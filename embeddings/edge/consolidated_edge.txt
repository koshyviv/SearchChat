Gartner, Inc. | G00753920 Page 1 of 19Building an Edge Computing Strategy
Published 3 September 2021 - ID G00753920 - 20 min read
By Analyst(s): Thomas Bittman
Initiatives:I&O Platforms
Edge computing projects are emerging in different parts of
enterprises. To accelerate deployments, ensure extensibility, and
drive efﬁciency and effectiveness, I&O leaders must create an
edge computing strategy.
Additional Perspectives
Overview
Key Findings
Recommendations
I&O leaders responsible for cloud and edge infrastructure projects should:Summary Translation: Building an Edge Computing Strategy
(26 September2021)■
Edge computing can be used to accelerate an enterprise’s digital business strategies,
but there is little consistency, understanding or expertise about how to do that.■
Disjointed edge computing projects are appearing in many different parts of
enterprises, designed to ﬁll a speciﬁc need, but without a guiding strategy to ensure
enterprise efﬁcacy, efﬁciency and leverage.■
Edge computing projects have common challenges that require invention to solve —
but solutions are usually siloed and rarely shared across projects.■
Edge computing deployments are often designed to deliver a single use case or
workload, but inevitably the requirements expand, and lack of extensibility is a key
inhibitor.■
Create a vision for edge computing by linking up with business and other technology
strategies, and communicating a measurable goal for edge computing.■
G00753920 Page 2 of 19Strategic Planning Assumptions
By year-end 2023, 50% of large enterprises will have a documented edge computing
strategy, compared to less than 5% in 2020.
Through 2025, 50% of edge computing solutions deployed without an enterprise edge
computing strategy in place will fail to meet goals in deployment time, functionality
and/or cost.
By year-end 2023, 20% of installed edge computing platforms will be delivered and
managed by hyperscale cloud providers, compared to less than 1% in 2020.
Introduction
Edge computing projects are increasing, based on inquiry discussions.
Edge computing is part of a distributed computing topology where
information processing is located close to the edge (the physical
location where things and people connect with the networked
digital world).Identify existing and potential use cases for edge computing through proactive
collaboration with business units and a clearinghouse to identify emerging use
cases.■
Accelerate expanded deployments of edge computing by identifying common
challenges and risks, and putting mitigation plans in place.■
Improve enterprise adoption of edge computing by building and communicating
standards, best practices, guidelines and skills requirements.■
Ensure success of edge computing projects through a structured plan to manage
unique edge computing requirements from proofs of concept (POCs) through
production.■
G00753920 Page 3 of 19Often, edge computing projects are deployed independently, as custom solutions focused
on a speciﬁc requirement, a speciﬁc use case or a speciﬁc part of the enterprise. However,
enterprise experience has shown that edge computing projects multiply independently in
different parts of the business, or expand from a single use case in a project to several
use cases. Diversity in use cases is the norm in edge computing. But as the edge
computing trend grows, synergy across projects and extensibility to enable new use cases
is critical in overcoming challenges, creating standards, choosing technologies and
managing costs. Edge computing should be pursued in a stepwise manner, but
strategically (see Figure 1). Enterprises should be proactive about their strategies because,
if they aren’t, by default they’re following the strategy determined by speciﬁc vendors,
which will be optimized to vendor offerings, rather than enterprise requirements, cost and
ﬂexibility.
Figure 1. Key Elements of an Edge Computing Strategy
There are ﬁve important elements to an edge computing strategy, designed to simplify,
synergize and systematize edge computing projects.
Analysis
G00753920 Page 4 of 19Create an Edge Computing Vision
What is edge computing, and why are you doing it? An edge computing vision states
speciﬁc business outcomes the enterprise can enable. An edge computing strategy should
be linked to other business and technology strategies (see Figure 2). It is critical to get
stakeholder buy-in and engagement.
Enterprise Strategies and Objectives
Edge computing will be integral to digital business transformation by enhancing and
expanding digital and physical interactions at the edge — where people and things are
located and interacting — and expanding the use of digital information to make faster and
better decisions. An edge computing strategy should be linked to the overarching
enterprise digital transformation strategy.
Figure 2. Mutually Supporting Enterprise and Technology Strategies
G00753920 Page 5 of 19An edge computing strategy should also target speciﬁc business objectives. Edge
computing can be used to deliver automation, improve efﬁciency or enhance quality, all of
which help to improve an enterprise’s bottom line. However, through the creation of net
new business interactions and agile use of information, edge computing is also being
used to improve customer experience and personalization or to create new business
opportunities. Based on Gartner client inquiry, about half of edge computing use cases are
targeting bottom-line efﬁciency, while the other half are focused on the top line and
growth.
An edge computing strategy should be explicitly linked to business objectives (cost,
quality, resilience, growth, agility, innovation, customer experience, etc.).
Technology Strategies
Edge computing is part of a distributed computing topology where information processing
is located close to the edge (the physical location where things and people connect with
the networked digital world). But its value is in complementing and enabling other
technology strategies.
For example, cloud computing is a style of computing in which scalable and elastic IT-
enabled capabilities are delivered as a service using internet technologies. In practice,
cloud computing is about taking your IT requirements to providers’ hyperscale data
centers. Edge computing does the obverse — it brings computing capability to your IT
(and OT) requirements, when that’s necessary. Edge computing complements (and
generally requires) cloud computing, integrating computing at or near the edge with back-
end data center processing. While edge computing is location-speciﬁc, many cloud
attributes can still be applied to edge computing solutions, such as continuous integration
and continuous deployment, DevOps, “as a service” hardware management, and operating
expense pricing. An edge computing strategy should be linked to the enterprise cloud
computing strategy.
On the other hand, while consistency is important, edge computing and the requirements,
technologies and capabilities at the edge are different than in hyperscale data centers
(see Figure 3). “Cloud native” implies exactly that: native to cloud computing, typically in
hyperscale and homogeneous data centers. “Edge native” is very real; where data is
distributed and often ephemeral, workloads tend to be real time with limited scalability
options, and processing location is not abstracted, it’s a deﬁning characteristic.
G00753920 Page 6 of 19Figure 3. Edge-Native
While cloud computing complements edge computing, edge use cases are either
approached from a cloud-out perspective (i.e., extending cloud capabilities and services
closer to the edge) or edge-in (i.e., focusing on rich edge ecosystems and integrating to
cloud services). As highly diverse edge systems evolve, the role of hyperscale providers
will focus on centralized value. By year-end 2023, 20% of installed edge computing
platforms will be delivered and managed by hyperscale cloud providers, compared to less
than 1% in 2020.
G00753920 Page 7 of 19Edge computing also enhances IoT. In many ways, Phase 1 of the IoT was simply about
digital connection: digitizing data, getting it to a data center and sending actions from the
data center. As the amount of data from things continues to grow, and as requirements for
lower-latency digital processing grow, relying on back-end data centers is insufﬁcient.
Edge computing provides the preprocessing or low-latency local processing for growing
IoT deployments. Further, as the interactions between people and things at the edge
multiply and become complex local systems, more of the digital processing needed is
local and latency is deﬁned as the latency of complex systems, not individual
transactions.
Immersive technologies are being used in a variety of ways to create the future of
experience, for employees and for customers. Immersive technologies will range in
capability from headsets to smartphones (doing augmented reality [AR]/virtual reality
[VR]) to intelligent speakers (providing a more natural digital interface). As systems of
interaction and collaboration at the edge continue to grow, computing for immersive
experiences will be distributed between immersive technologies themselves, the cloud and
edge computing.
Enhanced automation and intelligence at the edge will rapidly expand the use of machine
learning. While inference is usually implemented at the edge, training is often done by
sending massive datasets to central processing. However, as the cost of compute
continues to decline compared to the cost of bandwidth, training may also be done closer
to the edge where the inference models are deployed. In the future, more training might be
distributed in a federated or hierarchical way. Edge computing will be a part of deploying
inference models and a part of training. And edge computing requirements will push the
requirement for machine learning applications to be easy to deploy and easy to use.
An edge computing strategy should be linked to a variety of technology strategies an
enterprise is pursuing, including cloud computing, IoT, immersive experiences and
machine learning, among others.
Edge Computing Vision and Leadership
The purpose of a vision is to provide an understandable target state that can help
motivate and direct the team internally, and also that can be used to present measurable
results to the enterprise. How will the organization operate differently with edge
computing in ﬁve years? What new capabilities will be enabled? The edge computing
strategy is linked to enterprise and technology strategies — what will they look like in ﬁve
years?
G00753920 Page 8 of 19A vision for edge computing could include:
Key to success of an edge computing strategy will be executive buy-in and sponsorship.
For a strategy to be meaningful, there needs to be executive and staff support, and the
vision needs to be well communicated. As the strategy changes — and an edge computing
strategy will evolve as technologies and use cases emerge — updates to the strategy and
vision also need to be well communicated.
Recommendations
I&O leaders should:
Identify Edge Computing Use Cases
Edge computing use cases will be highly diverse, in diverse parts of the enterprise, with
diverse objectives. An edge computing strategy should include an understanding of edge
computing drivers, requirements and existing deployments. It should also have a process
for proactively ﬁnding new use cases and correctly identifying use cases as they emerge.An objective business impact, such as a percentage of digital business initiatives
that include edge computing, net new business transactions enabled by edge
computing, amount of money saved through edge computing initiatives, etc.■
Speciﬁc goals for edge computing use in the ofﬁce, the factory, the store or branch. ■
Percentage of customer interactions leveraging edge computing. ■
Number of automation projects completed. ■
Range of types of use cases deployed. ■
Deployment agility, i.e., the number of POCs or time to deployment. ■
Create a vision statement for edge computing, communicate it, and ensure executive
buy-in, sponsorship and alignment with staff.■
Create linkages between the edge computing strategy and the overall enterprise
strategy and objectives, especially those related to digital business.■
Link the edge computing strategy to other technology strategies, speciﬁcally cloud
computing, IoT, immersive technologies and machine learning.■
G00753920 Page 9 of 19Edge Computing Drivers
Edge computing ﬁlls a gap that other computing architectures can’t ﬁll. If a business
problem can be solved efﬁciently and effectively with a cloud computing or stand-alone
solution that should be the default. An edge computing strategy should deﬁne when an
edge computing solution should be pursued. There are four main drivers (see Figure 4),
and every use case may require one or more of them.
Figure 4. Edge Computing Drivers
G00753920 Page 10 of 19Latency/Determinism: This refers to when the response time needed for processing
requires that the compute be located physically close. In addition to low latency, the
use case might require deterministic, predictable response time (for example, at the
speed of an assembly line). Beyond individual transactions, there could be a number
of people or things interacting in sequence, and the latency of that overall system
might be the issue. Latency requirements can be addressed at different points in an
edge computing topology using:■
Speciﬁc cloud regions ■
Local data centers or content delivery networks (CDNs) ■
5G connections and multiaccess edge computing (MEC) capability ■
Micro data centers ■
Local edge servers ■
Intelligent gateways ■
Embedded processing ■
Or some combination ■
Data/Bandwidth: As data at the edge grows in volume, the cost of sending noisy,
ephemeral data elsewhere to be ﬁltered or processed might be more than moving the
compute to the data. Edge computing will be used for preprocessing, for ﬁltering or
to completely handle certain edge transactions. Most of the data produced at the
edge will never leave the edge.■
Limited Autonomy: Edge computing might be required to maintain a working
environment even when the connection to the central data center or cloud service
goes down or is unavailable for some time. This could include queuing transactions
for later processing, or it could be essentially a subset of the data center or cloud
capability that runs at the edge.■
Privacy/Security: As more and more intimate raw data about interactions and the
things and people in a location are digitized, that data might become regulated (e.g.,
facial recognition data). Regulatory requirements might be different for different
edge locations. In many cases, enterprises will self-regulate to keep certain data and
interaction data on-premises or on-device.■
G00753920 Page 11 of 19An edge computing strategy should explicitly discuss the requirements that will be
addressed by edge computing speciﬁc to enterprise requirements.
Existing Edge Computing Use-Case Landscape
Before there was an edge computing strategy, the enterprise encountered — and dealt with
— some edge computing use cases. On the manufacturing ﬂoor, this would include
existing OT (operational technology), programmable logic controllers (PLCs), and robotics.
These are often known as embedded systems. In the store, it could include smart check-
out. An edge computing strategy should identify all of the existing deployments that ﬁt
under its purview. The strategy should include how changes and new investments in those
deployments should be done to move them closer to chosen edge computing standards,
guidelines, technologies and best practices.
Potential Edge Computing Use Cases
An edge computing strategy should establish a proactive process to identify potential use
cases. This needs to be collaborative between IT and the various business units. There
should be well-deﬁned linkages between the edge computing and various enterprise
strategies, a clear vision and well-understood drivers. IT and business units should work
together to ﬁnd the art of the possible; but the goal would be to ﬁnd at least a single
prospect use case that directly supports the outcomes expressed in the edge computing
vision. There are 12 different categories of edge computing use cases, based on
enhancing interactions between business, things and people (see Figure 5).
G00753920 Page 12 of 19Figure 5. Edge Computing Use-Case Categories (With Examples)
Edge Computing Use-Case Clearinghouse
An edge computing strategy should include creating a clearinghouse for new use-case
ideas, that is, structured processes for how new use cases and edge workloads are
identiﬁed, vetted and prioritized. Linkages to enterprise digital business strategies and
technology strategies for cloud computing, IoT and others will facilitate the identiﬁcation
of edge computing candidates. Edge computing should be considered whenever latency,
edge data, semiautonomy or location privacy requirements cannot be solved by traditional
enterprise data center or cloud-based solutions.
Recommendations
I&O leaders should:
G00753920 Page 13 of 19Focus on Edge Computing Challenges
Edge computing creates risks that need to be mitigated and new challenges that need to
be overcome. An edge computing strategy needs to maintain a focus on them. Different
industry verticals may have unique edge challenges, or risks that need to be mitigated.
However, there are four challenges with edge computing that are applicable to the vast
majority of enterprises (see Figure 6).
Figure 6. Edge Computing ChallengesIdentify the speciﬁc requirements that edge computing can address for the enterprise
in the areas of latency, data, semiautonomy and privacy.■
Identify existing technologies and deployments that should be included within the
purview of the edge computing strategy.■
Identify potential use cases that could be addressed with edge computing,
proactively and collaboratively with business units.■
Create a use-case clearinghouse to identify edge computing candidates as they
emerge, with structured processes for how new use cases and edge workloads are
identiﬁed, vetted and prioritized.■
G00753920 Page 14 of 19Distributed Computing
Edge computing can be highly distributed in locations that have heat, air quality and
power challenges, with limited to zero IT skills. The number of nodes can be extreme, for
example, every store, throughout factories, in automobiles and homes. Different
geographies may have different environmental issues, varying network access, or
challenges in local skills and services availability. And different locations may have
different regulatory requirements. Resilience requirements may demand hardware,
software and networking that can ensure continuous operation, or processes to rapidly
replace bad equipment. Remote management and monitoring and zero touch
management are often essential, and a variety of edge-as-a-service offerings might be
important. As requirements change — software changes, new hardware requirements —
the enterprise needs to be able to adapt.
Security and Privacy
The distributed scale and lack of traditional physical data center security in edge
computing put a greater burden on ensuring that edge computing nodes are hardened and
that the enterprise is protected through defense in depth. Edge computing architectures
need to be protected from tampering, data rerouting, hijacking or denial-of-service attacks.
In addition, the data being captured may introduce new legal, regulatory and ethical
considerations that determine how (and if) data is stored and encrypted and where data
can be sent. As time goes on, workloads may change, capturing and processing new kinds
of data and creating new challenges.
Distributed Data
As more things produce data at the edge, and as more interactions at the edge become
digital, data growth outside of data centers will become signiﬁcant. Edge computing
creates a massive distributed data challenge — distributed governance, distributed data
integration, and distributed data and analytics architectures. The nature of data at the
edge is quite different from data in data centers. Data tends to be more ephemeral, noisy
and locally speciﬁc. Unlike data centers, where data management is primarily about data
preservation, a challenge at the edge is to destroy useless data as quickly as possible (to
avoid expensive storage and minimize governance risks). But determining data value
dynamically and not destroying data that has potential value are real challenges.
Extensibility
G00753920 Page 15 of 19In order to meet speciﬁc requirements but keep costs down, edge computing nodes will
typically be very purpose-speciﬁc to their environment and workloads. However, the
workloads in a location will change, and new ones will emerge. Edge computing platforms
need to be both special-purpose and extensible. An edge computing strategy should
attempt to predict future workloads and enable others beyond the planning horizon. A
typical enterprise may have a number of “edges,” in ofﬁces, equipment and factories. At
the same time, when it makes sense, common challenges should be solved with common
technologies and processes, rather than having entirely distinct management tools across
heterogeneous edge nodes, for example. In an emerging space like edge computing, new
technologies and vendors will arise, and dependence on speciﬁc vendors drives lock-in. An
edge computing strategy will need to manage partners and include exit plans.
Recommendations
I&O leaders should:
Build and Communicate Edge Computing Standards
Diversity is a deﬁning characteristic of edge computing use cases, but that makes it even
more important to ﬁnd and maintain synergies by leveraging technologies, platforms, best
practices, standards, processes and skills across disparate deployments. Many
enterprises have a cloud center of excellence (CCOE). A CCOE is a centralized enterprise
architecture function that leads and governs cloud computing adoption within an
organization. The CCOE could be expanded to include edge computing, or a separate
center of excellence could be created for edge computing. There are three main pillars of
an edge center of excellence.
Edge Computing Technologies
An edge computing strategy should lay out and maintain a set of technology and
architecture standards, frameworks, topologies and delivery models that will be used
across edge computing deployments. An enterprise should maintain a list of trusted
vendors (and a process for analyzing vendors).
The technology and architecture areas covered should include at least:Identify the risks, challenges and inhibitors that need to be overcome and mitigated,
and put special ongoing focus on those challenges in terms of management,
investment and skills.■
Cloud integration: Applications, data, networking, etc. ■
G00753920 Page 16 of 19Edge Computing Governance
An edge computing strategy needs to lay out policies for governance and security of edge
computing deployments, software and data, and measure the value and return on
investment. This includes standards for leveraging as-a-service providers (which could be
for any or all layers of the edge computing stack). Policies, guidelines and guardrails
should be put in place that can be used to guide all future edge computing deployments
and how the edge computing technologies will be used (and when they won’t be used). An
edge computing strategy must also factor in compliance requirements that might be
different in different locations, but also potential legal and ethical considerations (for
example, how facial recognition data is stored and used).
Edge Computing Best Practices and Skills
An edge computing strategy should include how best practices both outside the enterprise
and from enterprise deployments will be captured and leveraged. Since edge computing is
so diverse and so new, lessons learned will be extremely valuable to accelerate successful
deployments and avoid duplicate efforts.
The strategy should also include edge computing skills identiﬁcation, development, roles
and responsibilities, and organization and matrix organization structure.Delivery models: Edge-as-a-service options for hardware, software, service providers,
etc.■
Hardware: Ruggedized, ﬁt to purpose, composable, extensible, etc. ■
Application/software platform: Application architecture, development tools,
hypervisor, container architecture, etc.■
Management tools: For hardware, software platform, applications, etc. ■
Data management: Storage, governance, metadata, etc. ■
Data analytics: Traditional reporting and business intelligence (BI), advanced
analytics, artiﬁcial intelligence (AI), etc.■
Security technologies: All layers including hardware, software, data, encryption, etc. ■
Networking: Connectivity to devices, to the cloud, etc. ■
IoT technologies: IoT endpoints, platforms, device management, stream processing,
security, etc.■
G00753920 Page 17 of 19Recommendations
I&O leaders should:
Ensure Success of Edge Computing Execution
Because edge computing is an emerging concept where “ﬁrsts of a kind” predominate,
enterprises will often be pioneering new technologies and new uses of technologies, and
learning along the way. There are two unique aspects of edge computing that need to be
considered in an edge computing strategy.
Edge Computing POCs
Edge computing deployments will need to be piloted for efﬁcacy, manageability,
autonomy and scale. The most common reason that edge computing POCs tend to fail in
production is inability to efﬁciently scale (in terms of number of nodes and number of
locations). A pilot should attempt to evaluate how a project will operate and how it can be
monitored and managed at full scale (using vendor edge computing labs, simulations, or
evaluating similar reference customers). In addition, the ability to handle disconnection
needs to be tested in a variety of ways. The edge computing strategy should lay out
deﬁned steps for deployments from POCs through production, including centralized
coordination, measures of success and the capture of lessons learned.
Evolution Management
Deployments of edge computing evolve during development, and often evolve and greatly
expand after they’re in production. An edge computing strategy should have guidelines
and guardrails in place for how to monitor and manage changes in a deployment’s
requirements, workloads, data, scale and uses over time. The strategy should guide those
evolutions — and in some cases, the evolutions should cause the strategy to change.Create an architecture function focused on edge computing (e.g., “edge center of
excellence”).■
Build, maintain and communicate technology and architecture standards,
frameworks and topologies that will be used across edge computing deployments.■
Capture and maintain best practices both outside the enterprise and from enterprise
deployments.■
Identify and develop edge computing skills, roles and responsibilities, and
organization structure.
■
G00753920 Page 18 of 19Recommendations
I&O leaders should:
Evidence
This research is drawn from hundreds of inquiries with Gartner clients, including those
beginning edge computing deployments, those in the middle of the process, and those
that have completed a rollout.
Recommended by the Author
Some documents may not be available as part of your current Gartner subscription.
Infographic: Understanding Edge Computing
Hype Cycle for Edge Computing, 2021
2021 Strategic Roadmap for Edge Computing
Predicts 2021: Cloud and Edge Infrastructure
Why and How I&O Should Lead Edge Computing
Emerging Technologies: Emerging Edge AI Use Cases in RetailRequire POCs to test (or simulate) unique edge computing requirements, such as
scale and tolerance to disconnection.■
Build a process to manage the evolution of an edge computing deployment in
production, especially in terms of how workloads, data, scale and uses change over
time.■
G00753920 Page 19 of 19© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00767014 Page 1 of 9Cool Vendors in Edge Computing
Published 23 May 2022 - ID G00767014 - 10 min read
By Analyst(s): Bob Gill, Thomas Bittman, Sandeep Unni
Initiatives:I&O Platforms
IT leaders are ﬁnding that the diversity and complexity of edge
computing requires technologies that enable scale and
extensibility, and solutions that simplify and accelerate
deployments. We highlight four vendors that attack the edge scale
and complexity problem through management and orchestration,
or low code techniques.
Additional Perspectives
More on This Topic
This is part of an in-depth collection of research. See the collection:
Overview
Key FindingsSummary Translation: Cool Vendors in Edge Computing
(10 June 2022)■
2022 Cool Vendors Pave New Paths for Democratized Digital Delivery ■
Edge computing remains a complex, highly diverse trend — but signiﬁcant progress
toward simpliﬁcation and layered solutions has been made over the past year.■
Architected Edge Management and Orchestration (EMO) stacks are critical to
deployment of solutions that scale while remaining extensible.■
EMO stacks extending across functional and tool boundaries are simplifying
application development, deployment and management.■
Low code/no code “front ends” and studios are simplifying the development of
complex edge applications, such as machine learning (ML) at the edge, and fast
tracking ML, to be a game-changing application for the edge.■
G00767014 Page 2 of 9Recommendations
IT Leaders planning edge computing projects should:
Analysis
This research does not constitute an exhaustive list of vendors in any given technology
area, but rather is designed to highlight interesting, new and innovative vendors, products
and services. Gartner disclaims all warranties, express or implied, with respect to this
research, including any warranties of merchantability or ﬁtness for a particular purpose.
What You Need to Know
The wide diversity of edge use cases and deployment patterns make it very difﬁcult for
enterprises to develop solutions, deploy, observe — and operate them at scale, manage
change and enable extensibility.Create and follow an enterprise edge strategy by focusing ﬁrst on business beneﬁt
and holistic systems, not just point technical solutions or products.■
Establish a modular, ﬂexible edge approach through the use of emerging edge
frameworks, which allow for the mixing and matching of technologies based on
enterprise direction — not simply “what comes with the vendor solution.”■
Simplify solution-building and accelerate time to deployment by using solutions that
both bundle open source elements in preconﬁgured sets and output open code to
minimize lock-in and allow extension.■
Accelerate solution deployment by breaking down management and orchestration
tasks into functional layers — each addressing a foundational module of the
deployment, such as physical infrastructure, networking, security, software stack and
the applications themselves.■
Accelerate ML solutions for the edge by experimenting with studios and low code
options to empower business unit customers.■
G00767014 Page 3 of 9The complexity of an end-to-end edge solution can be minimized through two different
but complementary technology approaches. We describe these as Edge Management and
Orchestration (EMO) “stacks,” and application “studios” or front ends, which are tools for
creating complex edge applications through a low-code, or even no-code approach. The
EMO stack approach eases complexity in building and deploying distributed applications
at scale (e.g., deployment of Kubernetes clusters and associated modules, such as
providing a service mesh, distributed security, logging, etc.). The low code/no code studios
and languages allow business process experts and operational staff to create formerly
extremely complex applications, such as database-connected IoT solutions — or even to
build, train and deploy edge-located machine learning models without relying on the
involvement of data scientists (see Figure 1). One of the vendors in this report has built
such a studio for machine learning at the edge, with the goal of democratizing model
development.
Figure 1. Reducing Edge Complexity: Edge Management and Orchestration, and Low-
Code Approaches
Gartner
G00767014 Page 4 of 9EMO stacks provide discrete layers of control over server and device management,
network and security management, the infrastructure software stack and, in some cases,
the applications themselves. A layered approach to edge management and orchestration
— which previously was delivered through monolithic application stacks — allows the
assembly of functional, full-featured orchestration capabilities from subset-speciﬁc
offerings.
In the absence of available EMO components, edge applications will require costly and
time-consuming, custom-made, control stacks to be created again and again: EMO stacks
provide not only functionality of initial provisioning, but also ongoing management and
optimization simplifying operating at scale. A layered, integration-ready portfolio of EMO
elements will simplify the assembly of multi-vendor solutions based on common and
open frameworks. Three of the providers in this report offer differing approaches to the
EMO complexity problem.
Building a distributed edge system that is integrated with other enterprise systems is still a
daunting task, but we are making steady progress. Just a year or two ago, the advisability
of and implementation of Kubernetes for edge deployment was still being debated. Today,
complete, ready-to-deploy stacks based on Kubernetes at the core are commercially
available. The progress has been impressive, and we expect that it will continue into the
future.
Edge Impulse
San Jose, California, U.S. ( https://www.edgeimpulse.com/)
Analysis by Bob Gill
Why Cool:
G00767014 Page 5 of 9Edge Impulse provides a software studio and framework that simpliﬁes the collection of
use-case speciﬁc datasets, and the creation, training and deployment of machine learning
models for Edge Devices and Gateways in a way that is unique in the edge-computing
market. Unlike other tools masking the complexity of software development with a
simpliﬁed front end, Edge Impulse also creates the ML models as reusable, standard
Python code in parallel. These are to be used by experienced data scientists looking to
modify or extend the models, and to ensure that projects are not “locked in” to the Edge
Impulse environment, should alternative methods emerge. In this way, the Edge Impulse
studio complements the data science tool stack, rather than competing with or
contradicting it. Its template-driven approach allows subject matter experts, such as
engineers and business unit leaders to harness the power of ML at the edge, with little to
no data science background.
Challenges:
Creation and refreshing of simpliﬁed interfaces for immensely complex and rapidly
advancing models, such as those for ML across model types (i.e., image, video, audio,
NLP), across a growing market of hardware targets is a daunting task. In addition to the
boards they support with rapid prototyping and complete binary generation, Edge Impulse
supports other hardware targets through their ﬂexible ingestion tool.
Studio and model deployment in POCs is a huge step forward, but only fulﬁlls the initial
task of deploying and maintaining ML models at scale. A complete enterprise solution will
need to include integration with other frameworks, processes and pipelines the enterprise
is planning or may have in use — which Edge Impulse supports through an extensive set
of APIs and webhooks to integrate with their CI/CD workﬂow and third-party IoT
frameworks.
Who Should Care:
Edge architects and planners looking to deploy ML at the edge, and AI/ML teams looking
for an approachable, easy to use studio that democratizes ML development and brings
model creation and training to business unit subject matter experts. Enterprises that want
to allow this simpliﬁed/democratized development, while also requiring
reusable/extensible code for Data Scientists, will ﬁnd Edge Impulse offers both.
Spectro Cloud
San Jose, California, U.S. ( https://www.spectrocloud.com/)
G00767014 Page 6 of 9Analysis by Bob Gill
Why Cool:
Spectro Cloud provides a software platform for the management of large numbers of
distributed Kubernetes (K8s) clusters, whether centralized in a virtualized or bare metal
data center, cloud or distributed across thousands of individual nodes or clusters at the
edge. Spectro Cloud’s offering “Palette” is differentiated through its use of “cluster
proﬁles,” that allow the user to specify the complete stack as a declarative model, from OS
and K8s to apps — including logging/monitoring through service mesh, security, and on
down through infrastructure across dissimilar clusters and nodes. Users can either select
these elements from a drop-down menu, or via their own customer provided tools. Palette
will keep the clusters consistent at deployment time and runtime to prevent conﬁguration
drift, and make updates as easy as updating the “cluster proﬁle.” The platform also
provides comprehensive day-two operations for backup/restore, certiﬁcate rotation,
security and compliance scans, etc.
The ability to manage these nodes and clusters as both local groups, as well as a
complete whole, allows for powerful and unique edge management and orchestration
options.
Challenges:
While Spectro Cloud simpliﬁes and extends deployment of K8s and its stack, as K8s was
not originally designed for managing 1000s of lightweight edge devices, some architects
we speak to are still not convinced it is the best model for edge deployment.
A market challenge that Spectro Cloud faces is that its offering is targeted at enterprise
and cloud architects, as well as operational teams, and container and application owners.
These are not business unit overall solution owners, where edge solutions and funding are
currently stemming from.
Who Should Care:
Architects and planners looking to standardize on K8s at the edge, despite differences in
edge conﬁgurations or legacy hardware realities. As enterprises mature from edge POCs
to consider deploying at scale, management and orchestration stacks, such as those
created with Palette, will be critical — requiring edge planning to operate across the
enterprise, including enterprise and cloud architects.
G00767014 Page 7 of 9Avassa
Stockholm, Sweden ( https://avassa.io/)
Analysis by Tom Bittman
Why Cool:
Avassa is cool because it uniquely enables very rapid deployment of container
applications in many distributed edge locations, and management with a control plane
that is either enterprise-hosted or delivered as a service. The Avassa Control Tower
handles centralized management and monitoring of distributed edge resources,
applications and tenants — while the Avassa Edge Enforcer (installed at each edge node)
is an agent that handles local cluster management, application placement, scheduling
and security. New edge locations can be up and running in under one minute. Together,
they provide complete life cycle management for container applications at the edge.
Avassa has integrations with Amazon Web Services, Microsoft Azure and third-party
software.
Challenges:
As a small company, Avassa will be challenged to get noticed by prospects unless
brought in by system integrators and consultants for speciﬁc solutions — so partnerships
are critical. Distributed edge application management is a foundational edge computing
challenge with emerging competition. Avassa competes with IBM Edge Application
Manager and Spectro Cloud, among others. IBM’s Edge Application Manager is also the
basis for the LF Edge Open Horizon Project. However, edge computing use cases and
workload patterns are so diverse that there is room for competitors. Avassa will need to
ﬁnd the niches where it can lead, and partners to expand its market presence.
Who Should Care:
Architects and DevOps engineers looking to build, distribute and maintain container
applications across many distributed edge locations should consider Avassa. Systems
integrators, and other solution builders looking to ﬁll a technology gap in their edge
offerings, should consider Avassa.
Sunlight
Cambridge, U.K. ( https://sunlight.io/)
G00767014 Page 8 of 9Analysis by Sandeep Unni
Why Cool:
Sunlight provides a cool and differentiated software-deﬁned infrastructure platform
designed uniquely for the edge that combines compute, storage and networking — along
with the deployment and management plane, which allows both VM-based or cloud-native
applications to be run on a common framework. Its portfolio consists of two main
offerings: Sunlight Infrastructure Manager (SIM) and NexVisor Hypervisor. NexVisor is a
lightweight platform optimized to support low power processors at the edge with the
capability to run multiple applications on the same cluster, unlike typical HCI offerings
designed for data center locations. It results in reduced overall footprint and system
resource overhead needed, and increases overall performance efﬁciency. The
Infrastructure Manager offering enables deployment and management of clusters in the
cloud, on-premises or at the edge.
Challenges:
While Sunlight is designed for the edge, it competes with better-known, larger vendors who
are also trying to expand their offerings for the edge. SIM can be deployed on any cloud
provider, but is currently only offered directly (and most easily) in the AWS marketplace,
somewhat reducing its visibility.
Who Should Care:
Enterprise and edge architects planning to integrate newer, data-intensive workloads at the
edge while also looking to future-proof legacy hardware infrastructure costs and
application investments in a single infrastructure platform should evaluate Sunlight as
the migration onramp. Beneﬁts include its small footprint, simplicity of conﬁguration and
management, as well as lower cost of deployment.
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Predicts 2022: The Distributed Enterprise Drives Computing to the Edge
Building an Edge Computing Strategy
Get Ready For Data Management at the Edge: Key Considerations and Actions
G00767014 Page 9 of 9© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00768727 Page 1 of 129Hype Cycle for Cloud Computing, 2022
Published 12 July 2022 - ID G00768727 - 139 min read
By Analyst(s): Ed Anderson, David Smith
Initiatives:I&O Platforms
Cloud computing is used broadly and pervasively and continues to
spawn new technology innovations and trends. Infrastructure and
operations leaders must be pragmatic about the hype surrounding
cloud computing as new innovations emerge and cloud usage
expands to enable more use cases and outcomes.
Additional Perspectives
Summary Translation: Hype Cycle for Cloud Computing, 2022
(11 August 2022)■
G00768727 Page 2 of 129Analysis
What You Need to Know
Despite the broad and mainstream use of cloud computing, hype surrounding the beneﬁts
and limitations of cloud persists. Most organizations are using cloud computing in some
capacity and increasing investments in new cloud-based initiatives. As cloud adoption
expands to address new use cases, so does the demand for cloud capabilities to support
distributed scenarios, new application models, industry-speciﬁc requirements, application
operational needs, and new features and functionality. Other priorities and outcomes —
such as cloud sovereignty, sustainability, industry cloud capabilities, application
portability, and new service consumption models — drive further technology innovation
and vendor differentiation.
Cloud computing has largely lived up to its potential, particularly when employed to fully
embrace cloud operating principles. The effectiveness of cloud models in helping
organizations respond to unanticipated changes during the pandemic demonstrated the
ﬂexibility and adaptability of cloud models. Cloud markets continue to grow as cloud
services are employed to support new use cases such as smart cities, connected vehicles
and remote healthcare as businesses continue to push their move toward a digital
transformational model.
IT leaders should use this Hype Cycle to track new technology trends, understand the
continuing evolution of cloud computing, assess risks and guide their strategic planning
processes in pursuit of the full beneﬁts of cloud.
The Hype Cycle
The shift to cloud continues, as demonstrated by the continuing growth of the cloud
market — still the fastest growing IT market. Cloud computing supports mainstream
operations for many organizations and serves as a foundation for IT modernization as
well as the desired outcomes associated with digital business transformation. Cloud
operating principles are often espoused as the preferred style of operations in a
modernized business.
In pursuit of the full value of cloud, organizations are investing in cloud-native
applications, operating models and architectures. Cloud-native thinking drives many cloud
decisions today in conjunction with application modernization initiatives. The inﬂuence of
cloud on IT operating models extends through IT operations management, security,
networking, as well as business functions (such as procurement) where cloud
marketplaces are becoming more prominent in technology purchasing.
G00768727 Page 3 of 129Business requirements are driving consideration of cloud-based innovations such as
artiﬁcial intelligence, the Internet of Things (IoT), telco clouds driven by 5G, and edge
computing. New innovations such as quantum computing as a service will be delivered as
a cloud service to further support business growth initiatives.
Three growing trends are shaping cloud activities, as noted on the Hype Cycle:
Despite its overall maturity, cloud computing continues to garner industry attention as a
delivery vehicle for proven IT capabilities and as an innovation foundation for the
capabilities required by businesses across industries and throughout the world.
Figure 1: Hype Cycle for Cloud Computing, 2022
Source: Gartner (July 2022)Industry cloud capabilities, support for sovereignty requirements and sustainability
are increasingly guiding cloud purchase decisions.■
Applications are moving toward serverless models, which helps organizations spend
less time thinking about infrastructure and more time on business functionality.■
Composable applications and packaged business capabilities support the
requirement for businesses to be agile, dynamic and adaptive.■
G00768727 Page 4 of 129The Priority Matrix
The Hype Cycle for Cloud Computing represents a diverse collection of high-impact
technologies driving growth and disruption across markets. The dynamic nature of cloud
computing causes some cloud technologies and concepts to move through the Hype
Cycle at an accelerated rate. The transformational nature of cloud-based solutions
spawns an ever-increasing collection of innovations as illustrated by the innovations
featured on the Innovation Trigger in the Hype Cycle.
Many cloud computing technologies and concepts are two to ﬁve years away from
mainstream adoption, and they will continue to be impactful. Other technologies have
reached mainstream adoption and form the foundation for the next wave of innovations.
The relative impact of cloud and cloud-related technologies is high and often
transformational. Organizations building on a cloud foundation will embrace
transformational change more quickly and effectively than organizations bound to
traditional IT environments, which is why modernization initiatives always include cloud.
G00768727 Page 5 of 129Table 1: Priority Matrix for Cloud Computing, 2022
(Enlarged table in Appendix)
Off the Hype Cycle
G00768727 Page 6 of 129On the Rise
Augmented FinOps
Analysis By: Adam Ronthal, Dennis Smith
Benefit Rating: Transformational
Market Penetration: Less than 1% of target audience
Maturity: Embryonic
Definition:
FinOps applies the traditional DevOps concepts of agility, continuous integration and
deployment, and end-user feedback to ﬁnancial governance, budgeting and cost
optimization efforts. Augmented FinOps automates this process through the application
of artiﬁcial intelligence (AI) and machine learning (ML) practices — predominantly in the
cloud — enabling environments that automatically optimize cost based on deﬁned
business objectives.
Why This Is Important
As more workloads move to the cloud, the cost of individual workloads is exposed with
greater transparency than ever before. In the cloud, it is now possible to assess the cost of
a speciﬁc workload or collection of workloads assigned to a project. However, the
complexity and diversity of choice in underlying cloud infrastructure and service offerings
makes it difﬁcult to achieve optimal price performance outcomes. Augmented FinOps can
automate this process by applying AI/ML techniques.
Business Impact
The automation of cloud budget planning and ﬁnancial operations will allow businesses
to express their objectives — ideally in natural language — and allow their cloud
ecosystem to automatically optimize the underlying cloud resources to meet those
objectives. This will result in more efﬁcient use of resources, and therefore, optimal spend
by reducing/eliminating misaligned or poor use of cloud infrastructure and service
offerings.
Drivers
Increased realization on the part of practitioners that cloud is fundamentally a
complex cost optimization exercise.■
G00768727 Page 7 of 129Obstacles
User Recommendations
IT leaders looking to optimize their cloud spend should:Need to reduce the unpredictability of cloud spending when using cloud
infrastructure and services for analytics, operational database management
systems (DBMSs), data lakes and other applications.■
While commit-based usage mitigates some unpredictability, consumption-based
usage remains common in earlier stages of cloud adoption, driving the need for
augmented FinOps.■
Cloud adoption often results in decentralization of the IT function to the consuming
lines of business, wherein cost overruns are often downplayed/dismissed by
highlighting improved productivity and agility.■
Automation of ﬁnancial governance controls in cloud environments provides
increased predictability and cost optimization, with less operational effort.■
Solid ﬁnancial governance frameworks are positioning organizations to take
advantage of FinOps.■
Owing to their complexity, cloud environments are ideally suited for the application
of ML and AI methods to automate processes and track price and performance.■
Cloud service provider pricing models remain needlessly complex and diverse. ■
Cloud ecosystems are (and will remain) open to third-party participants, which
implies multiple commercial arrangements with multiple providers.■
Seek out service offerings to automate (via AI/ML) performance, consumption and
pricing options. Increasingly, incorporate these capabilities into cloud data
ecosystems that will learn from consumption patterns as they seek to optimize the
underlying resources, and by extension, cloud spending.■
Explore the use of augmented FinOps services for targeted and scoped use cases to
establish the capability and actionability of the provider’s product. An initial use case
of machine instance type or DBMS service offerings will allow for a controlled
evaluation and validation of the approach and capabilities available.■
G00768727 Page 8 of 129Sample Vendors
Anodot; Cloudwiry; Enteros; Oracle; OtterTune; Sync Computing; Unravel Data
Gartner Recommended Reading
CDOs and CFOs Must Join Forces in the Cloud to Connect Business Value With Cost
Predicts 2022: Data and Analytics Leaders Must Expand Value While Managing Cost and
Risk
Cool Vendors in Augmented Data Management
How to Identify Solutions for Managing Costs in Public Cloud IaaS
Solution Criteria for Public Cloud Third-Party Cost Optimization Tools
Quantum Computing as a Service
Analysis By: Chirag Dekate
Benefit Rating: Moderate
Market Penetration: Less than 1% of target audience
Maturity: Embryonic
Definition:
Quantum computing as a service (QCaaS) provides enterprises with access to quantum
computing (QC) systems and associated services, enabling them to explore enterprise-
relevant use cases and devise quantum algorithms for a highly specialized set of
problems. QCaaS provided by QC vendors enables access to their own technologies.
Some cloud service providers offer QCaaS that enables access to a variety of QC
implementations, vendors and solution approaches.
G00768727 Page 9 of 129Why This Is Important
The torrid pace of innovation in quantum computing systems means that on-premises
quantum systems are impractical for most users today, given their limited utility and rapid
aging. QCaaS enables enterprises to derisk quantum strategies and leverage cloud
services to access, test, validate and utilize diverse quantum technologies. QCaaS
environments enable enterprises to focus on exploring a variety of use cases and devising
quantum algorithms as opposed to negative ROI on-premises acquisitions.
Business Impact
Enterprises pioneering quantum initiatives today are focusing on ﬁve key applications:
optimization, simulation, search, linear systems and security-related use cases. QCaaS
enables enterprises to explore different types of quantum systems and accelerate
quantum skills development in a relatively low-risk environment. QCaaS continues to
evolve in maturity. However, these environments are not ready for production use cases,
primarily due to the limited scale of underlying quantum systems.
Drivers
Many scientiﬁc problems are unsolvable using traditional computing technology.
QCaaS offers access to quantum computing technologies for organizations
pursuing solutions to computationally hard problems without the risk and cost
associated with dedicated systems that will likely age faster, given the pace of
innovation in the industry.■
Rather than acquiring expensive quantum systems on-premises, enterprises can
minimize cost, complexity and time to value by using QCaaS-based quantum
computing services.■
Some leading cloud service providers offer access to diverse quantum systems,
simplify identity and data management and offer streamlined pricing across diverse
quantum providers. In some cases this approach can simplify exploration of
quantum technologies and signiﬁcantly lower risk.■
Continued scaling of underlying quantum computing systems and implicit
advancement of the ﬁeld (including scalable error correction schemes) is seminal to
the evolution and eventual success of QCaaS.■
The ability to address the growing set of use cases beyond the traditional ﬁve —
optimization, simulation, search, BQP and security — will be essential to create
virtuous business cycles.■
G00768727 Page 10 of 129Obstacles
User Recommendations
Sample Vendors
Amazon Web Services (AWS); DWave; Google; IBM; Microsoft; Oxford Quantum
Computing; Quantinuum; Rigetti; Xanadu
Gartner Recommended Reading
Predicts 2021: Disruptive Potential During the Next Decade of Quantum ComputingA lack of return on investment, limited applicability and inability to demonstrate
value creation are key business obstacles limiting enterprise investments in
quantum.■
A lack of sufﬁcient scale in underlying quantum computing systems powering
QCaaS limits the scale of applications that can be explored or run. Current classical
approaches deliver better, more impactful results than any quantum alternative.■
Quantum computing systems continue to be nascent in maturity, with more than
half a dozen different ways of representing qubits and organizing systems to deliver
error correction and scaling. Quantum technologies that look promising today may
not be the ones that will eventually deliver value in the future.■
There remains a lack of skills to leverage QCaaS effectively, including the
development of applications to fully exploit quantum computing capabilities.■
Leverage QCaaS to devise quantum initiatives over the next decade: Avoid
acquiring on-premises quantum systems. Rapid pace of innovation in quantum
technologies means that most on-premises systems will be obsolete faster as newer
systems and scalable technologies come online. QCaaS minimizes the risk
associated with these dynamics.■
Select single provider QCaaS for specialization and value creation: Direct QCaaS
capabilities enabled by quantum vendors can help provide highly specialized access
to quantum systems while derisking your strategies. Engage in this approach if your
main goal is value creation and scaling.■
Select multiquantum system QCaaS for exploration and broader enterprise cloud
strategy integration: Some CSPs offer access to multiple quantum providers,
enabling enterprises to evaluate diverse technologies and simpliﬁed integration to
existing cloud practices.■
G00768727 Page 11 of 129Innovation Insight for Quantum Computing for the Automotive Industry
4 Advanced Computing Algorithms That Lead to Next-Generation Proﬁts
Cloud Sustainability
Analysis By: Ed Anderson
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Emerging
Definition:
Cloud sustainability is the use of cloud services to achieve sustainability beneﬁts within
economic, environmental and social systems. As such, cloud sustainability refers to both
the sustainable operation and delivery of cloud services by a cloud service provider, as
well as the sustainable consumption and use of cloud services.
Why This Is Important
Cloud sustainability is a key emerging trend as organizations consider the use of
technology models to achieve their sustainability ambitions. Public cloud services offer
great potential to produce sustainability beneﬁts because of their ability to operate at
scale using a shared services model, which results in greater efﬁciency in the use of
computing resources. Cloud services can be physically located near renewable energy
sources, further extending their sustainability potential.
Business Impact
The worldwide focus on climate change is pressuring organizations to improve their
sustainability posture. Customers, investors, regulators, employees and the public at large
are driving increased attention to sustainability. Organizations can use technology,
including cloud, to help meet their sustainability objectives. Cloud computing has the
potential to improve the sustainability posture of an organization by moving workloads
from legacy data centers to more efﬁcient cloud environments.
G00768727 Page 12 of 129Drivers
Sustainability is a rising imperative for organizations across all industries and in all
countries and regions around the world, and encompasses environmental, social and
economic factors.■
Although there may be differing opinions on the need for sustainability action,
market data shows that customers, investors, regulators, citizens and employees
value organizations with commitments to achieving sustainability outcomes.■
Sustainability investments correlate with operational efﬁciency, which means that
most organizations operating in an increasingly sustainable fashion also recognize
other beneﬁts such as reduced spending on energy, reductions in waste and
improvements in water use.■
Cloud providers, being among the world’s largest data center operators, are making
progress toward delivering sustainable cloud service offerings. Sustainability
beneﬁts complement the other beneﬁts of moving applications and data to the
cloud.■
Regulatory and legislative mandates for sustainability are increasing and are likely
to be common in the next several years. Using cloud services provides a means to
track sustainability metrics such as carbon emissions from technology use, which
will help organizations comply with future regulatory reporting requirements.■
G00768727 Page 13 of 129Obstacles
Accounting for sustainability outcomes adds complexity to the existing challenges
of cloud operations, multicloud management, hybrid integrations, cost management
and optimization.■
Cloud providers claim to have made great strides in offering sustainable cloud
solutions, but these claims are often difﬁcult to verify. Currently, there are no
mandated sustainability reporting standards, making it difﬁcult to interpret provider
claims.■
Assessing the sustainability posture of a cloud provider is challenging, particularly
when assessing across the multiple dimensions of sustainability including
environmental, social and economic sustainability.■
Achieving cloud sustainability outcomes is a shared responsibility between the cloud
provider and the customer. The cloud provider must demonstrate progress in
sustainable cloud operations, and cloud consumers must employ sustainability
practices in their use of cloud services. If one of these parties doesn’t do its part, then
sustainability outcomes are compromised. Partners can be useful in helping
customers achieve their desired sustainability outcomes.■
The world is still developing new, sustainable energy sources. Currently, there aren’t
enough renewable energy sources to support all cloud service offerings. Further
improvement, including development, will be needed before sustainable cloud
services are pervasive in the industry.■
G00768727 Page 14 of 129User Recommendations
Sample Vendors
Amazon Web Services (AWS); Google; IBM; Microsoft; Oracle; Salesforce; SAP; VMware
Gartner Recommended Reading
Executive Leadership: Sustainability Primer for 2022
Leading Sustainability Ambition, Goals and Technology in the 2020s
How to Set Strategic Ambition for Sustainable Business
Positioning I&O for Environmental Sustainability
Sustainability: A Customer Priority and Provider Imperative
SaaSOps
Analysis By: Sid Nag
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: EmergingEmbrace the sustainability opportunities of using cloud by validating the
sustainability capabilities of speciﬁc cloud service offerings. Use sustainability
information to inform workload placement.■
Seek the beneﬁts of using cloud to meet sustainability ambitions including the
environmental, social and economic beneﬁts produced by cloud service providers.■
Engage in the shared responsibility model for sustainability by holding cloud service
providers accountable for their sustainability practices as well as your progress
toward sustainable use of cloud services.■
Track sustainability performance KPIs as published by cloud service providers to
ensure you are realizing the expected results from your use of cloud services. Bolster
these metrics with an audited assessment of technology use within overall progress
toward a sustainable organization.■
G00768727 Page 15 of 129Definition:
SaaS operations (SaaSOps) describe the operational aspects of end users that use SaaS
applications in the cloud, and those that deploy their own software in the cloud and want
to operate the software like a SaaS provider. It also includes functionalities such as ITOps,
PlatformOps, SecOps, AIOps, MLOps and Site Reliability Engineering (SRE) capabilities.
Why This Is Important
SaaSOps remains a fragmented model. Many SaaSOps aspects are performed separately.
However, there is no consolidated approach from SaaS providers to address observability,
application performance, security, platform and SRE functionality issues. The aspect of
choosing a SaaS application that runs on a preferred hyperscaler is also a part of this
functionality. Hyperscale cloud providers differ substantially, and choosing a SaaS
application by organizations that are on their preferred cloud provider can be daunting for
IT leaders.
Business Impact
Independent software vendors (ISVs) are increasingly moving to a SaaS model. Most
DevOps and I&O teams are not equipped to serve full life cycle demands of SaaS
applications when they deploy their own software in the cloud and want to operate it like
SaaS providers in application management and observability areas. This can impede
deployment and efﬁciency of such applications, and the end user experience.
Drivers
ISVs continue to move their commercial off-the-shelf (COTS) applications to a cloud
SaaS version.■
Applications are increasingly being modernized today by DevOps teams using
microservices and containers.■
I&O leaders don’t have the available tools and capabilities for observability,
application monitoring and security visibility for this new world of applications.
There is no integrated platform approach to these functionalities and tools.■
The number of SaaS applications in the cloud and organization’s that deploy their
own software in the cloud and want to operate the software like a SaaS provider are
growing. This has created an unmanageable amount of administrative work for IT.■
Lack of SaaSOps capabilities such as ITOps, PlatformOPs, AIOPs, MLOps, SecOps
and SRE services is impeding the continuous integration/continuous delivery
(CI/CD) pipeline.■
G00768727 Page 16 of 129Obstacles
User Recommendations
Sample Vendors
BetterCloud; Productiv; Sonar; Torii; Zluri
Gartner Recommended Reading
Market Guide for SaaS Management Platforms
Comparing Cloud Operations Approaches
Sovereign Cloud
Analysis By: Rene Buest, Neville Cannon, Gregor Petri, Tiny Haynes
Benefit Rating: Moderate
Market Penetration: 1% to 5% of target audience
Maturity: AdolescentLack of comprehensive SaaS operational and application management suites in the
market in the areas of ITOps, PlatformOPs, AIOPs, MLOps and SecOps■
DevOps teams and I&O teams within an organization still operating in silos ■
Lack of IT skills in areas such as AI/ML and SRE-type services ■
Institute an ITwide SaaSOps strategy and execution plan. ■
Bring the DevOps and IT teams to work together in a harmonious manner, thereby
making the CI/CD pipeline frictionless.■
Choose the right SaaSOps toolsets with a platform-based approach. ■
Force the SaaS providers to select the right cloud provider to run their SaaS
applications on.■
G00768727 Page 17 of 129Definition:
Sovereign cloud is the provision of cloud services within a jurisdiction meeting data
residency requirements and operational autonomy. It is intended to ensure that data and
infrastructure is free from control by external jurisdictions and protected from foreign
government access.
Why This Is Important
The public sector and commercial organizations increasingly depend on cloud services
leading to greater demand for control and autonomy, and hence more regulation. In many
jurisdictions data residency, data protection and privacy laws are increasing. Laws
combined with increasing geopolitical tensions, different economic ideologies, and
proliferating cybersecurity risks from a variety of directions, including state actors, are
steadily elevating interest in sovereignty.
Business Impact
Concerns about the sovereignty of data, infrastructure, and operations hosted in foreign-
owned and operated cloud service offerings has led to a surge in newly announced
sovereign cloud offerings. Legislative mandates are being applied to limit the ability of
organizations to use nondomestic vendor services. These impact buying decisions and
investments organizations are willing to make in cloud offerings. As a result, end users
could ﬁnd themselves in a fragmented market without access to the resources they need
to support their digital business initiatives.
G00768727 Page 18 of 129Drivers
Governments and commercial organizations are becoming increasingly aware of
and concerned about their dependence on foreign cloud infrastructure providers and
SaaS offerings. Reasons for this are tighter privacy and data protection regulations,
data sovereignty, data control and operational autonomy, as well as technological
sovereignty and independence.■
The market for digital and cloud technology and services is dominated by the U.S.
and Chinese technology and service providers. As a result, all non-US organizations
and companies mainly have to access foreign services and technology to build and
run digital business models. Hence, data is being stored within non-domestic cloud
and digital service providers, which creates political uneasiness.■
As digital services become increasingly important and critical, cloud customers and
regional trade bodies worry about retaining control over their data to stay compliant
with local regulations as well as their operational autonomy.■
Some more regulated industries and governments are particularly concerned by the
U.S. and Chinese legal frameworks that might allow the US government to access
cloud-stored data under speciﬁc circumstances.■
Businesses increasingly depend on technology platforms they don’t control.
Although the risk of deplatforming remains small, the growing number of platforms
enterprises use and the businesses’ growing dependence on platforms increase the
consequences of deplatforming.■
G00768727 Page 19 of 129Obstacles
User Recommendations
Sample Vendors
Atos; Bleu; Google; Microsoft; Oracle; OVHcloud; SAP; Telindus; Thales; T-SystemsThe market dynamics make it almost impossible for domestic providers to present a
viable alternative to hyperscale cloud offerings as the capabilities of hyperscaler far
exceed most sovereign cloud offerings. Considerable technical obstacles exist if
sovereign clouds are expected to deliver the maturity and level of scalability,
reliability and functionality of hyperscale offerings.■
Too few skilled engineers exist to replicate the design capabilities of hyperscale
cloud offerings to build comparable sovereign clouds. With lower levels of skills
being available, security and operational maturity will be compromised, potentially
leading to greater security and failure risks.■
An increasing number of announcements of sovereign cloud offerings from global
and national cloud providers are hitting the market, all based on various approaches
and delivery models.■
Individual governments each deﬁning their own requirements for sovereign cloud
offerings may lead to compliance regimes that break public cloud scale and
innovation.■
Subject proposals for sovereign cloud to the same level of risk assessment that
current cloud computing offerings are subjected to. Do not assume that the
sovereign cloud conveys any additional security measures in itself.■
Differentiate between different sovereign cloud approaches by type of workload,
data and infrastructure when making cloud deployment decisions. Doing so, classify
various delivery models between the cloud provider approaches to meet sovereignty
requirements.■
Explore evaluating locally provided cloud services for workﬂows that can be
provided locally and leverage third-party solutions to protect data and ensure it is
compliant with local requirements.■
Assess any considered sovereign cloud offerings against long term viability, also in
case legal requirements change or global offerings start to directly cater to national
sovereignty requirements.■
G00768727 Page 20 of 129Gartner Recommended Reading
What Are the Different Provider Approaches to ‘Sovereign Cloud’ Demands?
Market Trends: Europe Aims to Achieve Digital Sovereignty With GAIA-X
Emerging Technologies: Compliance Regimes Will Break Public Cloud Scale and
Innovation
Decide How to Mitigate the Risk of Being Deplatformed
What We Are Hearing About Cross-Border Data Transfers
Distributed Cloud
Analysis By: David Smith, Milind Govekar, Daryl Plummer
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Distributed cloud refers to the distribution of cloud services to different physical locations,
while operation, governance, updates and evolution of the services are the responsibility
of the originating cloud provider.
Why This Is Important
Distributed cloud enables organizations to use consistent cloud-based services wherever
needed, while the cloud service provider retains the responsibility of managing the
technology, implementation and evolution of the capabilities. It gives organizations the
ﬂexibility to support use cases that will beneﬁt from cloud services — regardless of their
dependence on speciﬁc locations. Organizations can use distributed cloud to reimagine
use cases where cloud computing is not currently feasible.
G00768727 Page 21 of 129Business Impact
A major notion of the distributed cloud concept is that the provider is responsible for all
aspects of delivery, and manages the distributed capabilities “as a service.” This restores
cloud value propositions that are broken when customers are responsible for a part of the
delivery, as is true in some hybrid cloud scenarios. The cloud provider must take
responsibility for how the overall system is managed and maintained. Otherwise, the
value proposition of distributed cloud is compromised.
Drivers
Distributed cloud computing is a style of cloud computing where the location of
cloud services is a critical component of the model.■
Historically, location has not been relevant to cloud computing deﬁnitions. In fact,
the variations on cloud (e.g., public, private, hybrid) exist because location can vary.■
It is a misconception that private cloud or hybrid cloud requires on-premises
computing, as they do not require private components for any speciﬁc location. With
the advent of distributed cloud, location formally enters the deﬁnition of a style of
cloud services.■
There are services outside of the provider’s data center for both approaches. That is
the key distinction, and shared with distributed cloud.■
In hyperscale public cloud implementations, the public cloud is the center of the
universe. There has been distribution of cloud services through worldwide regions in
public cloud practically since its inception. The major hyperscale cloud providers
have different geographic regions around the world, and all are centrally controlled
and managed, and provided by the public cloud provider. Most instances of
distributed cloud are distributed from public cloud providers. Alternative providers
are possible, but likely to be rare.■
Distributed cloud supports both tethered and untethered operations of like-for-like
cloud services from the cloud provider, “distributed” out to speciﬁc and varied
physical locations. This enables an important characteristic of distributed cloud
operation — low-latency compute where the compute operations for the cloud
services are closer to those that need the capabilities. This can deliver major
improvements in performance, as well as reduce the risk of global network-related
outages.■
Data sovereignty and other regulatory issues. ■
G00768727 Page 22 of 129ObstaclesPerceived and real security and privacy concerns with off-premises applications and
infrastructure.■
Latency needs of IoT/edge applications. ■
Distributed cloud is still a single-cloud provider, and the managed cloud assets are
still part of the cloud provider’s portfolio.■
Disconnected operations. ■
Customers can’t abandon existing technologies in favor of complete and immediate
migration to the public cloud, due to sunk costs, latency requirements, regulatory and
data residency requirements — and the need for integration.■
Different approaches to distributed cloud have different value propositions (e.g.,
portability, software, appliance).■
Distributed services are a relatively small subset of the centralized services and will
take time to expand and may never reach 100% parity.■
Distributed cloud in your data center will have limits to scale and elasticity that do
not exist with the centralized public cloud.■
More advanced approaches like distributed cloud embedded in networking or
telecom equipment — or delivered as metro area services — are very immature.■
The application and data architecture needs to take into account the different
compute components. Using a distributed cloud requires distinct attention from an
architectural perspective.■
G00768727 Page 23 of 129User Recommendations
Sample Vendors
Amazon Web Services; Google; IBM; Microsoft; Oracle
Gartner Recommended Reading
Top Strategic Technology Trends for 2021: Distributed Cloud
‘Distributed Cloud’ Fixes What ‘Hybrid Cloud’ Breaks
The Cloud Strategy Cookbook, 2021
Cloud HPC
Analysis By: Chirag Dekate
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: AdolescentAdopt distributed cloud by overcoming the fear of a single franchise controlling the
public cloud and on-premises cloud estates.■
Use the distributed cloud model to prepare for the next generation of cloud
computing by targeting location-dependent use cases, such as low latency, tethered
scale and data residency, that are enhanced by using a distributed cloud model.■
Identify scenarios where distributed cloud use-case requirements can be met by
evolution of a hybrid cloud model and where the requirements are substantially
different.■
Distributed cloud should be a preferred model (over private cloud). ■
G00768727 Page 24 of 129Definition:
Cloud high-performance computing (HPC) services deliver modernized supercomputing
environments that accelerate HPC-powered digital innovation using integrated cloud
services, automation, elasticity and AI-infused management. Cloud HPC providers deliver
platforms that support scalable access to specialized HPC architectures, enabling
enterprises to address diverse use cases, including traditional computational engineering
applications, advanced AI and simulation-driven digital twins.
Why This Is Important
Cloud HPC enables enterprises to develop new digital products, scale innovation faster,
modernize HPC applications and validate new technologies faster. Further, cloud HPC
offers exclusive capabilities including accelerator environments, automated management
services, workload-speciﬁc continuous cost optimization and early access to HPC
technologies. Cloud-native HPC applications interfaced with IoT data, can enable new
digital products, unlocking new value creation opportunities.
Business Impact
Cloud HPC delivers purpose-designed cloud services optimized for high-end performance,
with integrated HPC workﬂow automation, support for advanced compute environments
and AI- infused management that continuously maximize value. Cloud HPC services are
designed to augment or replace existing HPC environments, and enable HPC teams to
accelerate the modernization of HPC applications and enable new HPC-powered digital
products that are integrated with broader enterprise architectures.
G00768727 Page 25 of 129Drivers
IT leaders across enterprises are seeking to minimize analytics islands and are
focused on enabling new value creation use cases, including simulation-driven
digital twins and AI-augmented HPC, while ensuring continuity of existing advanced
analytics pipelines.■
In most enterprises, HPC systems tend to be oversubscribed with large-long running
jobs occupying large chunks of resources. As a result, smaller jobs wait in resource
management queues for inordinately long durations. HPC I&O teams are actively
seeking to minimize queue wait times for business stakeholders using cloud HPC to
ofﬂoad some of the smaller jobs.■
IT leaders are seeking to modernize HPC I&O practices that have remained relatively
unchanged for nearly two-and-a-half decades. In part, IT leaders are seeking to
leverage virtualization techniques including HPC-speciﬁc containers to enable
portability in some HPC applications■
Cloud HPC enables any type of enterprise to rapidly and cost-efﬁciently explore
newer accelerator technologies and validate applicability for their applications.■
Retaining HPC skills is becoming harder due to both the Great Resignation and
demand-driven talent attrition, resulting in some enterprises struggling to deliver
sustainable HPC services. Some IT leaders are leveraging cloud HPC to mitigate
these impacts.■
G00768727 Page 26 of 129Obstacles
User Recommendations
Sample Vendors
Alibaba Cloud; Amazon Web Services; Atos-Nimbix; Google Cloud Platform; Microsoft
Azure; Oracle; OVHcloud; Rescale; SURFsara; UberCloud
Gartner Recommended Reading
Understanding the Opportunity for Arm-Based Servers
2022 Strategic Roadmap for Compute InfrastructureAdvanced HPC and AI workloads require a large number of compute nodes for long
durations — in some, these jobs span thousands of cores and run for months at a
time. These large, long-running jobs result in pricing dynamics that are incompatible
with most major cloud pricing models.■
Cloud HPC environments are better-suited to loosely coupled, very parallel
workloads. Applications that require tightly coupled systems with high-performance
low-latency message passing interconnects will not operate cost-effectively or
efﬁciently in most general-purpose clouds.■
HPC middleware has largely remained unchanged for the last two-and-a-half
decades and has not aggressively adopted broader IT practices including
virtualization and containerization, due to the perceived performance impact of
virtualization techniques.■
HPC-speciﬁc instances are not available from cloud providers in sufﬁcient volumes
across the diverse regions, severely limiting enterprise ability to leverage these
systems.■
Devise a cloud-augmented supercomputing platform that enables your teams to
access supercomputing capabilities wherever they are needed (on-premises, cloud
and/or edge). Embrace Gartner’s platform ops approach to deliver these platforms■
Modernize HPC skills by upskilling and hiring for HPC talent that can augment cloud
HPC in your organization. Seek individuals with cloud competencies in addition to
HPC.■
Accelerate disruptive HPC innovation by encouraging rapid prototyping of HPC-
speciﬁc container and virtualization strategies.■
G00768727 Page 27 of 129Solution Criteria for Cloud Integrated IaaS and PaaS
Packaged Business Capabilities
Analysis By: Yeﬁm Natis
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Packaged business capabilities (PBC) are the building blocks of composable
architectures. They are software components that are wrapped (packaged) to maximize
the effectiveness of their reusability, autonomy, orchestration and discoverability by
business-oriented tools. These capabilities may represent business logic, process, data,
analytics, experience, platform technology or other packaged functionality.
Why This Is Important
Composable architecture is adopted to better support the pace of change required by the
fast-changing needs of the business. PBCs help separate the creators of the composable
modular building blocks and the composers of the delivered solutions. Such role
separation forms the foundation for composability and, consequently, better prepares
organizations to sustain and grow their businesses in the face of present and future
disruptions.
Business Impact
With composable architecture and PBCs:
Business technologists and fusion teams are better equipped to control and quickly
adapt business processes and experiences to the changing business needs,
improving the organization’s potential for resilience, adaptability and innovation.■
IT and business professionals are better equipped to combine their talents in the
creator-composer process, shifting organizations closer to an effective business-IT
operational continuum.■
G00768727 Page 28 of 129Drivers
High fragmentation, disruptive change and ﬁerce competition in some industries —
such as digital commerce, healthcare, or transportation and supply chain — demand
fast realignment and adjustment of business and its digital resources.■
To take advantage of the pace of business change, leading organizations seek
faster, safer and more-efﬁcient tools for digital business innovation.■
Increasing participation of business professionals in software engineering requires
more business-oriented expression in software modeling, replacing or augmenting
the traditional programmatic orientation.■
The increasing democratization of platform technologies is bringing more business
professionals to digitalized business capability (application) design work.■
The growing orientation of vendor applications (including SaaS) to API-ﬁrst and API-
only (“headless”) design is leading organizations toward composition and
integration, instead of the basic customizations of vendor applications.■
The increasing sophistication of agile development practices and managed-product
delivery of applications demands more-advanced modularity, autonomy,
orchestration and discovery for functions and features of applications.■
Software engineering teams are acting in a more autonomous manner and
enterprise architects need new ways to provide business-architectural guidance to
application architects.■
G00768727 Page 29 of 129Obstacles
User RecommendationsCultural resistance to change, fear of the shifting business priorities and familiarity
bias — form barriers to the rapid adoption of the architecture of composability.■
“Composable-washing” by some vendors discredits the model and delays adoption. ■
Many application design and composition initiatives remain, by tradition, in central
IT, limiting the direct participation of business.■
Fusion team effectiveness is limited, due to cultural challenges and inertia
compromising both the technology and the business value of their outcomes.■
Without an effective separation of technologist creators and fusion team
composers, the architecture fails to deliver the business effect of composability.■
Enterprise and business architecture designs often lack direct mapping to the
modularity of application architectures, making the business-IT design continuum
more difﬁcult to achieve.■
Prioritize expertise in API and event management as precursors of composable
architecture.■
Separate the responsibilities of the creators of PBCs and the composers that use
them to deliver application processes and experiences to business users.■
Reject any new monolithic solutions proposed by vendors or in-house developers,
and plan to renovate or replace the old ones, to enable their participation in
composition.■
Prioritize democratized tools suitable for business-IT fusion teams supporting
development of composed application experiences.■
Give preference to API-ﬁrst and API-only (headless) SaaS. ■
Build up the business-IT collaboration by forming fusion teams and promoting
shared objectives and incentives across areas of the organization contributing
resources to a common business outcome.■
Form specially targeted fusion teams that bring together enterprise and application
architects to bridge the gap between the business and technology architecture
thinking.■
G00768727 Page 30 of 129Gartner Recommended Reading
Innovation Insight for Composable Modularity of Packaged Business Capabilities
Use Gartner’s Reference Model to Deliver Intelligent Composable Business Applications
Kick-Start Your Composable Business Journey With 2 Key Strategies
How to Design Enterprise Applications That Are Composable by Default
Fusion Teams: A New Model for Digital Delivery
Business-Driven Cloud Strategy
Analysis By: David Smith, Lydia Leong
Benefit Rating: Transformational
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
A cloud strategy is a concise viewpoint on the role of cloud computing in an organization.
A true cloud strategy is business-driven, focused on “what” and “why” issues and
alignment to business goals. It is different from other efforts, often referred to as cloud
strategy, that in many cases are better described as cloud adoption or migration plans.
Why This Is Important
Every business requires a cloud strategy, regardless of where it is in its cloud journey.
Organizations lacking a cloud strategy don’t achieve as much from cloud computing as
those that have one. Devising a business-driven cloud strategy leads to secondary
questions about product choices, principles and business value prioritization. Effective
business-driven cloud strategies align these questions with other strategies and provide a
common view of “cloud,” and a launching point for more use-case scenarios.
G00768727 Page 31 of 129Business Impact
Many top questions in cloud computing revolve around cloud strategy. However, many
cloud discussions are typically focused purely on technology, not on how cloud
capabilities should be assessed against business outcome targets. Business-driven cloud
strategies need to be aligned with other strategies (e.g., data center, security and
architecture). Your organization’s business driven cloud strategy should enable cloud
adoption/migration (e.g., tactical implementation plans) leads to speciﬁc actions.
Drivers
Many organizations lack a cloud strategy. They don’t achieve as much from their use
of cloud computing as organizations with a cloud strategy.■
Devising a cloud strategy leads to many secondary questions about principles and
prioritization. Effective cloud strategies align these questions with other strategies.■
Cloud computing affects every aspect of organizations’ IT and business
environments, requiring coordination across multiple domains to ensure successful
and safe cloud exploitation.■
Organizations with documented and repeatable processes for evaluating the value
and risks using cloud services outperform those with ad hoc processes.■
Organizations that embrace cloud computing best practices across all IT functions
reduce the risk and increase the value of exploiting cloud computing.■
Organizations that do not have a high-level cloud computing strategy, driven by their
business and IT strategy, will signiﬁcantly increase their risk of failure and wasted
investment.■
G00768727 Page 32 of 129Obstacles
User Recommendations
Gartner Recommended Reading
The Cloud Strategy Cookbook, 2021Challenges in aligning your business-driven cloud strategy with existing technology
and business strategies, such as those for security, data center, and development
and architecture.■
Cloud myths or common mistakes like believing a high-level, aspirational statement
like “cloud ﬁrst” is a strategy — or considering implementation plans to be the
strategy, or that cloud migration automatically saves money.■
A lack of understanding or alignment across the organization regarding the intended
business strategy and potential use of technology.■
On the organizational front, lack of senior-level buy-in or backing for the strategy,
and failure to assemble a cloud council of interested parties across IT and the
business.■
Existing cloud adoption/migration plans often preclude plans for a business-driven
cloud strategy.■
Maximize the beneﬁts from your use of cloud computing by creating a business-
driven cloud strategy. Make it a living document that provides a concise view on the
role of cloud computing in your organization and its contribution to business value
results.■
Align your cloud strategy with other strategic plans (e.g., those for data center,
security and architecture) communicating with stakeholders.■
Plan for your cloud strategy to be the launching point for cloud activities like
architecture, assessment, migration and operations. Keep those activities in mind
when devising your cloud strategy.■
Safeguard your organization from potential problems if you withdraw from the cloud
by including an exit strategy describing the dependencies and choices involved in
cloud computing. Focus your overall and exit strategies on answering “what” and
“why” questions. Cover answers to “how” questions in a more detailed cloud
adoption and/or exit plan. Account for potential business impacts.■
G00768727 Page 33 of 129A CIO’s Guide to Strategy Development
Align Your Cloud Strategy With the Organizational Strategic Plans
Top 10 Tips for Avoiding the Most Common Mistakes in Cloud Strategies
Infographic: The Cloud Journey
Container-VM Convergence
Analysis By: Michael Warrilow, Tony Iams, Philip Dawson
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: Emerging
Definition:
Container-virtual machine (VM) convergence refers to the fusion of hypervisor and
operating system (OS) based virtualization technologies. By integrating and optimizing
the most desirable features of containers and virtual machines, container-VM
convergence delivers improved workload isolation and greater infrastructure utilization.
Underlying technologies include optimized virtual machine monitors, integrated container
runtimes, open APIs and highly optimized OS images.
Why This Is Important
Containers appeal to the need for modern, agile infrastructure whereas virtual machines
are a foundational element of data center infrastructure. Container-VM convergence
promises the “best of both worlds” and introduces competition in a software market that
is dominated by a few virtualization vendors. Using innovation from public cloud
providers and open source communities, container-VM convergence creates opportunities
to improve infrastructure elasticity, scalability and efﬁciency while supporting modern
application development.
G00768727 Page 34 of 129Business Impact
Container-VM convergence enables greater infrastructure agility without sacriﬁcing
security. By infusing a cloud-inspired approach to managing virtual infrastructure, it
enables I&O teams to manage hypervisor and OS-based virtualization together. It also
supports initiatives to increase developer and operational productivity. By supporting a
gradual transition to cloud-like infrastructure for both on- and off-premises, it preserves
existing infrastructure investments and reduces additional spend over the medium term.
Drivers
Many I&O teams desire increased competitive negotiation in the hyper-based
virtualization market. This has resulted from the market dominance of a small
number of providers with few alternatives for on-premises requirements. Container-
VM convergence has the potential to do this and disrupt the competitive landscape
before 2025.■
I&O teams struggle to meet the demands for cloud-like infrastructure in noncloud
environments. A major factor for this deﬁciency is that traditional IT infrastructure is
limited in the ability to deliver modern requirements such as immutable
infrastructure, agile delivery and API-driven management. Container-VM convergence
provides a technology to bridge these needs.■
CIOs will require technologies such as containers to satisfy business requirements.
In 2022, CIOs are investing in areas such as digital business transformation, API
integration and legacy application modernization. However, containers are becoming
a foundational element required to leverage many emerging technologies (AI/ML
and edge computing).■
Container-VM convergence can support current and future I&O requirements, and
help balance rising ﬁnancial pressures that will increasingly require I&O to refocus
on IT cost optimization. If existing infrastructure investments are maintained for
longer than was anticipated, container-VM convergence can reduce resulting
pressure to continue supporting digital transformation. This is achieved by avoiding
investment in new tools, skills and processes.■
For I&O leaders, the ﬁnancial outlay is limited to upgrading existing infrastructure
software; however, the beneﬁt will be supported for traditional and future
requirements. To remain competitive, incumbent virtualization providers will need to
update their products and innovative, and emerging container-VM convergence
technologies will be a major catalyst.■
G00768727 Page 35 of 129Obstacles
User RecommendationsPractices and technologies related to container-VM convergence favor Linux. For
non-Linux enterprise workloads, a different operation/production support model
must be maintained. As a mitigation, container-VM convergence could support
Windows as VMs, alongside Linux containers and/or micro-VMs.■
Successful adoption of new operational approaches requires cultural change. This
may constrain the uptake of container-VM convergence, which will lead to friction
with application developers, business-led IT projects and/or circumvention of
existing I&O capabilities. In turn, this will lead to compliance issues in regulated,
high-security environments.■
Similarly, ﬁnancially constrained organizations may struggle to see the beneﬁt of
additional investment over and above existing sunk costs.■
Container-VM convergence is not an established technology in enterprise
environments. Although some related technologies have been proven to work in
hyperscale cloud environments, container-VM convergence will need to establish
itself as being suited to the complexity of legacy enterprise environments.■
Plan for container-VM convergence to deliver modern, cloud-inspired infrastructure
supporting enterprise developers, while maintaining operational management and
security for production environments.■
Establish and maintain consensus on the role of virtual infrastructure in data center,
distributed cloud, public cloud and edge — recognizing that various virtualization
technologies may be necessary.■
As a precursor to selecting container-VM convergence, conduct or utilize a skills
inventory. An I&O skills roadmap will help to verify existing and future skills needed
to support emerging technology trends like this. Where skills deﬁciencies are
identiﬁed, adopt container-VM convergence in a more gradual fashion that supports
future requirements without detracting or unnecessarily complicating existing
service-level requirements.■
Validate potential beneﬁts of using cloud-based services can obviate the need for
software-based virtual infrastructure.■
G00768727 Page 36 of 129Sample Vendors
Amazon Web Services (AWS); Fly; Microsoft; Red Hat; SUSE; VMware
Gartner Recommended Reading
Market Guide for Server Virtualization
Market Guide for Container Management
Industry Cloud Platforms
Analysis By: Gregor Petri
Benefit Rating: Transformational
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Industry cloud platforms leverage underlying SaaS, platform as a service (PaaS) and
infrastructure as a service (IaaS) cloud services to offer industry-relevant packaged
business and technical capabilities to an identiﬁed vertical as a whole product offering.
Industry cloud platforms turn to composability and modularity to reduce the risk of
platform complexity and lock-in.
Why This Is Important
Cloud providers are launching industry cloud platforms by combining SaaS, PaaS and
IaaS offerings with industry-speciﬁc functionality and composable capabilities to create
more compelling propositions for mainstream customers. Emerging industry cloud
platforms are leveraging innovative approaches such as composable packaged business
capabilities (PBCs), PBC marketplaces, data grids and fusion teams to accommodate
faster change and platform adaptability.
Business Impact
Broader cloud adoption within enterprises will require more vertical-targeted whole-product
solutions that follow deﬁned industry scenarios and process models, rather than
technology-oriented solutions that enterprises have to largely conﬁgure and integrate
themselves. Industry clouds will have a lasting impact on cloud customers, blurring the
lines between established cloud services such as SaaS, PaaS and IaaS.
G00768727 Page 37 of 129Drivers
Industry cloud platforms can create value for enterprises by bringing traditionally
separately purchased solutions together in a composable and modular way. This
can simplify the sourcing, implementation and integration process.■
Leaders in this space are expected to leverage composable cloud and edge
approaches to create more holistic and comprehensive industry offerings, which
enterprises’ staff and talent will be able to recompose to meet unique or special
requirements.■
Currently, industry cloud platforms are largely being initiated and created by large
technology providers, although we see some enterprises considering creating —
either jointly or in collaboration with a technology provider — a dedicated industry
cloud platform as the basis for a more autonomous industry ecosystem.■
Enterprises can gain business value from industry clouds through shared best
practices; vertically specialized go-to-market and implementation teams; compliance
of the infrastructure platform with industry-speciﬁc regulations, such as HIPAA or
FedRAMP; analytical capabilities to integrally mine the data from existing and new
applications; industry-speciﬁc add-on functionality in front and back ofﬁce enterprise
applications; and fully vertical speciﬁc solutions, combined with collections of
composable building blocks available in industry cloud marketplaces.■
Providers are creating whole-product offerings that cater directly to the established
needs of vertical industry enterprises.■
G00768727 Page 38 of 129Obstacles
User Recommendations
Sample Vendors
Amazon Web Services (AWS); Google; IBM; Infor; Microsoft; Oracle; Salesforce
Gartner Recommended Reading
Quick Answer: What Makes Industry Clouds Different From Today’s Cloud Offerings?Industry clouds are at risk of following the same path as community clouds, such as
dedicated government clouds, where providers added speciﬁc vertical functionality.
This often led to breaking the compatibility and upgradability with the cloud it was
derived from and left enterprises on a long-term unsupported or unsupportable copy
of the cloud.■
Industry cloud platforms can be overwhelming in terms of the wide breadth of
functionality they potentially cover. Customers and providers must therefore be
disciplined and not burn precious resources on ﬁxing/replacing things that are not
broken. Implementing an industry cloud platform must be approached as adding an
exoskeleton, bringing new and improved capabilities rather than a vital organ
transplant, replacing or repairing functionality that was already present.■
To reach their full potential, industry clouds will need to evolve into something best
described as ecosystem clouds. Enterprises can leverage these ecosystems by
participating in shared (business) processes, such as procurement, distribution,
payment procession, and maybe even R&D and innovation. This will be a step
beyond the initial sharing of infrastructure and technology.■
Assess the industry-speciﬁc features promoted by various cloud providers, and
distinguish between real technology or functionality offerings versus marketing
messages.■
Evaluate long-term industry cloud roadmaps to understand the strategic focus of
different industry cloud offerings and how they evolve industry speciﬁcity, taking
into account that vendors choose different paths to add industry value.■
Establish communications with current application and infrastructure providers
about which industry cloud ecosystems they plan to support or envision to become
part of in the future.■
G00768727 Page 39 of 129Providers of Cloud Managed Services: Use Composable Industry Platforms to Productize
Your Offerings
Predicts 2022: The Cloud Moves From Technology Disruption to Business Disruption
Leverage Gartner’s Vertical Strategy Framework for Composable Industry Cloud Offerings
Create Differentiated Cloud Managed Services for the Banking and Investment Services
Industry
G00768727 Page 40 of 129At the Peak
Consumption-Based Model
Analysis By: Jeff Vogel, Philip Dawson
Benefit Rating: Moderate
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
A consumption-based sourcing model strategy for hybrid cloud on-premises data center
storage and compute infrastructure is an acquisition, deployment and support model that
includes a cloud-like consumption and platform services model with a variable payment
tied to measured use.
Why This Is Important
The cloud model has affected IT operations more broadly than simply bringing easy-to-
consume compute and cloud storage. It has brought a whole new way of procurement
sourcing and asset consumption, with pay-as-you-use and as-a-service platforms
becoming the preferred deployment methodology for storage and compute. The desire to
pay only for what is used has spread to storage hardware in particular, with vendors
offering licensing that allows customers to pay for what is committed and used.
Business Impact
A consumption-based sourcing model and services strategy will:
Shift responsibility for maintenance and support costs to vendors investing in
artiﬁcial intelligence for IT operations (AIOps) to automate IT operations and reduce
dependency on subject matter expertise■
Preserve cash by avoiding upfront capital expenditure (capex) in exchange for
strategic priorities■
Shift IT and ﬁnance resource budget cycles to a services-based platform delivery
model■
Provide a more ﬂexible and agile IT operations aligned with business demands ■
G00768727 Page 41 of 129Drivers
Many infrastructure and operations (I&O) leaders are embracing cloud-native hardware
and software consumption models as a strategy to replace owned, on-premises
infrastructure and to lower data center operations’ costs. This trend is driven by:
Obstacles
A consumption-based sourcing model may:The permanence of the cloud ■
The need for a more ﬂexible and agile IT operating model ■
The massive growth of enterprise data ■
Prolonged procurement lead time increases due to persistent supply shortages ■
The need for an application-aware services delivery model ■
The preference for operating expenditure (opex) to capital expenditure (capex) with
cloud-like beneﬁts, while avoiding risks or costs associated with moving mission-
critical workloads to the public cloud■
The need for a more cost-effective and efﬁcient sourcing strategy that aligns with
business demands■
The need to augment IT budget priorities to redirect investments to develop cloud-
native platform skills that support business growth initiatives■
The shift from exiting the life cycle management of infrastructure assets in the long
term to freeing up IT resources■
Be more expensive than a purchase ■
Be organizationally challenging to implement ■
Be unsuitable for IT operations that don’t require ﬂexibility to accommodate
uncertain growth and variability in forecast demand or lean toward sweating assets■
Require minimum-usage commitment levels, and can’t scale down to zero as a
minimum regardless of what is actually consumed■
Require three- to ﬁve-year contracts with vendor-centric services ■
G00768727 Page 42 of 129User Recommendations
By implementing a consumption-based model strategy, IT leaders should:
Sample Vendors
Cisco; Dell Technologies; Hewlett Packard Enterprise (HPE); IBM; Lenovo; NetAppLack the skills or culture alignment to shift from sourcing products to platform SLA
services■
Not take into account long-term supply chain price ﬂuctuations during the contract
period, when declining hardware costs or supply constraints are considered■
Conﬂict with ﬁnancial asset depreciation and amortization schedules or corporate
balance sheet objectives■
Conﬂict with established industry accounting standards and operational norms ■
Conﬂict with consumption-based hardware, making software licensing terms
impractical■
Adopt a cloud operating model as a platform services strategy to shift to an IT-as-a-
services-capable organization.■
Organize and align a joint team approach to include I&O, vendor management and
ﬁnance to establish a strategic sourcing strategy and implement it.■
Rightsize and align IT I&O resources to focus on business priorities. ■
Assess the economics and requirements against a range of vendor consumption
programs before committing.■
Ensure that contract terms match ﬁnancial requirements, accounting for capex
versus opex, and that contracts include appropriate end-of-term options, such as
book value buyout.■
Address licensing options and term constraints as they pertain to usage. ■
Align and link consumption-based costs to speciﬁc usage at agreed-upon elastic
utilization levels, along with remediation terms to enforce minimum levels.■
Retire legacy technical debt and onerous support fees, and modernize systems and
processes.■
G00768727 Page 43 of 129Gartner Recommended Reading
Enterprise Storage as a Service Is Transforming IT Operating Models
Market Guide for Consumption-Based Models for Data Center Infrastructure
How to Evolve Your Physical Data Center to a Modern Operating Model
Key Considerations for CSOs Moving to a Consumption-Based Subscription Model
API-Centric SaaS
Analysis By: Yeﬁm Natis, Anne Thomas, Mark O'Neill
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
API-centric (“headless”) SaaS is a cloud application service whose primary interface
method is programmatic, accessed via request- or event-based APIs. A user interface may
be provided, but the strategic intent for API-centric SaaS is to contribute a set of API
building blocks for further development of custom-composed application functionality,
processes and experiences.
Why This Is Important
API-centric SaaS allows packaged business functionality, such as banking, commerce or
mapping services, to be used as building blocks in larger applications. This empowers
organizations to create application experiences — more advanced than relying entirely on
in-house resources, and better targeted than relying entirely on the SaaS provider. Greater
creativity in application engineering translates to business empowerment for faster, safer
and more efﬁcient innovation.
G00768727 Page 44 of 129Business Impact
Business organizations equipped to capitalize on API-centric SaaS can create new
application experiences for their employees and customers through composition of the
API-packaged business functionality. They gain an opportunity for faster, safer and more
efﬁcient innovation. Vendors of API-centric SaaS engage a new business model by
extending the more traditional direct-to-user application business with cooperative
developers composing customized user applications.
Drivers
The modern application design relies on cross-application integration and
composition to a degree that application vendors that want to be included must
offer their services as published and packaged business APIs.■
The technology and skills for integration, including management, of APIs are
widespread, allowing some leading businesses to package their business
functionality behind APIs for customers’ and partners’ customized use. API-centric
SaaS serves that purpose.■
The demands for the depth of customized applications in business organizations
have evolved to the point that SaaS vendors must allow rearrangement of their
business functionality by their customers. API-centric SaaS serves that purpose.■
Increasing use of advanced digital twins serves as the early experience of API-
packaged business functionality for organizations strongly invested in IoT.■
Many older applications are increasingly accessed via APIs to include them in
modernization and innovation of organizations’ IT. This prepares organizations’
skills and technologies to include the API-centric SaaS capabilities into their
software engineering practices.■
Business application design has become signiﬁcantly partitioned into the back-end
functionality with its APIs and the front-end multiexperience user interface, creating
each side using different tools and design expertise. Some business-oriented
application vendors ﬁnd it convenient to concentrate on the back-end data and
business logic, and leave the ﬁnalized user experience to separate teams, including
the customer’s own developers.■
G00768727 Page 45 of 129Obstacles
API-centric SaaS is a relatively new phenomenon. Both SaaS vendors and business
developers can lack the required skills and tools. Offered APIs may be hard to use,
and the adopted APIs may be hard to manage.■
The best procurement practices of API-centric SaaS are not well-developed, delaying
adoption or increasing its costs.■
Using multisourced API-centric components for assembling new application
processes and experiences requires some integration work that is not supported in
low-code development tools. This conﬁnes many use cases to advanced software
engineering skills and delays adoption of API-centric SaaS by mainstream
organizations.■
Reduced or absent user interfaces packaged with an API-centric SaaS assume and
require that the customer implement their own differentiated application and user
experience. What is a welcome opportunity for innovation for some can be a burden
to others, delaying adoption of API-centric SaaS.■
G00768727 Page 46 of 129User Recommendations
Sample Vendors
Algolia; Alloy; Clearbit; Cloudinary; Lob; MessageBird; Plaid; Strapi; Stripe; Twilio
Gartner Recommended Reading
Accelerate Digital Transformation With an API-Centric (Headless) Architecture for
Enterprise Applications
How to Successfully Implement API-First Integration
Banking Product Leader Insight: Think Beyond APIs to Address Composability
Tool: API Product Manager Job Description
Kick-Start Your Composable Business Journey With 2 Key StrategiesBuild the tools and skills of API management that recognize the added requirements
to govern access to imported third-party APIs.■
Give preference to SaaS offerings that expose more of their business functionality
as APIs and/or event streams.■
Plan for the gradual shift of development to composition and integration of API-
centric business functionality components.■
Ensure clean API-based separation of the back-end business logic and the front-end
user experience in all applications, to maximize future beneﬁts of adopted API-
centric SaaS capabilities.■
Avoid vendor applications that lock your organization into their user experience
implementations.■
Give preference to application platform offerings that are well-equipped for access
to external APIs and event sources.■
Practice use and governance of APIs in preparation for greater adoption of API-
centric SaaS.■
Watch for opportunities to experiment by offering some of your business
functionality packaged as priced API products.■
G00768727 Page 47 of 129Composable Applications
Analysis By: Yeﬁm Natis
Benefit Rating: Transformational
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Composable applications are built, in part or in whole, as ﬂexible assemblies
(compositions) of well-encapsulated (packaged) modules of business application
capabilities that may originate from one or multiple sources. The role of the “composers”
is typically performed by members of business-IT fusion teams, while the creators of the
packaged building blocks may be application vendors or central IT software engineering
teams.
Why This Is Important
Composable applications are designed to support fast-paced business change while
protecting the integrity of the outcomes. They use coarse-grained business-centric
software modularity to better match digital application capabilities to business
operations. Organizations that use composable applications can achieve faster change.
Composable applications help support resilience, adaptability and growth of business in
the context of increasingly frequent challenges, disruptions and opportunities.
Business Impact
The more composable the organization’s application portfolio is, the better the
organization is prepared to support changing business requirements through digital
innovation. In return, greater conﬁdence in the agility of applications promotes faster
business thinking. The improved agility of business technology strengthens the ability of
an organization to maintain and grow their business, a high value in the modern context
of frequent disruptions and opportunities.
G00768727 Page 48 of 129Drivers
In the continuously changing business context, demand for business adaptability
directs organizations toward technology architecture that supports fast, safe and
efﬁcient application change.■
The demand for active participation of business decision makers in the design of
their digital experiences promotes adoption of technology models that are
accessible and useful to business experts in addition to, and in cooperation with,
technical professionals.■
The increasing number of vendors offering API-centric SaaS (also known as API
products or “headless” SaaS) builds up a portfolio of available business-centric
packaged application components — the building blocks of composable business
applications.■
Increasing mainstream use of low-code application, integration and automation
platforms supports composition of applications using APIs and other forms of
encapsulated business software, preparing organizations for composable
application engineering.■
Fast-growing competence in mainstream organizations for management of broad
collections of APIs and event streams creates a technology foundation for safe
operation of a composable business technology environment.■
G00768727 Page 49 of 129Obstacles
Limited experience of composable thinking and planning in most software
engineering organizations complicates composable design efforts and transition
plans.■
Limited practice of business-IT collaboration for application design delays the
effective composable design that depends on the complementary expert talents in
multidisciplinary fusion teams.■
Most legacy applications can participate in composition via their APIs and/or event
streams, but their architecture provides only minimal autonomy, delaying the full
positive effect of composable architecture.■
Lack of componentry, development and platform tools dedicated to composable
application architecture limits the early success to advanced design teams capable
of adapting precursor technologies to new objectives.■
Insufﬁcient mapping of architectural thinking and models between business and
technology planners makes digital representation of business functionality less
prepared to track the real-world business change.■
G00768727 Page 50 of 129User Recommendations
Sample Vendors
Alloy; Boost; Contentful; Elasticpath; Shippo; Stripe
Gartner Recommended Reading
Becoming Composable: A Gartner Trend Insight Report
Predicts 2022: Composable Applications Accelerate Digital Business
Use Gartner’s Reference Model to Deliver Intelligent Composable Business Applications
How to Design Enterprise Applications That Are Composable by Default
Intercloud Data Management
Analysis By: Adam Ronthal, Donald Feinberg
Benefit Rating: HighPromote composable thinking throughout the organization. All delivered application
designs must be ready to participate in the changing business context by providing a
collection of published business-centric APIs. More advanced designs would
optimize business granularity, autonomy, orchestration and independent discovery
of the application components.■
Build competence in API and event stream management as the precursor to
managing composable application building blocks.■
Use low-code technologies to facilitate design collaboration of business and
technology experts.■
Prioritize formation of business-IT fusion teams to support faster and more effective
adaptive change of business applications.■
Build an investment case for composability by highlighting how aging digital assets
endanger the future success of the business by forming barriers to keeping up with
the pace of market change.■
Gradually modernize (or replace) existing applications toward architecture of
business-centric modularity.■
G00768727 Page 51 of 129Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Intercloud data management is the process of actively managing data in multiple cloud
providers as part of a cohesive application and data management strategy. It builds on
the foundation of multicloud capabilities, but adds the ability to access and use data
across clouds in an operational context. It can be done at the cloud object store, DBMS or
application tiers.
Why This Is Important
The vast majority of organizations using the public cloud are storing data on more than
one cloud. Today, most of that data remains siloed — accessed and managed in the
context of a single cloud environment. As data’s center of gravity shifts to the cloud (or
more likely to multiple clouds), data and analytics leaders will seek out means to unite
that data in a logical and cohesive consumption tier.
Business Impact
The ability to access data — regardless of where it is located — is a potentially
transformational capability that will serve to break down barriers to access for end users
and applications. Intercloud data management is cloud-agnostic and will allow
enterprises to access their data in any cloud at any time, and by any means. It will enable
globally distributed applications that span cloud providers and geographies, providing
resilience and avoiding lock-in to any single cloud service provider.
Drivers
Enterprises seeking to adopt intercloud data management approaches tend to be large,
global enterprises with speciﬁc application requirements; most small to midsize
organizations do not have these requirements. Though actual market penetration remains
minimal, several drivers affect adoption, including:
Regulatory requirements — Some industries are starting to mandate the use of
multiple clouds for resilience and availability reasons, and some countries have
strict data sovereignty laws.■
Global applications — Applications that operate on a global basis may require the
use of multiple clouds to meet latency and performance requirements.■
G00768727 Page 52 of 129Obstacles
User Recommendations
Weigh trade-offs in optimization and ﬂexibility when making design decisions. Take into
account the three primary deployment options for intercloud data management:Distributed teams — Organizations using more than one cloud may need to work
with data in more than one cloud and provide continuity in their multicloud
environments.■
Integration — Distributed data must be integrated (in storage and/or logically) to
achieve maximum business value.■
Intercloud data management typically involves investment in highly specialized
technologies from relatively niche vendors. This must be weighed against existing
strategic relationships with large cloud service providers, which generally do not
support intercloud capabilities broadly at this time. Because the supporting vendors
tend to be specialized, most applications will need to be custom-built and written
with speciﬁc intercloud vendor capabilities in mind. This adds risk because
switching may come with a high cost and technical difﬁculty.■
Any intercloud data management application will also be subject to the laws of
physics. Practitioners will need to be aware of both performance implications and
trade-offs in consistency and availability, and ensure that their applications are both
aware of and designed with these trade-offs in mind.■
As with any data, intercloud data must be governed and integrated in storage and/or
logically to achieve maximum business value.■
Object store — If data is distributed at the cloud object store (COS) layer, there is not
a single service — native or third-party — that cannot read and write from the local
COS. This enables diversity of choice in selecting a data management offering for
the last-mile delivery.■
DBMS — Also called “distributed SQL.” The database management system
distributes and manages data in a geodistributed cluster. Nodes can reside in
multiple clouds and/or on-premises. This approach can support local, low-latency
read/write apps with global read capabilities, and data sovereignty enforcements.■
G00768727 Page 53 of 129Sample Vendors
Aerospike; Cockroach Labs; DataStax; Denodo; HPE; MariaDB; MongoDB; Snowﬂake;
WANdisco; Yugabyte
Gartner Recommended Reading
How to Plan for Optimal Multicloud and Intercloud Data Management
What Are the Key Factors to Consider When Choosing a Cloud Data Management
Architecture?
6 Best Practices to Create a Cloud Management Services Offering in the World of
Multicloud and Hybrid Cloud
Cloud-Out and Edge-In: How Cloud Service Providers Can Leverage the Two Edge
Computing Architectures
Distributed Cloud: Does the Hype Live Up to Reality?
Cloud-Native
Analysis By: David Smith, Michael Warrilow
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Cloud-native refers to something created to optimally leverage or implement cloud
characteristics. Those cloud characteristics are part of the original deﬁnition of cloud
computing, and include capabilities delivered as a service. Cloud computing
characteristics also include scalable and elastic, shared, metered by use, service-based,
and ubiquitous by means of internet technologies.Application — Think of this as an after-the-fact solution. Data may already be in
multiple clouds and require a means of bringing it together. This approach
essentially defers data integration to the point of consumption.■
G00768727 Page 54 of 129Why This Is Important
Cloud-native is a popular term. Depending on its meaning, it can be described as taking
full advantage of the cloud capabilities of a cloud provider, or by using approaches
pioneered in the cloud to deliver beneﬁts wherever needed, via speciﬁc technologies such
as containers.
Business Impact
Cloud-native is a popular, hyped concept that aspires to attain and maximize the beneﬁts
of cloud computing; however, the realization of those beneﬁts varies. For example, if a
traditional, noncloud application is migrated to the cloud with a lift-and-shift approach,
the application is unlikely to fully leverage cloud characteristics and deliver the maximum
beneﬁts. An application rewritten to take advantage of cloud capabilities is more likely to
deliver the expected cloud outcomes.
Drivers
The primary driver for cloud-native is the desire to “get the most out of the cloud.”
The cloud itself means different things to different constituencies, so it’s not
surprising that cloud-native means different things. What drives people to one or
another of these approaches varies.■
Cloud-native can optimally leverage cloud technologies and beneﬁts. The two most
common meanings in use are contradictory. CSP-native is all about using native
features and, therefore, locking yourself into a provider. Container-native focuses on
containers, and may evolve into other technologies. This doesn’t guarantee
portability, but is directionally consistent with the goal.■
There are multiple aspects to cloud-native, ranging from design to architectural to
operational practices. Examples include LIFESPAR and 12-Factor Apps (i.e., cloud-
native application design) and DevOps (cloud-native operations).■
Cloud-native is a concept that can be expressed in degrees. The more something
aligns with core cloud characteristics, the more we consider it to be cloud-native and
the more cloud-native outcomes the thing will produce.■
Cloud-native can be viewed on a continuum. It’s not a question of whether
something is cloud-native or not; it’s the degree to which it is. The more it aligns with
cloud characteristics, the more cloud-native it is.■
Organizations should make informed decisions regarding the extent to which they
invest in cloud-native for traditional workloads and processes. The investment
required to make something “cloud-native” will not always be justiﬁed.■
G00768727 Page 55 of 129Obstacles
User Recommendations
Gartner Recommended Reading
The Cloud Strategy Cookbook, 2021Cloud-native is confusing due to its many interpretations. It’s especially challenging
with respect to hype, because confusion ampliﬁes hype. The biggest obstacle is
getting beyond the confusion to focus on desired outcomes.■
It is essential to be realistic about the portability that can be achieved and the cost.
Otherwise, these features may not be used “with your eyes open,” and you may not
be aware you are doing so.■
In cloud strategy efforts, principles are the most important component. Cloud-native
and multicloud are often stated as principles in a cloud strategy. These principles
can contradict each other, and require further explanation.■
Use of the term “cloud-native” requires clariﬁcation of which meaning is being used.
This is a function of the hype surrounding cloud-native. Being clear about goals is
key to optimally leveraging cloud-native.■
Assuming that containerizing an application will inherently make it cloud-native is
an obstacle. We call this “container-native.”■
Focus on the outcomes you want from using the cloud, rather than focusing purely
on the deﬁnition of cloud-native. The more your use cases align with core cloud
characteristics, the more likely you are to realize the beneﬁts of using the cloud.■
Assess vendor claims about their cloud-native capabilities with skepticism. Vendors
use the term “cloud-native” to promote their offerings, regardless of how cloud-native
their offerings are.■
Ensure that the supporting tools, processes and operations support cloud
characteristics when building or acquiring cloud-native applications or services. The
value of cloud-native applications can be subverted when the approaches of the
supporting elements are not cloud-native.■
Embrace services designed to bring you closer to cloud-native outcomes. These can
include containers, microservices architecture, serverless design, functions and
many platform as a service (PaaS) services. However, using these technologies
should be a means, not a goal.■
G00768727 Page 56 of 129Deﬁne and Understand New Cloud Terms to Succeed in the New Cloud Era
Infographic: Cloud Native and Multicloud — Buzzwords or Key Principles in Your Cloud
Strategy
The CTO’s Guide to Cloud-Native: Answering the Top 10 FAQs
Multicloud Network Software
Analysis By: Andrew Lerner
Benefit Rating: Moderate
Market Penetration: 1% to 5% of target audience
Maturity: Adolescent
Definition:
Multicloud networking software (MCNS) enables the design, deployment and operation of
a network within multiple public cloud environments. MCNS enables uniﬁed networking
policy, network security and network visibility across multiple cloud environments. These
products address trafﬁc routing, secure ingress/egress and integrate with public cloud
services. MCNS can be self-managed and/or delivered as a service, and is accessible via
APIs and UIs.
Why This Is Important
MCNS enhances networking within public cloud environments. Enhancements include
consistent management and visibility across multiple cloud environments and/or feature
depth within public cloud environments, beyond what the cloud provider offers natively.
MCNS can be extended to data center and edge locations as well. The market is small but
growing from both a revenue and customer perspective.
Business Impact
Enterprises are heavily investing in public cloud services to modernize and digitally
transform. Public cloud providers’ native networking capabilities are “good enough” for
many use cases, but have some notable gaps. MCNS directly addresses these gaps.
G00768727 Page 57 of 129Drivers
ObstaclesOrganizations that desire consistent networking features and management within
and across multiple cloud environments that may otherwise have disparate
capabilities and management platforms.■
Simpliﬁes and maintains Day 2 operations within public cloud environments. ■
Some enterprises need enhanced functionality, scalability and simplicity compared
to what a single public cloud provider can offer. Clients have used multicloud
networking products within a single-cloud provider’s environment when speciﬁc
cloud-native functionality was inadequate.■
Speciﬁc shortcomings of native-cloud-provider network capabilities that enterprises
cite when using MCNS include administrative overhead, bandwidth limitations,
limited “full-stack” networking capabilities, limited advanced routing features and
inadequate troubleshooting/visibility.■
Most enterprises start with their cloud providers’ native stack, which is often good
enough for initial use cases, and then becomes very “sticky.”■
MCNS is mostly viewed as a “nice to have” and not a “must have” as there are
alternatives that are good enough for most use cases.■
Time required to certify, integrate and support delays multicloud networking support
for new features from public cloud providers.■
MCNS vendors add purchasing complexity and operational expense (via additional
software, conﬁgurations and installations to manage).■
Increased risk of vendor lock-in at the networking orchestration layer because
enterprises rely on the common networking stack provided by the multicloud
networking vendor.■
There are lesser-known vendors in the market and/or vendors with small install-
bases, which prevents, deters or delays investment.■
G00768727 Page 58 of 129User Recommendations
Sample Vendors
Alkira; Aviatrix; Cohesive Networks; F5; Prosimo
Gartner Recommended Reading
Market Guide for Multicloud Networking Software
Cloud Portability
Analysis By: Lydia Leong, David Smith
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: EmergingPrefer the native capabilities of the cloud providers when starting out, and avoid
forklifting data center networking designs and vendors in the public cloud.■
Invest in MCNS when networking feature depth or operational network consistency
across clouds is critical to the business.■
Invest in MCNS when a consistent set of broader “full-stack” L3-7 networking and
network security capabilities (i.e., routing, DNS, CDN, WAF, Firewall, observability,
etc.) are desired via a single management platform, across multiple providers.■
Prefer MCNS offerings that are API-ﬁrst and “cloud-ﬂuent” (meaning they
dynamically interact with their surrounding environment, leveraging the cloud
platform API).■
Prefer MCNS vendors that offer consumption-based pricing, particularly if your
needs are unpredictable or highly dynamic.■
Prefer MCNS offerings that offer either strong security features (e.g., segmentation,
encryption, ﬁrewall) and/or provide turnkey integration with your selected security
vendors.■
G00768727 Page 59 of 129Definition:
Cloud portability is the ability for a customer to move a cloud-based application or
workload from one cloud provider to another.
Why This Is Important
The ability to migrate between cloud providers (whether IaaS, PaaS or SaaS) is important
for reducing both vendor lock-in and the impact of vendor concentration risk. Cloud
portability offers customers more freedom and ﬂexibility to alter workload placement
based on evolving business or technical needs. It is sometimes perceived, especially when
coupled with the myth of container portability, as offering a negotiation advantage when
contracting with cloud providers.
Business Impact
When a cloud-based application is not portable, the customer is dependent on a single
cloud provider for that application. This exposes the customer to a range of vendor-related
risks and potential concerns about the provider’s ﬁnancial viability, service outages,
alignment of the provider to the customer’s long-term needs and negotiation leverage.
However, achieving cloud portability decreases agility, slows cloud implementations,
increases complexity and increases costs.
G00768727 Page 60 of 129Drivers
Flexibility: Organizations seek cloud portability to be able to potentially replace one
provider with another provider (reducing “lock-in”) or, less commonly, to create the
possibility of repatriating workloads on-premises. However, lock-in is not caused
solely by differentiated capabilities or proprietary API dependencies. Processes,
tools, data gravity, the cloud provider’s ecosystem, employee skills and contractual
obligations all prevent customers from easily and economically switching between
providers.■
“Cloud drift” not “cloud exit”: Organizations are increasingly shifting away from
focusing on portability that would enable an immediate “disaster recovery” cloud
exit. Rather, they have begun to focus on portability that would allow a gradual exit
from a cloud provider over a period of two years or more. This allows the “drift” of
workloads from one cloud to another cloud, based on the natural life cycle of each
application. This is better aligned to shifts in the organization’s business needs and
vendor preferences over time.■
Scenario-based plans: Organizations are building contingency plans that envision
speciﬁc scenarios and risks that would result in a desire to switch cloud providers.
Scenario-based cloud exit planning allows addressing cloud risks in a more speciﬁc
fashion. Organizations may also be regulatorily required to address cloud provider
concentration risks through this sort of planning. While portability needs to be a
consideration from the beginning of cloud application architecture and development,
shorter exit time frames increase architectural restrictions and portability complexity.■
Container myths: Cloud portability is aggressively hyped by vendors, especially in
the context of containers and Kubernetes. However, containers provide limited
portability beneﬁts and do not address most of the underlying causes of cloud
provider lock-in. Management tools that claim to commoditize the underlying cloud
services generally reduce cloud beneﬁts, add unnecessary cost — and create a
different, often riskier, point of lock-in.■
G00768727 Page 61 of 129Obstacles
User RecommendationsSaaS applications and platforms are normally entirely nonportable. An exit requires
completely replacing the application.■
Open source or COTS-based PaaS may be replaced by running that software directly.
Proprietary PaaS is nonportable and must be replaced by an alternative solution.■
The portability of cloud IaaS workloads may be limited by provider differentiation in
infrastructure capabilities. Furthermore, the customer must replace the service’s
security, management and automation capabilities, including revising DevOps and
CI/CD toolchains.■
Portability is a program requiring ongoing effort and investment. In addition to the
direct technology impacts, customers must consider the impact of cloud provider
replacement on the organization’s internal skills, its contractual relationships
(including relationships with ISVs whose software they license to run on IaaS) and
dependencies upon vendors in the cloud provider’s ecosystem.■
Address SaaS vendor risks through contractual means and integration strategies.
SaaS portability is impractical.■
Treat the need for cloud IaaS and PaaS portability as an application portability
challenge, not as an infrastructure lock-in problem.■
Balance portability costs and drawbacks against its beneﬁts. Require a business
case for larger investments in portability.■
Decide how much portability a particular application needs based on business
requirements, and then make architectural choices for that application that reﬂect
the portability requirements.■
Focus on scenario-based exit planning and take a risk management approach to
cloud portability.■
Find the balance between desired short-term and long-term business outcomes, and
the potential future risks of cloud provider dependence. Cloud portability requires
compromises between risk reduction and the negative impacts of increased
complexity, cost and time to deliver cloud solutions.■
G00768727 Page 62 of 129Gartner Recommended Reading
Infographic: Mitigate Cloud Risks With Realistic Exit Planning
How to Create a Public Cloud Integrated IaaS and PaaS Exit Plan
A Guidance Framework for Managing Vendor Lock-In Risks in Cloud IaaS
Assessing Kubernetes for Hybrid and Multicloud Application Portability
A Guidance Framework for Architecting Portable Cloud and Multicloud Applications
Edge Computing
Analysis By: Bob Gill, Philip Dawson
Benefit Rating: Transformational
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Edge computing describes a distributed computing topology in which data storage and
processing are placed in an optimal location relative to the location of data creation and
use. Edge computing locates data and workloads to optimize for latency, bandwidth,
autonomy and regulatory/security considerations. Edge-computing locations extend
along a continuum between the absolute edge, where physical sensors and digital
systems converge, to the “core,” usually the cloud or a centralized data center.
Why This Is Important
Edge computing has quickly become the decentralized complement to the largely
centralized implementation of hyperscale public cloud. Edge computing solves many
pressing issues, such as unacceptable latency and bandwidth requirements, given the
massive increase in edge-located data. The edge-computing topology enables the
speciﬁcs of Internet of Things (IoT), digital business and managed distributed IT
solutions, serving as a foundational element for next-generation applications.
G00768727 Page 63 of 129Business Impact
Edge computing improves efﬁciency and cost control through processing close to the
edge, where the data is generated or acted upon (e.g., better automation and quality
control), and offers more business opportunities and growth (e.g., customer experience
and new real-time business interactions). Early implementations have succeeded at
enterprises that rely on operational technology (OT) systems and data outside core IT,
such as the retail and industrial sectors.
Drivers
Growth of hyperscale cloud adoption has exposed the disadvantages of extreme
centralization. Latency, bandwidth requirements, the need for autonomy and data
sovereignty or location requirements may be optimized by placing workloads and
data closer to the edge, rather than centralizing in a hyperscale data center.■
Data growth from interactive applications and systems often cannot be
economically funneled into the cloud.■
Applications supporting customer engagement and analysis favor local processing
for speed and autonomy.■
IoT use cases are expanding from the industrial sector to other verticals, driving a
move toward a hierarchical and distributed model.■
Edge computing’s inherent decoupling of application front ends and back ends
provides a perfect means of fostering innovation and enhanced ways to do
business. For example, using technologies such as machine learning and industrial
sensors to perform new tasks at locations where business and operational events
take place, or at the point of interaction with a retail customer, can drive signiﬁcant
business value.■
G00768727 Page 64 of 129Obstacles
User Recommendations
IT leaders responsible for cloud and edge infrastructure should:The diversity of devices, software controls and application types all amplify
complexity issues.■
Widespread edge topology and explicit application and networking architectures for
edge computing are not yet common outside vertical applications, such as retail and
manufacturing.■
Edge success in industrial IoT applications and enhancing customer experience in
retail are well understood, but many enterprises still have difﬁculty understanding
the beneﬁts, use cases and ROI of edge computing.■
A lack of broadly accepted standards slows development and deployment time,
creating lock-in concern for many enterprise users.■
Edge physical infrastructure is mature, but distributed application management and
orchestration challenges are still beyond most vendor-supplied component
management offerings. The tasks of securing, maintaining and updating the
physical infrastructure, software and data require improvement before management
and orchestration can mature.■
Create and follow an enterprise edge strategy by focusing ﬁrst on business beneﬁt
and holistic systems, not simply focusing on technical solutions or products.■
Establish a modular, extensible edge architecture through the use of emerging edge
frameworks and design sets. This will enable the mixing and matching of
technologies based on enterprise direction, not simply “what comes with the vendor
solution.”■
Accelerate time to beneﬁt and de-risk technical decisions through the use of
vertically aligned systems integrators and independent software vendors that
demonstrate an understanding of, and ability to, implement and manage the full
orchestration stack from top to bottom.■
Evaluate “edge-as-a-service” deployment options, which deliver business-outcome-
based solutions that adhere to speciﬁc SLAs while shifting deployment, complexity
and obsolescence risk to the provider.■
G00768727 Page 65 of 129Gartner Recommended Reading
Cool Vendors in Edge Computing, 2021
G00768727 Page 66 of 129Sliding into the Trough
Site Reliability Engineering
Analysis By: George Spafford, Daniel Betts
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Site reliability engineering (SRE) is a collection of systems and software engineering
principles used to design and operate scalable resilient systems. Site reliability engineers
work with the customer or product owner to understand operational requirements and
deﬁne service-level objectives (SLOs). Site reliability engineers work with product or
platform teams to design and continuously improve systems that meet deﬁned SLOs.
Why This Is Important
SRE emphasizes the engineering disciplines that lead to resilience; but individual
organizations implement SRE in widely varying ways such as a deﬁned role or a set of
practices. SRE teams can serve as an operations function, and nearly all such teams have
a strong emphasis on blameless root cause analysis. This is to decrease the probability
and/or impact of future events and to enable organizational learning, continual
improvement and reductions in unplanned work.
Business Impact
The SRE approach to improving reliability is intended for products and platforms that
need to deliver customer value at speed at scale while managing risk. The two primary
use cases are to improve the reliability of existing products/platforms or to create new
products or platforms that need reliability from the start.
Drivers
Clients are under pressure to meet customer requirements for reliability while scaling
their digital services and are looking for guidance to help them.■
G00768727 Page 67 of 129Obstacles
User RecommendationsWhile Google originated what became known as SRE and continued to evolve it,
practitioners are developing and sharing new practices as well. Potential
practitioners looking for pragmatic guidance to improve the reliability of their
systems have a rich body of knowledge they can leverage that works well with agile
and DevOps.■
Organizations are adopting highly skilled automation practices (usually DevOps),
and usage of infrastructure-as-code capabilities (which usually requires a cloud
platform) to deliver digital business products reliably.■
The most common use-case based on inquiry calls with clients is to leverage SRE
concepts to improve the reliability of existing systems that are not meeting customer
requirements for availability, performance or are proving difﬁcult to scale.■
Insufﬁcient internal marketing to understand what agile, DevOps or Product teams
need or would value and then explaining how the value SRE can deliver will justify
the costs and risks incurred. Without marketing its beneﬁts, SRE adoption tends to
be less certain or slower. “SRE” the concept by itself is insufﬁcient — people must
continuously believe it is worthwhile.■
Finding SRE candidates who have the right mix of development, operations and
people skills is a big challenge for clients. Impacts on initial adoption and scaling
efforts as well.■
Clients have voiced problems with product owners who overly focus on functional
requirements and not nonfunctional requirements thus slowing support of SRE
within the organization. This impacts both the ability to start and then scale as well
because those people performing a SRE role on a team will have limited
opportunities to improve reliability and without sufﬁcient value being demonstrated,
then others will be reluctant to adopt SRE.■
Leverage practices pragmatically based on need. Don’t feel that you must implement
SRE exactly the way Google does it, learn what works for you.■
Detect an opportunity to begin that is politically friendly, will demonstrate sufﬁcient
value and has an acceptable risk proﬁle.■
G00768727 Page 68 of 129Sample Vendors
Atlassian; Blameless; Datadog; Dynatrace; New Relic; OpsRamp; PagerDuty; Splunk
Cloud-Optimized Hardware
Analysis By: Alan Priestley
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Adolescent
Definition:
Cloud-optimized hardware includes servers, networking, storage and custom silicon,
designed speciﬁcally for use in cloud operating environments. Optimizations include
conﬁguration via software control planes and tuning of hardware functionality for speciﬁc
software workloads. Cloud-optimized hardware is developed primarily by hyperscale cloud
providers to support execution of cloud services, but is increasingly becoming available to
enterprise IT for use in their infrastructure.Start small, focus, learn, improve, and demonstrate value — do not try to change
everything at once.■
Work with the customer or product owner to deﬁne clear, obtainable SLOs based on
their needs.■
Implement monitoring and improve observability to objectively report on actual
performance relative to the SLOs.■
Product owners must be accountable for functional and non-functional requirements
of their products.■
Instill collaborative working between site reliability engineers, developers and other
stakeholders to help them learn how to design, build and evolve their products to
meet SLOs.■
Create a community, implement effective organizational learning practices and
evolve SRE practices.■
G00768727 Page 69 of 129Why This Is Important
Efﬁcient delivery of cloud-based services at scale requires optimization of the data center
infrastructure to ensure ease of deployment and operation. Power, cost and consumption
also have a material impact on operational costs. Cloud service providers are working
with equipment supply chains to source custom hardware optimized for deployment
within their speciﬁc infrastructure. This includes servers, networking and storage
equipment and, most recently, optimized chip designs.
Business Impact
Cloud-optimized hardware can enable enterprise architects planning large-scale
infrastructure deployments:
Deliver efﬁciencies not possible when using standard OEM equipment. ■
Lower the cost of data center operations. ■
Deploy infrastructure targeted to speciﬁc workloads. ■
G00768727 Page 70 of 129Drivers
ObstaclesHyperscale cloud service providers’ requirements for increased operational efﬁciency
and lower component costs in their data center infrastructure.■
Initiatives like OCP Foundation, encouraging original design manufacturers (ODMs)
— such as Inventec and Wistron — to produce cloud-optimized servers for a wider
range of hosting providers, value-added colocation providers, telecom providers,
SaaS independent software vendors (ISVs) and large enterprises.■
Major semiconductor vendors offering custom versions of their standard chips,
tuned for use in cloud infrastructure.■
Development of custom microprocessors and application-speciﬁc integrated circuits
(ASICs) for workload acceleration. For example, AWS Graviton processors and
Google’s Tensor Processing Units (TPUs) for AI workloads.■
Requirements for network switches and interfaces that support and isolate multiple
workloads and users on servers hosting multiple virtual instances.■
Function accelerators that support ofﬂoad of storage and network processing from
the CPU.■
The use of custom chips to establish a hardware-based root of trust to guarantee the
integrity of the hardware and ﬁrmware. For example, Amazon’s Nitro-based services.■
While it is possible for smaller organizations to work with ODMs to access this
hardware, ODMs have a very different business model, often having minimum
delivery quantities of tens of thousands of standard units.■
Although it is possible to specify precisely what the system conﬁgurations will be
(CPU, memory, storage and network), there is often no ﬂexibility to vary
speciﬁcations.■
ODMs provide minimal support in terms of software conﬁgurations, long-term
maintenance contracts or postsales support.■
For many smaller-scale deployments, the cost savings from buying cloud-optimized
hardware can be offset by the increase in resources and skill sets necessary to
maintain the equipment and associated software stacks.■
G00768727 Page 71 of 129User Recommendations
Sample Vendors
Amazon Web Services (AWS); Flex; Inspur; Open Compute Project (OCP) Foundation
Gartner Recommended Reading
Top Strategic Technology Trends for 2021: Distributed Cloud
Market Trends: Emerging Opportunities for Semiconductor Vendors at the Hyperscale
Cloud Service Providers
Cloud and Edge Infrastructure Primer for 2022
Market Trends: Arm in the Data Center: Act Now to Develop Plans to Address This Shifting
Market
Container Management
Analysis By: Dennis Smith, Michael Warrilow
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstreamBe aware that cloud-optimized hardware is designed for deployment at scale within
hyperscale data centers.■
Acknowledge that customized hardware solutions often use “nonstandard”
components, impacting software conﬁgurations and workload optimizations.■
Be prepared to manage an ODM-based supply chain, which is very different from
managing traditional vendors and often lacks postsales and long-term maintenance
support.■
Track the use of cloud-optimized hardware by cloud service providers. For workloads
based on interpreted languages (such as Java or Python, where an Arm-based
interpreter is available), evaluate the use of cloud instances based on proprietary
CPU designs, such as the Arm-based AWS Graviton processor.■
G00768727 Page 72 of 129Definition:
Container management tools automate provisioning, starting/stopping and maintaining
of runtime images for containers and dependent resources via centralized governance
and security policies. They include a management interface, registration of container
images and integration with container management brokers.
Why This Is Important
Container runtimes simplify use of container functionality and enable integration with
DevOps tooling and workﬂows. Productivity and/or agility beneﬁts of containers include
accelerating and simplifying the application life cycle, enabling workload portability
between different environments and improving resource utilization. Container
management makes it easier to achieve scalability and production readiness for
containerized workloads.
Business Impact
Gartner surveys and client interactions show that demand for containers continues to rise.
This trend is due to application developers’ and DevOps teams’ preference for container
runtimes, which have introduced container packaging formats. Developers have quickly
progressed from leveraging containers on their desktops to needing environments that
can run and operate containers at scale, introducing the need for container management.
G00768727 Page 73 of 129Drivers
Container runtimes, frameworks and other management software provide
capabilities such as packaging, placement and deployment, and fault tolerance (for
example, clusters of nodes running the application).■
A vibrant open-source ecosystem and competitive vendor market has culminated in
a wide range of container management offerings. Many vendors enable
management capabilities across hybrid cloud or multicloud environments by
providing an abstraction layer across on-premises and public clouds. Container
management software can run on-premises, in public infrastructure as a service
(IaaS) or simultaneously in both.■
Container-related edge computing use cases have increased in industries that need
to get compute and data closer to the activity (for example, telcos, manufacturing
plants, etc.).■
Data analytics use cases have emerged over the past few years, as have operational
control planes that enable the management of container nodes and clusters.■
All major public cloud service providers now offer on-premises container solutions. ■
Independent software vendors (ISVs) are starting to package their software for
container management systems.■
Some enterprises have scaled sophisticated deployments, and many more have
recently begun or are planning container deployments. This trend is expected to
increase as enterprises continue application modernization projects.■
G00768727 Page 74 of 129Obstacles
User Recommendations
Sample Vendors
Amazon Web Services (AWS); Google; IBM; Microsoft; Mirantis; SUSE (Rancher Labs); Red
Hat; VMware
Gartner Recommended Reading
Market Guide for Container Management
Multicloud
Analysis By: David SmithMore abstracted, serverless offerings may enable enterprises to forgo container
management. Among these services are Knative-based services, AWS Lambda and
Fargate, Azure Functions and Google’s Cloud Run. These services embed container
management in a manner that is transparent to the user.■
Third-party container management software faces huge competition in the container
offerings from the public cloud providers, both with public cloud deployments and
the extension of software to on-premises environments. These offerings are also
challenged by ISVs that choose to craft open-source components with their software
during the distribution process.■
Organizations that perform relatively little app development or make limited use of
DevOps principles are served by SaaS, ISV and/or traditional application
development packaging methods.■
Determine if your organization is a good candidate for container management
software adoption by weighing organizational goals of increased software velocity
and immutable infrastructure, and its hybrid cloud requirements, against the effort
required to operate third-party container management software.■
Leverage container management capabilities integrated into cloud IaaS and PaaS
providers’ service offerings by experimenting with process and workﬂow changes
that accommodate the incorporation of containers.■
Avoid using upstream open source (e.g., Kubernetes) directly unless the organization
has adequate in-house expertise to support.■
G00768727 Page 75 of 129Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Multicloud refers to the intentional use of cloud services from multiple public cloud
providers for the same general class of IT solutions or workloads. It is much more
common in infrastructure as a service (IaaS) and converged IaaS/platform as a service
(PaaS) scenarios than SaaS. While multi-SaaS environments are possible, these would
typically be stovepiped situations.
Why This Is Important
Multicloud has the potential to lower the risk of cloud provider lock-in, can provide best-of-
breed capabilities for speciﬁc use cases and can provide service resilience and migration
opportunities, in addition to the core cloud beneﬁts of agility, scalability and elasticity. It
also may be used to obtain public cloud services in different geographic locations for
global companies.
Business Impact
Multicloud provides agility and potential of cost optimization. It can also provide a basis
to lower cloud provider lock-in and increase workload migration opportunities. However,
multicloud can also create additional complexity and, therefore, cost increases. Also,
many organizations end up in a multicloud environment accidentally due to
uncoordinated adoption and poor planning.
G00768727 Page 76 of 129Drivers
Many organizations end up in a multicloud environment unintentionally due to
uncoordinated cloud adoption, poor planning or through acquisitions and mergers.
Unintended multicloud environments can be rationalized into a purposeful
multicloud strategy.■
Enterprises typically start with one provider but, over time, become concerned about
lock-in. Thus, the ﬁrst use of multicloud is often procurement-based to encourage
competition, or as result of a merger/acquisition.■
As multiple cloud providers are in use, the need to manage and govern those
services becomes important.■
Eventually, some enterprises get to multicloud architectures. These rely on
architectural principles and portability solutions, and can potentially enable even
cloudbursting and other dynamic placement efforts. Many deliberate multicloud
strategies are designed to take advantage of differentiated capabilities within the
same general class (e.g., IaaS), from multiple cloud providers while applications run
in a single cloud provider stack. Some applications may have a multicloud
architecture themselves.■
The hype around multicloud is driving adoption, as providers often use this industry
buzz term to justify why their offering should be considered when another cloud
service already exists.■
G00768727 Page 77 of 129Obstacles
User Recommendations
Gartner Recommended Reading
The Cloud Strategy Cookbook, 2021
Technology Insight for Multicloud NetworkingMulticloud is often confused with hybrid cloud. The reality is that multicloud and
hybrid cloud often coexist in a multi-hybrid cloud environment that spans multiple
public cloud providers, as well as between public and private implementations.■
Multicloud is not a practical solution to higher availability/DR/Business Continuity,
as these goals are more effectively achieved in other ways within a provider's
ecosystem.■
Multicloud environments are complex and often result in cost increases. More time,
effort and cost are often required to secure and manage multiple cloud
environments.■
The scope of leveraging more advanced services from each provider is reduced. ■
The need to invest in the right skills to manage and deal with more complex
integration solutions.■
Ensure your multicloud strategy is purposeful and coordinated with your overall
cloud strategy. When embracing multicloud approaches, account for the tools, skills,
processes and other resources to ensure you achieve the right outcomes.■
Establish security, management, governance guidelines and standards to manage
cloud service sprawl and increasing cost, and develop decision criteria to decide
placement of services.■
Focus on coordination and strategy across the enterprise to identify the types of
services needed to deliver the beneﬁts of a cloud environment.■
Be prepared to incur additional expenses on training and skill development across
roles, including engineers and operators.■
Do not just shift vendor lock-in to a cloud management platform (CMP) and/or a
cloud service brokerage (CSB), even though they may enable governance and
optimizations in a multicloud environment.■
G00768727 Page 78 of 129Comparing Cloud Workload Placement Strategies
A Multicloud Strategy Is Complex and Costly, but Improves Flexibility
Market Guide for Multicloud Networking Software
Serverless Infrastructure
Analysis By: Jeffrey Hewitt
Benefit Rating: Transformational
Market Penetration: 20% to 50% of target audience
Maturity: Adolescent
Definition:
Serverless infrastructure is a model of IT service delivery in which the underlying enabling
resources are used as an opaque, unlimited, shared pool that is continuously available
without advance provisioning and priced in the units of the consumed IT service. The
runtime environment (that is, the compute, storage, networking and language execution
environment) required to execute an application or service is automatically provisioned
and operated.
Why This Is Important
Accelerating the development and delivery of software is a core imperative for I&O
leaders. Not only do serverless technologies enable organizations to build and deliver
software faster, but they also entail low operational overheads and an elastic pricing
model. Cloud providers and open-source software (OSS) vendors are all innovating and
making serverless products available for a broad set of use cases.
Business Impact
Serverless technologies enable organizations to build cloud-native applications with
newer application architectures — such as microservices, which can usher higher degrees
of resiliency, elasticity and agility for digital workloads. Serverless technologies also
enable consumption of platform services by developers and business users, with the
infrastructure provisioning and life cycle management abstracted away from the
consumer.
G00768727 Page 79 of 129Drivers
ObstaclesEvolution — In the past few years, “serverless infrastructure” as a term has evolved
to include much more than function as a service (FaaS) products. Currently, it refers
not only to a programming model such as FaaS, but also to an operational model
where all provisioning, scaling, monitoring and conﬁguration of the compute
infrastructure are delegated to the platform. Examples of such architectures include
serverless containers and serverless databases.■
Operational simplicity — serverless infrastructure obviates the need for IT
departments to do the infrastructure setup, conﬁguration, provisioning and
management.■
“Built-in” scalability — Infrastructure scaling is automated and elastic. ■
Cost-efficiency — You only pay for infrastructure resources when they are needed to
support requested transactions.■
Developer productivity and business agility — Serverless infrastructure abstracts
infrastructure architecture and allows developers to focus on writing code and
application design.■
Vendor lock-in — Like most cloud functionality, the leading serverless
implementations are proprietary to a speciﬁc cloud provider. If the application has to
move from one cloud platform to another, then it will have to be signiﬁcantly
reengineered.■
Low degree of control — The managed service model and runtime virtualization of
serverless technologies bestow huge beneﬁts, but at the cost of little to no control of
the service. The environment is a “black box” that must be used as is.■
Skills gap — Serverless operations require a major shift in skills and best practices,
with much more code and API-oriented service delivery.■
G00768727 Page 80 of 129User Recommendations
Gartner Recommended Reading
A CIO’s Guide to Serverless Computing
A CTO’s Guide to Top Practices for Open-Source Software
Compute Evolution: VMs, Containers, Serverless — Which to Use When?
SASE
Analysis By: Neil MacDonald, Andrew Lerner
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Secure access service edge (SASE) delivers converged network and security as a service
capabilities, including SD-WAN, SWG, CASB, NGFW and zero trust network access (ZTNA).
SASE supports branch ofﬁce, remote worker and on-premises secure access use cases.
SASE is primarily delivered as a service and enables zero trust access based on the
identity of the device or entity, combined with real-time context and security and
compliance policies.Ensure cost governance and budget control by evaluating the cost implications of
event-driven application architectures and the pricing models of different vendors.
Consider API gateway, network egress and other costs.■
Revise data classiﬁcation policies and controls to account for the fact that objects in
a content store can now represent code as well as data.■
Rethink IT operations from infrastructure management to application governance,
with an emphasis on ensuring that security, monitoring, debugging requirements and
application SLAs are being met. In those cases where an on-premises deployment is
merited, I&O teams can support FaaS in the role of service provider.■
G00768727 Page 81 of 129Why This Is Important
SASE is a key enabler of modern digital business transformation, including work from
anywhere and the adoption of edge computing and cloud-delivered applications. It
increases visibility, agility, resilience and security. SASE also dramatically simpliﬁes the
delivery and operation of critical network and security services mainly via a cloud-
delivered model. SASE can reduce the number of vendors required for secure access to
one to two over the next several years.
Business Impact
SASE enables:
DriversNew digital business use cases (such as branch ofﬁce transformation and hybrid
workforce enablement) with increased ease of use, while reducing costs and
complexity via vendor consolidation and dedicated circuit ofﬂoad.■
Infrastructure and operations and security teams to deliver a rich set of networking
and network security services in a consistent and integrated manner to support the
needs of digital business transformation, edge computing and work from anywhere.■
SASE is driven by enterprise digital business transformation including the adoption
of cloud-based services by mobile workforces, edge computing and business
continuity plans that must include ﬂexible, anywhere, anytime, secure access, and
use of the internet and cloud services.■
The need to ﬂexibly support digital business transformation efforts with a zero trust
security architecture while managing complexity is a signiﬁcant factor for the
adoption of SASE, primarily delivered as a cloud-based service (see 2021 Strategic
Roadmap for SASE Convergence). The rapid shift to hybrid work models accelerated
these trends.■
For IT, SASE can reduce the deployment time for new users, locations, applications
and devices as well as reduce the attack surface and shorten remediation times.■
Network security models based on data center perimeter security are ill-suited to
address the dynamic needs of a modern digital business and its distributed digital
workforce. This is forcing a transformation of the legacy perimeter into a set of
cloud-based, converged capabilities created when and where an enterprise needs
them — that is, a dynamically created, policy-based SASE.■
G00768727 Page 82 of 129Obstacles
User RecommendationsOrganizational silos, existing investments and skills gaps: A full SASE
implementation requires a coordinated and cohesive approach across security and
networking teams, which is challenging given refresh/renewal cycles, silos, and
existing staff expertise.■
Organizational bias and regulatory requirements for on-premises deployment:
Some customers have an aversion to the cloud and want to maintain control.■
Global coverage: SASE depends upon cloud delivery, and a vendor’s cloud footprint
may prevent deployments in certain geographies, such as China, Africa, South
America and the Middle East.■
SASE maturity: SASE capabilities vary widely. Sensitive-data visibility and control is
often a high-priority capability, but it is difﬁcult for many SASE vendors to address.
While your preferred single vendor may lack the capabilities you require, two-vendor
partnerships can be a viable approach.■
Involve the chief information security ofﬁcer (CISO) and network architect when
evaluating offerings and roadmaps from incumbent and emerging vendors to ensure
an integrated approach.■
Leverage WAN, ﬁrewall, VPN hardware refresh cycles or software-deﬁned WAN (SD-
WAN) deployments to update network and network security architectures.■
Strive for no more than two vendors for all core services to minimize complexity and
improve performance.■
Identify required capabilities for networking and security, including latency,
throughput, geographic presence, and endpoint types to develop evaluation criteria.■
Focus on vendors who invest signiﬁcantly in sensitive data discovery and protection
capabilities for their SASE covering multiple data exﬁltration vectors and serving
verticals with highly advanced requirements for data security.■
Combine branch ofﬁce and remote access in a single implementation to ensure
consistent policies and minimize the number of vendors required.■
Leverage branch ofﬁce transformation and dedicated circuit ofﬂoad projects to
adopt SASE for security services.■
G00768727 Page 83 of 129Sample Vendors
Cato Networks; Cisco; Forcepoint; Fortinet; Juniper; Netskope; Palo Alto Networks; Versa
Networks; VMware; Zscaler
Gartner Recommended Reading
2021 Strategic Roadmap for SASE Convergence
Quick Answer: Does SSE Replace SASE?
The Future of Network Security Is in the Cloud
Magic Quadrant for WAN Edge Infrastructure
Magic Quadrant for Security Service Edge
AI Cloud Services
Analysis By: Van Baker, Bern Elliot
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Artiﬁcial intelligence (AI) cloud services provide AI model building tools, APIs for prebuilt
services and associated middleware that enable the building/training, deployment and
consumption of machine learning (ML) models running on prebuilt infrastructure as cloud
services. These services include vision and language services and automated machine
learning to create new models and customize prebuilt models.
Why This Is Important
The use of AI cloud services continues to increase, with vendors competing to become the
platform of choice for developers and citizen data scientists. Applications will
increasingly use AI cloud services in language, vision and machine learning in
applications to help automate and accelerate achievement of business objectives.
Developers are aware of these offerings and are using both prebuilt and customized ML
models in applications. The adoption of these services is rapidly accelerating.
G00768727 Page 84 of 129Business Impact
The impact of AI will extend to the applications that enable business, allowing developers
and data scientists to enhance the functionality of these applications. With the
incorporation of forecasts, next best actions and other capabilities, including automation
of many workﬂows that are currently handled manually, these AI cloud services will enable
advanced machine-learning-enabled applications that improve business performance.
Drivers
Opportunities to capitalize on new insights. The explosion of data from both
internal and third-party sources enables insight that has previously been unavailable
to the business.■
Support demand for conversational AI. The need for human-to-machine interactions
that are based on conversational capabilities is increasing.■
To meet business key performance indicators (KPIs). There is a mandate for
businesses to automate processes to improve accuracy, improve responsiveness
and reduce costs by deploying both AI and machine learning models.■
Reduced barriers of entry. The ability to do one shot or few shot learning has
reduced need for large quantities of data to train models and accessibility for
developers and citizen data scientists to AI and machine learning services due to the
availability of API callable cloud-hosted services will expand the use of AI.■
Relevance for key use cases. There are multiple use cases that span language,
vision and automated machine learning services.■
AutoML as an enabler for custom development. Use of automated machine learning
to tailor the off-the-shelf services to more precisely address the speciﬁc needs of the
business is increasing.■
A wide range of AI cloud services. AI cloud services from hyperscaler cloud providers
as well as specialized providers in the market including orchestration layers to
streamline deployment of solutions are available.■
Increasing deployments of sensor networks. Sensor networks in IoT-based solutions
that facilitate data use to drive model development and facilitate proactive response
to changes in the data rather than reactive responses are increasingly deployed.■
Emerging AI model marketplaces. New marketplaces should help developers adopt
these techniques through AI cloud services.■
G00768727 Page 85 of 129Obstacles
User Recommendations
Sample Vendors
Amazon Web Services; Clarify; Dataiku; Google; H2O.ai; IBM; MicrosoftLack of understanding by developers and citizen data scientists about these
services and how they can be applied to speciﬁc business use cases.■
Pricing models for AI cloud services that make it challenging for businesses to
determine the costs associated with use of these services.■
Lack of guidance for solutions that utilize multiple services to address speciﬁc use
cases for developers and citizen data scientists including guidance on how to use
automated machine learning to supplement and enhance the standard language
and vision services.■
Minimal marketplaces for prebuilt machine learning models that could be used by
developers and citizen data scientists.■
Serious lack of ModelOps capabilities that contribute to challenges in integration of
AI into applications.■
Choose AI cloud services over building custom models to address a broader range of
use cases and for quicker deployment and built-in scalability.■
Improve the chances of success of your AI strategy by experimenting with different
AI techniques and AI cloud services providers, using the exact same dataset and
then selecting one that best addresses your requirements. Consider using an A/B
testing approach.■
Use AI cloud services to build less complex models, giving the enterprise the beneﬁt
of more productive AI while freeing up your data science assets for higher-priority
projects.■
Empower non-data-science users with automated features. Use features like
automated algorithm selection, dataset cleansing and preparation, and feature
engineering for project elements, and leverage existing expertise on operating cloud
services. This will assist technical professional teams with little to no data science
expertise.■
G00768727 Page 86 of 129Gartner Recommended Reading
Critical Capabilities for Cloud AI Developer Services
Magic Quadrant for Cloud AI Developer Services
Cloud Marketplaces
Analysis By: Ed Anderson
Benefit Rating: Moderate
Market Penetration: More than 50% of target audience
Maturity: Early mainstream
Definition:
Cloud marketplaces are online storefronts through which customers can ﬁnd and
subscribe to cloud service offerings, including IaaS, PaaS and SaaS. Other resources can
also be found and procured through cloud marketplaces such as curated datasets,
machine learning modules, packaged virtual machine images and cloud-related IT
services. Cloud marketplaces may be public or private, targeting B2B or B2C buyers, and
possessing specialized sub-storefronts.
Why This Is Important
Cloud marketplaces are growing in inﬂuence as a destination to ﬁnd and procure cloud
service offerings. Cloud service providers (CSPs), distributors, resellers, managed service
providers and internal IT organizations build and deliver cloud marketplaces to their
constituents. Cloud app stores, cloud service catalogs and portals are also types of cloud
marketplaces, but they are typically built for consumption by a closed community of
users.
Business Impact
As more cloud services are discovered and procured via cloud marketplaces, they become
increasingly important as a technology and architectural control point. Providers motivate
customers to use cloud marketplaces through discounts, additional services, solution
validations and ease of use. CSPs also often apply marketplace spending to customer
spending commitments, thereby unlocking additional volume discounts. Third-party cloud
marketplaces (such as technology distributors) cater to multicloud requirements.
G00768727 Page 87 of 129Drivers
CSPs use cloud marketplaces to highlight their own cloud service offerings as well
as their ISV and IT service partners’ offerings. They also use marketplaces to
strengthen their ecosystem of solutions, which in turn strengthens their respective
cloud platforms and increases cloud service consumption.■
Cloud marketplaces may be dissociated from any speciﬁc cloud platform/service
and offered by independent cloud service resellers or distributors. Many traditional
software and hardware distributors provide cloud marketplaces for their customers.■
Cloud service subscribers use cloud marketplaces to simplify the process of ﬁnding,
purchasing, deploying and using cloud services and ISV content. Cloud
marketplaces can also be used to isolate and deliver only validated or sanctioned
services, which can complement cloud governance policies enforcing the use of only
approved cloud services. Cloud service aggregation features, such as uniﬁed billing,
software license and entitlement management are often included in marketplace
offerings.■
CSPs often provide ﬁnancial incentives, such as volume discounts, for cloud
spending commitments including both cloud services from the CSP as well as
services purchased through the CSP marketplace.■
Third-party cloud marketplaces will be most popular with smaller organizations that
beneﬁt from the value-added capabilities of cloud marketplace providers including IT
services. Larger organizations will also make use of cloud marketplaces associated
with their chosen cloud platform(s).■
Internal cloud marketplaces may be established by organizations with sophisticated
IT capabilities to facilitate use of internal and external cloud services, and to provide
management and governance over the organization’s use of cloud services.■
G00768727 Page 88 of 129Obstacles
User Recommendations
Sample Vendors
Alibaba Cloud; Amazon Web Services (AWS); CDW; Google; IBM; Ingram Micro; Microsoft;
Salesforce
Gartner Recommended Reading
Incorporate Cloud Marketplaces in Your Digital Delivery Strategy
Emerging Technologies: Digital Solutions to Enable Ecosystems and Marketplaces
Create Enterprise Marketplaces to Accelerate Digital BusinessUsing CSP marketplaces may require organizations to support multiple
marketplaces when they have multicloud requirements.■
Cloud marketplaces typically charge marketplace solution vendors a commission on
marketplace sales, which could impact the pricing of marketplace offerings.■
There is no common standard for cloud marketplaces, which results in wildly
different capabilities and interfaces for each marketplace offering. Organizations
will ﬁnd procuring cloud services from multiple marketplaces to be complex and
likely to result in cost inefﬁciencies. Terms related to marketplace purchases may
also vary both within and across marketplaces, resulting in inconsistency in asset
and entitlement management.■
Use cloud marketplaces to ﬁnd solutions compatible with speciﬁc cloud platforms
and validated by the hosting cloud provider.■
Include cloud marketplace spending in your contractual commitments with cloud
providers, which may result in pricing discounts.■
Use non-CSP cloud marketplaces when looking for a multicloud marketplace
solution. These marketplaces are typically offered by technology resellers and
distributors, and often include additional IT services such as consulting.■
Leverage the full capabilities of cloud marketplaces including metering, reporting,
billing, integration, localization and management services. Seek out marketplace
offerings beyond packaged cloud service offerings, such as machine learning (ML)
modules, prepackaged containers and validated datasets.■
G00768727 Page 89 of 129IoT Platform
Analysis By: Alfonso Velosa, Eric Goodness
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
An Internet of Things (IoT) platform enables the connection and capture of data from IoT-
enabled assets to develop, deploy, and manage business solutions that improve
operations such as monitoring remote assets or optimizing maintenance. Capabilities
include device management, integration data management, analytics, application
enablement and management, and security. It may be delivered as edge or on-premises
software, or cloud IoT platform as a service, or a hybrid combination.
Why This Is Important
Enterprises continue adding IoT capabilities to assets and products, to achieve beneﬁts
such as cost optimization, process optimization, improved customer experience, and new
opportunities such as product as a service. The complexity, scale and business value of
these interactions call for specialized technology resources, most often implemented as
an IoT platform.
Business Impact
IoT platforms are required to implement IoT-enabled assets in order to make better
business decisions from the data and information generated by connected products.
Goals include:
Differentiated smart products ■
Cost optimization strategies centered on improved maintenance ■
Process improvement by using assets at their optimal state ■
Opportunities to sell new services and data products or adopt new business models
such as product servitization■
G00768727 Page 90 of 129Drivers
ObstaclesBoth asset-intensive and asset-light industries are increasingly implementing IoT
projects based on a broad range of business objectives.■
IoT platforms help enterprise teams to accelerate time to market and improve the
quality of smart products, while consolidating and structuring the data.■
Enterprises are ﬁnding IoT platforms incorporated in their equipment and operations
to help them lower operating costs, improve processes, avoid unplanned downtime,
and enhance worker safety.■
Technology providers’ marketing continues to drive enterprises to implement IoT
projects. In parallel, their investments in improved ecosystems and channel partners
make it easier for companies implementing IoT-enabled solutions to achieve
business value.■
In parallel, technology providers continue to improve their IoT platform technology,
customer experience and vertical market templates, to ensure they can deliver
business solutions at scale for their customers.■
IoT platforms require extensive customization to achieve business outcomes for
large-scale deployments, driving up cost and schedule.■
Many enterprises approach IoT projects as technology projects, instead of business
projects that use IoT platforms to achieve business outcomes.■
Many enterprises operate in siloed fashions, adopting different IoT platforms for
each use case, limiting their ability to scale and adding complexity.■
Enterprise leaders often underinvest in culture change processes or in training key
employees. This leads employees to underuse or reject the data produced by the IoT
platform, leading the project to underperform against its objectives.■
IoT’s technical implementation and complexity challenges remain barriers to deploy
at large scale at enterprises.■
Technology providers have yet to develop a clear value proposition and sales
strategy that help their customers leverage their platforms on scaled up levels.■
G00768727 Page 91 of 129User Recommendations
Sample Vendors
ABB; Braincube; COPA-DATA; Envision Digital; PTC; ROOT CLOUD; Samsung SDS; Software
AG; Toshiba; XCMG HAHNYUN
Gartner Recommended Reading
Magic Quadrant for Industrial IoT Platforms
Critical Capabilities for Industrial IoT Platforms
Toolkit: 5 Digital Twin/IoT Project Success Drivers
Repatriation
Analysis By: Ed Anderson, Lydia Leong
Benefit Rating: Low
Market Penetration: 5% to 20% of target audience
Maturity: LegacyStart small. Treat initial IoT platform projects as IT and business capability
programs to acquire implementation lessons, identify challenges and opportunities,
and verify alignment to business KPI requirements.■
Develop a scenario analysis for the probability IT will have to assume IoT platform
budget and long-term management.■
Identify the differing enterprise needs for IoT platforms. These include simple
projects with new assets using new protocols versus complex projects in legacy
plants that need to connect a range of assets and protocols. Use these insights to
establish a multiplatform architecture.■
Use a skills gap for IoT platforms to map a path to improve the IT organization’s
capabilities such as integration or digital twin development or security.■
Reuse an existing platform provider for their IoT platform. Key evaluation criteria
include vertical market expertise, proof of value projects, the ability to drive large-
scale deployments and a partner ecosystem.■
G00768727 Page 92 of 129Definition:
In the context of cloud computing, repatriation is the return of an application or data that
has been migrated to a public cloud back to its original on-premises environment.
Repatriation may occur when an application that has been migrated to a public cloud
does not deliver the expected beneﬁts, such as cost savings.
Why This Is Important
Applications deployed in a cloud environment may fail at a technical level or may fail to
deliver desired outcomes such as lower costs, better performance and reliability, enhanced
security, and innovation. One approach to remediate these shortfalls, and which has
experienced some hype, is to repatriate the application back to its original environment.
While rare, repatriation has been hyped to refute the beneﬁts of public cloud or to elevate
the beneﬁts of on-premises environments.
Business Impact
While repatriation can address the shortcomings of a cloud migration, it is typically not
the most efﬁcient approach. Most organizations desire the beneﬁts of cloud, thus
pursuing optimization strategies rather than moving applications back to an on-premises
environment. Also, many cloud migrations are associated with data center divestiture
programs or hardware end-of-life events, meaning the previous environment may no
longer exist, which could result in signiﬁcant expense to reinstantiate.
G00768727 Page 93 of 129Drivers
The concept of repatriation had been signiﬁcantly hyped in recent years, but the
hype is beginning to fade.■
Repatriation has been hyped as a means of redress when public cloud services fail
to deliver on their purported beneﬁts by returning migrated applications back to on-
premises environments.■
Repatriation hype is declining due to the success of the majority of cloud migration,
even when migrations don’t produce the expected results.■
Cloud projects often experience temporary failures and setbacks, a very small
minority of which result in returning applications to their previous on-premises
environment for a period of time. Failures in execution are most common in projects
that are poorly conceived or inadequately planned. In the speciﬁc context of data
center migrations to cloud infrastructure as a service (IaaS), the most critical
problems are related to the migration strategy, rationale or approach. For instance, a
lift-and-shift migration designed to treat the cloud provider as just a virtualization
platform is usually costly and often fails to deliver the expected cloud beneﬁts. In
such cases, a migration pause may occur while the project is rethought and
rescoped.■
Ungoverned consumption can also cause cloud expenses to rise and may lead to a
slowdown or pause in the adoption of cloud IaaS and platform as a service (PaaS).■
True migration reversals are rare. The most commonly cited examples typically have
unique circumstances. While organizations do sometimes rethink their IT direction in
a way that may involve a decrease in future cloud adoption, few organizations are
willing to give up the beneﬁts of the cloud services they already enjoy.■
G00768727 Page 94 of 129Obstacles
User Recommendations
Sample Vendors
Amazon Web Services; Cisco; Dell; Google; Hewlett Packard Enterprise; Microsoft; OracleRepatriation has the potential to create distractions during a time when thoughtful
analysis should be devoted to cloud initiatives. Public cloud environments are not
well-suited for every application or workload type. Prudent organizations employ a
thorough process of planning and due diligence, using a business-driven cloud
strategy, before moving any workload to the cloud to avoid the need for repatriation
in the future.■
The process of repatriation assumes the original environment used to host the
application before the cloud migration still exists. This may not be the case,
particularly when cloud migration events are accompanied by data center
divestitures.■
The process of repatriation assumes that the customer can host the application or
otherwise replicate its functionality, including cloud operating characteristics, on-
premises. This is often not the case. Therefore, most organizations pursue other
remediation strategies rather than repatriation when cloud challenges arise.■
Assess repatriation anecdotes with skepticism, especially when delivered from
entities with a bias toward preserving traditional data center businesses. While
repatriation may be mentioned in industry discussions, actual repatriation events are
rare.■
Establish a thorough cloud strategy, including a process for assessing the viability of
application migration initiatives. Identify the expected beneﬁts of moving an
application to the cloud before the migration begins, and then purposefully pursue
those beneﬁts. Cloud migration results should improve over time and with
experience, thereby reducing potential repatriation considerations.■
Employ best practices in tooling and operations to optimize workloads that have
already migrated to the cloud. Ongoing optimization should be a continual element
of cloud operations. Consider cloud-native application design, architecture and
operations to get the greatest beneﬁts from public cloud.■
G00768727 Page 95 of 129Gartner Recommended Reading
Moving Beyond the Myth of Repatriation: How to Handle Cloud Project Failures
Market Trends: Public Cloud Repatriation Remains the Exception, Not the Rule
The Cloud Strategy Cookbook, 2021
Decision Point for Migrating Your Data Center to Public Cloud IaaS and PaaS
Hybrid Cloud Computing
Analysis By: David Smith, Milind Govekar
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Hybrid cloud computing comprises one or more public and private cloud services that
operate as separate entities, but are integrated. A hybrid cloud computing service is
automated, scalable and elastic. It has self-service interfaces and is delivered as a shared
service using internet technologies. Hybrid cloud computing needs integration between
the internal and external environments at the data, process, management or security
layers.
Why This Is Important
Hybrid cloud theoretically offers enterprises the best of both worlds — the cost
optimization, agility, ﬂexibility, scalability and elasticity beneﬁts of public cloud, in
conjunction with the control, compliance, security and reliability of private cloud. As a
result, virtually all enterprises have a desire to augment internal IT systems with external
cloud services. Note that many organizations start with Hybrid IT, which lessens the
requirement of a true private cloud.
G00768727 Page 96 of 129Business Impact
Hybrid cloud computing enables an enterprise to scale beyond its data centers to take
advantage of the public cloud’s elasticity. It is transformational, because changing
business requirements drive the optimum use of private and/or public cloud resources.
This approach improves the economic model and agility, and sets the stage for new ways
for enterprises to work with suppliers, partners (B2B) and customers (B2C). However,
relying on advanced “cloudbursting” types of approaches to implement limits feasibility.
Drivers
The key driver for hybrid cloud is a desire to evolve data centers to become more
cloud-like and, therefore, have a private cloud that has cost and other characteristics
that are more like a public cloud, while maintaining “in-house” infrastructure for key
privacy, security, data residency or latency needs.■
As more providers deliver hybrid cloud offerings, they increasingly deliver a
packaging of the concept. “Packaged hybrid” (a ﬂavor of distributed cloud) means
that you have a vendor-provided private cloud offering that is packaged and
connected to a public cloud in a tethered way. Azure Stack HCI from Microsoft is a
good example of this packaging, but there is another approach as well. We call these
two main approaches “like-for-like” hybrid and “layered technology” hybrid (spanning
different technology bases). Packaged hybrid cloud is a key component of the
distributed cloud concept.■
The solutions that hybrid cloud provides include service integration,
availability/disaster recovery, cross-service security, policy-based workload
placement and runtime optimization, as well as cloud service composition and
dynamic execution (e.g., cloudbursting).■
G00768727 Page 97 of 129Obstacles
User RecommendationsHybrid cloud computing is different from multicloud computing, which is the
deliberate use of cloud services from cloud providers for the same class of IT
service.■
Hybrid cloud computing complements multicloud computing. Although most
organizations are integrating applications and services across service boundaries,
few large enterprises implemented hybrid cloud computing, and for few services.
Most companies will use a hybrid cloud computing during the next two years, but
more advanced approaches lack maturity and suffer from setup and operational
complexity.■
Hybrid cloud is different from hybrid IT, where IT organizations act as service brokers
as part of a broader IT strategy, and may use hybrid cloud computing. Hybrid IT can
also be enabled by service providers focused on delivering cloud service brokerage,
multisourcing, service integration and management capabilities. These services are
provided by vendors, such as Accenture, Wipro and TCS, and other service providers
and system integrators.■
Note that internally run, virtualized environments are often recast as “private clouds,”
then integrated with a public cloud environment and called a “hybrid cloud.” Hybrid
cloud assumes that the internal environment is truly a private cloud. Otherwise, the
environment is hybrid IT.■
Establish security, management, and governance guidelines and standards when
using hybrid cloud computing services to coordinate the use of these services with
public and private services.■
Approach sophisticated cloudbursting and dynamic execution cautiously, because
these are the least mature and most problematic hybrid approaches.■
Create guidelines/policies on the appropriate use of the different hybrid cloud
models to encourage experimentation and cost savings, and to prevent
inappropriately risky implementations.■
Coordinate hybrid cloud services with noncloud applications and infrastructure to
support a hybrid IT model.■
G00768727 Page 98 of 129Gartner Recommended Reading
Top Strategic Technology Trends for 2021: Distributed Cloud
‘Distributed Cloud’ Fixes What ‘Hybrid Cloud’ Breaks
Predicts 2021: Building on Cloud Computing as the New Normal
G00768727 Page 99 of 129Climbing the Slope
Cloud Managed Services
Analysis By: Craig Lowery
Benefit Rating: High
Market Penetration: More than 50% of target audience
Maturity: Mature mainstream
Definition:
Cloud managed services are IT service offerings that provide for the day-to-day
management of, and operational responsibility for, cloud service environments. A select
set of professional services is typically offered and highly coordinated with the managed
services to assist with cloud strategy, workload migration, solution architecture and
ongoing transformation efforts. Cloud service brokerage is often delivered as a cloud
managed service.
Why This Is Important
Cloud adoption is a critical strategic objective for most organizations, but most need help
in achieving it. Cloud managed service providers (MSPs) deliver a mix of professional and
managed services that guide their customers through typical cloud adoption patterns and
into an ongoing, evolving operational state. This improves the organization’s degree of
success with a faster time to value.
Business Impact
Business beneﬁts:
DriversThe primary beneﬁt of engaging a cloud MSP is to augment a cloud-adopting
organization’s expertise with certiﬁed, experienced personnel to provide advice and
convey best practices.■
The secondary beneﬁt is to provide the difﬁcult-to-source tooling and day-to-day
management of a highly dynamic operating environment.■
A long-term beneﬁt of transformative outcomes occurs when organizations work
with providers to unlock the disruptive potential possibilities of cloud computing.■
G00768727 Page 100 of 129Obstacles
User RecommendationsAs demand for public cloud services has grown steadily, so has the need for
professional and managed services to successfully adopt those services.■
Organizations seeking to adopt public cloud lack the experience, skills, tools and
stafﬁng to successfully navigate key milestones: setting strategy, planning,
implementation, optimization and ongoing evolution of cloud deployment.■
Strong cloud MSPs provide cloud capabilities aligned with hyperscale cloud
infrastructure and platform services (CIPS) providers, unlocking technology
innovations such as artiﬁcial intelligence, automation, data services and edge
computing.■
The move to more cloud-native solutions and complex deployment scenarios such
as hybrid cloud and multicloud have substantially emphasized the need for
professional services expertise.■
Cloud MSPs have varying levels of capability based on the clouds they support,
speciﬁc use cases, and the geographies and industries they target, as well as the
technologies and personnel roles they use to deliver their services. This can make it
difﬁcult to choose the right MSP for an organization’s purposes.■
Becoming heavily dependent on an MSP can make it difﬁcult to leave it later. MSPs
differentiate from each other in how they automate the delivery of their professional
and managed services. They often create proprietary tools or trade secret
integrations of open-source solutions on which a customer can become dependent.■
MSPs face the same challenges as end users in developing and retaining a skilled
workforce. Although MSPs are generally better-positioned to attract and cultivate
those resources, customers may have highly variable service delivery experiences
from one interaction to the next or when compared with other customers.■
Assess providers to ensure the provider has up-to-date expertise and a track record
of success.■
Assess providers with capabilities across the adoption spectrum — from initial and
ongoing advisory services (design) to implementation services (build) and managed
services (run).■
G00768727 Page 101 of 129Sample Vendors
Accenture; Bespin Global; Capgemini; Cognizant; Deloitte; Logicworks; Rackspace
Technology; SMX; TCS; Wipro
Gartner Recommended Reading
Magic Quadrant for Public Cloud IT Transformation Services
Critical Capabilities for Public Cloud IT Transformation Services
CCOE
Analysis By: Lydia Leong
Benefit Rating: High
Market Penetration: More than 50% of target audience
Maturity: Early mainstream
Definition:
A cloud center of excellence (CCOE) is a centralized enterprise architecture function that
leads and governs cloud computing adoption within an organization.Give ﬁrst consideration to providers that demonstrate partnership status and
accomplishments with the organization’s primary public cloud provider.■
Assess providers’ expertise and resources by industry, region and country. ■
Plan for long-term value by choosing providers that can deliver innovations and
support additional use cases and cloud providers as your needs change.■
Assess providers’ ability to deliver to the organization’s speciﬁc hybrid deployment
patterns, which range from management of on-premises virtualization farms to
distributed container and Kubernetes deployments to distributed public cloud
solutions.■
G00768727 Page 102 of 129Why This Is Important
A CCOE is an effective way to drive enterprise-scale cloud adoption that results in positive
business outcomes. As an enterprise architecture (EA) function, it leads organizationwide
cloud governance and brokerage, and guides cloud transformation. Although the CCOE is
responsible for developing cloud computing policies, it primarily inﬂuences, rather than
controls. It enables and empowers, rather than executes — depending on other teams for
implementation.
Business Impact
The CCOE drives strategic cloud adoption through three core functions — cloud brokerage,
cloud governance and cloud transformation. It serves as an internal cloud consulting
practice that delivers cloud architectures and recommended solutions. It partners with the
sourcing team to provide cloud vendor management, including cloud cost governance. It
raises the organization’s level of cloud expertise and supports transformation efforts
through its leadership of a cloud community of practice.
Drivers
The impetus to form a CCOE generally comes from the realization that the
organization needs to mature or scale its cloud adoption, or to ensure that cloud
computing delivers desired business beneﬁts. Speciﬁcally, the organization needs to
ensure that the “path of least resistance” for cloud use is also the path that is well-
governed and meets the organization’s security and regulatory compliance
requirements.■
The organization needs a guide on its cloud journey, as cloud use grows within an
organization and the organization discovers the best practices for cloud usage that
are speciﬁc to its business needs. The CCOE, governance guidelines and guardrails,
and solutions must evolve with the business and its cloud use.■
Many businesses need help with cloud-enabled digital transformation, ideation of
new cloud-enabled business innovations and “evangelism” to encourage cloud
adoption.■
The creation of a CCOE is strongly encouraged by many cloud providers, as well as
cloud managed service providers (MSPs). However, cloud providers often encourage
the creation of vendor-speciﬁc CCOEs, rather than broad CCOEs that cut across IaaS,
PaaS and SaaS lines. MSPs often promote CCOEs because they tend to result in
signiﬁcant managed or professional services revenue.■
G00768727 Page 103 of 129Obstacles
User Recommendations
Gartner Recommended Reading
How to Deploy a Cloud Center of Excellence
Infographic: Gartner’s Reference Cloud Operating ModelThe CCOE needs the sponsorship and mandate of the CIO or other C-level executive
to be effective.■
Many organizations make mistakes in setting the structure and mission of the CCOE,
resulting in a failure of the CCOE to make the desired business impact. The CCOE is
fundamentally a business-outcome-driven enterprise architecture function, not a
sourcing or infrastructure and operations function.■
The CCOE is typically small, and its effectiveness depends on educating and
inﬂuencing others throughout the organization who are actually implementing the
use of cloud services.■
Stafﬁng the CCOE is often challenging, even though it is typically small. There is
signiﬁcant labor-market competition for this skill set, and it may, therefore, be
necessary to open headcount for cloud architects well in advance of demand.
Understafﬁng the CCOE can be a major threat to its inﬂuence and success. However,
the CCOE should not be fully outsourced, due to its strategic nature.■
A CCOE is an EA function that should be led by a chief cloud architect and staffed by
cloud architects. Some organizations hire an individual contractor with very strong
knowledge of their primary cloud services to lead their CCOE, but such an individual
needs an enterprise architect partner with very strong knowledge of the business and
the necessary cross-functional relationships. In most cases, these are full-time,
senior-level, individual-contributor roles.■
The chief cloud architect should also lead a cross-functional cloud computing
advisory committee that contains representatives from the business; technical end-
user teams (such as the application development teams); infrastructure and
operations; and sourcing, security, compliance, risk management and legal teams.
This committee is primarily concerned with strategy and policy.■
Neither the committee nor the CCOE should be a cloud implementation function.
That role is typically occupied by a cloud operations function.■
G00768727 Page 104 of 129How to Design a Cloud Operating Model and Build a Cloud Center of Excellence
Solution Path for Implementing a Cloud Center of Excellence
Innovation Insight for the Cloud Center of Excellence
Cloud Networking
Analysis By: Sid Nag
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Cloud networking incorporates a broad set of technologies that underpin cloud computing
environments. This includes networking connectivity to and across cloud environments,
and networking functionality within cloud environments. Speciﬁc product categories that
help enable cloud networking include CSP network offerings, SDCI services, MCNS and
SD-WAN, among others.
Why This Is Important
Cloud networking — which includes cloud interconnect, VPC networking, zero-trust
networking and network services — is a major factor in the selection of cloud providers by
75% of Gartner-surveyed enterprises.
Business Impact
A cloud service is only as good as the network it runs on. Cloud networking directly
impacts application performance and availability based on the latency and resiliency of
the underlying network. Cloud networking continues to be a top challenge to the adoption
of cloud and is a critical selection criteria for cloud services.
G00768727 Page 105 of 129Drivers
ObstaclesAdoption of public cloud networking requires investment in cloud networking
capabilities such as VPNs and VPCs.■
Over 76% of organizations had adopted or planned to adopt multicloud
environments by the end of 2021, making cloud networking an important
technology.■
In cloud networking, traditional network functions and services, including
connectivity, security, management and control, are delivered as a service in the
cloud.■
Organizations increasingly want to consume networking in a more ﬂexible, agile and
cloudlike way versus traditional mechanisms such as charging a ﬂat rate for devices
or bandwidth.■
Cloud networking can often be confused with traditional networking products,
although such products are key to cloud networking services.■
Cloud networking is speciﬁc to connecting to and across multicloud environments
and providing network functionality within cloud environments.■
Cloud networking today is often done via inelegant methods by leveraging exchange
providers as opposed to using native exterior gateway protocols such as BGP.■
There will often be a requirement for the application running in a cloud provider’s
data center to interoperate with other applications that reside in the customer’s data
centers or in a different cloud provider.■
As multicloud adoption increases, organizations need to be familiar with multiple
cloud service provider (CSP) stacks.■
As more enterprises add more providers, assuring security and performance
becomes increasingly complex.■
G00768727 Page 106 of 129User Recommendations
Sample Vendors
Alkira; Aviatrix; Equinix; VMware
Gartner Recommended Reading
Market Guide for Multicloud Networking Software
Cool Vendors in Cloud Networking
Cool Vendors in Cloud Computing
Infographic: How to Best Connect to Public Cloud Services
CIPS
Analysis By: Sid Nag
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstreamSelect cloud networking offerings that include capabilities to select the right
connectivity provider.■
Institute a requirement where QoS, latency and availability issues are addressed by
your cloud provider, especially for mission-critical applications that are running in the
cloud.■
Mandate capabilities that include provisioning of MPLS, VPN and SD-WAN support
for complex overlays for connecting your on-premises locations into the cloud
provider’s data center.■
Exploit the beneﬁts of public cloud providers by preferring their native capabilities
when starting out. Don’t “forklift” traditional networking strategies into the public
cloud.■
G00768727 Page 107 of 129Definition:
The cloud infrastructure and platform services (CIPS) market is where cloud providers
offer IaaS and PaaS capabilities in an integrated manner. The degree of integration
between IaaS and PaaS may vary, but it includes the use of a single self-service portal and
catalog, shared identity and access management, a single integrated low-latency network,
uniﬁed security, uniﬁed monitoring, and uniﬁed billing.
Why This Is Important
CIPS is important because:
Business Impact
A well-functioning CIPS will offer enterprises a more natural, ﬂexible and comprehensive
cloud computing environment for their workloads, thereby addressing today’s IT needs
from an application perspective. Vendors also beneﬁt from CIPS — those coming from
IaaS and those specialized in PaaS increase their customer value proposition and ability
to compete when covering the broader set of capabilities.Customers are looking for integrated platforms to simplify development, deployment
and operations.■
CIPS offerings are the most complete platforms in the industry, thereby driving
signiﬁcant market consolidation around these platforms.■
ISVs, SIs and MSPs have embraced the leading CIPS platforms, which makes them
the foundation for most organizations’ cloud operations.■
Workloads of today are complex. and cloud providers are addressing the problem by
offering CIPS.■
G00768727 Page 108 of 129Drivers
ObstaclesThe appeal for CIPS is not necessarily in best-of-breed offerings, but in the
uniﬁcation and integration of platform capabilities across these services enabling
broad support of workloads ranging from ERP to cloud-native.■
Most customers that use a hyperscale CIPS provider, such as Amazon Web Services
or Microsoft Azure, have adopted a blend of the provider’s IaaS and PaaS
capabilities. Indeed, the availability of this broad portfolio of services is a key aspect
of choosing a strategic cloud platform provider. Hyperscale CIPS providers deliver
PaaS services with a direct dependency on their IaaS services. As a customer,
whether you are using PaaS services or IaaS services, they are built on a common
substrate. The combination of these services means you are making a strategic bet
on the cloud provider.■
The complexity and level of investment required to offer a full, integrated portfolio of
multifunctional PaaS and IaaS services will likely limit the vendor options in this
market to a handful of hyperscalers. Some hyperscalers will form ecosystems,
enabling smaller PaaS specialists to be included in this market. However, the
maturity of this technology will be primarily dependent on the capabilities of the
hyperscalers.■
Public CIPS markets are consolidating around the market leaders. ■
IaaS-only or PaaS-only cloud providers will continue to exist, but only as secondary
cloud providers compared with CIPS providers.■
This, in turn, could make it a market dominated by a handful of cloud providers,
which could stiﬂe competition and drive stand-alone cloud providers out of the
market.■
G00768727 Page 109 of 129User Recommendations
Sample Vendors
Alibaba Cloud; Amazon Web Services; Google Cloud Platform; Microsoft Azure
Gartner Recommended Reading
Magic Quadrant for Cloud Infrastructure and Platform Services
Critical Capabilities for Cloud Infrastructure and Platform Services
What Buyers Want From CIPS Providers
Risk and Opportunity Index: Cloud Infrastructure and Platform Services
Extending the CIPS Business to New Markets and Opportunities
CSPM
Analysis By: Neil MacDonald, Charlie Winckless
Benefit Rating: HighUse CIPS in both cloud-native and legacy migration projects to expand your design
and deployment options. In some cases, this may involve using capabilities from
multiple cloud providers.■
Prioritize consolidating systems on a hyperscaler CIPS offering when you are
operating and governing ﬂeets of applications at enterprise scale. This improves
your economies of scale, skills and resources through standardization and
consistency across your company and industry.■
Treat integrated CIPS providers as long-term application platforms. They should be
managed as such, with appropriate attention to potential application portability
issues.■
Do not assume that all services of the provider are of the same maturity, functional
completeness or quality of service.■
Give extra credit to those that are multicloud, and therefore, can be colocated with
multiple larger suites of CIPS capabilities, when considering a smaller specialist
PaaS provider.■
G00768727 Page 110 of 129Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Cloud security posture management (CSPM) offerings continuously manage IaaS and
PaaS security posture through prevention, detection and response to cloud infrastructure
risks. The core of CSPM offerings applies common frameworks, regulatory requirements
and enterprise policies to proactively and reactively discover and assess risk/trust of
cloud services conﬁguration and security settings. If an issue is identiﬁed, remediation
options (automated or human-driven) are provided.
Why This Is Important
The complexity of a modern IaaS or PaaS environment makes validating secure
conﬁguration extremely difﬁcult. Even simple misconﬁguration issues, such as open-
storage objects, represent signiﬁcant and often unidentiﬁed risk. The speed and scale of
modern cloud deployments compound the impact of misconﬁguration, and makes it
effectively impossible to address cloud risk without automation. This is an urgent
problem, and one that is encouraging rapid growth in the availability and maturation of
this category.
Business Impact
CSPM offerings provide business and security leaders assurance that their cloud services
are implemented in a secure and compliant fashion despite the speed, complexity,
dynamics and scale of IaaS and PaaS deployments. For enterprises that have a
multicloud strategy, CSPM offerings provide a single way to implement and monitor
security and compliance guardrails across multiple IaaS providers.
G00768727 Page 111 of 129Drivers
ObstaclesMultiple mature offerings are now available from established vendors. ■
Hyperscale cloud service providers offer built-in CSPM capabilities suitable for
single-cloud deployments.■
Most cloud-native application protection platforms (CNAPP), cloud workload
protection platform (CWPP) and security service edge (SSE) vendors now offer
CSPM capabilities as a result of acquisitions or open-source integration.■
CSPM tools offer an abstraction layer that allows for consistent policy management
across multiple clouds — a feat that borders on the impossible if you rely entirely on
each CSP’s native console.■
Several open-source software (OSS) options are emerging with enterprise offerings
available.■
Some emerging CSPM platforms leverage graph and relationship mapping
technologies that enable rich simulation, detection and forensic use cases.■
Vendors are starting to offer full-stack risk visibility with an understanding of
vulnerabilities within the workload itself (virtual machine [VM] or container), typically
achieved by taking a snapshot of the running workload.■
The market for CSPM is maturing and consolidating. ■
Emerging CNAPP offerings subsume CSPM capabilities and offer a longer-term
alternative more integrated approach.■
The market is increasingly looking for tooling that shifts left and offers
infrastructure as code scanning capabilities. Not all vendors offer this or only offer a
limited set of infrastructure as code scripting languages.■
CSPM capabilities are available in many adjacent markets, making it difﬁcult for end
users to select the best approach.■
There is no standard way to remediate issues identiﬁed and approaches vary. ■
Organizations are reluctant to enable automated remediation from SaaS-based
CSPM offerings (that require read/write access), and prefer remediation within the
context and control of their own CSP tenancy.■
G00768727 Page 112 of 129User Recommendations
Sample Vendors
Check Point Software Technologies; Lacework; OpsCompass; Orca Security; Palo Alto
Networks; Rapid7; Skyhigh Security; Tenable; Trend Micro; Wiz
Gartner Recommended Reading
How to Make Cloud More Secure Than Your Own Data Center
How to Protect Your Clouds With CSPM, CWPP, CNAPP and CASB
Market Guide for Cloud Workload Protection Platforms
Innovation Insight for Cloud Security Posture Management
Innovation Insight for Cloud-Native Application Protection Platforms
Cloud Workload Protection Platforms
Analysis By: Charlie Winckless, Neil MacDonald
Benefit Rating: Moderate
Market Penetration: 20% to 50% of target audienceInvestigate and see if you already have suitable CSPM capabilities from your IaaS
provider, CWPP vendor, SSE/cloud access security broker (CASB) vendor and IT
operations team. The IaaS provider might have sufﬁcient CSPM capabilities built in
and the IT operations team may have purchased a cloud management platform for
billing/utilization, but many also have suitable CSPM capabilities.■
Treat investments as tactical if a point solution is used. Limit contracts to one to two
years as the market matures and further consolidates.■
Evaluate the CSPM provider’s cloud infrastructure entitlements management
capabilities as an alternative to purchasing a stand-alone solution for this.■
Extend scanning into development, including infrastructure as code scanning. ■
Evaluate and compare the response options when an out-of-compliance
conﬁguration is encountered, including alerting and automated remediation
alternatives.■
G00768727 Page 113 of 129Maturity: Mature mainstream
Definition:
Cloud workload protection platforms (CWPPs) are workload-centric security offerings that
protect server workloads in hybrid and cloud deployments. CWPPs provide consistent
visibility and control for physical machines, virtual machines, containers and serverless
workloads, regardless of location. CWPP offerings protect the workload using a
combination of system integrity protection, application control, behavioral monitoring,
intrusion prevention and optional anti-malware protection.
Why This Is Important
As enterprises spread workloads across data centers and public clouds, they need ways to
integrate security into the workload creation toolchain and maintain the visibility and
control of the workload at runtime regardless of location. The only way to address the
speed, scale and complexity of workload protection when cloud is in the equation is to use
an appropriately designed offering. Simply using a solution designed for on-premises
data centers or end-user endpoints won’t work.
Business Impact
Enterprises are implementing hybrid data center architectures, with workloads spanning
on-premises and public cloud IaaS providers, container-based implementations and
serverless functions. These workloads have unique security requirements that differ
signiﬁcantly from end-user systems. In order to secure these workloads and realize the
beneﬁts of cloud native applications, it is necessary to use the appropriate tools.
Drivers
The only way to address the speed, scale, complexity, and the ephemeral and elastic
nature of cloud workload protection is to use a tool designed for how these
workloads are deployed.■
Simply using a solution designed for on-premises data centers or end-user endpoint
protection platforms (EPPs) won’t work. Thus, many vendors are now explicitly
targeting the CWPP market, including multiple startup point solutions. Established
EPP vendors are adding dedicated CWPP products to support this trend.■
Cloud server workload protection strategies must be based on a foundation of solid
operational hygiene including proper administrative control, patching discipline (or
the use of immutable infrastructure) and conﬁguration management.■
G00768727 Page 114 of 129ObstaclesWorkloads are no longer remotely homogeneous; tools must protect containers, VM
and serverless workloads, and grant the appropriate levels of security to each.■
Unlike end-user endpoints, server workloads do not commonly encounter and
execute unknown arbitrary code, thus lending themselves to a default deny,
application-control-based protection strategy.■
As vendor convergence becomes more important to Gartner clients, the convergence
of CWPP and CSPM into a cloud-native application platform supports fewer tools
that still provide the same value.■
Some organizations are maturing their approach to cloud protection and have not
identiﬁed a need for cloud-native security toolsets, or prefer to continue with existing
endpoint tools despite their lack of suitability for cloud deployments.■
Organizations still wish to extend on-premises controls and control patterns to the
cloud, regardless of suitability.■
Single cloud-using organizations may wish to use CSP-native tools. This can be
shortsighted due to multicloud deployments, increasing options for cost and feature
improvements.■
Not all vendors offer all capabilities. Some specialize in only one or two forms of
workload protection.■
Not all vendors offer support for physical servers or out-of-support and older
operating systems that still require protection.■
Serverless functions and PaaS security protection capabilities require new
approaches that don’t require agents or privileged containers.■
G00768727 Page 115 of 129User Recommendations
Sample Vendors
Aqua Security; Bitdefender; CrowdStrike; Lacework; Microsoft; Palo Alto Networks; Skyhigh
Security; Sophos; Trend Micro; VMware
Gartner Recommended Reading
Market Guide for Cloud Workload Protection Platforms
How to Make Cloud More Secure Than Your Own Data Center
Magic Quadrant for Endpoint Protection Platforms
Private Cloud Computing
Analysis By: Thomas Bittman
Benefit Rating: Moderate
Market Penetration: More than 50% of target audienceDon’t use an end-user EPP solution to protect cloud server workloads. ■
Architect for consistent visibility and control of all workloads regardless of location,
size, or type as well as for cases where runtime agents may not be used or may not
make sense.■
Require vendors to support native integration with leading cloud platforms. ■
Prioritize “zero-trust execution”/default deny/application control approach. ■
Extend workload scanning and compliance efforts into development (DevSecOps),
especially for containers and serverless functions.■
Require CWPP offerings to expose all functionality via application programming
interfaces.■
Require vendors to provide solutions for the protection of containers and Kubernetes
with the ability to scan the containers in development for vulnerabilities before
deployment into production.■
Require vendors to provide solutions for serverless functions, both in terms of
scanning static code and validating runtime behavior.■
G00768727 Page 116 of 129Maturity: Mature mainstream
Definition:
Private cloud computing is a form of cloud computing used by only one organization or
one that ensures an organization is completely isolated from others. As a form of cloud
computing, it has full self-service, full automation behind self-service, and usage metering.
It does not have to be on-premises, or owned or managed by the enterprise.
Why This Is Important
Cloud services offer many beneﬁts, but isolation may be important for security or
regulatory reasons. Private cloud offers complete isolation, while providing the
convenience and ease of use of a cloud service. Private and public cloud are at opposite
ends of the isolation spectrum. Public cloud providers have offered virtual private cloud,
dedicated instances, dedicated hosts, and distributed cloud offerings. Thus, there are a
variety of isolation choices between public and private cloud.
Business Impact
Organizations that build a private cloud service are emulating public cloud computing
providers to acquire similar beneﬁts — mainly agility for new cloud-native applications,
and business value and growth. When the goals are IT modernization or efﬁciency for
existing applications, cloud-inspired deployments — that are more customized and less
automated — are more appropriate.
G00768727 Page 117 of 129Drivers
ObstaclesRegulatory or privacy reasons: Private cloud may be useful where data or
applications cannot reside on a public cloud, or need to reside in a speciﬁc location
or on-premises.■
Unique cloud service requirements: Due to speciﬁc enterprise requirements (or
support of existing applications), public cloud providers may not offer speciﬁc
capabilities needed by the enterprise.■
Evolution of virtualization: Private cloud can be seen as a natural evolution of an
existing virtualized environment — virtualizing all infrastructure and providing a
service interface.■
Standardization: Speciﬁc private cloud services can be used to drive users to more
standard offerings, further reducing costs and increasing automation.■
Platform-level services: Cloud services can provide rapid deployment, agile change,
rapid scaling and innovation at the platform level.■
Building a custom private cloud can be very costly and complex (and most
deployments that are called “private cloud” actually don’t have cloud
characteristics).■
Building an initial self-service offering is one thing, but maintaining and adding new,
innovative features (similar to public cloud providers) is usually untenable for
enterprises.■
Private clouds being built to support existing applications require signiﬁcant
nonstandardization, which reduces automation potential.■
Users often have little motivation to move to a more standard model, unless they
make fundamental business changes, such as adopting usage-based pricing for
services.■
G00768727 Page 118 of 129User Recommendations
Sample Vendors
HPE; IBM; VMware
Gartner Recommended Reading
Rethink Your Internal Private Cloud
Building ‘Just Enough’ Private Cloud With Virtualization Automation
CASBs
Analysis By: Craig Lawson, Neil MacDonald
Benefit Rating: Transformational
Market Penetration: More than 50% of target audience
Maturity: Mature mainstreamRule out public cloud offerings — including forms of distributed cloud — before
investigating private cloud.■
Evaluate third-party hosting options, and avoid building your own, if possible. ■
Choose cloud-inspired technologies rather than true cloud if IT efﬁciency or
modernization for existing applications is the goal.■
Focus on business and application needs ﬁrst, and let that determine the cloud
service offerings.■
Focus on services that ﬁt the cloud model — standard, high volume and self-service;
those that require agility and horizontal scalability; and usages that might be short-
lived.■
Build or buy private cloud services with the potential to interoperate with, integrate
with or migrate to public cloud services in the future. Develop an exit strategy.■
Manage the scope of work — start small and expand based on the business case. ■
Build expertise in managing multicloud — going beyond just the private cloud. ■
G00768727 Page 119 of 129Definition:
Cloud access security brokers (CASBs) provide critical controls to allow for the secure use
of cloud services, with key features being visibility, compliance, data security and threat
protection. They consolidate multiple types of security enforcement into one place that
can span SaaS, IaaS and PaaS.
Why This Is Important
CASBs are critical for organizations to secure usage of business-critical cloud services.
The four key areas — visibility, compliance, data security and threat protection — are the
primary value propositions for the use of CASBs.
Business Impact
CASBs enable the secure use of cloud services, are suitable for organizations of all sizes
in all industries and can demonstrate that organizational cloud usage is well-governed.
With continued feature expansion, ongoing convergence with secure web gateway (SWG)
and zero trust network access (ZTNA) into security service edge (SSE), and relative ease
of switching providers, we recommend preferencing an SSE solution when renewing or
selecting CASB features. One year contract terms are still recommended for this evolving
market unless substantial discounts can be obtained and you are satisﬁed with that
vendor’s roadmap execution.
Drivers
End-user organizations need to: secure use of business-critical, cloud-delivered
applications and infrastructure; secure general internet to prevent threats to users,
regardless of their location; and improve access to existing services while taking
advantage of zero trust concepts. Today, CASB is converging with SWG and ZTNA to
deliver this “three-legged stool” concept to support all these use cases.■
With CASB vendors enabling secure use of business-critical cloud applications and
infrastructure, and SWG vendors expanding functionality for general internet security
and access to existing services, security leaders are now able to successfully deliver
on the above-mentioned three capabilities from an increasing number of vendors
providing all three.■
The past few years have seen increased focus on two speciﬁc use cases that CASB
technology directly helps with: the huge shift to remote working and the
continuously increasing use of cloud services critical to business.■
G00768727 Page 120 of 129Obstacles
User Recommendations
The CASB market has now converged into the security service edge (SSE) market and, as
such, Gartner has depreciated the stand-alone CASB and SWG Magic Quadrants.
Therefore, we recommend you:
Sample Vendors
Broadcom; Cisco; iboss; Lookout; Microsoft; Netskope; Palo Alto Networks; Skyhigh
Security; Versa; Zscaler
Gartner Recommended Reading
2021 Strategic Roadmap for SASE Convergence
Magic Quadrant for Security Service EdgeUnclear and often distributed organizational ownership of cloud services can lead to
a CASB implementation that fails to secure these services adequately.■
Overlapping CASB functionality from a number of vendors leads to duplication and
confusion. Lack of an effective data security policy can lead to frustration, with a
CASB trying to enforce an ineffective policy resulting in issues like false positives
and risk of data loss.■
A subset of controls are offered by some cloud service providers themselves. For
example, Microsoft 365’s native security features and Salesforce Shield continue to
see interest from users.■
Some cloud workload protection platform (CWPP)/cloud native application
protection platform (CNAPP) offerings also overlap in the area of IaaS security.■
Move to a consolidated SSE offering during upcoming refresh cycles. ■
Read the Magic Quadrant for Security Service Edge for a more detailed analysis of
the SSE market where we have detailed evaluations of vendors that can help you
secure access to the web, cloud services and private applications.■
Seek support for multiple modes of operation, namely forward proxy, reverse proxy
(or RBI) and API for the best support of managed and unmanaged devices and cloud
services via a CASB.■
G00768727 Page 121 of 129Critical Capabilities for Security Service Edge
Market Guide for Zero Trust Network Access
G00768727 Page 122 of 129Entering the Plateau
Cloud Migration
Analysis By: Craig Lowery
Benefit Rating: High
Market Penetration: More than 50% of target audience
Maturity: Mature mainstream
Definition:
Cloud migration is the process of planning and executing the movement of applications
or workloads from on-premises infrastructure to external cloud services, or between
different external cloud services. At a minimum, applications are rehosted (moved largely
as-is to public cloud infrastructure), but are ideally modernized through refactoring or
rewriting, or potentially replaced with software as a service (SaaS).
Why This Is Important
Cloud migration is a necessary step for organizations wishing to maximize the business
beneﬁts of their use of public cloud computing services. When applications and data in
the organization’s private data centers are moved into a public cloud, new opportunities
are unlocked for the beneﬁt of the business. A structured, well-informed approach to such
a move is necessary to avoid negative impacts such as wasted time and investment, or
business disruptions.
Business Impact
The business impacts of cloud migration are:
DriversThe business is able to access the beneﬁts of public cloud without building all of its
cloud application portfolio from scratch.■
Existing applications can be migrated into the public cloud and modiﬁed to take
some advantage of cloud capabilities, yielding near-term cloud-related beneﬁts.■
Cloud migration enables a business to adopt public cloud with the least disruption
by evolving organizational structures, operational capabilities and user experiences
over time.■
G00768727 Page 123 of 129Obstacles
User RecommendationsOrganizations see beneﬁt in public cloud deployments and seek to move all or a
portion of their IT data center deployments there to derive positive business impacts.■
Migration allows an organization to leverage existing deployments in their private
data centers rather than building everything anew in the cloud.■
Competitors who built their businesses in the cloud or migrated earlier have cloud-
based advantages that the organization must counter as quickly as possible.
Conversely, getting to cloud before competitors can give an organization a
competitive cloud-based advantage.■
Most organizations lack the tools, expertise and resources to plan and execute a
migration. Although some existing tools and skills can be leveraged, they are usually
not sufﬁcient to effect a successful migration.■
Organizations often struggle with the new operational aspects of cloud computing,
which can result in technically successful migrations that fail to meet business
objectives.■
A recent and well-justiﬁed emphasis on application modernization as part of
migration has further confused the market on best practices and strategies.■
Not everything should be moved to the cloud and most organizations will be in a
hybrid deployment for some period of time. There are many approaches to achieving
a hybrid deployment and organizations may ﬁnd it difﬁcult to identify, understand
and act on their options.■
Use an external service provider, such as a cloud MSP, to improve the chances of a
successful migration. Almost all successful large-scale migrations to public cloud
infrastructure and platform services (CIPS) are done in conjunction with a service
provider. They provide consulting for strategy and planning, tools, and technical
staff to implement the move.■
Set a strategy based on business objectives, provide CSP recommended training
(badges) for key personnel, and source migration tools from ISVs or the CSP’s native
toolset if you are an I&O leader electing to perform your own migration.■
G00768727 Page 124 of 129Sample Vendors
Accenture; Bespin Global; Capgemini; Cognizant; Deloitte; Logicworks; Rackspace
Technology; SMX; TCS; Wipro
Gartner Recommended Reading
Magic Quadrant for Public Cloud IT Transformation Services
Critical Capabilities for Public Cloud IT Transformation Services
Cloud Cost Scenario Planning Tool
Ignition Guide to Creating a Migration Plan for Public CloudChoose the best approach for modernizing an existing application. Although
rehosting without modiﬁcation might be easiest, it brings far fewer beneﬁts than
some degree of modernization or outright replacement. Some rehosting migrations
are still done for expediency but expectations of cloud-native beneﬁts have become
mainstream.■
G00768727 Page 125 of 129Appendixes
Figure 2: Hype Cycle for Cloud Computing, 2021
Source: Gartner (July 2021)
G00768727 Page 126 of 129Hype Cycle Phases, Beneﬁt Ratings and Maturity Levels
Table 2: Hype Cycle Phases
(Enlarged table in Appendix)
G00768727 Page 127 of 129Table 3: Benefit Ratings
Source: Gartner (July 2022)T r a n s f o r m a t i o n a l Enables new ways of doing business across
industries that will result in major shifts in
industry dynamics
H i g h Enables new ways of performing horizontal
or vertical processes that will result in
significantly increased revenue or cost
savings for an enterprise
M o d e r a t e Provides incremental improvements to
established processes that will result in
increased revenue or cost savings for an
enterprise
L o w Slightly improves processes (for example,
improved user experience) that will be
difficult to translate into increased revenue
or cost savingsBenefit Rating Definition
G00768727 Page 128 of 129Table 4: Maturity Levels
(Enlarged table in Appendix)
Document Revision History
Hype Cycle for Cloud Computing, 2021 - 14 July 2021
Hype Cycle for Cloud Computing, 2020 - 31 July 2020
Hype Cycle for Cloud Computing, 2019 - 8 August 2019
Hype Cycle for Cloud Computing, 2018 - 31 July 2018
Hype Cycle for Cloud Computing, 2017 - 1 August 2017
Hype Cycle for Cloud Computing, 2016 - 3 August 2016
Hype Cycle for Cloud Computing, 2015 - 5 August 2015
Hype Cycle for Cloud Computing, 2014 - 24 July 2014
Hype Cycle for Cloud Computing, 2013 - 31 July 2013
Hype Cycle for Cloud Computing, 2012 - 1 August 2012
Hype Cycle for Cloud Computing, 2011 - 27 July 2011
Hype Cycle for Cloud Computing, 2010 - 27 July 2010
G00768727 Page 129 of 129Hype Cycle for Cloud Computing, 2009 - 16 July 2009
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Understanding Gartner’s Hype Cycles
Create Your Own Hype Cycle With Gartner’s Hype Cycle Builder 2021
Cloud and Edge Infrastructure Primer for 2022
Predicts 2022: The Cloud Moves From Technology Disruption to Business Disruption
Top Four Trends Are Shaping the Future of Public Cloud
The Cloud Strategy Cookbook, 2021
© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00768727 Page 1A of 5ATable 1: Priority Matrix for Cloud Computing, 2022
Benefit Years to Mainstream Adoption
Transformational CASBs Cloud Sustainability
Composable Applications
Edge Computing
SASE
Serverless Infrastructure
Site Reliability EngineeringAugmented FinOps
Business-Driven Cloud
Strategy
Industry Cloud Platforms
High CCOE
Cloud Managed Services
Cloud Migration
Cloud NetworkingAI Cloud Services
API-Centric SaaS
CIPS
Cloud HPC
Cloud-Optimized Hardware
Container Management
CSPM
Hybrid Cloud Computing
IoT Platform
Multicloud
Packaged Business
Capabilities
SaaSOpsCloud-Native
Cloud Portability
Container-VM Convergence
Distributed Cloud
Intercloud Data ManagementLess Than 2 Years 2 - 5 Years 5 - 10 Years More Than 10 Years
G00768727 Page 2A of 5ASource: Gartner (July 2022)Benefit Years to Mainstream Adoption
Moderate Cloud Workload Protection
Platforms
Private Cloud ComputingCloud Marketplaces
Consumption-Based Model
Multicloud Network Software
Sovereign CloudQuantum Computing as a
Service
LowLess Than 2 Years 2 - 5 Years 5 - 10 Years More Than 10 Years
G00768727 Page 3A of 5ATable 2: Hype Cycle Phases
Innovation Trigger A breakthrough, public demonstration, product launch or other event
generates significant media and industry interest.
Peak of Inflated Expectations During this phase of overenthusiasm and unrealistic projections, a flurry of
well-publicized activity by technology leaders results in some successes, but
more failures, as the innovation is pushed to its limits. The only enterprises
making money are conference organizers and content publishers.
Trough of Disillusionment Because the innovation does not live up to its overinflated expectations, it
rapidly becomes unfashionable. Media interest wanes, except for a few
cautionary tales.
Slope of Enlightenment Focused experimentation and solid hard work by an increasingly diverse
range of organizations lead to a true understanding of the innovation’s
applicability, risks and benefits. Commercial off-the-shelf methodologies and
tools ease the development process.
Plateau of Productivity The real-world benefits of the innovation are demonstrated and accepted.
Tools and methodologies are increasingly stable as they enter their second
and third generations. Growing numbers of organizations feel comfortable
with the reduced level of risk; the rapid growth phase of adoption begins.
Approximately 20% of the technology’s target audience has adopted or is
adopting the technology as it enters this phase.
Years to Mainstream Adoption The time required for the innovation to reach the Plateau of Productivity.Phase Definition
G00768727 Page 4A of 5ASource: Gartner (July 2022)
Table 3: Benefit Ratings
Source: Gartner (July 2022)Phase Definition
Transformational Enables new ways of doing business across industries that will result in
major shifts in industry dynamics
High Enables new ways of performing horizontal or vertical processes that will
result in significantly increased revenue or cost savings for an enterprise
Moderate Provides incremental improvements to established processes that will result
in increased revenue or cost savings for an enterprise
Low Slightly improves processes (for example, improved user experience) that will
be difficult to translate into increased revenue or cost savingsBenefit Rating Definition
G00768727 Page 5A of 5ATable 4: Maturity Levels
Source: Gartner (July 2022)Embryonic In labs None
Emerging Commercialization by vendors
Pilots and deployments by industry leadersFirst generation
High price
Much customization
Adolescent Maturing technology capabilities and process
understanding
Uptake beyond early adoptersSecond generation
Less customization
Early mainstream Proven technology
Vendors, technology and adoption rapidly evolvingThird generation
More out-of-box methodologies
Mature mainstream Robust technology
Not much evolution in vendors or technologySeveral dominant vendors
Legacy Not appropriate for new developments
Cost of migration constrains replacementMaintenance revenue focus
Obsolete Rarely used Used/resale market onlyMaturity Levels Status Products/Vendors
G00770252 Page 1 of 101Hype Cycle for Edge Computing, 2022
Published 12 July 2022 - ID G00770252 - 110 min read
By Analyst(s): Bob Gill, Philip Dawson, Thomas Bittman
Initiatives:I&O Platforms
Interest in edge computing is increasing usage across industry
verticals and use cases. Due to the diversity of edge computing,
innovations are bespoke and varied. As infrastructure and
operations leaders shift from ﬁrst deployments to expansion,
innovations, standards and markets will accelerate.
Additional Perspectives
Analysis
What You Need to Know
Based on client inquiries, market hype for innovations associated with edge computing
remains much higher than mature technology reality, but that is changing. As a trend,
edge computing continues to grow, driven by the overall trend toward digital
transformation (as well as the need to collect, analyze, and monetize data and
interactions at the edge). Enterprises are using cloud computing to enable more agility
and innovation for back-ofﬁce services, and are putting more attention on increasing
solution agility, responsiveness and intelligence close to the edge. Various technologies
that enable the edge computing trend are evolving, but the huge diversity in use cases and
challenges for vendors to monetize that diversity result in slower progress. Regardless, a
large percentage of enterprises have at least started deploying edge computing solutions,
initially focused on one use case. Inevitably, enterprises that deploy edge computing for a
single use case expand workloads rapidly, which will eventually drive more standard
offerings in industry verticals (like retail), and more layered horizontal solutions across
vertical industries (e.g., edge management and orchestration). As markets form up, edge
computing will accelerate.Summary Translation + Localization: Hype Cycle for Edge Computing, 2022
(25 July 2022)■
G00770252 Page 2 of 101The Hype Cycle
There are three important factors that have affected the edge computing trend in the last
year:
Innovations that enable edge computing evolving differently in the Hype Cycle:Drivers Are Changing: While low latency tends to be the primary driver for edge use
cases, the growth of IoT has increased the importance of the latency of connected
systems at the edge, pushing the location of computing closer to the edge. At the
same time, because data production at the edge continues to grow rapidly, reducing
backhaul bandwidth costs by managing and processing more of that data at or near
the edge will become the primary driver (by 2025, bandwidth cost will be the primary
driver for new edge computing deployments, versus latency in 2021). More data is
also increasing the need for processing power, the importance of machine learning
architectures and creating more demand to train models closer to the edge —
including federated and swarm learning.■
Diversity of Requirements Remains a Challenge: The majority of edge computing
solutions continue to have unique use cases or vertical industry requirements, with
very different technologies and topologies. Customized, ﬁrst-of-a-kind deployments
are common in edge computing in 2022. In addition to net new use cases,
enterprises continue evaluating how to integrate with and modernize their OT
solutions, and there is a growing trend to look at modernizing remote-ofﬁce/branch-
ofﬁce solutions to also support new edge workloads.■
Enterprises Are Getting More Strategic: While enterprises are usually drawn into
edge computing by a use case, they inevitably look to expand capabilities near the
edge for more use cases. Gartner is seeing more inquiries about edge computing
standards, extensibility and building edge computing strategies.■
Slow Movers: IT/OT Integration remains an organizational and cultural challenge,
while emerging technologies like Edge AI Hardware and Software are still maturing.■
Moving Quickly: SASE has been developing quickly (primarily as a service), as well
as intelligent technologies for the extreme edge, like Single-Board Edge Computers.■
Peak Hype: There’s signiﬁcant market interest driving the hype for Edge as a Service,
Edge AI and Edge PaaS.■
G00770252 Page 3 of 101Figure 1: Hype Cycle for Edge Computing, 2022
Source: Gartner (July 2022)
The Priority Matrix
The Priority Matrix provides perspective on the edge computing innovations that will have
a bigger impact, and those that might take longer to fully mature.Very Early Innovations: Peer-to-Peer Edge innovations are emerging as the density of
computing (and the data) at the edge grows (e.g., in factories).■
Consider now: Some core innovations are nearing maturity, including IoT in general,
computing hardware like Edge Servers and Industrial IoT Gateways, Hardware-Based
Security and integrated Low-Power, Wide-Area (LPWA) network technologies. Also
consider Private 5G now (even though full mainstream adoption is more than ﬁve
years away).■
Evaluate in the near-term: A few transformation innovations will mature in the two
to ﬁve year time frame, including Edge AI, Edge Analytics, Micro Datacenters and
SASE.■
G00770252 Page 4 of 101Table 1: Priority Matrix for Edge Computing, 2022
(Enlarged table in Appendix)Monitor but give them time: A number of edge computing innovations will take
more than ﬁve years to become mainstream, including Neuromorphic Computing,
Distributed Cloud, Edge Security and Peer-to-Peer Edge.■
G00770252 Page 5 of 101On the Rise
Peer-to-Peer Edge
Analysis By: Thomas Bittman, Bob Gill
Benefit Rating: Moderate
Market Penetration: Less than 1% of target audience
Maturity: Embryonic
Definition:
Peer-to-peer edge computing enables distributed computing across an edge environment
for resilience, workload orchestration, horizontal scaling, swarm learning, and interaction
and cooperation between edge computing nodes using local or mesh networking as an
enabling technology.
Why This Is Important
Much of the early focus on edge computing has been extending intelligent things to the
cloud, and extending cloud capabilities closer to users and things at the edge. As more
things connect at the edge, there will be a growth in capability, processing, interaction and
decision making across things — creating systems of interaction at the edge. These
systems can leverage each other for resilience, horizontal scaling and orchestration of
work.
Business Impact
Peer-to-peer edge capability will enable enterprises with dense digital locations (especially
factories) to maximize digital interactions and decision-making locally, increasing
efﬁciency, lowering cost, improving resilience and improving latency:
DriversPeer-to-peer edge processing will leverage compute and storage that will proliferate
at the edge, reduce latency and response time for complex local computations, and
enable collaborative immersive experiences.■
Peer-to-peer edge processing is the infrastructure underpinning swarm and federated
learning- especially valuable for highly-dense edges.■
The growth of complex systems and interactions at the edge. ■
G00770252 Page 6 of 101Obstacles
User Recommendations
Sample Vendors
Alef Mobitech Solutions; Equinix; Fasetto; HPE; mimik technology; Rajant; Storj
Gartner Recommended ReadingThe importance of semiautonomy when the edge is disconnected. ■
Unpredictable edge workload peaks requiring efﬁcient orchestration. ■
Emergence of osmotic computing technologies. ■
The need for local training, maintaining data locally and the growth of swarm
learning.■
The growing maturity of blockchain technologies as an enabler. ■
Mobility and coordination of endpoints (such as robot swarm decisions). ■
Collaborative immersive computing. ■
Mesh networking — allowing a remote device to communicate small packets of
information via other devices connections.■
Diversity and lack of standards will limit peer-to-peer edge computing to speciﬁc use
cases, but will gradually generalize to broader use cases as standards evolve.■
Evolving technology stack and lack of integration and architecture skill sets in I&O. ■
Security challenges inherent in local networking. ■
Complexity caused by rapidly-changing nodes. ■
Deploy custom solutions for peer-to-peer edge when the business case is very strong,
and complex and lengthy development and rollout process is worth the investment.
Enterprises should do this carefully until broad solutions and standards appear.■
Evolve peer-to-peer edge in a stepwise manner: basic connectivity and simple
interactions ﬁrst, then resilience and workload orchestration, and ﬁnally horizontal
scaling and distributed processing.■
G00770252 Page 7 of 101Innovation Insight for Federated Machine Learning
Building an Edge Computing Strategy
Designing Blockchain Smart Contract Security and Access Control
Emerging Technologies: Critical Insights on Metaverse
Edge Data Management
Analysis By: Ted Friedman
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Edge data management comprises the capabilities and practices required to capture,
organize, store, integrate and govern data outside of traditional data center and public
cloud environments. An increasing number of digital business use cases, including those
based on IoT solutions, will leverage data in edge environments. This creates tremendous
opportunities to optimize resources and drive real-time decisions and actions, but also
brings challenges in complexity and governance concerns.
Why This Is Important
Valuable data is increasingly generated and used outside of traditional data centers and
cloud environments. This data often has a shorter useful life span, requiring value to be
captured near the place and time of its origin. This is the role of edge-computing
environments closer to assets in the physical world. Edge data management will both
impact and enable IT leaders and their teams, requiring new capabilities and skills while
also opening up new opportunities to deliver value.
Business Impact
Edge data management creates value in various ways:
By distributing data management to edge environments, data-centric solutions better
support demand for local and real-time data availability.■
G00770252 Page 8 of 101DriversMore solutions, such as for IoT use cases, must operate in disconnected (or
intermittently connected and low-bandwidth) scenarios.■
It enables smarter physical assets and collections of assets, including remote
management or autonomous behavior via onboard (edge) data.■
It addresses inconsistencies, protection, sovereignty and other governance issues
that arise from siloed edge environments.■
Extreme speed: By placing data, data management capabilities and analytic
workloads at optimal points ranging all the way out to endpoint devices, enterprises
can enable more real-time use cases. In addition, the ﬂexibility to move data
management workloads up and down the continuum from centralized data centers
or the cloud to edge devices will enable greater optimization of resources.■
Data gravity: Bandwidth costs and scenarios with limited or intermittent connectivity
demand the ability to organize and process data closer to the edge.■
Expanded scale and reach: By using distributed computing resources, and spreading
the load across the ecosystem, enterprises can broadly scale their capabilities and
extend their impact into more areas of the business. This includes use cases and
outcomes traditionally managed only via operational technology teams, such as
those managing equipment in industrial settings. Dedicated hardware for edge
processing of data will continue to amplify these beneﬁts.■
Resiliency: Pushing data management capabilities toward edge environments can
also bring beneﬁts in the form of greater fault tolerance and autonomous behavior. If
edge environments do not require centralized resources, then issues with
connectivity to, or unplanned downtime of, those centralized resources don’t disrupt
processes that rely on local edge capabilities.■
G00770252 Page 9 of 101Obstacles
User Recommendations
To capture opportunities and minimize risk from edge data management, IT leaders and
their teams should:Management of distributed data architectures: Data management has been largely
based on principles of centralization — bringing data to central data stores (e.g.,
data warehouses), then processing that data to create value. Edge environments
break that model via distributed data architectures, raising complex choices of where
to locate and aggregate data on the continuum of cloud/data center to edge. This
includes ﬁnding the right balance of latency and consistency.■
Governance: With the distribution and complexity of edge environments, data
governance becomes challenging. Organizations should extend their governance
practices to address edge-resident data stores and processing capabilities, including
policies for disposal of ephemeral or nonvalue event data.■
Organizational and skills considerations: Many modern applications are being
developed and deployed by OT teams lacking data management skills and
oversight, or by IT teams lacking edge computing skills and experience.■
Identify use cases where data management capabilities in edge environments can
enable differentiated products and services by collaborating with OT and IT
personnel working in edge locations.■
Expand the skill sets of their teams to include edge platforms and the technologies
required to manage data and data-intensive workloads on them.■
Augment existing data management infrastructure to support edge deployment by
partnering with product teams that are implementing IoT platforms and similar
distributed computing architectures.■
Place a greater emphasis on end-to-end system design. Understanding the
dependencies between all components of distributed data pipelines, analytic
workloads and AI models will be crucial to success.■
Ensure safety and control by extending existing governance capabilities to apply to
edge data environments.■
G00770252 Page 10 of 101Sample Vendors
EdgeDB; FairCom; IBM; Microsoft; ObjectBox; Xencia
Gartner Recommended Reading
Get Ready For Data Management at the Edge: Key Considerations and Actions
Building an Edge Computing Strategy
Forecast: Internet of Things, Endpoints and Communications, Worldwide, 2020-2030,
4Q21 Update
Forecast Analysis: Edge Hardware Infrastructure, Worldwide
Edge AI Software
Analysis By: Chirag Dekate, Carlton Sapp, Eric Goodness
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Edge AI software refers to the use of systems for orchestrating, integrating, deploying and
monitoring machine learning (ML) models across edge environments. While
predominantly focused on inference, advanced systems may enable training at or near the
edge based on local context data.
Why This Is Important
Unlike traditional enterprise stacks, managing deployment of AI requires meticulous
model versioning, A/B (champion/challenger) testing, and optimized delivery for compute
accelerators in the data center or edge. Edge AI software technologies and toolkits, such
as containers (e.g., Docker Swarm) and AI frameworks (e.g., TensorFlow Lite, Caffe2),
enable enterprises to accelerate the orchestration, integration and deployment of AI in
edge computing.
G00770252 Page 11 of 101Business Impact
Using edge AI software technologies, enterprises can improve management of edge AI for
outcomes such as:
Drivers
Edge AI software needs to support four common edge AI architectural patterns found in
cloud service providers (CSPs) and IoT environments:
In each of the above contexts, trained AI models need to be:Use of streaming AI analytics using event-based architecture platforms and
streaming data contexts.■
Manage versioning, roll out/roll back and monitoring of AI models deployed. ■
Reduce communications cost with less data trafﬁc across the edge and the cloud. ■
Increase availability even when the edge is disconnected from the network. ■
Latency sensitive decision making for business-critical applications. ■
Enable Vision Solutions: Enable mature solutions for smart video to monitor
environments for much lower costs.■
Control patterns: Supervisory deployment conﬁguration that focuses on executing
highly regulated actions of IoT (IT and OT) endpoints. For example, smart city
deployments and intelligent industry 4.0 applications.■
Graph patterns: Highly interconnected and relational IoT endpoints in distributed
edge environments. They are commonly deployed in MEC applications to solve
bandwidth optimization problems.■
Swarm patterns: A deployment conﬁguration that focuses on decentralized edge
environments or collective actions of IoT endpoints. For example, logistics ﬂeet
optimization and management to enhance routes and extend the life of service
vehicles.■
Optimized for delivery (tuned for speciﬁc architecture, optimized for hardware or
operating environment constraints, etc.).■
Managed (with roll back, and champion/challenger testing). ■
G00770252 Page 12 of 101Obstacles
User RecommendationsMonitored (for changes in state or model drift, need for redeployment or retraining,
etc.).■
Paired with lower latency inference technologies at the edge enabling faster decision
making.■
Many of these applications are in trial phases and broad adoption is a few years
away. Some use cases with comparatively mature system stacks (for example,
surveillance cameras or video recognition systems) are closer to productization.■
Most use cases require a high degree of contextualization and customization. For
most enterprises, edge AI software will offer limited capabilities in self-serve
modalities. Maximizing value from edge AI software will require vendor or third-party
professional services, driving higher costs and consequently creating adoption
challenges.■
Edge AI is often deployed in operational environments — requiring coordination,
cooperation and communication between IT and OT or engineering teams which can
be challenging in some organizations.■
Deploy analytics closer to where the data is created and managed by curating AI
edge aligned with relevant streaming data ecosystems.■
Architect middleware that enables orchestration, integration and deployment of AI in
edge systems.■
Augment IoT analytics architectures with edge AI architecture patterns to take
advantage of data and model parallelism across IoT endpoints.■
Leverage model compression to push AI to brownﬁeld assets (MPU and MCU, lower
cortex) to create intelligence in aged assets. This has helped OEMs create premium
service contracts on equipment that has been end-of-life/end-of-sales for years.■
Select edge AI software technologies that support the broadest set of relevant edge
AI architectural patterns (e.g., swarm, control, graph).■
Organizations need to develop capabilities for better IT-OT collaboration and
coordination for effective edge AI strategies.■
G00770252 Page 13 of 101Sample Vendors
Amazon Web Services (AWS); C3 AI; Eurotech; FogHorn; Google; Hewlett Packard
Enterprise (HPE); IBM; Intel; Microsoft; NVIDIA
Gartner Recommended Reading
Survey Analysis: AI on the Edge Demonstrates Practical Value in IoT Projects
Emerging Technologies: Provider Positioning Patterns in Edge AI
Emerging Technologies: Use-Case Patterns in Edge AI
Emerging Technologies: Research Roundup for Edge AI as a Platform for Growth
Edge Management and Orchestration
Analysis By: Bob Gill
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Adolescent
Definition:
Edge management and orchestration (EMO) stacks provide discrete layers of control over
server and device management, network and security management, the infrastructure
software stack, and the applications themselves. A layered approach to edge
management and orchestration — which previously was delivered through monolithic
application stacks — supports the assembly of functional, full-featured orchestration
capabilities from subset-speciﬁc offerings.
Why This Is Important
The complex task of developing, delivering and managing distributed applications across
device types, networking models and infrastructure stacks has been solved by building
monolithic edge and Internet of Things (IoT) applications. Current edge commercial EMO
offerings often solve only subsets of the overall requirement. By breaking the task into
discrete layers that map to function-based offerings from component providers,
enterprises can assemble a complete stack, while avoiding lock-in.
G00770252 Page 14 of 101Business Impact
In the absence of available EMO components, edge applications will require costly and
time-consuming, custom-made, control stacks to be created repeatedly:
Drivers
Viable products have come to market in the past year, with most based on the integration
of open-source elements and a layered approach.
Drivers of a stack approach include:
Obstacles
Creating a broadly applicable set of services that are open enough to be modiﬁed, yet
integrated enough to appear as a single solution is an extremely complex balancing act.
Obstacles to creating such solutions include:EMO stacks provide functionality of initial provisioning, as well as ongoing
management and optimization simplifying operating at scale.■
A layered, integration-ready portfolio of EMO elements will simplify the assembly of
multivendor solutions based on common and open frameworks.■
The need for end-to-end management and orchestration of complex distributed
applications is holding back enterprise adoption of many edge applications beyond
the simple proof of concept (POC) phase.■
Enterprise planners are demanding ﬂexible architectures and stacks that protect
against lock-in and rising costs, while many edge applications have been built on
custom-made and proprietary stacks.■
Early examples of management and orchestration stacks tend to concentrate on
subsets of the overall solution, and often only address a subset of the task (e.g.,
device management).■
Endpoint device options, interfaces and APIs are still emerging ■
A lack of standards beyond simple protocols ■
Most current options offer isolated and often monolithic silos, leading to a lack of
awareness of other layers■
G00770252 Page 15 of 101User Recommendations
Single-Board Edge Computers
Analysis By: Tony Harvey
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Single-board edge computers are small, very low-cost general-purpose systems that
perform functions such as anomaly detection or AI inferencing e.g., image recognition at
the edge. Based on a system-on-chip (SoC) solution, single-board computers are designed
with the minimum capability to perform the tasks required. I/O interfaces will vary but at a
minimum include a wired or wireless network interface.Inconsistent approaches and feature depth driven in part by a lack of clarity about
who the target buyer or implementor is: infrastructure and operations (I&O) teams or
application developers?■
Break down management and orchestration tasks into functional layers — each
addressing a foundational module of the deployment, such as physical
infrastructure, networking, security, software stack and the applications themselves.
This pertains especially to enterprises building solutions themselves.■
Examine the solution to ensure that each of these layers are being addressed —
either through modules or functionality built into a monolithic application. This is
especially relevant to enterprises implementing externally developed applications, or
those offered as a service.■
Factor EMO into management and governance planning sections of the edge
strategy.■
Ask every app and systems provider how they map to the model and ensure
ﬂexibility, as well as overall coverage.■
Employ a layered approach to ensure that all needs are met in a mix-and-match
approach.■
G00770252 Page 16 of 101Why This Is Important
Single-board edge computers enable general purpose compute devices to be located as
close to the edge device as possible. This enables, for example, low-cost image
recognition of dials and gauges so that they can be continuously and automatically
monitored, rather than requiring periodic human inspection. The low cost and easy
programmability will enable more widespread edge computing services across a broad
range of solutions.
Business Impact
Single board computers provide an intermediate form factor that can be positioned at the
edge, but are less expensive than standard “servers,” providing cost-effective edge
processing in more locations. They provide lower cost intelligence for automation,
monitoring and reporting of edge-located sensors and equipment that could not otherwise
be managed due to cost constraints. This improves efﬁciency and enables better decision
making, delivering both bottom-line and top-line beneﬁts.
Drivers
Cost and scale: Edge locations require devices in tens or hundreds of thousands of
locations to provide programmable real-time/low latency responses to sensors and
actuators. Single board edge computers provide a low-cost, highly programmable
solution.■
Automated monitoring of unintelligent devices: Many manufacturing and process
control solutions are designed for human inspection. Single board edge computers
can provide low cost computer vision and recognition solutions to automate
monitoring for dials and gauges that require visual monitoring.■
Speed to value: Single board edge computers are low cost and use high-level
programming languages that make it easy to prototype and deliver solutions. In
addition many are integrated into existing AI frameworks e.g., NVIDIA VisionWorks
for image recognition or eKuiper for streaming analytics.■
Edge AI: New AI-optimized inferencing chipsets and application-speciﬁc integrated
circuits (ASICs) are being incorporated into single board edge computers. Gartner’s
2021 Emerging Technologies Impact Radar identiﬁes a range of AI-enabled edge
technologies that are poised to broadly disrupt markets in the coming decade,
including edge computer vision, neuromorphic computing, advanced swarm
intelligence, the orbital edge and Blockchain for IoT (see Emerging Technologies
Impact Radar: Edge AI).■
G00770252 Page 17 of 101Obstacles
User Recommendations
Sample Vendors
Arduino; Digilent; NVIDIA; Raspberry Pi Foundation; TI; VersaLogic
Gartner Recommended Reading
Tech Providers 2025: The Future of Edge
Infographic: Understanding Edge Computing
Emerging Technologies Impact Radar: Edge AILimited application portability: A wide variety of manufacturers and a lack of
platform standards may constrain deployment of edge applications to speciﬁc
vendor platforms.■
Methods for managing and updating a very large number of distributed devices are
relatively immature in the IT/OT space.■
Enabling remote automated control of systems that control industrial processes also
opens up the potential for security breaches on systems where failures can result in
injury or loss of life.■
Evaluate the use of single-board edge computers for edge projects where a large
number of low-cost devices will be required to provide capabilities such as data
processing, anomaly detection, image recognition, voice recognition or AI inferencing
capabilities.■
Choose single-board edge computers that can be rolled out rapidly, without skilled
staff on-site, that can easily be managed and updated in the ﬁeld.■
Build security into the system and evaluate potential vendors for security across all
areas, including physical, data storage, communications, management and updates.■
Use systems with ASICS that support existing Internet of Things (IoT) and artiﬁcial
intelligence (AI) frameworks such as NVIDIA VisionWorks or, Amazon SageMaker
Neo when selecting a single-board edge computer.■
G00770252 Page 18 of 101Private 5G
Analysis By: Sylvain Fabre
Benefit Rating: High
Market Penetration: Less than 1% of target audience
Maturity: Adolescent
Definition:
A private 5G network is based on 3rd Generation Partnership Project (3GPP) technology
and spectrum to provide uniﬁed connectivity, optimized services and security for
enterprises. A 5G private mobile network (PMN) is used to interconnect people and also
things in an enterprise, and consists of private infrastructure. Deployments can be hybrid
with on-premises radio and local breakout, and connections to the telco core, and/or on a
public cloud or fully on-premises.
Why This Is Important
Multiple verticals will require 5G PMN deployments to realize the full effect of their digital
transformation initiatives. It can provide the required functionality, with guaranteed
performance levels, earlier than communications service providers (CSPs) can offer on
their public infrastructure. Distinct from the public network, it supports voice, video,
messaging, data and IoT with higher performance requirements and can optimize cost or
connectivity (e.g., cheaper than Wi-Fi for large area coverage).
Business Impact
Private 5G enables transformational digital use cases for industry, especially in
conjunction with other technologies, such as factory digital twin or edge AI for computer
vision. 5G PMN can offer enterprises improved security, independence and enable
efﬁciency gains. For example, BMW Brilliance Automotive (BBA, BMW’s joint venture in
China) claims complete 5G coverage in all of its factories, with speeds over 1 Gbps, and is
exploring 5G Edge and AI use cases.
G00770252 Page 19 of 101Drivers
Applicability to connected industrial applications — While 5G standards are deﬁned
by 3GPP, other bodies are now contributing. For example, the 5G Alliance for
Connected Industries and Automation (5G-ACIA) or the 5G Automotive Association
(5GAA).■
Requirement for full, reliable network coverage for machines, sensors and
equipment, including indoor, outdoor, ofﬁce and industrial areas at lower cost than
Wi-Fi.■
Performance proﬁle for demanding industrial use cases is a key justiﬁcation for
deploying a private 5G network, in particular when low-latency, high-bandwidth,
especially uplink communication and reliability are required.■
A private setup is required if network performance requirements exceed the
capabilities of the shared public infrastructure.■
There is also another class of use cases, not focused on mobility initially but
requiring a high-performance backbone where wiring is complex and costly — such
as in a factory deployment.■
Interest from telecommunications service providers (TSPs) that can offer 5G PMN to
various verticals, such as I4.0 factory automation, mining, oil, utility and railroad
companies. IoT providers, universities, stadiums and so on are thereby expanding
into industries and generating new revenue.■
Alternative provider types beyond the CSPs, such as integrators, infrastructure
vendors and hyperscalers, all of which are driving new deployments and POCs.■
Some enterprises deploy private networks because they want to run their network
more independently, as their own infrastructure, with limited outside dependency. For
example, data privacy can be a key concern that requires more control and visibility
over their data, with data loss prevention (DLP) security controls in place to ensure
that sensitive information does not leave the enterprise perimeter.■
Some defense and government clients have indicated a wish to have more control
and visibility into the vendors involved in the mobile services provision, which can be
an issue over a shared public network built and managed by a CSP.■
G00770252 Page 20 of 101Obstacles
User Recommendations
IT leaders should:Unclear business models and value justiﬁcation vs. alternatives (e.g., 4G PMN) ■
Perception that real value begins from 3GPP R16, and that maturity and availability
of R16 solutions is still a work in progress, for example, with network slicing■
Complex deployment and operation ■
Networks and endpoints cost ■
Lack of outcome-based pricing models ■
Spectrum availability and/or cost in some countries ■
Perception of risk regarding timing and relevance of private 5G ■
Feedback from some industrial clients mentioned that the majority of their use cases
could be serviced by a 4G private network, and/or NarrowBand-Internet of Things
(NB-IoT) and other low-power wide-area network (LPWAN), such as LoRaWAN.■
Differentiate from other providers, like large equipment vendors, system integrators
(SIs), resellers, smaller specialist network vendors and hyperscalers, by integrating
PMN with other functions like supplier information management (SIM), IoT
platforms, edge computing, design and managed services, and national roaming.■
Co-create networks by partnering with SIs and consultancies that have the required
industry skills for design, deployment, and managed services engineering headcount
and evaluation test bed environments. For example, build manufacturing 5G PMN
with connectivity, security and AI capabilities.■
Design licensed and unlicensed/shared spectrum options where available. ■
Supplement your engineering teams by working with IT service providers. Do not
expect or plan on public 5G replacing WLAN in large portions of your environment.
Instead, IT leaders should select private 5G for specialized use cases with large
coverage areas and known application performance requirements.■
G00770252 Page 21 of 101Sample Vendors
AT&T; Athonet; China Mobile; Druid Software; Ericsson; Huawei; Nokia; T-Mobile;
Vodafone; ZTE
Gartner Recommended Reading
Creating Your Enterprise 4G and 5G Private Mobile Network Procurement Strategy and
RFQ
Architecting a Reference Framework for 5G Private Mobile Networks
Market Guide for 4G and 5G Private Mobile Networks
4 Hype Cycle Innovations That Should Be on the Private Mobile Networks Roadmap for
5G Security, CSP Edge and Slicing
Market Guide for 5G Network Ecosystem Platform Providers
Distributed Cloud
Analysis By: David Smith, Milind Govekar, Daryl Plummer
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Distributed cloud refers to the distribution of cloud services to different physical locations,
while operation, governance, updates and evolution of the services are the responsibility
of the originating cloud provider.
Why This Is Important
Distributed cloud enables organizations to use consistent cloud-based services wherever
needed, while the cloud service provider retains the responsibility of managing the
technology, implementation and evolution of the capabilities. It gives organizations the
ﬂexibility to support use cases that will beneﬁt from cloud services — regardless of their
dependence on speciﬁc locations. Organizations can use distributed cloud to reimagine
use cases where cloud computing is not currently feasible.
G00770252 Page 22 of 101Business Impact
A major notion of the distributed cloud concept is that the provider is responsible for all
aspects of delivery, and manages the distributed capabilities “as a service.” This restores
cloud value propositions that are broken when customers are responsible for a part of the
delivery, as is true in some hybrid cloud scenarios. The cloud provider must take
responsibility for how the overall system is managed and maintained. Otherwise, the
value proposition of distributed cloud is compromised.
Drivers
Distributed cloud computing is a style of cloud computing where the location of
cloud services is a critical component of the model.■
Historically, location has not been relevant to cloud computing deﬁnitions. In fact,
the variations on cloud (e.g., public, private, hybrid) exist because location can vary.■
It is a misconception that private cloud or hybrid cloud requires on-premises
computing, as they do not require private components for any speciﬁc location. With
the advent of distributed cloud, location formally enters the deﬁnition of a style of
cloud services.■
There are services outside of the provider’s data center for both approaches. That is
the key distinction, and shared with distributed cloud.■
In hyperscale public cloud implementations, the public cloud is the center of the
universe. There has been distribution of cloud services through worldwide regions in
public cloud practically since its inception. The major hyperscale cloud providers
have different geographic regions around the world, and all are centrally controlled
and managed, and provided by the public cloud provider. Most instances of
distributed cloud are distributed from public cloud providers. Alternative providers
are possible, but likely to be rare.■
Distributed cloud supports both tethered and untethered operations of like-for-like
cloud services from the cloud provider, “distributed” out to speciﬁc and varied
physical locations. This enables an important characteristic of distributed cloud
operation — low-latency compute where the compute operations for the cloud
services are closer to those that need the capabilities. This can deliver major
improvements in performance, as well as reduce the risk of global network-related
outages.■
Data sovereignty and other regulatory issues. ■
G00770252 Page 23 of 101ObstaclesPerceived and real security and privacy concerns with off-premises applications and
infrastructure.■
Latency needs of IoT/edge applications. ■
Distributed cloud is still a single-cloud provider, and the managed cloud assets are
still part of the cloud provider’s portfolio.■
Disconnected operations. ■
Customers can’t abandon existing technologies in favor of complete and immediate
migration to the public cloud, due to sunk costs, latency requirements, regulatory and
data residency requirements — and the need for integration.■
Different approaches to distributed cloud have different value propositions (e.g.,
portability, software, appliance).■
Distributed services are a relatively small subset of the centralized services and will
take time to expand and may never reach 100% parity.■
Distributed cloud in your data center will have limits to scale and elasticity that do
not exist with the centralized public cloud.■
More advanced approaches like distributed cloud embedded in networking or
telecom equipment — or delivered as metro area services — are very immature.■
The application and data architecture needs to take into account the different
compute components. Using a distributed cloud requires distinct attention from an
architectural perspective.■
G00770252 Page 24 of 101User Recommendations
Sample Vendors
Amazon Web Services; Google; IBM; Microsoft; Oracle
Gartner Recommended Reading
Top Strategic Technology Trends for 2021: Distributed Cloud
‘Distributed Cloud’ Fixes What ‘Hybrid Cloud’ Breaks
The Cloud Strategy Cookbook, 2021
Public Cloud for Mobile Edge
Analysis By: Sylvain Fabre
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: AdolescentAdopt distributed cloud by overcoming the fear of a single franchise controlling the
public cloud and on-premises cloud estates.■
Use the distributed cloud model to prepare for the next generation of cloud
computing by targeting location-dependent use cases, such as low latency, tethered
scale and data residency, that are enhanced by using a distributed cloud model.■
Identify scenarios where distributed cloud use-case requirements can be met by
evolution of a hybrid cloud model and where the requirements are substantially
different.■
Distributed cloud should be a preferred model (over private cloud). ■
G00770252 Page 25 of 101Definition:
ETSI MEC ISG speciﬁes requirements for multivendor, multiaccess CSP edge computing
environments. This infrastructure is deployed in the CSP network that hosts cloud
compute, storage and networking, runs telecom workloads and can be CSP-owned, or
public-cloud-managed. It can be deployed at the cloud provider’s edge or within a telecom
CSP’s network, and host a variety of telecom applications and workloads — from the
telecom back ofﬁce to the mobile packet core and edge.
Why This Is Important
Many technologies and skills are needed to deliver and orchestrate applications and
services that are cloud-based and telecom-network-enabled. CSPs and public cloud
providers can offer the capabilities required for these needs. Public cloud for mobile edge
implements a mobile edge cloud architecture for core telecom network functions and
value-added services on public cloud edge infrastructure, deployed within the carrier
network itself, in partnership between CSPs and public cloud providers.
Business Impact
Adoption of public cloud for mobile edge computing is impactful in several ways:
Provides a strong alternative to traditional vendors with lower infrastructure
commitment and a mature technology partner ecosystem.■
Public cloud providers can reach new markets for mobile and real-time edge use
cases in B2B and B2C realms.■
The ISVs deploying over that infrastructure are the real OTT providers, and MEC
allows carriers to control the point of distribution and capture margin from these
OTT providers.■
G00770252 Page 26 of 101Drivers
ObstaclesHCPs are increasingly moving into the telecom space, by adapting their IaaS and
PaaS infrastructure to better integrate with and complement telco CSP network
environments. This is so that they can “tether” the mobile domains to the public
cloud and manage them from there, at least partially.■
“Edge PaaS” platforms are emerging in the market (e.g., Avassa, F5 Distributed
Cloud Platform, etc.) that provide the basis for building a next generation of highly
distributed, edge-aware applications that are optimized for 5G networks, using
standard cloud tools and techniques.■
Digital business solutions for CSPs and their enterprise clients increasingly demand
faster data distribution and reduced latency, requiring more data management and
processing outside of cloud and traditional data center environments.■
Bandwidth and bandwidth costs will improve, but more slowly than the cost of
computing and the rate of data growth.■
Deep learning at the edge positions the edge device as an algorithmic or model-
based intelligent data capture mechanism for further upstream processing and
analytics.■
Public cloud for mobile edge solutions increase integration burden and security risk,
as they involve complexity and multivendor scenarios. This includes the CSP, the
public cloud provider, a third-party mobile application ISV, a system integrator, carrier-
neutral access network providers and software-deﬁned cloud interconnection
providers.■
Sharing of revenue, margins and customer control between these parties will be
obstacles. CSPs will be wary of taking a heavy dependence on a cloud provider
without assurance of appropriate revenue sharing for the OTT services delivered on
the provider’s platform. Cloud providers will be wary of giving CSPs control over their
larger enterprise customers.■
Some CSPs are cautious of partnerships with public cloud providers due to
perceived risk for CSPs when public cloud providers compete in providing private
mobile networks to enterprises. Interventions by regulators, who may be concerned
about hyperscalers’ market power, could slow down adoption.■
G00770252 Page 27 of 101User Recommendations
Sample Vendors
Alibaba Cloud; AWS; Google; HPE; IBM; Microsoft; VMware
Gartner Recommended Reading
Market Guide for 5G Network Ecosystem Platform Providers
Creating Your Enterprise 4G and 5G Private Mobile Network Procurement Strategy and
RFQ
Edge AI Hardware
Analysis By: Alan Priestley
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: EmergingFocus on business outcomes that can be achieved through leveraging the public
cloud enterprise channel and developer ecosystem.■
Increase differentiation by enabling integration of the edge computing offering from
the public cloud providers into private mobile network solutions for industries.■
Retain long-term value by designing an edge architecture with edge public cloud
providers so that the CSP can secure the most value from enterprise networks and
related IoT data.■
Preempt competing offers by cloud providers for mobile private networks by
enabling complementary solutions such as “slice as a service” to cloud providers
and mobile virtual network operators (MVNOs), using both shared and licensed
spectrum options.■
Leverage what public cloud providers bring with something the telco CSPs have
never been able to develop such as a truly horizontal application hosting and
management environment.■
G00770252 Page 28 of 101Definition:
Edge artiﬁcial intelligence (AI) hardware comprises a wide range of systems, add-in
boards and chips designed and optimized to execute deep neural network (DNN)-based
data analytics applications within edge hardware infrastructure deployments.
Why This Is Important
Many edge computing and endpoint deployments require the use of applications that
leverage DNN-based algorithms to analyze complex datasets (such as video, images,
audio or sensor data) close to their point of capture. The processing required by these
applications is often beyond that which can be easily supported by traditional
microprocessors and microcontrollers. Consequently, a wide range of new edge AI
hardware designs optimized speciﬁcally for these tasks are being developed.
Business Impact
Edge AI hardware bring signiﬁcant business beneﬁt in designs that:
Drivers
Edge AI hardware enables data analysis to be undertaken at — or close to its point of —
capture.Require sophisticated AI or machine learning (ML)-based applications to be executed
in low-power edge computing and endpoint devices.■
Need local analysis of captured data. ■
Demand low-latency decision making. ■
Have limited, intermittent or costly connectivity to central data centers. ■
Face governance or regulatory restrictions on the type of data that can be captured
and stored.■
A key element of many system designs is being able to interpret this data and make
decisions on the data content in a timely and efﬁcient manner. However, data
volume and complexity captured by edge computing and endpoint devices are
increasing.■
G00770252 Page 29 of 101Obstacles
To ﬁt within the power and price constraints, edge computing and endpoint device
deployments require optimized AI hardware and have the potential to impact overall
analytics performance.For many deployments it may not be practical to transfer this data to the cloud or a
remote data center and utilize hosted analytics services. This is especially the case
where low latency, interactivity, autonomy, privacy and security are required, or
communication services may be intermittent or expensive.■
Recent developments by semiconductor vendors are delivering chips optimized to
efﬁciently execute DNN-based analytics. Managing this within constrained power
and form factor has made it viable to deploy AI-based analytics into a wide range of
edge computing and endpoint applications.■
Edge AI hardware can be utilized in many deployments that would previously have
required transferring the data to the cloud or remote data center in order to analyze.
Example use cases include video surveillance, facial/gesture recognition, factory
automation, voice response/control and monitoring a wide range of sensor data.■
Edge AI hardware can complement the processing capabilities deployed in edge
computing and endpoint devices freeing resources for local decision making and
process control.■
This may limit the range of deployments where this equipment can be utilized,
necessitating the use of cloud or data-center-based service to implement the desired
analytics functions.■
Many DNN-based applications are developed within data centers using high-
performance graphics processing unit (GPU) based systems. This may limit the
ability of these applications to be deployed on edge AI hardware.■
Even if the edge AI hardware is GPU-based, applications may still require
optimization before they can be deployed in lower-performance non-data-center
locations to ensure latency/responsiveness targets are met.■
G00770252 Page 30 of 101User Recommendations
Sample Vendors
Advantech; Dell; Gyrfalcon Technology; Hailo; Intel; NVIDIA
Gartner Recommended Reading
Emerging Technologies: Neuromorphic Computing Impacts Artiﬁcial Intelligence Solutions
Emerging Technologies: Critical Insights on AI Semiconductors for Endpoint and Edge
Computing
Forecast: AI Semiconductors, Worldwide, 2020-2026
Emerging Technologies and Trends Impact Radar: Artiﬁcial Intelligence, 2021
Cloud-Out to Edge
Analysis By: Ed Anderson, Bob Gill
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: AdolescentDetermine the need to leverage edge AI hardware by assessing the types and
complexity of data to be analyzed.■
Simplify development workﬂow by leveraging cloud-based services and toolsets. ■
Evaluate deployment via the use of cloud and data-center-based AI services versus
local data analytics using edge AI hardware. Take into account the sophistication of
data-center-based analytics services versus those that can be implemented using
edge AI hardware on remote decision making.■
Evaluate the cost, availability and reliability of data communication to remote
locations.■
Examine the regulatory and governance impacts of where data is stored and
analyzed.■
G00770252 Page 31 of 101Definition:
Cloud-out to edge describes an architectural construct where a centrally managed cloud
environment, typically using a hyperscale cloud, establishes cloud service capabilities that
are extended to edge environments. In a cloud-out to edge architecture the cloud control
plane, including security, identity and access management, governance, operations,
programming models and interfaces, and other control elements, originate in the cloud
and are then instantiated at the edge.
Why This Is Important
The move to public cloud drives centralization of operating processes, including the
controls used to govern the environments. Cloud-out to edge is an architectural construct
that supports the extension of public cloud control models to edge environments. Cloud-
out to edge complements edge-in to cloud models. Cloud-out to edge is popular when
organizations standardize IT operational control through centralized, public cloud
environments.
Business Impact
IT environments are growing in complexity due to the adoption of public cloud services.
Without some unifying capabilities, the complexity creates operational risks, including
cost increases. Cloud-out to edge models extend cloud capabilities, including the cloud-
based control plane, to other environments, including systems operating at the edge.
Extending public cloud capabilities, including operational control, to edge environments
can be a means to address the complexities of distributed hybrid environments by
unifying IT operations under a uniﬁed operational framework.
G00770252 Page 32 of 101Drivers
ObstaclesAdoption of hyperscale public cloud services continues unabated. Gartner predicts
continued growth in public cloud adoption with IT spending rates on public cloud
services expected to grow almost 20% through 2026.■
Cloud operations have become critical for most organizations, driving increased
investment in tools and skills in cloud management practices.■
Centralized, hyperscale cloud services are not well-suited for all application
scenarios, particularly those better-suited to run at the edge. This creates an
architectural divide between cloud and edge, which impacts operations,
programming interfaces, security, identity and access, and application compatibility.■
Distributed architectures, including edge computing and distributed cloud, can
beneﬁt from the distribution of cloud services from centralized environments to
edge. Cloud-out to edge models can extend the cloud control plane to provide
management, governance and oversight to edge environments.■
Standardizing technologies around cloud technologies and unifying operations
using the cloud control plane can reduce complexity and help manage costs.■
Cloud-out to edge methodologies assume a centralized cloud system. Organizations
that are immature in their cloud adoption should establish a foundational public
cloud presence before pursuing cloud-out to edge approaches.■
Cloud-out to edge assumes the standardization of architecture, technologies and
operational control using a single, centralized cloud service. While there may be
value in extending public cloud capabilities to edge environments, this approach can
drive increased dependence on a single cloud provider.■
Multicloud strategies, which are common with most organizations, may conﬂict with
the cloud-out to edge approach. Cloud-out to edge drives uniﬁcation of technology,
architecture and control to a single cloud environment, which may conﬂict with an
organization’s stated multicloud preference.■
Cloud provider offerings purported to support cloud-out to edge implementations are
still maturing, and generally struggle to deliver the full beneﬁts of distributed cloud
approaches.■
G00770252 Page 33 of 101User Recommendations
Sample Vendors
Alibaba Cloud; Amazon Web Services; Google; IBM; Microsoft; Oracle; VMware
Gartner Recommended Reading
Cloud and Edge Infrastructure Primer for 2022
Cloud-Out and Edge-In: How Cloud Service Providers Can Leverage the Two Edge
Computing Architectures
A Guide to Distributed Cloud: The Next Frontier of Cloud Computing
Top Strategic Technology Trends for 2021: Distributed Cloud
Streamline Digital Delivery With Cloud-to-Edge Computing
Neuromorphic Computing
Analysis By: Alan Priestley
Benefit Rating: Transformational
Market Penetration: Less than 1% of target audienceFocus on the needs of use cases operating at the edge to determine whether an
edge-in to cloud or a cloud-out to edge approach will work best.■
Establish a comprehensive cloud and edge strategy to guide cloud-out to edge and
edge-in to cloud implementations. Let business value and operational beneﬁts lead
cloud and edge decisions.■
Build strong, centralized cloud operating capabilities before pursuing cloud-out to
edge strategies. Cloud competencies will be critical to achieving success in cloud-out
to edge implementations.■
Assess the risks and beneﬁts of a cloud-out to edge approach, particularly if you
have a stated multicloud strategy. Cloud-out to edge approaches can potentially
complicate or compromise multicloud strategies.■
Seek expert help from system integrators and managed service providers with
expertise in both cloud and edge environments.■
G00770252 Page 34 of 101Maturity: Embryonic
Definition:
Neuromorphic computing leverages semiconductor devices inspired by neurobiological
architectures. Neuromorphic processors feature non-von Neumann architectures and
implement spiking neural network execution models that are dramatically different from
traditional processors. They are characterized by simple processing elements, but very
high interconnectivity.
Why This Is Important
Currently, most AI development leverages parallel processing designs based on GPUs.
These are high-performance, but high-power-consuming, devices that are not applicable in
many deployments. Neuromorphic computing utilizes asynchronous, event-based designs
that have the potential to offer extremely low power operation. This makes them uniquely
suitable for edge and endpoint devices, where their ability to support object and pattern
recognition can enable image, audio and sensor analytics.
Business Impact
AI techniques are rapidly evolving, enabled by radically new computing designs.
Today’s deep neural network (DNN) algorithms require the use of high-performance
processing devices and vast amounts of data to train these systems, limiting scope
of deployment.■
Neuromorphic computing designs can be implemented using low-power devices,
bringing the potential to drive the reach of AI techniques out to the edge of the
network, accelerating key tasks such as image and sound recognition.■
G00770252 Page 35 of 101Drivers
ObstaclesNeuromorphic computing leverages the concept of spiking neural networks (SNNs)
to model a biological brain.■
Different design approaches are being taken to implement neuromorphic computing
designs — large-scale devices for use in data centers, and smaller-scale devices for
edge computing and endpoint designs. Both these paths implement asynchronous
designs that have the beneﬁt of being extremely low power when compared with
current DNN-based designs.■
Semiconductor vendors are developing chips that utilize SNNs to implement AI-
based solutions.■
Neuromorphic computing architectures have the potential to deliver extreme
performance for use cases such as DNNs and signal analysis at very low power.■
Neuromorphic systems can be trained using smaller datasets than DNNs, with the
potential of in situ training.■
Accessibility: GPUs are more accessible and easier to program than neuromorphic
computing. However, this could change when neuromorphic computing and the
supporting ecosystems mature.■
Knowledge gaps: Programming neuromorphic computing will require new
programming models, tools and training methodologies.■
Scalability: The complexity of interconnection challenges the ability of
semiconductor manufacturers to create viable neuromorphic devices.■
Integration: Signiﬁcant advances in architecture and implementation are required to
compete with other DNN-based architectures. Rapid developments in DNN
architectures may slow advances in neuromorphic computing, but there are likely to
be major leaps forward in the next decade.■
G00770252 Page 36 of 101User Recommendations
Sample Vendors
ABR; AnotherBrain; BrainChip; GrAi Matter Labs; Intel; Natural Intelligence; SynSense
Gartner Recommended Reading
Emerging Technologies: Neuromorphic Computing Impacts Artiﬁcial Intelligence Solutions
Emerging Technologies: Critical Insights on AI Semiconductors for Endpoint and Edge
Computing
Emerging Technologies: Top Use Cases for Neuromorphic Computing
Forecast: AI Semiconductors, Worldwide, 2020-2026
Emerging Technologies and Trends Impact Radar: Artiﬁcial Intelligence, 2021Prepare for future utilization as neuromorphic architectures have the potential to
become viable over the next ﬁve years.■
Create a roadmap plan by identifying key applications that could beneﬁt from
neuromorphic computing.■
Partner with key industry leaders in neuromorphic computing to develop proof-of-
concept projects.■
Identify new skill sets required to be nurtured for successful development of
neuromorphic initiatives, and establish a set of business outcomes/expected value
to set management’s long-term expectations.■
G00770252 Page 37 of 101At the Peak
CDN Developer Edge Node
Analysis By: Bob Gill
Benefit Rating: Moderate
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
CDN providers continue to provide developer tools, APIs and runtime environments at their
edge nodes. CDN developer edge nodes provide either stateful- or stateless-based
processing environments that support the execution of specialized code. Technologies
deployed in CDN edge nodes include containers, Kubernetes clusters, key value stores,
NoSQL databases and Chrome V8 engines.
Why This Is Important
CDN providers have been on a long, slow path to manage their investments in commodity
networks and value-added services. Severless and containerized microservices
architecture has gained considerable enterprise popularity as a way to build cloud-native
applications and enable continuous delivery. As developers venture into applications
requiring low latency and high performance, edge environments that can support some
degree of stateful processing will become another valuable endpoint.
Business Impact
Adoption of CDN and edge-based serverless architecture will:
DriversLead to increased scalability, reduced costs and faster time to market for IT-
supported business initiatives■
Help achieve the true beneﬁts of cloud-native operations and create a more
consistent and manageable environment for cloud operations■
Require adjustments to the organization’s practices and strategies around security,
application development, infrastructure and operations, mandating closer
coordination■
G00770252 Page 38 of 101Obstacles
Obstacles for adoption could include:
User Recommendations
Sample Vendors
Akamai; Cloudﬂare; Fastly; Macrometa; Rafay Systems; StackPath
Gartner Recommended ReadingAs many enterprises deploy applications and workloads that cannot tolerate latency
delay (4K video, gaming and trading), consumers of CDN will begin to look beyond
content delivery and engage in developer edge services.■
Gartner sees typical edge use cases, including geographic fencing, A/B testing and
stage migrations. Developer edge offerings from CDNs vary based on maturity, so
enterprises should consider test and Q/A projects as candidates for deployment.■
Disparate technology stacks that fail to deliver performance and low latency. ■
Regional compliance and privacy considerations that preclude code or data from
existing outside a speciﬁc location.■
Failure of vendors to support multiple development languages beyond JavaScript. ■
Preset limits around compute or storage that limit use cases’ applicability. ■
Stagnated development by vendors to develop innovation roadmaps around edge
delivery.■
Lack of API and container registries. ■
Evaluate CDN developer edge services for use cases extending cloud-native
applications, microservices implementations or service integrations to achieve
improved productivity and cost efﬁciency.■
Avoid CDN edge delivery if the project requires ﬁne-grain control over application
infrastructure operations, or where cost estimates are excessive.■
Choose a CDN provider that has both edge functionality and developer integrations
for developer-led workloads. Ensure that your provider shares developer tooling
roadmaps and beta product invitations.■
G00770252 Page 39 of 101Market Guide for Global CDN
Emerging Technologies: IoT Platforms at the Edge Needs Secured and Composable
Architectures/Design
Edge PaaS
Analysis By: David Wright
Benefit Rating: Moderate
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Edge PaaS, or platform services at the edge, is an application platform-as-a-service
infrastructure designed to run in capacity-constrained edge environments and support
latency-sensitive and edge-aware applications, such as interactive gaming, network
function virtualization, local device management and real-time analytics.
Why This Is Important
Standard cloud PaaS platforms are not optimized for edge environments. Edge PaaS
platforms adapt the beneﬁts of the cloud to the constraints of the edge. They make
possible a new type of edge-native, highly distributed and cloud-aware application
architecture by offering:
Business Impact
Edge PaaS will enable new enterprise and vendor opportunities:Container-based platform capabilities, running on lean edge hardware servers ■
Autonomous, self-organizing physical sites ■
Cloud-based application management ■
Location-aware security ■
Compatibility with cloud-native tools for application development and delivery ■
G00770252 Page 40 of 101Enterprises can leverage PaaS services at the edge that are consistent with public
cloud standards■
Cloud providers and ISVs can extend their products to support edge use cases and
5G networks■
Enterprises can use cloud-native tools to build and deploy custom solutions at the
edge■
Telecom providers, hardware OEMs and managed service providers (MSPs) can
resell and support edge cloud solutions■
G00770252 Page 41 of 101Drivers
An increasing number of viable business use cases at the edge. Gartner’s current
ﬁve-year analysis of the Edge Infrastructure market forecasts high growth in areas
such as distributed business processing, device control, personal monitors and
immersive experiences (see Forecast Analysis: Edge Hardware Infrastructure,
Worldwide).■
Programmable edge hardware. Resource-efﬁcient chipsets and programmable
function accelerator cards (FACs) are being packaged into lightweight edge servers.
These enhancements give edge applications access to hardware acceleration for
efﬁcient networking, security and data processing.■
Kubernetes at the edge. CNCF-compatible initiatives, such as MicroK8S, K3S and
KubeEdge — as well as emerging third-party edge container platforms — will place
the edge within easy reach of a vast number of cloud developers who will program
the next generation of solutions.■
Distributed cloud computing. Public cloud providers are offering distributed
infrastructure solutions capable of supporting globally scalable “cloud-tethered”
applications. These applications will deploy into geographically dispersed edge
clusters, interact with their local environments in microsecond time, share collective
state across a wide-area network and “manage themselves” by tethering to public
cloud regions for updates and other services as needed.■
Edge AI. New AI-optimized inferencing chipsets and ASICs are being incorporated
into programmable edge hardware. Gartner’s 2021 Emerging Technologies Impact
Radar identiﬁes a range of AI-enabled edge technologies that are poised to broadly
disrupt markets in the coming decade, including edge computer vision,
neuromorphic computing, advanced swarm intelligence, the orbital edge and
Blockchain for IoT.■
G00770252 Page 42 of 101Obstacles
User Recommendations
Sample Vendors
Avassa; F5 Distributed Cloud Platform; Platform9; Section; SpectroCloud; Sunlight.io;
SUSE Rancher K3S
Edge Stream Analytics
Analysis By: Paul DeBeasi
Benefit Rating: HighLack of edge-optimized application development tools. Current enterprise tools do
not yet fully support development, and management of next-generation “edge-
native” applications.■
Incomplete platform services. If independent edge PaaS vendors do not allow for
sufﬁcient extensibility and/or integration with public cloud services, they may
struggle to offer a complete value proposition.■
Limited application portability. Lack of platform standards may constrain
deployment of edge applications to speciﬁc vendor platforms.■
Lack of standard interfaces for hardware acceleration. Without common FAC
interfaces and drivers, applications may not easily deploy across diverse hardware
environments.■
Identify and prioritize edge business use cases that are best addressed by edge-
native cloud applications, and build early prototypes using Edge PaaS technologies
that validate this approach.■
Align your Edge PaaS platform and technology selection process to your existing
cloud strategy, to ensure consistency in services and tools.■
Vet the process, not the products: quantify the early potential of Edge PaaS by using
cloud-native toolchains and agile DevOps methodologies to integrate and deliver test
applications onto the current early generation of vendor edge platforms.■
Develop a longer-term edge AI strategy by determining how edge computing and
edge AI affect your core business, and how you connect with customers.■
G00770252 Page 43 of 101Market Penetration: 20% to 50% of target audience
Maturity: Adolescent
Definition:
Edge stream analytics is a technology that drastically reduces the time from data
ingestion to business insight in comparison to batch-oriented architectures. Prior to the
emergence of edge stream analytics, data from industrial devices was ﬁrst stored in
repositories such as data lakes and historians, and then analyzed by business intelligence
applications. Edge stream analytics inverts the traditional model of store ﬁrst and then
analyze, to analyze ﬁrst and then store.
Why This Is Important
Digital transformation is distributing the enterprise to the edge where customers,
employees and enterprise assets are located. Gartner predicts that by 2025, more than
50% of enterprise-managed data will be created and processed outside the data center or
cloud. Edge stream analytics is a speciﬁc type of edge analytics that processes streams
of data that emanate from physical and digital sources (e.g., connected products,
industrial devices, ﬁnancial transactions).
Business Impact
Edge stream analytics accelerates business value creation by analyzing streams of data
as they are being ingested. It can improve overall equipment effectiveness (OEE) by
processing unstructured, high-volume, high-velocity, diverse, time-series data from
industrial equipment with low latency. Edge stream analytics can operate when the edge
is disconnected, thus supporting use cases that require the ability to operate
autonomously from other edge, cloud or data center locations.
Drivers
Proliferation of Industry 4.0 initiatives. ■
Integration of operational technology (OT) and information technology (IT). ■
Use cases that require autonomous edge operation. ■
Increasing need to produce data-driven business insight. ■
Greater availability of stream analytics platforms designed for small footprints. ■
Expanded deployment of edge computing technology. ■
G00770252 Page 44 of 101Obstacles
User Recommendations
Sample Vendors
Amazon Web Services (AWS); KX Insights; Microsoft; PTC; SAP; SAS; Software AG
Gartner Recommended Reading
Design IoT Stream Analytics From Edge to Platform
Market Guide for Event Stream Processing
5 Essential Practices for Real-Time AnalyticsLack of technical skills ■
Product immaturity ■
Product incompatibility ■
Integration complexity ■
Rapidly changing technology ■
Deﬁne speciﬁc use cases with measurable business outcomes. Edge stream
analytics initiatives fail when use cases are vague and desired outcomes are not
measured.■
Develop an edge stream analytics proof of concept on a cloud platform before doing
so at the edge. This approach will shorten your learning time because cloudstream
analytics products are more mature than edge stream analytics products.■
Invest in training on how to develop, integrate and use event stream analytics.
Include the operation, engineering and IT teams.■
Use a phased approach by starting small and then scaling up the implementation as
the team gains experience.■
Deﬁne your service-level requirements and capacity constraints. Your design must
have the capacity to ingest and process the streams.■
G00770252 Page 45 of 101Immersion Cooling
Analysis By: Jeffrey Hewitt, Philip Dawson
Benefit Rating: Moderate
Market Penetration: 1% to 5% of target audience
Maturity: Adolescent
Definition:
Immersion cooling is a type of data center server cooling system that immerses server
boards in a nonconductive heat transfer liquid, typically built using an immersion
container in a dense, closed system. Immersion systems deliver well-above-average power
efﬁciency, enabling compute systems to run at high performance while requiring less ﬂoor
space.
Why This Is Important
Immersion cooling shifts power from cooling to computing, potentially doubling the
compute density in power-constrained locations. Immersion cooling allows servers to
operate in constrained environments, such as 5G network control nodes and IoT edge
servers.
Immersion cooling signiﬁcantly increases the efﬁciency of enterprise servers by reducing
the need for air cooling. We estimate that fans account for 20% of server power
consumption, and 47% of power consumption in a typical (PUE 1.7) enterprise data
center.
Business Impact
Immersion cooling systems enable enterprises to deliver on sustainability and deploy
higher levels of compute capability to strategic locations than is possible with
conventional air-cooled racks. Key applications include data centers in facilities with
limited space, factory automation, edge data centers and data centers in remote or
unattended locations.
Immersion cooling is well-suited to the small remote data centers that will support 5G
mmWave deployments.
G00770252 Page 46 of 101Drivers
ObstaclesImmersion cooled systems are smaller, quieter and more efficient than traditional
rack systems. Their initial value will most likely come from outside the data center,
where they enable higher compute density at higher energy efﬁciency and lower
noise. Although the capital cost of the system is typically higher because of the
mechanical and cooling infrastructure involved, there are environments where these
systems outperform any alternative.■
Immersion cooling can recover expensive server space and power costs. For an
enterprise constrained to operate servers in expensive spaces, there is often a ﬂoor
power budget that covers both equipment and cooling. An immersion cooling
system could improve power efﬁciency by 40%, theoretically enabling 67% more
energy for computing within the same power budget. These systems may also
recover ﬂoor space, and are quiet enough that they need no sound bafﬂing.■
Immersion cooling enables edge servers to operate in otherwise hostile locations.
Medium-scale edge computing nodes or wireless telecom nodes often operate under
the thermal, spatial and power constraints of a remote server bunker, pole or closet.
Shipboard or truck-based mobile data centers also beneﬁt from these space and
power efﬁciencies. For certain GPU-centric small-scale supercomputing tasks, these
systems represent a practical on-premises solution. Isolation of the components
also facilitates their use in locations with high levels of particulate pollutants like
dust.■
Data centers must be replumbed for immersion cooling. Immersion cooling requires
redundant plumbing for the warm water loop. Immersion cooling is also most
efﬁcient when used with a passive heat exchanger.■
Cooling fluids require special handling. Immersion systems use vegetable oil or
ﬂuorocarbons. Oil-based systems require that staff handle and bag oil-coated
boards for repair or replacement. This requires skills normally not possessed by IT
administrators. Fluorocarbon systems operate with robotic, sealed pods as all ﬂuids
have to be recovered and contained.■
Nonstandard compact server motherboards deliver the best economics. To achieve
the ﬂoor space reductions that liquid cooling offers, most systems require smaller
motherboards as many systems are horizontal, rather than vertical. Vertical systems
can use standard motherboards and are well-suited to environments such as 5G
closets, where the vertical form factor is a better ﬁt.■
G00770252 Page 47 of 101User Recommendations
Sample Vendors
Asperitas; Green Revolution Cooling (GRC); Iceotope; Immersion Systems; QCooling;
Submer; TMGcore
Gartner Recommended Reading
How to Turn Old Data Centers Into Critical IT Assets
How to Create a Data Center Cost Model Suitable for Public Cloud Comparison
Edge as a Service
Analysis By: Bob Gill
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Edge as a service (EaaS) describes a model in which some or all of edge software and/or
hardware is offered via provider-owned and operated assets, requiring little to no
ownership of infrastructure on the part of the customer.Evaluate immersion cooling for environments where power and space are
expensive. Immersion cooled systems are signiﬁcantly smaller, quieter and more
energy efﬁcient than traditional data center racks. These systems will prove cost-
effective in space- and power-constrained environments.■
Fully comprehend how to communicate the use of vegetable oil in environments.
Despite marketing terms, the cooling ﬂuids are primarily vegetable-derived and
present manageable ﬁre and mess risks. Understanding how to present and defuse
these issues will be important in review meetings.■
Plan to use immersion cooling for larger edge server deployments. Edge systems
inevitably face power, cooling and space constraints. Immersion cooling
signiﬁcantly eases cooling design by eliminating the need to pass air over
components. Immersion cooling also improves reliability through lower overall
operating temperatures, and exclusion of oxygen from contacts.■
G00770252 Page 48 of 101Why This Is Important
A lack of standards and the diversity of edge devices and workloads make
implementation challenging for even the most tech-savvy enterprises, while the potential
for selecting a technical “dead end” is high. Edge as a service features a delivery model
for edge computing in which a system integrator, independent software vendor and/or
cloud provider offers some or all of the infrastructure required to deliver edge-based
applications, shielding the customer from technical complexity and market volatility.
Business Impact
Drivers
Edge as a service solves for:Most enterprises do not possess the skills or experience to build and manage
complex, distributed systems incorporating a diversity of end systems and software
stacks.■
EaaS delivers and maintains prebuilt solutions with a primary focus on meeting
service-level agreements (SLAs) based on business outcomes.■
EaaS simpliﬁes adoption of complex edge initiatives, while lessening complexity, risk
of obsolescence and technical debt incurred.■
Organizations looking to deploy edge computing are ﬁnding that the crowded and
rapidly shifting technology space is making vendor selection a nearly impossible
task.■
Enterprises looking to limit risk ﬁnd EaaS to be a safe and defensible choice. Rather
than making technology choices, they can contract for a business outcome (for
example, retail store operations with explicit SLAs) and be insulated from the
infrastructure that the provider uses to deliver the solution.■
Lack of skills and experience in building and operating edge computing solutions. ■
Exposure to technology obsolescence. ■
The business unit preference for business-outcome-based solutions rather than
technology.■
Unpredictable costs when the number of sites is either growing or shrinking. ■
G00770252 Page 49 of 101Obstacles
User Recommendations
Gartner Recommended Reading
2021 Strategic Roadmap for Edge Computing
Edge-In to Cloud
Analysis By: Bob Gill
Benefit Rating: High
Market Penetration: 5% to 20% of target audienceEdge use cases are so individual that providers may not be in a position to solve all
enterprise requirements economically.■
Ongoing operations at scale may be more costly than if the enterprise operated the
infrastructure at a high degree of efﬁciency and automation.■
Placing all responsibility in the hands of the provider naturally drives vendor lock-in,
and some EaaS offerings may not support future edge requirements.■
Sourcing a solution externally may limit integration with other internal applications
and systems.■
Evaluate edge as a service offerings by creating a build versus buy model for edge
deployment and operations.■
Reduce initial cost outlays and pressure on accurate conﬁguration sizing by
positioning the edge capabilities as a more elastic service, rather than explicit
hardware conﬁguration and purchase.■
Weigh enterprise needs for customization and differentiation against realistic
assessments of in-house technical expertise, the organization’s stance on “opex
versus capex,” and the breadth of the solution (targeted, speciﬁc application set
versus a more general, distributed infrastructure platform).■
Examine EaaS to speed time to market for many use cases by lowering the initial
cost outlays, technical hurdles, operational expertise required and “platform risk”
present in such a nascent market.■
G00770252 Page 50 of 101Maturity: Adolescent
Definition:
Edge-in to cloud describes an architecture where edge applications, servers and gateways
make use of cloud-oriented technologies and connect as needed to public cloud services
— but they are deployed and operated independently from the public cloud. Rather than
using the programming models, or the IAM capability of a hyperscale cloud platform
(“cloud-out”), the application is centered around its role at the edge ﬁrst and its need for
hyperscale cloud application services second.
Why This Is Important
Many edge solutions are designed to operate independently of the public cloud while
maintaining connections as needed with one or more hyperscale cloud providers. These
“edge-in to cloud” architectures leverage some cloud services, but use edge-speciﬁc
functionality to support edge requirements, rather than pushing public cloud architecture
to the edge as a complete platform. This is critical where cloud independence, multicloud
core or predominantly brownﬁeld infrastructures are present.
Business Impact
Making the decision whether to adopt a “cloud-out” vs. “edge-in” model is fundamental to
the set or services available and extensibility of the platform providing edge computing in
the enterprise. This decision affects:
DriversCloud independence ■
Application and platform availability ■
Technology stack availability ■
Evaluations of whether an edge-centric approach may better align with a brownﬁeld
modernization effort■
While the hyperscale cloud providers continue to grow their portfolio of cloud to edge
solutions, enterprises report that such offerings are still functionally incomplete, not
optimized for speciﬁc solutions, nor well supported. Enterprise architects we speak
to indicate that they want to avoid single cloud vendor solutions and must reduce
the chance for lock-in.■
G00770252 Page 51 of 101ObstaclesEdge-centric approaches can provide solutions that are more tuned to edge
requirements.■
They reduce concerns over hyperscale lock-in. ■
They provide cloud independence for enterprises that are planning to be multicloud. ■
They provide greater ﬂexibility when dealing with legacy “brownﬁeld” environments. ■
They come with a greater likelihood of compatibility with existing vertically focused
devices, application software and system integrators.■
Edge-in architectures require implementation expertise beyond a normal IT skill set,
leading to cost overruns and mistakes in implementation.■
The lack of “off the shelf” edge in solutions is more likely to create a long-term
dependence on a system integrator or value-added reseller (VAR).■
Some edge-in platforms are generic and do not offer an application ecosystem,
while others may be too focused on a speciﬁc vertical subset and the investment
cannot be recouped across a range of use cases.■
Many edge-in platform vendors are early stage startups, providing increased risk due
to inability to support projects at a very large scale. These vendors may lack the
staying power to survive in a rapidly changing marketplace.■
Absence of common standards or frameworks for implementation will result in
highly custom solutions that will be harder to extend or replace over time■
G00770252 Page 52 of 101User Recommendations
Sample Vendors
LF Edge; Mirantis; Platform9; Rancher
Gartner Recommended Reading
Cool Vendors in Edge Computing, 2021
Cloud-Out and Edge-In: How Cloud Service Providers Can Leverage the Two Edge
Computing Architectures
The Edge of the Edge Overview
Edge AI
Analysis By: Eric Goodness
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: EmergingUse overall requirements to determine whether an application is ideally an edge
extension of a cloud-based application or an edge application that may use some
cloud resources on the back end.■
Determine whether the edge and Internet of Things (IoT) portfolio will be used in
conjunction with a single hyperscale cloud provider, or be multicloud.■
When adopting a newer edge-in platform, consider your need for long-term support
and have a backup plan in place if your platform vendor does not thrive or is
purchased by another vendor.■
Ensure architectural compatibility with your systems integrator (SI) or independent
software vendor (ISV), as these solution or verticals-speciﬁc providers deliver the
actual business value of the effort. Ensure alignment with any edge-in or cloud-out
preference they have.■
G00770252 Page 53 of 101Definition:
Edge AI refers to the use of AI techniques embedded in IoT endpoints, gateways and edge
servers, in applications ranging from autonomous vehicles to streaming analytics. While
predominantly focused on AI inference, more sophisticated systems may include a local
training capability to provide in-situ optimization of the AI models.
Why This Is Important
An increasing number of edge computing use cases are latency sensitive, data intensive
and require an increasing amount of autonomy for local decision making. This creates a
need for AI-based applications in a wide range of edge computing and endpoint solutions.
Examples include video analytics which, driven by the rapid growth in use of surveillance
cameras and the need for real-time interpretation of captured video, is starting to see
adoption.
Business Impact
The business beneﬁts of deploying edge AI include:
Drivers
Overall, edge AI has benefited from improvements in the capabilities of AI in general.
This includes:Improved operational efﬁciency, such as manufacturing visual inspection systems. ■
Enhanced customer experience. ■
Reduced latency in decision making, with the use of local analytics. ■
Connectivity cost reduction, with less data trafﬁc between the edge and the cloud. ■
Persistent solution availability, irrespective of network connectivity. ■
Reduced storage demand through a more reactive exploitation of the data. ■
Preserved data privacy at the endpoint. ■
The maturation of MLOps and ModelOps tools and processes ■
The improved performance of combinatorial techniques and a concomitant increase
in data availability■
G00770252 Page 54 of 101Business demands for new and improved outcomes solely achievable from the use of AI
at the edge, such as:
Reducing FTEs based on vision-based solutions used for surveillance or inspections ■
Improving manufacturing production quality by automating various processes ■
Optimizing operational processes across industries ■
New approaches to customer experience such as personalization on mobile devices
or changes in retail brought about by edge-based smart check-out POS■
Increasing numbers of users look to enable legacy systems and infrastructure in
brownfield environments. AI systems can be hosted within an edge computer or a
gateway (aggregation point). In this architecture, the IoT endpoint is peripheral to the
AI system. An example of this is environmental sensors deployed for a smart
agriculture application.■
Increasing numbers of manufacturers seek to embed AI in the endpoint as an
element of product servitization. In this architecture, the IoT endpoints, such as in
automobiles, consumer white goods or within commercial building infrastructure, are
capable of running AI models to interpret data captured by the endpoint and drive
some of the endpoints’ functions. In this case, the AI is trained and updated on a
central system and deployed to the IoT endpoint. Examples of the use of embedded
(edge) AI are medical wearables and AGVs and other robotic products that possess
some levels of intelligence and autonomy.■
There is increasing demand for R&D into training AI models at the edge for
decentralized machine learning. These emerging solutions are driven by explicit
needs such as privacy preservation or the requirement for machines and processes
to run in disconnected (from the cloud) scenarios.■
G00770252 Page 55 of 101Obstacles
User Recommendations
Sample Vendors
Akira; Chooch.AI; Edge Impulse; Falkonry; Imagimob; Litmus; MicroAI; Modzy; Octonion;
PalantirEdge AI is constrained by the application and design limitations of the equipment
being deployed; this includes form factor, power budget, data volume, decision
latency, location and security requirements.■
Systems deploying AI techniques can be nondeterministic. This may impact
applicability in certain use cases, especially where safety and security requirements
are important.■
The autonomy of edge AI-enabled solutions, built on some ML and deep learning
techniques, often presents questions of trust, especially where the inferences are not
readily interpretable or explainable.■
The lack of quality and sufﬁcient data for training is a universal challenge across AI
usage.■
Deep learning neural networks is a compute-intensive task, often requiring the use of
high-performance chips with corresponding high power budgets. This can limit
deployment locations, especially where small form factors and lower power
requirements are paramount.■
Determine whether the new AI developments are applicable to their IoT deployments,
or whether traditional centralized data analytics and AI methodologies are adequate.■
Evaluate when to consider AI at the edge versus a centralized solution. Applications
that have high communications costs, are sensitive to latency or ingest high
volumes of data at the edge are good candidates for AI.■
Assess the different technologies available to support edge AI and the viability of the
vendors offering them. Many potential vendors are startups, which may have
interesting products but limited support capabilities.■
Use edge gateways and servers as the aggregation and ﬁltering points to perform
most of the edge analytics functions. Make an exception for compute-intensive
endpoints, where AI-based analytics can be performed on the devices themselves.■
G00770252 Page 56 of 101Gartner Recommended Reading
Building a Digital Future: Emergent AI Trends
Emerging Technologies: Neuromorphic Computing Impacts Artiﬁcial Intelligence Solutions
Emerging Technologies: Edge Technologies Offer Strong Area of Opportunity — Adopter
Survey Findings
Emerging Technologies Impact Radar: Edge AI
Edge Computing
Analysis By: Bob Gill, Philip Dawson
Benefit Rating: Transformational
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Edge computing describes a distributed computing topology in which data storage and
processing are placed in an optimal location relative to the location of data creation and
use. Edge computing locates data and workloads to optimize for latency, bandwidth,
autonomy and regulatory/security considerations. Edge-computing locations extend
along a continuum between the absolute edge, where physical sensors and digital
systems converge, to the “core,” usually the cloud or a centralized data center.
Why This Is Important
Edge computing has quickly become the decentralized complement to the largely
centralized implementation of hyperscale public cloud. Edge computing solves many
pressing issues, such as unacceptable latency and bandwidth requirements, given the
massive increase in edge-located data. The edge-computing topology enables the
speciﬁcs of Internet of Things (IoT), digital business and managed distributed IT
solutions, serving as a foundational element for next-generation applications.
G00770252 Page 57 of 101Business Impact
Edge computing improves efﬁciency and cost control through processing close to the
edge, where the data is generated or acted upon (e.g., better automation and quality
control), and offers more business opportunities and growth (e.g., customer experience
and new real-time business interactions). Early implementations have succeeded at
enterprises that rely on operational technology (OT) systems and data outside core IT,
such as the retail and industrial sectors.
Drivers
Growth of hyperscale cloud adoption has exposed the disadvantages of extreme
centralization. Latency, bandwidth requirements, the need for autonomy and data
sovereignty or location requirements may be optimized by placing workloads and
data closer to the edge, rather than centralizing in a hyperscale data center.■
Data growth from interactive applications and systems often cannot be
economically funneled into the cloud.■
Applications supporting customer engagement and analysis favor local processing
for speed and autonomy.■
IoT use cases are expanding from the industrial sector to other verticals, driving a
move toward a hierarchical and distributed model.■
Edge computing’s inherent decoupling of application front ends and back ends
provides a perfect means of fostering innovation and enhanced ways to do
business. For example, using technologies such as machine learning and industrial
sensors to perform new tasks at locations where business and operational events
take place, or at the point of interaction with a retail customer, can drive signiﬁcant
business value.■
G00770252 Page 58 of 101Obstacles
User Recommendations
IT leaders responsible for cloud and edge infrastructure should:The diversity of devices, software controls and application types all amplify
complexity issues.■
Widespread edge topology and explicit application and networking architectures for
edge computing are not yet common outside vertical applications, such as retail and
manufacturing.■
Edge success in industrial IoT applications and enhancing customer experience in
retail are well understood, but many enterprises still have difﬁculty understanding
the beneﬁts, use cases and ROI of edge computing.■
A lack of broadly accepted standards slows development and deployment time,
creating lock-in concern for many enterprise users.■
Edge physical infrastructure is mature, but distributed application management and
orchestration challenges are still beyond most vendor-supplied component
management offerings. The tasks of securing, maintaining and updating the
physical infrastructure, software and data require improvement before management
and orchestration can mature.■
Create and follow an enterprise edge strategy by focusing ﬁrst on business beneﬁt
and holistic systems, not simply focusing on technical solutions or products.■
Establish a modular, extensible edge architecture through the use of emerging edge
frameworks and design sets. This will enable the mixing and matching of
technologies based on enterprise direction, not simply “what comes with the vendor
solution.”■
Accelerate time to beneﬁt and de-risk technical decisions through the use of
vertically aligned systems integrators and independent software vendors that
demonstrate an understanding of, and ability to, implement and manage the full
orchestration stack from top to bottom.■
Evaluate “edge-as-a-service” deployment options, which deliver business-outcome-
based solutions that adhere to speciﬁc SLAs while shifting deployment, complexity
and obsolescence risk to the provider.■
G00770252 Page 59 of 101Gartner Recommended Reading
Cool Vendors in Edge Computing, 2021
Edge Analytics
Analysis By: Eric Hunter, Ted Friedman
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
Analytics is the discipline that applies logic (e.g., “rules”) and mathematics (“algorithms”)
to data to provide insights that drive organization strategy and decision making. “Edge”
analytics means that the analytics are executed in distributed devices, servers or
gateways located outside of data centers and public cloud infrastructure closer to where
the data and decisions of interest are created and executed.
Why This Is Important
Gartner client inquiries related to the data and analytics (D&A) implications of edge
continue to increase. With a growing relevance, by 2025, more than 50% of enterprise-
managed data will be created and processed outside the data center or cloud. Demands
for real-time decision making closer to where the data of interest is created and stored are
one of many drivers for edge analytics.
Business Impact
The origins of edge analytics offerings were primarily in the support of decentralized
deployments for device-isolated insights. However, connectivity advances, demands for
cross-device analytics and innovations surrounding IoT have dramatically increased the
scale and complexity of edge analytics use cases. Real-time event analytics and decision
making, autonomous behavior of assets and fault-tolerant applications hold tremendous
potential value for enterprises in many industries.
G00770252 Page 60 of 101Drivers
Advantages of edge analytics include faster response times, reduced network
bottlenecks, data ﬁltering, reliability, increased access to data and reduced
communications costs.■
Data sovereignty and governance issues related to sensitive/regulated data can
constrain D&A teams from adopting centralized/cloud-based environments —
moving data outside its originating geography can violate sovereignty regulations.
By locating analytics in edge environments, the data remains in the originating
locations, increasing the likelihood of compliance.■
The increase of distributed cloud and hyperconverged solutions from public cloud
providers including Amazon Web Services (AWS Outposts), Microsoft (Azure Stack
Hub), Google Cloud (Anthos) are further decentralizing previously cloud-restricted
workloads. This perimeter expansion of the cloud brings compute and storage closer
to the edge — creating new possibilities for edge-centric analytic workloads.■
5G networks continue to grow in relevancy and, combined with mobile edge
computing, will increase edge analytics use cases — particularly for latency-sensitive
deployments.■
More analytics solutions, such as those supporting IoT use cases, need to operate in
disconnected (or intermittently connected) scenarios. By bringing more powerful
analytics capabilities to edge environments, these solutions need not rely on
centralized data centers or cloud resources. As demand grows for “smarter” physical
assets in many industries, supporting autonomous behavior will be a common
requirement.■
G00770252 Page 61 of 101Obstacles
User Recommendations
Analytics leaders should consider edge analytics across the following ﬁve imperatives:Some of the disadvantages of edge analytics include increased complexity, lack of
cross-device analytics, overhead of device maintenance and technical currency
demands.■
Architectural design and development best practices for traditional or cloud-resident
analytics typically assume or prioritize data/analytics centrality and do not carry
over directly for edge analytics use cases.■
Vendor choices include two extremes in terms of provider scale — with early and
unknown startups competing head to head with global mega vendors — driving a
mix of platform/protocol standards and complicating going concern considerations
for prospective buyers.■
Edge analytics can increase the complexity of enterprise standards and governance
(data privacy, security, etc.) which has the potential to delay overall value realization
objectives.■
Provide analytic insights for individual devices, assets or a larger distributed site
even in the midst of disconnection from cloud or data center infrastructure and
resources (e.g., driverless cars).■
Provide data sovereignty. Many regulations or data privacy laws require data be kept
in the location of origin or the organization deems the transfer of data to introduce
too many security vulnerabilities.■
Adapt to scenarios where network connectivity does not have the ability to support
desired latency or stability requirements.■
Address scenarios where cross-device interdependencies serving as part of a larger
system require edge-resident analytics.■
Redesign analytic strategies where it costs too much to upload the full volume or
ﬁdelity of generated data and that there is no beneﬁt to moving device-level data to a
central location for aggregated analysis.■
G00770252 Page 62 of 101Sample Vendors
Amazon Web Services; Arundo; CloudPlugs; FogHorn; Microsoft; PTC; Samsara; TIBCO
Software
Gartner Recommended Reading
Top Trends in Data and Analytics, 2022
Predicts 2022: The Distributed Enterprise Drives Computing to the Edge
The Edge of the Edge Overview
Emerging Technologies Impact Radar: Edge AI
G00770252 Page 63 of 101Sliding into the Trough
5G
Analysis By: Sylvain Fabre
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstream
Definition:
5G is the next-generation cellular standard by the 3rd Generation Partnership Project
(3GPP). The standard targets maximum downlink and uplink throughputs of 20 Gbps and
10 Gbps, respectively. Latency is as low as 4 milliseconds in a mobile scenario and can be
as low as 1 millisecond in ultrareliable low-latency communication scenarios, down to 10
m precision positioning and massive IoT scalability. New system architecture includes
core slicing as well as wireless edge.
Why This Is Important
5G supports eMBB, URLLC and MIoT which play a vital role in supporting the digital
economy and enterprise transformation. 3GPP 5G standards releases deliver incremental
functionality: in R15, extreme mobile broadband; then, in R16: industrial IoT (massive IoT,
slicing and security) — this the latest commercially available release; later, in R17: MIMO
enhancements, sidelink, DSS, IIoT/URLLC, bands up to 71GHz, nonterrestrial networks and
RedCap. Finally, R18 is under deﬁnition.
Business Impact
The business impacts are:
5G enables three main technology deployment, which each support distinct new
services for multiple industries and use cases of digital transformation, and possibly
new business models (such as latency as a service), namely: enhanced mobile
broadband (eMBB) for HD video; mMTC for large IoT deployments; and URLLC for
high-availability and very low-latency use cases, such as remote vehicle operations.■
Promising applications include ﬁxed wireless access, IoT support and private mobile
networks.■
G00770252 Page 64 of 101Drivers
ObstaclesOver 255 operators rolled out 5G (see  GSA), 30% of public mobile networks. ■
5G capability is penetrating lower cost smartphones in vendors’ portfolios. ■
Increasing device penetration: Gartner estimates that 5G-capable handset
penetration will reach 87% in 2023 in Western Europe, similar to North America.■
Gartner estimates that 5G-capable handset share of sales will reach 83% in 2023 in
Western Europe from 51% in 2021. North America share will rise to close to 90%.■
5G capability is starting to deliver value in emerging always-on wearables use cases. ■
Increased data usage per user and device requires a more efﬁcient infrastructure. ■
Operational cost savings and growth with new vertical industry use cases for
industry use cases.■
Requirements from industrial users value 5G lower latency from ultrareliable and
low-latency communications (URLLC) and expect 5G to outperform rivals in this
area.■
Demand for massive machine-type communications (mMTC) to support scenarios
of very dense deployments up to 5G target of 1 million connected sensors per square
kilometer.■
Increased availability of industry-speciﬁc spectrum options (e.g., CBRS). ■
Competitive pressures, if one CSP launches 5G in the market others usually have to
follow or risk losing market share.■
Availability and cost of spectrum, in particular for industrial private networks, in
some countries.■
Security concerns when using 5G in critical industrial scenarios. ■
Availability and pricing of networks and modules for R16 solutions. ■
Need to upgrade to 5G SA (stand-alone) Core for more advanced R16 release (such
as slicing).■
G00770252 Page 65 of 101User Recommendations
Sample Vendors
Cisco; Ericsson; Huawei; Mavenir; Nokia; Qualcomm; Samsung; ZTE
Gartner Recommended Reading
Competitive Landscape: Emerging Providers of 5G Platform Infrastructure
Creating Your Enterprise 4G and 5G Private Mobile Network Procurement Strategy and
RFQUse of higher frequencies and massive capacity requires denser deployments with
higher frequency reuse, which could raise network costs.■
Uncertainty about use cases and business models that may drive 5G for many CSPs,
enterprises, and technology and service providers (TSPs).■
Feedback from some industrial clients mentioned that the majority of their use cases
could be serviced by a 4G private network, Wi-Fi and/or NB-IoT, and other LPWA such
as LoRa.■
While diverse networks can offer adequate and cost-effective alternatives to 5G for
many use cases (e.g., LPWA, NB-IoT, LoRa, Wi-SUN), overall TCO and future
proofness may not be as good.■
Enable 5G for enterprise connectivity for mobile, nomadic and FWA
secondary/tertiary use cases for branch location redundancy, as long as 5G is not
the primary link for high-volume or mission-critical sites, unless there are no other
options.■
Provide clear SLAs for network performance by testing installation quality for
sufﬁcient and consistent signal strength, signal-to-noise ratio, video experience,
throughput and coverage for branch locations.■
Ensure backward compatibility to 4G devices and networks, so 5G devices can fall
back to 4G infrastructure.■
Focus on architecture readiness — such as SDN, NFV, CSP edge computing and
distributed cloud architectures, and end-to-end security — in preparation for 5G.■
Build their ecosystem of partners to target industry verticals more effectively with 5G
and before competition.■
G00770252 Page 66 of 1015G Providers Must Grasp the Scope of Hyperscalers’ Announcements for CSP Network
Infrastructure
Top Technology Trends for CSPs in 2022: 5G as a Catalyst for Platform Strategy and
Culture Change
Edge Servers
Analysis By: Thomas Bittman
Benefit Rating: High
Market Penetration: 1% to 5% of target audience
Maturity: Early mainstream
Definition:
Edge servers collect and deliver data, and perform analytics and inference close to IoT
data producers (e.g., sensors and cameras) and data consumers (e.g., people and IoT
actuators). They are often ruggedized for deployment outside of data centers, and have
broader and more general capabilities than gateway servers, but are less powerful than
micro data centers.
Why This Is Important
As IoT and data produced by things grow at the edge, and as varied use cases at the edge
increase, computing power is needed to aggregate and correlate this data, and turn many
connected things into smart systems. Edge servers that can handle harsh environmental
conditions and power limitations, with zero-touch remote management, will ﬁll that
requirement.
Business Impact
Edge servers improve the bottom line through increased plant automation, predictive
maintenance, better efﬁciency and quality control. They improve the top line by enabling
faster decision making for opportunities, more business interactions and better customer
experiences. Whether owned by enterprises or acquired as a service, edge servers will
become a critical part of most enterprises’ infrastructure topologies and will be critical for
digital business strategies.
G00770252 Page 67 of 101Drivers
Obstacles
User Recommendations
Sample Vendors
ADLINK; Cisco; Dell Technologies; Eurotech; Hewlett Packard Enterprise (HPE); Lenovo
Gartner Recommended Reading
Building an Edge Computing StrategyIncreased requirement for compute in locations where responses must be low-
latency, in real time or must continue in the event of an internet failure■
Increased data production at the edge (video, sensors, etc.) and the relative low cost
of computing versus bandwidth■
Increased number of near-real-time digital interactions between people and things at
the edge■
Growth in varied use cases at the edge ■
Technologies that enable high-volume remote management with zero touch are
immature.■
Diversity of edge interactions drives a diversity of compute requirements. ■
Scale requirements at the edge can be very small or very large and demand can grow
quickly.■
Choose edge servers that can be deployed rapidly and are easily ﬂexible and
extensible to match changing requirements.■
Evaluate edge servers for zero-touch remote management. ■
Avoid lock-in where possible and plan for technology changes as the market rapidly
evolves.■
Make security an upfront design requirement in any edge server deployment. ■
Evaluate as-a-service options rather than acquiring hardware and software. ■
G00770252 Page 68 of 101Predicts 2022: The Distributed Enterprise Drives Computing to the Edge
Market Guide for Edge Computing Solutions for Industrial IoT
2021 Strategic Roadmap for Edge Computing
Micro Data Centers
Analysis By: Jeffrey Hewitt
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Early mainstream
Definition:
Micro data centers are modular and smaller than a computer room — usually no more
than a rack of equipment or less. All required facilities and IT functionalities (such as
uninterruptible power supply, servers, storage, networking, cooling and monitoring) are
contained in the micro data center. It is designed for speciﬁc needs (for example,
accumulating sensor data or small remote ofﬁce support) at distributed locations, and is
typically managed remotely as part of a broader infrastructure.
Why This Is Important
Localized or micro data centers have been in use for some time, and their use cases in
edge and IoT environments are increasing. By applying a self-contained, scalable and
remotely managed solution and process, CIOs can reduce costs, improve agility, and
introduce new levels of compliance and service continuity.
Business Impact
Creating micro data centers is something companies have done for years but often in an
ad hoc manner. By partnering with vendors and creating a consistent and standardized
architecture, enterprises can regain control of these critical assets and increase the ability
to rapidly introduce site-speciﬁc services, while reducing risks and operational costs and
improving service levels.
G00770252 Page 69 of 101Drivers
Consistency: For organizations with a large number of sites, consistency in the IT
and networking architecture, consistency in the installation of said architecture, and
consistency in the operational procedures to maintain, update and manage the
environment will enhance the ability to deploy site-speciﬁc workloads quickly using a
standardized toolset. By creating a consistent environment, the overall costs can be
reduced through standardization of vendors, architecture and software.
Implementation and operational costs can also be reduced by creating a repeatable
and easy-to-implement solution for local building managers.■
Control: Whether on manufacturing plant ﬂoors, IoT installations, or in retail outlets
or professional ofﬁce areas, control of remote IT assets is extremely complex. A
micro data center implementation provides greater control of the assets through
remote tools, while also allowing standardized methods for releasing new
applications, software upgrades, maintenance patches and equipment upgrades in
environments that may be hostile and/or harsh.■
Continuity of service: Micro data centers operate independently, regardless of what
happens at the home ofﬁce. If a primary data center goes down for any reason,
remote sites still have a business to run, and the ability to provide continuous
operations independently for short periods of time is a critical design criterion.■
Compliance: The deployment of well-deﬁned operations controls that can be
effectively applied to support regulatory issues, audit, data location, latency and
localized business requirements is crucial. Creating a standardized, controlled
environment at each site, helps increase the level of compliance while decreasing the
level of risk to the business.■
Security: Micro data centers provide a physical security barrier that can be placed
around valuable IT equipment and data. Sensors and intrusion monitoring systems
enable IT to detect theft or vandalism.■
G00770252 Page 70 of 101Obstacles
User Recommendations
Sample Vendors
Compass Datacenters; DartPoints; EdgeConneX; Schneider Electric; Vertiv
Edge Video Analytics
Analysis By: Bob Gill
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Edge video analytics is the real-time analysis for object detection/classiﬁcation and event
recognition of video data, either executed in a video camera or edge computing system.
Edge video analytics systems leverage deep-neural-network-based algorithms running on
either a dedicated AI chip, microprocessor, or application processor with the camera or
edge computer.Local control of business at remote sites has often dictated how and why speciﬁc
technologies were deployed, and changing that mindset is a challenge.■
Implementation of a consistent operating methodology (purchase, deployment,
monitoring, management) across many sites is required.■
Vendor selection, but more importantly vendor strategy, needs to align with IT
strategy to create a consistent, repeatable deployment model. This alignment can be
difﬁcult to achieve.■
Focus on maximizing operational independence and energy efﬁciency, while
minimizing IT skills and risks at remote sites.■
Design micro data centers to run autonomously but to be controlled centrally. ■
Create a standard, self-contained micro data center solution designed for easy
deployment, simple replacement, and remote monitoring and management.■
G00770252 Page 71 of 101Why This Is Important
Video cameras are being deployed in a wide range of applications such as surveillance,
security, factory inspection systems, facial recognition (security authentication and
personalized retail experiences), and autonomous vehicles. This creates a massive
volume of data that must be analyzed and used for decision making. For many
applications, the data analysis must be automated, and it is time-critical, requiring
processing either close to or at the point and time of data capture.
Business Impact
Edge Video Analytics provide visual decision making more quickly, consistently and at a
greater scale than the human experts they replace. Business targets include:
DriversBusiness units looking to automate the evaluations and recommendations of
experts in visual tasks.■
Organizations looking to improve safety through the enforcement of safety rules,
use of protective equipment, etc.■
Retailers looking to automate inventory management, merchandise identiﬁcation for
check-out and theft reduction.■
Video and image usage at the edge has advanced from simple surveillance and
monitoring to include operational tasks, such as classifying objects for parts picking
or retail check-out, or taking corrective action when an object is detected.■
While historically, video captured has been stored and analyzed at a later date —
often by humans, emerging applications enable the decision making and
subsequent action taken to be advised by the ML algorithms at the edge.■
Deployment of video analytics into edge systems and devices is being enabled by
the development of semiconductor devices such as visual processing units, graphics
processing units and application-speciﬁc integrated circuits that can execute the
deep neural network (DNN) algorithms.■
The use of higher resolution cameras, with consequent higher data volumes, drives
the need for high-performance edge video analytics systems to lessen bandwidth
appetites.■
G00770252 Page 72 of 101Obstacles
User Recommendations
Sample Vendors
AMD (Xilinx); Hikvision; Intel; Mobileye; NVIDIA
Gartner Recommended Reading
Leading the Edge: Gartner’s Initial Edge Hardware Infrastructure Forecast
Emerging Technologies: Critical Insights on AI Semiconductors for Endpoint and Edge
Computing
Tech Providers 2025: The Future of EdgeFor many deployments, implementing the analytic capabilities in the camera may be
an ideal solution. However, this often requires the use of new camera technology and
power and form factor constraints can limit the type of analytics capabilities that
can be deployed in a camera.■
In many deployments, there is the need to support updating the video analytics
algorithms to implement new models, which can drive up the cost and complexity of
the hardware and software at the edge.■
Improved Operational AI systems will be required to ensure life cycle management of
models and model compression capabilities to reduce the churn of cameras based
on processing capabilities.■
Evaluate if the deployment location will be at the edge or in a central data center, i.e.,
cloud or on-premises.■
Perceive the number of cameras and volume of captured video data. ■
Explore the need for real-time analysis and decision making. ■
Compare the complexity of the DNN algorithms being proposed with the capabilities
of the processing hardware in the edge systems and endpoint devices.■
Assess maturity of the DNN algorithms and associated video analytics applications. ■
Check for data communications availability. ■
G00770252 Page 73 of 101Emerging Technologies: Top Use Cases for Neuromorphic Computing
Emerging Technologies: Using Neuromorphic Neural Networks to Advance IoT Vision
Projects
Low Code for Edge
Analysis By: Bob Gill
Benefit Rating: High
Market Penetration: 5% to 20% of target audience
Maturity: Emerging
Definition:
Low-code solutions for the edge allow simpliﬁed development of edge applications
through the assembly of prebuilt blocks of code into a working application. The interface
may offer a visual “drag and drop” approach, where I/O, functions and processes are
represented by visual elements positioned and linked on a “canvas.” Code blocks, or
“nodes,” may be created or modiﬁed through the use of lower-level languages for
extensibility, while prebuilt apps or frameworks can assist the new user.
Why This Is Important
Edge computing applications are often complex, multiplatform, distributed applications
made even more difﬁcult by the diversity of devices at the edge and a lack of established
standards. Edge-speciﬁc low-code solutions provide prebuilt and conﬁgured “chunks” of
code, which are dragged, dropped and linked together based on business logic or data
ﬂows to quickly create functioning edge applications without knowledge of low-level
interfaces or structures.
Business Impact
Low-code development for edge applications can provide simplicity, speed and agility to
the development of edge applications. They:
Break complex applications into prebuilt and preintegrated templates and “applets.” ■
Enable business unit and operational users to create their own applications. ■
Speed time to deployment of apps. ■
G00770252 Page 74 of 101Drivers
Obstacles
The current level of granularity and performance of resulting applications may uncover
that low-code tools face the following challenges:Break the development bottleneck of waiting for IT and provide a self-service model. ■
The complexity of writing and maintaining distributed applications in rapidly
changing operational settings.■
The need to mask the complexity of diverse devices, data formats and upstream
targets■
The Complexity of integrating remote distributed applications with existing back end
processes and systems, often “owned” by other business units■
IT-driven development may not possess the business-context and process expertise
of the OT-based systems experts■
The need to provide operational users the ability to exercise the infrastructure
directly■
The ability to continue to innovate after the SI’s initial building phase. ■
There is a lack of standards or clear leaders. ■
Low code’s coarse-grained approach may limit the extensibility of the solution. ■
There is a lack of training in app development “hygiene” among nontechnical staff. ■
Deeper evaluation of complex apps is needed, especially long-term scalability and
total cost of ownership (TCO).■
Putting tools into the hands of business users is powerful, but some degree of
governance, particularly regarding manipulation of enterprise data, must be
exercised.■
G00770252 Page 75 of 101User Recommendations
Sample Vendors
Node-RED; Tulip; Vantiq; Virtuoso
Edge Security
Analysis By: Neil MacDonald
Benefit Rating: Moderate
Market Penetration: 1% to 5% of target audience
Maturity: Emerging
Definition:
Edge security offerings protect the integrity of the hardware, operating system, application
platform, application workload, its data and its network access at distributed edge
computing locations.
Why This Is Important
Like any IT system, edge computing locations are targets for attack, including data theft,
data poisoning, denial of service and placement of malware (for example, Bitcoin mining).
Edge security combines the requirements of data center computing security with the scale,
remoteness and heterogeneity of mobile and Internet of Things (IoT) computing security.Detail who will be using these tools and to what end. Governance is still required. ■
Evaluate offerings that tend to specialize in the platforms you are implementing. ■
Involve IT development for assistance in creating new application modules or
scripting to modify existing modules.■
Consider low code a user tool for data analysis and prototyping, but look to IT
development or system integrators to move prototypes to enterprisewide, scalable
performant systems.■
G00770252 Page 76 of 101Business Impact
For edge computing strategies to succeed, the integrity of edge computing workloads and
data must be protected. Without protection, edge computing capabilities could be
rendered inaccessible and the data could be stolen, copied or tampered with and adjacent
edge nodes and devices could be attacked. Edge attacks can lead to potentially
catastrophic results that could threaten health and safety if monitoring and control
signals are lost, tampered with or spoofed.
Drivers
Driven by network constraints and costs as well as privacy and compliance
requirements, organizations are expanding workloads to the edge, and security
needs to be a part of this.■
Sensitive data and intellectual property will be stored at and transmitted from the
edge and must be appropriately protected.■
New solutions and approaches are needed that securely support intermittent access
and protect from physical tampering and theft.■
The standardization of edge computing software stacks around containers and
Kubernetes will allow cloud-native security concepts to be modiﬁed for the edge.■
Enabling remote access/automated control of systems that control industrial
processes where failures can result in injury or loss of life requires a high level of
security.■
G00770252 Page 77 of 101Obstacles
User RecommendationsThe historic diversity of hardware and application platforms makes it difﬁcult to
develop a single-edge security strategy.■
The market for edge security is emerging with a large number of overlapping
offerings.■
Most edge hardware devices were not designed with adequate security controls. ■
Security wasn’t a high priority during the procurement of most edge hardware
devices.■
Most edge computing platforms won’t have the capacity for a standard security
stack.■
Complete edge security protection strategies must address the entire stack:
hardware, OS, application platform, application, data and network security.■
Design protection that treats the network as compromised, hostile and intermittent. ■
Make tamper resistance a part of security control evaluation, assuming that
hardware will be attacked, tampered with or stolen.■
Use your protection requirements to drive vendor selection. Include at least
mandatory data encryption, network security, workload integrity, application control,
memory protection, behavioral monitoring and intrusion detection/prevention.■
Favor offerings that are centrally managed, ideally cloud-based, and provide for
tightly controlled administrative access.■
Require the use of identity-based policy management using provisioned unique
identities of edge equipment, typically certiﬁcate-based.■
Restrict access to the edge platform using a zero trust network access or SASE
offering.■
Require vendors to support Linux containers and container-based security offerings,
which are expected to be widely used along with virtual machines for distributed
edge computing architectures.■
G00770252 Page 78 of 101Sample Vendors
Amazon Web Services; Appgate; Dell Technologies; Fortinet; Hewlett Packard Enterprise
(HPE); Red Hat; StorMagic; Tempered; VMware; ZEDADA
Gartner Recommended Reading
2021 Strategic Roadmap for SASE Convergence
Building an Edge Computing Strategy
SASE
Analysis By: Neil MacDonald, Andrew Lerner
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Secure access service edge (SASE) delivers converged network and security as a service
capabilities, including SD-WAN, SWG, CASB, NGFW and zero trust network access (ZTNA).
SASE supports branch ofﬁce, remote worker and on-premises secure access use cases.
SASE is primarily delivered as a service and enables zero trust access based on the
identity of the device or entity, combined with real-time context and security and
compliance policies.
Why This Is Important
SASE is a key enabler of modern digital business transformation, including work from
anywhere and the adoption of edge computing and cloud-delivered applications. It
increases visibility, agility, resilience and security. SASE also dramatically simpliﬁes the
delivery and operation of critical network and security services mainly via a cloud-
delivered model. SASE can reduce the number of vendors required for secure access to
one to two over the next several years.
Business Impact
SASE enables:
G00770252 Page 79 of 101DriversNew digital business use cases (such as branch ofﬁce transformation and hybrid
workforce enablement) with increased ease of use, while reducing costs and
complexity via vendor consolidation and dedicated circuit ofﬂoad.■
Infrastructure and operations and security teams to deliver a rich set of networking
and network security services in a consistent and integrated manner to support the
needs of digital business transformation, edge computing and work from anywhere.■
SASE is driven by enterprise digital business transformation including the adoption
of cloud-based services by mobile workforces, edge computing and business
continuity plans that must include ﬂexible, anywhere, anytime, secure access, and
use of the internet and cloud services.■
The need to ﬂexibly support digital business transformation efforts with a zero trust
security architecture while managing complexity is a signiﬁcant factor for the
adoption of SASE, primarily delivered as a cloud-based service (see 2021 Strategic
Roadmap for SASE Convergence). The rapid shift to hybrid work models accelerated
these trends.■
For IT, SASE can reduce the deployment time for new users, locations, applications
and devices as well as reduce the attack surface and shorten remediation times.■
Network security models based on data center perimeter security are ill-suited to
address the dynamic needs of a modern digital business and its distributed digital
workforce. This is forcing a transformation of the legacy perimeter into a set of
cloud-based, converged capabilities created when and where an enterprise needs
them — that is, a dynamically created, policy-based SASE.■
G00770252 Page 80 of 101Obstacles
User RecommendationsOrganizational silos, existing investments and skills gaps: A full SASE
implementation requires a coordinated and cohesive approach across security and
networking teams, which is challenging given refresh/renewal cycles, silos, and
existing staff expertise.■
Organizational bias and regulatory requirements for on-premises deployment:
Some customers have an aversion to the cloud and want to maintain control.■
Global coverage: SASE depends upon cloud delivery, and a vendor’s cloud footprint
may prevent deployments in certain geographies, such as China, Africa, South
America and the Middle East.■
SASE maturity: SASE capabilities vary widely. Sensitive-data visibility and control is
often a high-priority capability, but it is difﬁcult for many SASE vendors to address.
While your preferred single vendor may lack the capabilities you require, two-vendor
partnerships can be a viable approach.■
Involve the chief information security ofﬁcer (CISO) and network architect when
evaluating offerings and roadmaps from incumbent and emerging vendors to ensure
an integrated approach.■
Leverage WAN, ﬁrewall, VPN hardware refresh cycles or software-deﬁned WAN (SD-
WAN) deployments to update network and network security architectures.■
Strive for no more than two vendors for all core services to minimize complexity and
improve performance.■
Identify required capabilities for networking and security, including latency,
throughput, geographic presence, and endpoint types to develop evaluation criteria.■
Focus on vendors who invest signiﬁcantly in sensitive data discovery and protection
capabilities for their SASE covering multiple data exﬁltration vectors and serving
verticals with highly advanced requirements for data security.■
Combine branch ofﬁce and remote access in a single implementation to ensure
consistent policies and minimize the number of vendors required.■
Leverage branch ofﬁce transformation and dedicated circuit ofﬂoad projects to
adopt SASE for security services.■
G00770252 Page 81 of 101Sample Vendors
Cato Networks; Cisco; Forcepoint; Fortinet; Juniper; Netskope; Palo Alto Networks; Versa
Networks; VMware; Zscaler
Gartner Recommended Reading
2021 Strategic Roadmap for SASE Convergence
Quick Answer: Does SSE Replace SASE?
The Future of Network Security Is in the Cloud
Magic Quadrant for WAN Edge Infrastructure
Magic Quadrant for Security Service Edge
Edge Asset Life Cycle Management
Analysis By: Thomas Bittman
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Adolescent
Definition:
Edge asset life cycle management includes onboarding, monitoring, change management
and decommissioning of edge computing systems. Edge computing systems can be
intelligent gateways, edge computing servers and workstations deployed in remote ofﬁces
or sites, or Internet of Things (IoT) systems, such as embedded devices or IoT servers.
They may also be part of an operational technology (OT) vendor system.
Why This Is Important
Edge asset life cycle management software provides better control and visibility to the
edge infrastructure. This can include edge servers, intelligent gateways or embedded
devices. Edge asset life cycle management software ensures that infrastructure life cycle
management policies that are applied in the data center or the cloud can be similarly
applied to edge environments.
G00770252 Page 82 of 101Business Impact
As enterprises build distributed IT architectures, infrastructure and operations (I&O)
leaders must ensure they can centrally manage the health and life cycle of these systems,
regardless of location. System management software is crucial, and, when organizations
begin to scale their edge projects and deploy large numbers of endpoints, gateways and
edge servers, centralized management of these distributed systems is imperative. This
ensures better visibility and control of the environment.
Drivers
Edge environments, such as remote ofﬁces, are often managed by data center
system management software, which is less capable of handling the scale and
distribution of edge assets.■
Gateways provided by OT vendors typically lack consistent life cycle management
capabilities.■
System management for IoT-centric edge environments is still nascent. In this case,
IoT asset management software is usually provided by the hardware vendors as
software as a service (SaaS) offerings.■
Cloud vendors and emerging asset management vendors now offer IoT platforms
that support remote management of IoT gateways and embedded devices.■
G00770252 Page 83 of 101Obstacles
User RecommendationsEdge life cycle management requires agreement between the responsible operations
teams (IT and OT) on how asset health is to be managed, and the types of asset
updates that will be allowed.■
The success of the update often depends on network bandwidth and availability. ■
Although it is relatively easy to update remote assets through wired networks, over-
the-air (OTA) updates via cellular networks are often challenging and may result in
inconsistent results.■
Updating is subject to device availability, which may be restricted to production
shutdown periods for OT systems.■
Updating low-power-constrained devices operating on low-power wide-area networks
(LPWANs) is a challenge. In some cases, the cost of OTA updates on constrained
devices may be higher than the cost of replacing the asset.■
Vendor-led asset management tools become untenable as heterogeneous edge
assets grow.■
Leverage separate edge life cycle platforms when the ownership of edge assets (e.g.,
the operations team versus IT infrastructure and operations teams) and policies for
edge life cycle management are distinctly different.■
Choose platforms that support easier packaging and delivery of software updates
(such as containers) to the edge system to manage IoT assets.■
Synchronize work with planned maintenance shutdowns to manage OT-related edge
devices.■
Consolidate IoT functionality — analytics, visualization, application and device life
cycle management — onto a common IoT platform.■
Ensure that updates are delivered in a secure manner and the base OS/ﬁrmware of
the asset has a roll-back option.■
Focus on the usability aspect of these platforms, which is equally important,
particularly when the enterprise begins to scale from a small number of assets to
thousands.■
G00770252 Page 84 of 101Sample Vendors
Amazon Web Services; CloudPlugs; Inﬁot; ioTium; Microsoft; SECO
Gartner Recommended Reading
Market Guide for Edge Computing Solutions for Industrial IoT
Market Guide for Industrial IoT Gateways
G00770252 Page 85 of 101Climbing the Slope
IoT
Analysis By: Alfonso Velosa, Peter Havart-Simkin
Benefit Rating: Transformational
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
The Internet of Things (IoT) is an enabler and accelerator for digital transformation of
enterprises. IoT, through assets with embedded technology to communicate and
sense/interact with their internal states or external environment, enables enterprises to
improve business processes and enhance decisions with real- time information. IoT
solutions span assets, communications, applications, data and analytics. IoT supports
enterprises growing and developing new revenue and operating models.
Why This Is Important
IoT is an enabler and accelerator for digital and composable business initiatives. Most
enterprises lack accurate information about the state of their assets or products, adding
cost and inefﬁciency to their business processes. For operators of assets such as hotels
or manufacturers, adding IoT capabilities enables them to gain new understanding of the
asset to operate it at the optimal level. For OEMs, IoT enables them to know how their
products are used and how they should be improved.
Business Impact
IoT will impact most enterprises’ business operations, customer engagement, competitive
position and product strategies by enabling:
Optimization of business processes: This covers the spectrum from costs to
operations, while improving the use of assets and conserving resources.■
New revenue strategies: This includes generating revenue via improved products,
services and data monetization.■
Safety and compliance focus: This includes meeting regulatory certiﬁcation and
driving employee safety.■
G00770252 Page 86 of 101Drivers
Obstacles
User RecommendationsEnterprises, on a global basis, have shifted from implementing narrower IoT
technology solutions on a limited basis toward more widely implementing business
solutions that use IoT capabilities at large scale.■
OEMs increasingly add IoT-enabled capabilities to improve value, meet competitive
pressure, drive differentiation and add new revenue streams.■
Leading-edge enterprises are using IoT to drive transformative strategies. For
example, product as a service or SLA-compliance asset uptime or new digital twin or
data management value propositions.■
Typical enterprise clients seek average payback targets of six- to 18-month paybacks
on clearly deﬁned business projects using IoT capabilities.■
Technology and service providers have realigned their go-to-market strategy to
express clearer value propositions to their enterprise customers.■
Many leaders avoid investing the necessary political capital to support IoT projects,
since these are really business transformation projects that require engagement with
business and frontline OT workers.■
Business leaders often fail to set clear objectives or KPIs for IoT projects, or
communicate their importance.■
Lack of a center of excellence or other central team to develop best practices and
share them with the organization, drive IT-OT alignment, and allocate budgets,
personnel and resources.■
Lack of standards inhibit scaling for solutions that involve multiple vendors, from
sensors, gateways and communications, to implementation and analysis.■
Difﬁculty of integrating elements of an end-to-end IoT solution from assets and
other data sources to the business applications.■
Given the business transformations involved with IoT projects, the cluttered market
of IoT hardware and software vendors lack the vertical domain understanding to
provide effective IoT business solutions.■
G00770252 Page 87 of 101Sample Vendors
Accenture; Amazon Web Services (AWS); ClearBlade; GE Digital; Haier Digital; Hitachi;
Microsoft; Panasonic; Vodafone
Gartner Recommended Reading
Quick Answer: What Industries and Use Cases Are Driving IoT in the Commercial Sector?
Emerging Technologies: Top Sustainability Trends for Technology and Service Providers
Toolkit: 5 Digital Twin/IoT Project Success Drivers
IT/OT Integration
Analysis By: Kristian Steenstrup
Benefit Rating: High
Market Penetration: 20% to 50% of target audience
Maturity: Early mainstreamBuild relationships across business units as IoT centers on business transformation.
Invest time and effort on culture change, such as incentives to foster cross-
organizational collaboration around desired IoT-enabled business outcomes.■
Contribute to an IoT center of excellence composed of IT, operational, and business
leaders and other stakeholders. Use it to drive enterprisewide best practices and
objectives.■
Ensure the teams focus on both the IT and operational architectures to address key
technology complexity, security and integration challenges.■
Particularly when implementing multiple use cases at scale, plan to implement a
multivendor approach for IoT platforms, analytics and applications.■
Establish accountability, participation, predictability and transparency policies for
IoT to address sponsorship, budgets, digital ethics, data ownership and rights to
monetize IoT data.■
Develop multiyear budgets as IoT-enabled initiatives will be decades long. ■
G00770252 Page 88 of 101Definition:
Information technology/operational technology (IT/OT) integration is the end state
sought by organizations (most commonly, asset-intensive organizations) where IT and OT
are not separated as technology areas with different areas of authority and responsibility.
Instead, there is an integrated process and information ﬂow. Integration includes
infrastructure, software, processes and potentially resources.
Why This Is Important
Few organizations have a mature, systemic approach to IT/OT resource integration. For
most, IT and OT are managed by separate groups with different approaches to technology
and different vendors in use. Integration can be initiated by IT departments; however,
operational business units may also seek integration when trying to solve other
challenges such as dealing with cybersecurity, rising support costs, safety concerns,
disaster recovery or software administration.
Business Impact
Opportunities and beneﬁts from transparency, and an integrated value chain based on
data come from integrating the systems. As IT and OT platforms, and technologies
converge (become more alike) through increasing use of IT products within OT, a
successful digital business manages both IT and OT together. There is a shared
responsibility, even though direct reporting lines may not shift. Data can be shared, and
process ﬂows become continuous and coherent, with minimal interruptions.
Drivers
With IT/OT integration for asset-intensive digital businesses, organizations will be much
more capable of managing, securing and exploiting data, information and processes.
IT/OT integration results in integrated systems, processes and teams of people as
technology domains with different areas of authority and responsibility come
together.■
A common driver is for better reliability and maintenance strategies through more
direct access to condition and the use of data both on-premises and SaaS solutions
for plants and equipment.■
Integrated operational intelligence will provide better production management,
quality control and responses to events in the supply chain, and more efﬁcient
production processes. The result will be a more agile and responsive organization.■
G00770252 Page 89 of 101Obstacles
User Recommendations
Sample Vendors
Accenture; Cisco; Eurotech; NTT DATA; PTC; Rockwell AutomationThe data from OT systems will be the fuel for better decision making in areas such
as operations (adjusting and responding to production events), energy consumption,
material consumption, and product quality, safety and reliability.■
A lack of common governance structures because of a siloed approach to managing
technology in the past has to be overcome.■
Without incentives, this will not change because historically, IT and OT had little
contact and have different reporting lines.■
Completely integrated approaches to IT and OT are difﬁcult to achieve because of
the deeply rooted tradition in many businesses, where engineers and operations
staff have been the “exclusive owners and operators” of OT.■
Many companies have disparate standards of technology in IT and OT, and even
different standards for documenting the technologies, making initial planning
difﬁcult.■
An integrated data model spanning IT and OT rarely exists. ■
Evaluate the IT/OT integration challenges and beneﬁts in your speciﬁc company. ■
Achieve consensus across groups and with senior management, and create an
alignment activity ﬁrst to manage governance and standards. Sustainable
integration needs well-planned IT/OT alignment.■
Add a more integrated approach to technology progressively. This integration should
extend at least to data exchange and platform maintenance, with particular attention
paid to communications, cybersecurity and enterprise architecture. In some
companies, that commonality will lead to an organization no longer delineated
between IT and OT.■
Initiate IT/OT alignment discussions to arrive at common standards for platforms,
security and architecture.■
G00770252 Page 90 of 101Gartner Recommended Reading
Quick Answer: What Are IT/OT Alignment and IT/OT Integration?
Manufacturing Insight: How to Position Hybrid IT/OT Offerings
How IT Standards Can Be Applied to OT
Survey Analysis: IT/OT Alignment and Integration
When Does a CIO Need to Be Involved in OT?
LPWA
Analysis By: Tim Zimmerman, Aapo Markkanen, Bill Menezes
Benefit Rating: Moderate
Market Penetration: 5% to 20% of target audience
Maturity: Adolescent
Definition:
Low-power wide-area (LPWA) network technologies are a set of wireless wide-area
network solutions designed for applications that are characterized by their long signal
range, low cost and low throughput. Communications are typically over 5 miles/10
kilometers and involve small amounts of data at throughput of 0.3 Kbps to 50 Kbps
between low-powered devices such as sensors that have a battery life of several years.
Why This Is Important
Smart cities, agriculture, oil and gas, and mining have use cases where devices in remote
or hard-to-reach areas have a small amount of data that needs to be transmitted to an
application. Sensors with integrated LPWA radios, which may be battery-operated or
connected to power, are used in solutions that communicate data over vast distances.
G00770252 Page 91 of 101Business Impact
LPWA network technologies allow sensors to communicate status or change in status,
enabling organizations to act on or react to information that can improve operational
efﬁciency, reduce cost, and reduce risk and liability. LPWA solutions also allow the
elimination of human labor to collect the data from disparate sensors such as meters,
pipes or machinery, while operating with relatively low power. Similarly, the labor costs
associated with battery replacements can be reduced considerably.
Drivers
LPWA solutions offer:
Obstacles
User RecommendationsThe ability to collect information from widely disparate locations using low-power
technology■
The cost savings from applications such as metering, where currently personnel
manually collect data from an end device, or service it in remote or challenging
locations■
The availability of standards-based technologies such as NB-IoT, which is classiﬁed
as part of 5G introduced in 2016 3GPP 5G release 13, or LTE-M, which is part of 4G
LTE.■
The expense of proprietary radio infrastructure to collect the information ■
The proprietary tags and protocols that limit interoperability (e.g., LoRa) ■
Immaturity of standards for eSIM-related use cases ■
Organizations with large numbers of geographically dispersed sensors should
evaluate LPWA for sensor communications including smart metering, asset
management or remote sensors used for monitoring applications such as
agriculture, construction and utilities.■
All organizations should evaluate 5G NB-IoT because radio coverage will be part of
5G rollouts.■
Enterprises must continue to monitor data plan pricing to ensure that any solutions
provide the required ROI.■
G00770252 Page 92 of 101Sample Vendors
AT&T; Deutsche Telekom; Sierra Wireless; Verizon; Vodafone
Gartner Recommended Reading
Magic Quadrant for Managed IoT Connectivity Services, Worldwide
Critical Capabilities for Managed IoT Connectivity Services, Worldwide
IIoT Gateways
Analysis By: Thomas Bittman
Benefit Rating: High
Market Penetration: More than 50% of target audience
Maturity: Mature mainstream
Definition:
An industrial Internet of Things (IIoT) gateway is a bridge between a ﬁeld network and the
IoT platform or, sometimes, a business application. IIoT gateways can be viewed as data
aggregation points for ﬁeld devices or wireless networks. They provide local storage and
compute capabilities, as well as UIs for data processing and system management.
Why This Is Important
IIoT gateways can be viewed as a critical point of data aggregation for most industrial IoT
projects. These gateways enable efﬁcient transmission of data from endpoints to the IoT
platform by signiﬁcantly reducing bandwidth costs by normalizing and ﬁltering most data
at the edge.
Business Impact
IIoT gateways are viewed as a critical component of any industrial IoT project as these
gateways have transitioned from systems that aggregate, ﬁlter and forward data, to
systems that can provide localized and near-real-time insights. Manufacturing,
automotive and utility sectors are increasingly deploying IIoT gateways as part of the
Industrie 4.0 project initiatives. IIoT gateways also play a critical role in building
management and smart city initiatives.
G00770252 Page 93 of 101Drivers
Obstacles
User RecommendationsThe growth of IoT devices and data production at the edge requires gateways for
connectivity and data processing — especially where low latency is a requirement.■
While more general-purpose edge servers will take on deeper analysis and inference
roles, the larger demand for light data processing and local aggregation of IoT
interactions are best handled by gateways.■
Most industrial gateway vendors offer in-house-developed gateway management
utilities, with limited integration to Tier 1 IoT platforms, complicating deployment,
change management and scalability.■
A myriad of edge network protocols and rapidly evolving cellular network technology
warrant a modular gateway architecture. However, most IIoT gateways do not offer
this, demanding frequent reinvestment and system redesign.■
Gateways in industrial settings are deployed in physically challenging and mission-
critical environments and, therefore, need to meet stringent safety and security
requirements.■
Users face challenges when technically integrating with existing PLCs and other
legacy equipment, as well as organizational, cultural and process challenges
working across the traditional IT/OT divide.■
Create a roadmap for the current and future role of gateways in the IoT architecture,
and invest in a combination of low-end IIoT gateways and intelligent IoT gateways
to address various use cases.■
Standardize on gateways that can be managed and programmed by preferred IoT
platforms and can provide software development kits for custom integration and
management.■
Extend the usable life of IIoT gateways and reduce the need for new investments by
choosing gateways that are modular and can accommodate new peripherals.■
Select IIoT gateways that are certiﬁed for international, as well as country- and
industry-speciﬁc safety standards, and provide an adequate level of overall system
security.■
G00770252 Page 94 of 101Sample Vendors
Advantech; Aspen Technology; Cisco; ClearBlade; Dell Technologies; Eurotech; Hewlett
Packard Enterprise (HPE); Lantronix
Gartner Recommended Reading
Market Guide for Edge Computing Solutions for Industrial IoT
Hardware-Based Security
Analysis By: Tony Harvey, Neil MacDonald
Benefit Rating: Moderate
Market Penetration: 5% to 20% of target audience
Maturity: Early mainstream
Definition:
Hardware-based security uses chip-level techniques for the protection of critical security
controls and processes in host systems independent of OS integrity. Typical control
isolation includes encryption key handling, secrets protection, secure I/O, process
isolation/monitoring and encrypted memory handling.
Why This Is Important
The integrity of a computing system starts with the integrity of the hardware. Hardware-
based security uses the strong isolation and protection characteristics of hardware to
extend assurance and protection into the software layers running above it. For example, if
system ﬁrmware or a hypervisor is compromised, any upper layer security controls can be
disabled and sensitive data in memory stolen. The use of shared compute services such
as IaaS, PaaS and SaaS exacerbates this issue.
Business Impact
Hardware-based security provides the foundation of a secure enterprise by ensuring that
the hardware, ﬁrmware and boot process have not been tampered with. In addition,
hardware-based security functions like conﬁdential computing, and encrypted VMs and
containers ensure that data cannot be leaked due to activity by compromised or malicious
environments on the same system.
G00770252 Page 95 of 101Drivers
ObstaclesThe desire to extend trust from the hardware level of a system, through the OS, to
applications and workloads, including containers that run above it. This root of trust
needs the strong foundation that hardware-based security provides.■
Software-based isolation of security controls is inevitably fallible and will be
attacked, increasing interest in protection approaches rooted in hardware.■
Requirements for data sovereignty or the need to use IaaS providers in potentially
hostile parts of the world and protect these workloads from OS compromise or
virtual machine and memory snapshotting are increasing.■
Most hardware platforms for servers and mobile devices, including Android and iOS
devices, now include hardware-based isolation capabilities.■
In public clouds, enterprises do not have access to the underlying hardware and
must rely on hardware-based attestations provided by the CSP.■
Approaches to hardware-based conﬁdential computing vary across microprocessor
vendors, complicating application deployment using these techniques. No single
approach covers all use cases. Abstraction layers, such as Asylo (open source) or
Anjuna (commercial), may help, but add another layer of complexity and are not
widely adopted.■
Hardware-based security is strong, but may potentially still be broken by software
ﬂaws or side-channel attacks, such as Spectre and Meltdown.■
G00770252 Page 96 of 101User Recommendations
Sample Vendors
Amazon Web Services (AWS); AMD; Anjuna; Apple; Bitdefender; Fortanix; Google; Intel;
Microsoft; Samsung Electronics
Gartner Recommended Reading
How to Make Cloud More Secure Than Your Own Data Center
Select the Right Key Management as a Service to Mitigate Data Security and Privacy
Risks in the CloudMake strong isolation of sensitive code and security controls a mandatory part of IT
systems procurement, especially IaaS.■
Make a secure root of trust that can detect ﬁrmware tampering and monitor the boot
process a requirement for all hardware and IaaS suppliers.■
Enable “secure boot” features on all devices, servers, desktops and laptops wherever
possible.■
Evaluate the need for conﬁdential computing capabilities only for the most critical
applications that move to public cloud infrastructure.■
Check for compatibility issues with third-party approaches that also use
virtualization techniques before activating OS-based virtualization-based security.■
Explore the use of hypervisor-based approaches with security rooted in hardware
virtualization techniques to achieve similar levels of strong isolation.■
Plan different strategies for different devices and server platforms, as none of these
mechanisms are interoperable.■
G00770252 Page 97 of 101Appendixes
Figure 2. Hype Cycle for Edge Computing, 2021
G00770252 Page 98 of 101Hype Cycle Phases, Beneﬁt Ratings and Maturity Levels
Table 2: Hype Cycle Phases
(Enlarged table in Appendix)
G00770252 Page 99 of 101Table 3: Benefit Ratings
Source: Gartner (July 2022)T r a n s f o r m a t i o n a l Enables new ways of doing business across
industries that will result in major shifts in
industry dynamics
H i g h Enables new ways of performing horizontal
or vertical processes that will result in
significantly increased revenue or cost
savings for an enterprise
M o d e r a t e Provides incremental improvements to
established processes that will result in
increased revenue or cost savings for an
enterprise
L o w Slightly improves processes (for example,
improved user experience) that will be
difficult to translate into increased revenue
or cost savingsBenefit Rating Definition
G00770252 Page 100 of 101Table 4: Maturity Levels
(Enlarged table in Appendix)
Document Revision History
Hype Cycle for Edge Computing, 2021 - 4 August 2021
Hype Cycle for Edge Computing, 2020 - 31 July 2020
Hype Cycle for Edge Computing, 2019 - 9 August 2019
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Understanding Gartner’s Hype Cycles
Create Your Own Hype Cycle With Gartner’s Hype Cycle Builder 2021
Predicts 2022: The Distributed Enterprise Drives Computing to the Edge
Hype Cycle for Edge Computing, 2021
Cool Vendors in Edge Computing
Infographic: Understanding Edge Computing
G00770252 Page 101 of 101The Edge of the Edge Overview
Hype Cycle for the Internet of Things, 2021
© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00770252 Page 1A of 6ATable 1: Priority Matrix for Edge Computing, 2022
Benefit Years to Mainstream Adoption
Transformational Edge AI
Edge Computing
Edge Video Analytics
IoT
Micro Data Centers
SASENeuromorphic ComputingLess Than 2 Years 2 - 5 Years 5 - 10 Years More Than 10 Years
G00770252 Page 2A of 6ABenefit Years to Mainstream Adoption
High Edge Servers
IIoT Gateways5G
Cloud-Out to Edge
Edge AI Hardware
Edge AI Software
Edge Analytics
Edge as a Service
Edge Asset Life Cycle
Management
Edge Data Management
Edge-In to Cloud
Edge Management and
Orchestration
Edge Stream Analytics
IT/OT Integration
Low Code for Edge
Public Cloud for Mobile Edge
Single-Board Edge ComputersDistributed Cloud
Private 5G
Moderate Hardware-Based Security
LPWACDN Developer Edge Node
Edge PaaS
Immersion CoolingEdge Security
Peer-to-Peer Edge
Low
Priority Matrix for Edge Computing, 2022Less Than 2 Years 2 - 5 Years 5 - 10 Years More Than 10 Years
G00770252 Page 3A of 6ASource: Gartner (July 2022)
G00770252 Page 4A of 6ATable 2: Hype Cycle Phases
Innovation Trigger A breakthrough, public demonstration, product launch or other event
generates significant media and industry interest.
Peak of Inflated Expectations During this phase of overenthusiasm and unrealistic projections, a flurry of
well-publicized activity by technology leaders results in some successes, but
more failures, as the innovation is pushed to its limits. The only enterprises
making money are conference organizers and content publishers.
Trough of Disillusionment Because the innovation does not live up to its overinflated expectations, it
rapidly becomes unfashionable. Media interest wanes, except for a few
cautionary tales.
Slope of Enlightenment Focused experimentation and solid hard work by an increasingly diverse
range of organizations lead to a true understanding of the innovation’s
applicability, risks and benefits. Commercial off-the-shelf methodologies and
tools ease the development process.
Plateau of Productivity The real-world benefits of the innovation are demonstrated and accepted.
Tools and methodologies are increasingly stable as they enter their second
and third generations. Growing numbers of organizations feel comfortable
with the reduced level of risk; the rapid growth phase of adoption begins.
Approximately 20% of the technology’s target audience has adopted or is
adopting the technology as it enters this phase.
Years to Mainstream Adoption The time required for the innovation to reach the Plateau of Productivity.Phase Definition
G00770252 Page 5A of 6ASource: Gartner (July 2022)
Table 3: Benefit Ratings
Source: Gartner (July 2022)Phase Definition
Transformational Enables new ways of doing business across industries that will result in
major shifts in industry dynamics
High Enables new ways of performing horizontal or vertical processes that will
result in significantly increased revenue or cost savings for an enterprise
Moderate Provides incremental improvements to established processes that will result
in increased revenue or cost savings for an enterprise
Low Slightly improves processes (for example, improved user experience) that will
be difficult to translate into increased revenue or cost savingsBenefit Rating Definition
G00770252 Page 6A of 6ATable 4: Maturity Levels
Source: Gartner (July 2022)Embryonic In labs None
Emerging Commercialization by vendors
Pilots and deployments by industry leadersFirst generation
High price
Much customization
Adolescent Maturing technology capabilities and process
understanding
Uptake beyond early adoptersSecond generation
Less customization
Early mainstream Proven technology
Vendors, technology and adoption rapidly evolvingThird generation
More out-of-box methodologies
Mature mainstream Robust technology
Not much evolution in vendors or technologySeveral dominant vendors
Legacy Not appropriate for new developments
Cost of migration constrains replacementMaintenance revenue focus
Obsolete Rarely used Used/resale market onlyMaturity Levels Status Products/Vendors
G00775218 Page 1 of 5Infographic: Identify the Business Challenges That
I&O Can Solve With Edge Computing
Published 26 September 2022 - ID G00775218 - 1 min read
By Analyst(s): Jeffrey Hewitt, Bob Gill, Thomas Bittman
Initiatives:Infrastructure and Operations Leaders
Infrastructure and operations leaders are using a variety of edge
computing solutions to solve challenges that they face. Examples
of these solutions provide useful insights as to how edge
computing can be used to overcome various I&O hurdles faced in
vertical industries.
G00775218 Page 2 of 5Identify the Business Challenges That I&O Can Solve With Edge Computing
G00775218 Page 3 of 5
G00775218 Page 4 of 5Overview
Infrastructure and operations leaders can use edge computing solutions to solve business
problems. There are many use cases where edge computing can be applied effectively.
Data/event reporting is one such use case.
Examples of data/event reporting include automated notiﬁcations of building
exits/entries, when thresholds are reached (e.g., heat, cold, humidity) and event time-frame
frequencies. For this use case, edge computing can be used to reduce costs by thinning
data before it is transferred to a cloud provider for additional processing.
G00775218 Page 5 of 5Personal monitors like those for health monitoring and personal tracking devices can also
beneﬁt from edge computing. Edge computing can be used to meet security and privacy
requirements in the ﬁeld while supporting optimal medical treatment for patients.
A third use case where edge computing can solve a business problem is system
automation. When autonomous forklifts, vehicles, robots, manufacturing plant
automation or smart agriculture are in play, edge computing can be used to improve
quality control and proactively identify machinery issues that could disrupt processes or
production.
These examples of solving business problems provide aspects of edge computing
solutions that can be applied by infrastructure and operations leaders to other use cases
to solve business problems that they will face.
About This Research
The basis for this infographic is to answer the ongoing question from Gartner I&O clients
as to where they can start to understand how edge computing can help them solve some
of their current challenges. Data for this infographic was derived from provider websites
and brieﬁngs and Gartner client interactions.
© 2022 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00750874 Page 1 of 13Innovation Insight for Edge AI
Published 28 November 2022 - ID G00750874 - 14 min read
By Analyst(s): Arun Chandrasekaran, Eric Goodness
Initiatives:Technology Innovation; Artiﬁcial Intelligence; Cloud and Edge Infrastructure
The intersection of edge computing and AI offers potential for
augmented, real-time decision making and many innovative use
cases. Technology innovation leaders need to create a cohesive
edge AI strategy to harness the full potential of industry
innovations, while mitigating challenges and risks.
Additional Perspectives
Summary Translation + Localization: Innovation Insight for Edge AI
(05 January 2023)■
G00750874 Page 2 of 13Overview
Key Findings
Recommendations
EA/TI leaders, including CTOs driving business transformation through technology
innovation, should:The interest in edge AI is being driven by the need for real-time analytics and
stringent data privacy requirements. While there is broad-based interest across
industries, early adoption is in automotive, healthcare, retail, transportation and
process manufacturing sectors, such as consumer packaged goods, food and
beverage and Oil & Gas.■
Visual monitoring, predictive maintenance and operational efﬁciency are the key use
cases for edge AI today.■
Both large vendors and startups are recalibrating their strategy for edge AI with two
distinct architectural approaches: Cloud-Out and Edge In. While the vendor
ecosystem is nascent and fragmented across these approaches, the innovations
across these approaches will deﬁne the future evolution of edge AI.■
While edge AI can enable a plethora of new digital use cases, many organizations
struggle with articulating a clear strategic business value and are constrained by the
technical debt in their edge environments.■
Several innovations are emerging across the edge AI landscape with innovation in
computation chips, containers, TinyML, Federated learning, privacy preserving
computation and ModelOps being the most signiﬁcant.■
Create and empower a core innovation team to assess use cases & ROI, select
strategic suppliers and guide edge AI implementations across the organization.■
Carefully assess potential use cases for edge AI and weigh them against
measurable business value metrics, such as cost savings, top-line growth, customer
satisfaction scores and improved physical safety or security, with a clear timeline for
these goalposts.■
Commence an audit of your edge and IoT environments and task your core team to
identify gaps and risks, and mitigate them before rolling out edge AI use cases.■
G00750874 Page 3 of 13Introduction
The intersection of AI, IoT and big data processing is giving rise to exciting technology
developments and new use cases for edge computing, the most signiﬁcant of which is
edge AI. There are several factors driving the adoption of edge AI, such as need for real-
time processing, growth in IoT endpoints, stringent privacy requirements and enterprise
infrastructure, and workforce becoming more distributed (the recent pandemic has been a
huge driver of this).
Although most enterprise data is currently generated inside centralized data centers or
cloud regions, this pattern will change dramatically in the future. We predict that, by 2025,
75% of data will be generated outside these centralized facilities. As demand grows to
process this data at the point of creation — in order to gain real-time insights —
applications, AI training and inferencing will need to move closer to edge environments
near IoT endpoints. We forecast that there will be 11.7 billion IoT devices by the end of
2025, and their capabilities will expand as compute, security and bandwidth technology
evolves (particularly with more pervasive adoption of 5G). This will create a rich
foundation for edge analytics capabilities to be widely available. The beneﬁts, risks and
key use cases for edge AI are summarized in Figure 1 below:This core team needs to work closely with developers, data scientists, network
engineers, security engineers, platform team, AI engineers and trust ofﬁcers to ensure
robust and responsible AI implementation.■
Track emerging developments in AI to ensure continuous improvement and progress
of your edge AI strategy, and provide frequent feedback to your suppliers to ensure
that they meet your evolving needs.■
G00750874 Page 4 of 13Figure 1: Edge AI — Benefits, Risks and Key Use Cases
Description
Gartner deﬁnes edge AI as “A class of analytical and inferencing techniques deployed in
endpoint devices and their associated gateways and local servers. Edge AI systems can
reduce latency, reduce data transport consumption and improve local processing
capabilities.”
The growth of real-time interactive applications, such as immersive collaboration,
autonomous vehicles and virtual gaming with augmented reality has been a key driver for
the adoption of edge AI solutions. There have been key technology innovations in
hardware accelerators, storage & memory, 5G, TinyML (light-weight neural nets), federated
learning and synthetic data that point to a vibrant suite of technologies and ecosystems
to enable cutting edge use cases.
What will drive the future of edge AI? Figure 2 below summarizes the key technologies
driving the future of edge AI:
G00750874 Page 5 of 13Figure 2: Technologies Driving the Future of Edge AI
Advancements in AI hardware: Edge AI hardware includes a broad range of hardware
accelerators and custom chips optimized to execute deep neural network based
analytics applications. AI chips feature parallel processing capabilities to execute a
much larger number of calculations with low energy consumption that is needed for
the edge. The possible options with their pros/cons include:■
CPUs offer cost and compatibility advantages, but have limitations in terms of
performance and density.■
GPUs are highly popular in cloud and data center environments and can be
used for edge AI due to their high performance and programmability, but have
higher power consumption and large footprint.■
FGPAs can provide high-performance hardware accelerators in a compact form
factor, but suffer from programming complexity.■
ASICs lend themselves to a range of IoT devices and sensors at a lower cost
with lower power consumption, but are limited to the speciﬁc applications they
are programmed for.■
G00750874 Page 6 of 135G & multi-access edge (MEC): 5G can signiﬁcantly mobilize data speed and
capacity, as well as enhance business functionality. MEC represents an advanced
edge compute option intended to bring cloud storage and computing closer to the
network edge for sophisticated capabilities. 5G and multi-access edge computing
are highly appealing for edge AI use cases due to their ability to reduce latency,
provision massive bandwidth and move cloud processing closer to the edge.
Combined, 5G & MEC can enable next generation AI-powered use cases, such as
autonomous vehicles, immersive (such as AR/VR) experiences, real-time visual
inspection, low-latency gaming and interactive digital health solutions, to name a
few.■
DevOps: Agile software engineering is critical in creating a consistent and dynamic
software pipeline for edge AI projects. DevOps tools are critical in ensuring tight
application packaging, model service granularity, consistent code versioning and
automated software delivery. Some key DevOps tools that can aid include:■
Code repositories that help with source code management and container build
and management tools, which can simplify model packaging, scanning,
orchestration and distribution.■
Continuous integration/continuous delivery (CI/CD) pipeline tools that can
automate and accelerate software delivery. Model-serving tools streamline the
deployment of AI models to a target environment, such as Kubernetes.■
GitOps tools can be used to introduce version control, automation,
collaboration and compliance of infrastructure. All of this translates directly to
transparency, better resiliency and faster time to market.■
Containers are highly appealing for packaging and shipping applications to the
edge due to their light-weightedness, modularity and immutability.■
Security tools for build, runtime scanning, secrets management and workload
protection.■
G00750874 Page 7 of 13Benefits and Uses
Here are some of the drivers, beneﬁts and use cases of edge AI:TinyML: TinyML is a class of neural network implementations native to low-power
microcontrollers. TinyML moves inferencing to the edge and IoT endpoints, enabling
low-power devices to analyze and perform simple actions based on signals they
receive. In the short term, more edge or even possibly IoT devices will be embedded
with AI models, where the inferring is done locally while the training is done in a
centralized cloud environment. TensorFlow Lite is a popular open source TinyML
framework. While TinyML adoption is nascent, a number of vendors offer pretrained
models for edge use cases, such as object detection, conversation agents and
recommendation engines.■
Advancements in ModelOps: The advancements in newer AI techniques, automated
ML (such as AutoML) and ModelOps software that allows deployment, management
and synchronization of AI models will enable more robust governance and
operationalization of AI services on the edge. This is particularly critical for the
operationalization and automation of streaming analytics using event-based
architecture patterns.■
Privacy preserving computation: while edge AI use cases look feasible on paper, they
often suffer from data privacy and security challenges. The usage of trusted
execution environments with hardware level protection, along with synthetic data
(i.e., artiﬁcially generated data), can be used to further enhance privacy at the edge.
In addition, federated learning holds a lot of promise in the future.■
Federated learning & swarm intelligence: federated machine learning aims at
training machine learning (ML) algorithms on multiple local datasets contained in
local nodes without moving data to an external cloud or data center environment.
Federated ML enables more-personalized experiences without compromising privacy
for edge environments. Federated learning is foundational for enabling swarm
intelligence. Swarm intelligence is emerging as a promising approach in
decentralized ML, uniting edge computing, peer-to-peer networking and intelligent
coordination. The business use cases that swarm intelligence could enable include
superior ﬂeet management, fuel consumption reductions, more efﬁcient vehicle
routing and better network routing optimization.■
G00750874 Page 8 of 13Reduced Costs and Improved Performance — Operational Intelligence, Asset
Monitoring and Management: Demand for edge AI solutions that leverage data
generation for operational insights to include analytics, predictions and cuts across
all industries. Healthcare diagnostic centers, construction, mining, utilities and other
businesses with high-value assets use edge AI to manage and improve their
business operations and processes. The edge AI architecture is emerging as
preferred for operational intelligence use cases because of its ability to provide real-
time, actionable insights across distributed assets in potentially remote
environments. This use of edge AI provides a level of operational insight and asset
health that does not exist within traditional industrial applications and OT systems.■
Real-time analytics: An emerging use case is applying AI on video streams for real-
time inference for operational and business intelligence that are pushed from core
compute services to resource constrained edge hardware. There are numerous
computer vision (CV) techniques, including image, action and pattern recognition;
object counting and classiﬁcation; and facial authentication. In all cases, these CV
techniques run embedded within a non-IT asset, a device such as a gateway, or on a
local edge server. For the edge CV design pattern, CV alerts are generated on edge,
and most CV analytics are performed on edge. Use cases that are driving signiﬁcant
adoption of edge CV include quality assurance on manufacturing production lines,
crop and animal monitoring and management in agriculture, shopping cart creation
and check-out in grocery, and smart space management in commercial and
multifamily real estate. Visual inspection is a use case that is common across all
industries that must observe regulatory requirements for site inspections and
certiﬁcations. Computer vision on the edge is generally able to provide improved
capabilities at a lower cost.■
Product Servitization for New Business Models: Product servitization is a business
model wherein product functionality, not the commercial or industrial asset, is sold
as a recurring revenue service or a metered usage-base service. The future success
of product servitization lies in the use of IoT, distributed computing and AI to
improve product performance, and facilitate remote intervention and control when
needed. Edge AI ensures that product servitization achieves cost-effectiveness,
proﬁtability and that product functionality meets and exceeds customer
expectations. Edge AI also provides critical capabilities for asset autonomy,
improved service levels and avoidance and remediation of service impacting events.■
G00750874 Page 9 of 13A Catalyst for IT/OT Integration: IT/OT integration is an architectural trend and
organizational end state within industrial enterprises. IT/OT integration harmonizes
the authority and responsibility for both Information Technology (IT) and
Operational Technology (OT) estates. Edge computing and edge AI are vital
catalysts to accelerate convergence, alignment and integration of IT and OT. For
example, the ability to provide new and innovative real-time AI techniques, while
remaining disconnected from the cloud for extended periods of time, is a signiﬁcant
driver of edge AI across various industries. These include discrete and process
manufacturing, electrical generation plants, municipal gas and water networks,
commercial rail, pipelines and other sub-sectors in transportation and logistics.■
Deepening Real-Time, Secure Value With Distributed and Decentralized MLs: R&D
and commercialization of distributed and decentralized learning are key to the future
of edge AI. These techniques enable real-time value and privacy preservation, while
obviating a core compute resources for ongoing training. As an example, federated
learning is a distributed learning technique to train ML algorithms on multiple local
datasets on local nodes without exchanging data samples as part of the training
process. Local learning occurs on resource constrained edges, such as smartphones,
softbots, vehicles with some level of autonomy or on IoT edge devices (including
embedded technologies within machines and non-IT enterprise assets and
infrastructure).■
G00750874 Page 10 of 13Risks
Adoption Rate
Edge AI is an emerging technology that has an overall market adoption of less than 5%.
Some use cases, such as visual inspection and predictive maintenance, are becoming
more common across industries. Adoption of edge AI for real-time analytics is
accelerating, particularly in industrial scenarios. Edge AI models are used to analyze
sensor data from IoT endpoints with enhanced value being delivered by further integrating
it with other applications, such as inventory management, supply chain and physical
safety systems. Edge AI will be a highly transformative trend for the next decade, with
proliferation of IoT endpoints and more pervasive network connectivity via 5G.Security Risks: The distributed nature of edge environments make them more
vulnerable to security risks. AI services in general are vulnerable to newer forms of
attack, such as data poisoning and model reengineering, where edge AI services are
also vulnerable. However, a combination of poor data security practices, software
heterogeneity and hardware constraints exacerbate the risks at the edge. The
probability of software design ﬂaws, device misconﬁgurations and deployments
bugs are higher in edge computing due to technical debt and chronic under-
management.■
Poor Model Accuracy: Organizations often train their models using large datasets in
cloud or data center environments. However, the accuracy of those models can
dramatically wane as they encounter new datasets on the edge that they haven’t
been trained on before. On a similar note, taking pretrained models and using them
for even allied applications can often make those models error prone and complex to
evolve for ﬁtting the use cases. Placing a high degree of emphasis on data quality,
implementing real-time model monitoring and a thorough POC on use case ﬁt are
critical to avoiding risks.■
Limited Processing Power: AI is one of the most computationally intensive
applications. Most edge devices tend to have limited computer power, storage
performance and network bandwidth, which can impact both the type of use cases
that can be pursued, as well as accuracy of the AI models.■
Heterogeneity: Edge environments are characterized by a high degree of
heterogeneity, which exacerbates reliability and manageability. Creating self-healing
architectures and common governance and management is a complex process that
involves signiﬁcant planning, capital outlay, know-how and operational excellence.■
G00750874 Page 11 of 13Alternatives
Cloud AI and on-premises data center-based AI deployments are an alternative to edge AI.
Cloud AI will continue to be popular due to the massive compute available to process
large amounts of data that are often needed for AI model accuracy. Apart from scalability,
Cloud AI also stands out for higher reliability, easier centralized operations and its ability
to provide a platform for complex AI use cases.
Most of AI training is still performed in a cloud or similar centralized environment, while
edge AI is emerging primarily as an inference platform. Cloud and edge AI will be hugely
complementary to each other in the future.
Recommendations
Create and empower a core innovation team to assess use cases and ROI, select
strategic suppliers and guide edge AI implementations across the organization.■
Carefully assess potential use cases for edge AI and weigh measurable business
value metrics, such as cost savings, top line growth, customer satisfaction scores
and improved physical safety or security with a clear timeline for these goalposts.■
Commence an audit of your edge and IoT environments and task your core team to
identify gaps and risks and mitigate them before rolling out edge AI use cases.■
This core team needs to work closely with developers, data scientists, network
engineers, security engineers, platform team, AI engineers and trust ofﬁcers to ensure
robust and responsible AI implementation.■
Track emerging developments in AI to ensure continuous improvement and progress
of your edge AI strategy and provide frequent feedback to your suppliers to ensure
they meet your evolving needs.■
G00750874 Page 12 of 13Representative Providers
Evidence
The research is based on 100+ inquiries handled on the topic of edge AI by the authors. In
addition, more than 50 brieﬁngs were conducted with edge AI vendors.
Forecast Analysis: Enterprise and Automotive IoT, Worldwide
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Applying AI in Industries
Invest Implications: Market Guide for Edge Computing Solutions for Industrial IoT
Get Ready For Data Management at the Edge: Key Considerations and Actions
Building an Edge Computing Strategy
Three Critical Use Cases for Privacy-Enhancing Computation Techniques
G00750874 Page 13 of 13© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00750874 Page 1A of 1AMarket Category List of Sample Vendors
Edge AI Hardware Advantech, AMD, ARM, Blaize, Brainchip, Cisco, Deep Vision, Dell, Gyrfalcon
Technologies, HPE, Hailo, Intel; Kneron, Mythic, NVIDIA, Syntiant.
Edge AI Software Amazon Web Services, C3 AI, Eurotech, Falkonry, FogHorn, Google, IBM, Intel,
Microsoft, NVIDIA, OctoML, Qeexo
Computer Vision Chooch AI, Deepen, Eyesight Technologies, Nauto, Neurala
Vertical Edge AI (Healthcare) Caption Health, Recursion, Overjet, Unlearn, Olive, Owkin, Insitro
Vertical Edge AI (Financial Services) Tractable, Zesty, Akur8
Vertical Edge AI (Manufacturing) Drishti, Landing AI
Vertical Edge AI (Food & Agriculture) Prospera, Aquabyte, Seewise
Vertical Edge AI (Retail & CPG) MSight, Syte, AIFi, Vue.ai
Vertical Edge AI (Transportation) Aurora, Algolux, Ghost, Momento, Konux, Deepmap
G00765954 Page 1 of 26Market Guide for Edge Computing
Published 4 October 2022 - ID G00765954 - 26 min read
By Analyst(s): Thomas Bittman, Bob Gill, Sylvain Fabre, Paul DeBeasi, Eric Goodness, Ted
Friedman, Tony Harvey, Jeffrey Hewitt, Sandeep Unni, Scot Kim, Tim Zimmerman, Mohini
Dukes, Eric Hunter
Edge computing is pushing a variety of vendors into a new
marketplace. I&O leaders can use this Market Guide to understand
the many facets of edge computing solutions, how vendors will
create strategies and offerings to support edge computing, and the
direction of this evolving market.
Additional Perspectives
Overview
Key Findings
Recommendations
I&O leaders responsible for edge computing projects should:Summary Translation + Localization: Market Guide for Edge Computing
(07 November 2022)■
Edge computing is rapidly evolving, with a broad range of existing and new IT
vendors creating or repositioning offerings.■
There are at least eight edge computing submarkets, with offerings often
overlapping between submarkets, but required for an end-to-end solution.■
Early adopters of edge computing are often focused initially on use cases versus
platforms and architectures, but as the trend continues to mature and extensibility
becomes central, offerings and the overall market will consolidate into a framework
of required capabilities/functionality.■
Enable extensibility in deployments by building an edge computing strategic plan
and focusing on platforms, frameworks and interoperability in early stages.■
G00765954 Page 2 of 26Market Definition
The edge computing market provides the hardware, software and services to extend an
agile digital enterprise to the edge, enabling lower latency, reduced data trafﬁc, and
semiautonomous computing. It must be easy to use and manage, connect seamlessly
and enable compute capabilities from simple ﬁltering to rich machine learning where
needed in the distributed computing value stream. Edge computing can be delivered as a
service, or through hardware and software deployed in the path of data ﬂow — at or near
the edge. Edge computing is used by enterprises of all industries to collect, process, ﬁlter,
augment and/or forward information within or from any location (e.g., ofﬁces, stores,
factories, vehicles) to achieve the stated business outcomes. Edge computing consists of
a number of overlapping submarkets, which are still forming.
Market Description
The edge computing market is rapidly evolving, as vendors in existing markets modify
their offerings to support new requirements, and as new technologies and vendors ﬁll
gaps. In this early phase of edge computing, the huge diversity of use cases leads to
many solutions that are ﬁrst-of-a-kinds, or highly customized. As standards and
repeatable patterns emerge, the overlapping submarkets of edge computing will resolve
themselves into a number of well-delineated, mature markets over the next six years or so.
Until then, we will discuss edge computing as a marketplace of overlapping submarkets.
Gartner believes the overlap between submarkets will reduce as vendors focus on speciﬁc
capabilities and partner with other vendors for total solutions. Gartner has identiﬁed eight
submarkets (see Figure 1):Prepare for the inevitable vendor and market consolidation as edge computing
matures by choosing partners that have broad market potential, and maintaining a
fallback migration strategy.■
Enable ﬂexibility by choosing solutions based on the total solution ecosystem of
partners, rather than only on speciﬁc vendors or speciﬁc vendor offerings.■
Edge management and orchestration ■
Internet of Things (IoT) platforms ■
Edge data management ■
Edge analytics and machine learning ■
Edge computing server solutions ■
G00765954 Page 3 of 26Figure 1. Edge Computing Submarkets
Gartner (2022)
This market guide does not cover some adjacent markets, such as IoT devices, or cloud
infrastructure and platform services.Edge communications infrastructure ■
Data center and content delivery network (CDN) edge services ■
Edge vertical industry solutions ■
G00765954 Page 4 of 26Market Direction
In 2022, edge computing is primarily driven by individual use cases, and not yet by
broader edge computing strategies. Use cases are highly diverse — by vertical industry, by
requirements (e.g., low latency or high-volume data), and by unique interactions (between
people, things and the business). Edge computing solutions are often part of speciﬁc
business improvements (e.g., improving the customer experience), or operations
improvements (e.g., factory automation). As such, the initial decision makers tend to be
business people (for example, responsible for retail store operations) or plant operations
professionals, not the IT organization, and they tend to be focused on business outcomes
rather than IT architectures. At the broadest level, edge computing solutions tend to be
part of an enterprise digital transformation (e.g., Industry 4.0 in manufacturing, or retail
store of the future). Currently, solutions are usually led by consulting, or systems
integrators (who often build around their own frameworks). However, edge computing
solutions today are often assembled as customized “ﬁrst-of-a-kinds.” Gartner describes
this as Phase 1 of the edge computing market, which we’ve been in for several years (see
Figure 2).
Figure 2. Phases of Edge Computing Market Focus
Gartner (2022)
G00765954 Page 5 of 26After enterprises start to deploy solutions for speciﬁc edge computing use cases, they
inevitably identify new use cases and workloads for their environments that have other
edge computing requirements (low latency, high-volume local data, tolerance to
disconnection, data privacy). Within vertical industries, this expansion will require similar
solutions, driving vertically oriented edge computing suites and packages. Gartner
describes this as Phase 2 of the edge computing market.
Many technology requirements for edge computing will span verticals. Rather than invest
in these technologies and services, vendors focused on speciﬁc vertical industries will
eventually leverage horizontal solutions. Requirements within enterprises will also extend
from the customer-facing storefront edge to the factory edge and the workplace edge —
putting more requirements on holistic, horizontal processes and technologies to improve
integration and security, and to reduce costs. Edge computing strategies and architectures
will drive the edge computing market in Phase 3.
Forward-thinking enterprises are focused on edge computing strategies and architectures
today — but they are not the primary buyers in the edge computing market yet. As strategy,
extensibility and architectures for edge computing mature, the marketplace for edge
computing will shake out and resolve into speciﬁc submarkets, and ecosystems of
partners delivering total solutions.
Market Analysis
The edge computing market will be in ﬂux for the next four to six years. The huge diversity
of edge computing and the customer focus on speciﬁc use cases (and on business cases
based just on those use cases) make it difﬁcult for vendors to build scalable business
models. Vendors are combining four approaches to grow their opportunity:
1. Build partnerships with vertical industry vendors, consultants and systems
integrators who already have customer relationships.
2. Find partners who can ﬁll capability gaps for unique use case and vertical industry
requirements.
3. Lower the barrier to entry with some form of usage-based pricing (even when the
hardware and/or software platforms have broader capability).
4. Create platforms and frameworks that enable extensibility to greater scale and more
use cases.
G00765954 Page 6 of 26In other words, while customers are usually use-case-focused, vendors are being much
more strategic. This “land and expand” approach helps vendors build a foundation for a
doable business model amid signiﬁcant usage diversity as customers inevitably expand
their edge computing use.
IT leaders who focus strictly on the best (often customized) solution for a single use case
risk:
The market for edge computing includes eight submarkets that are still forming up and
often overlap signiﬁcantly. Individual vendors may have offerings that span submarkets,
or individual offerings in several. In the next few years, as vendors seek volume
opportunities and partnerships, they may choose to expand into a submarket, or focus on
speciﬁc submarket(s). This volatility makes it very important for IT leaders to choose solid
vendor ecosystem solutions, or require a rapid return on investment and assume that
solutions may need to change. The eight submarkets of edge computing offer a way to
understand how the market is forming, and what different types of vendors are doing for
edge computing.
Edge Management and Orchestration
Analysis by Tom Bittman
Edge computing nodes require remote monitoring and management of possibly
thousands of edge nodes, without relying on manual intervention or on-location skills.
Edge management and orchestration (EMO) solutions provide layers of control over server
and device management, network and security management, the infrastructure software
stack and the applications themselves. EMO solutions provide for edge servers, edge
gateways and other edge nodes what uniﬁed endpoint management solutions provide for
mobile and client devices.
EMO solutions have several capabilities:Building a dead-end solution that doesn’t enable extensibility as their needs grow ■
Buying into a strategic vendor platform or framework that isn’t optimal for their long-
range requirements■
Fleet monitoring, management ■
G00765954 Page 7 of 26In these early days of edge computing, EMO offerings are either edge-native (and net
new), or may be evolving from existing data center, cloud or IoT management tools and
platforms. Because most net new workloads will be based on containers, EMO will often
focus on platforms that support containers and forms of Kubernetes. However, given the
amount of legacy operational technology (OT) and remote-ofﬁce/branch-ofﬁce solutions,
they may also support virtual machine platforms.
EMO can be provided centrally (e.g., from the cloud), or can be distributed, all the way to
the physical edge. In fact, as the density of computing at the edge increases (for example,
in a factory), orchestration across the edge may also become a requirement (orchestrating
local workloads, enabling horizontal scaling at the edge).
IoT Platforms
Analysis by Paul DeBeasi
The submarket for IoT platforms is growing, as both the capabilities and commercial
deployments are increasing. Despite the breadth of the IoT market, the most common use
cases focus on asset tracking, condition monitoring and compliance reporting. These use
cases aim to increase enterprise situational awareness and thus improve decision-making
quality. Asset-intensive industries such as oil and gas, chemical production and
manufacturing lead the way in demand for IoT platforms and services. These industries
rely heavily on edge infrastructure. Many organizations in these industries are in the midst
of edge infrastructure modernization initiatives that will use IoT solutions to help them
achieve their modernization goals. As a result, IoT platforms are a very important edge
computing submarket.Edge design studios ■
Edge application deployment and updates ■
Edge orchestration ■
Edge software platforms and platform as a service ■
Edge security technologies ■
G00765954 Page 8 of 26Every IoT system consists of edge and platform components. The edge is where data is
sampled and collected from the environment by instrumented “things” or devices. The
edge may also contain optional edge gateways that can provide stream processing, data
transformation and local storage, as well as integrate other systems to IoT platforms (e.g.,
data historians). The IoT edge is usually implemented in one of three ways:
The platform is where the IoT solution performs systemwide actions in collaboration with
the edge. The platform may perform device management, stream processing, advanced
analytics and workload orchestration. It may also invoke enterprise applications. The
platform is usually implemented on top of a composable set of IoT services developed by
one or more commercial providers. Taken together, the edge and platform form a
distributed system.
For more information about IoT platforms and services, refer to:
Edge Data Management
Analysis by Ted Friedman
Edge data management comprises the capabilities required to capture, organize, store,
integrate and govern data outside of traditional data center and public cloud
environments. Valuable data is increasingly generated and used outside of traditional
data centers and cloud environments. This data often has a shorter useful life span,
requiring value to be captured near the place and time of its origin. This is the role of data
management capabilities closer to assets in the physical world.
A number of factors are driving rising demand in the data management segment of the
edge computing market:IoT service provider software ■
Commercial gateway software (that integrates OT systems with the IoT platform) ■
Open-source software that provides linkages ■
Technology Opportunity Prism: Internet of Things ■
How IoT Service Providers Should Address Key Trends and Technologies ■
Create an Optimal IoT Architecture Using 5 Common Design Patterns ■
G00765954 Page 9 of 26While data management capabilities of various types exist in the portfolios of many
vendors targeting edge computing scenarios, the increasing focus speciﬁcally on
collecting, controlling and leveraging edge data has caused more vendors to develop
targeted data management functionality. The majority of these are focused on data
persistence (edge storage and database capabilities) and on integration (capabilities to
connect to, ingest from and provision data to edge environments).
For more insights on edge data management issues and challenges, refer to:
Edge Analytics and Machine Learning
Analysis by Eric Goodness, Eric HunterValue of real time: By placing data, data management capabilities and analytic
workloads at optimal points ranging all the way out to endpoint devices, enterprises
can enable more real-time use cases. In addition, the ﬂexibility to move data
management workloads up and down the continuum from centralized data centers
or from the cloud to edge devices will enable greater optimization of resources.■
Data gravity: Bandwidth costs and scenarios with limited or intermittent connectivity
demand the ability to organize and process data closer to the edge.■
Expanded scale and reach: By using distributed computing resources and spreading
the load across the ecosystem, enterprises can broadly scale their capabilities and
extend their impact into more areas of the business. This includes use cases and
outcomes traditionally managed only via operational technology teams, such as
those managing equipment in industrial settings. Dedicated hardware for edge
processing of data will continue to amplify these beneﬁts.■
Resiliency: Pushing data management capabilities toward edge environments can
also bring beneﬁts in the form of greater fault tolerance and autonomous behavior. If
edge environments do not require centralized resources, then issues with
connectivity to, or unplanned downtime of, those centralized resources don’t disrupt
processes that rely on local edge capabilities.■
Get Ready for Data Management at the Edge: Key Considerations and Actions ■
Top Trends in Data and Analytics, 2022 ■
G00765954 Page 10 of 26Analytics is the discipline that applies logic (“rules”) and mathematics (“algorithms”) to
data to provide insights. Machine learning (ML) is an advanced type of analytics and is a
subset of artiﬁcial intelligence (AI) that solves problems using algorithms and statistical
models to extract knowledge from data. “Edge” analytics and ML means that the
analytics and ML are executed within endpoint assets, gateways, edge devices and local
servers closer to where the data and decisions of interest are created and executed.
Some of the key drivers advancing both innovation and demand for edge analytics and
ML offerings include:
Data sovereignty and governance: Issues related to sensitive/regulated data can
constrain data and analytics teams from adopting centralized/cloud-based
environments.■
Evolving cloud perimeter and architectures: The increase of distributed computing
and hyperconverged solutions from public cloud providers are further decentralizing
previously cloud-restricted workloads. This perimeter expansion of the cloud brings
compute and storage closer to the edge — creating new possibilities for edge-centric
analytic workloads.■
Emerging edge-in to cloud architectures: Edge-in to cloud describes an architecture
where edge applications, servers, gateways, and embedded technology stacks are
designed and built independently from any hyperscale cloud they may connect to for
cloud services. Edge-in architectures develop AI/ML capabilities. Rather than using
the hyperscale programming models, or identity and access management of a
hyperscale cloud platform (cloud-out), the application is centered around its role at
the edge ﬁrst and its need for hyperscale cloud application services second.■
Pervasive legacy environments: As data-centric outcomes continue to align as top
priorities across the enterprise, legacy environments and platforms are increasingly
coupled with edge-based analytics and ML solutions to support critical use cases.
Noteworthy is the growth in the use of ML and analytics at the edge in industrial
enterprises as they seek to augment legacy OT systems. Similarly, in commercial
environments, edge analytics and ML are augmenting and replacing functions and
capabilities within building intelligence and management.■
Purpose-built hardware: Semiconductor vendors are delivering chips optimized to
efﬁciently execute increasingly sophisticated deep learning analytics (a subset of
ML) to support increasingly advanced use cases at the edge.■
G00765954 Page 11 of 26Continued advances to edge analytics and ML will occur through advances in hardware,
software and development/implementation standards. With the cost of data latency,
unstable network connectivity and increasingly rigid data sovereignty and governance
demands, IT leaders must increase the awareness of edge analytics and ML capabilities.
This will ensure they align the best technical solution in support of vital business outcome
demands.
For more insights on edge analytics and ML, refer to:
Edge Computing Server Solutions
Analysis by Tony Harvey, Jef Hewitt
Edge computing server solutions providers comprise a heterogeneous group. These
providers offer hardware and/or software that is designed to be an actual vehicle for edge
computing or some infrastructure element supporting it. They may sell solutions from the
gateway edge up to the regional data center. These edge computing infrastructure
offerings may include:Ultra-low-power ML innovations: Technology advances and increased standards
development for small-scale ML deployments (referred to as “TinyML”) supports
systems with a target energy cost of 1 mW or less per inference action.■
Federated ML: Federated ML is an emerging and important edge-centric technique.
Federated ML trains ML algorithms on multiple local datasets contained in local
nodes without sharing data samples. Federated ML is an element of privacy
protection and it enables ML (speciﬁcally, deep neural networks) to use more data. It
also resolves data transfer bottlenecks and empowers collaborative learning for
better accuracy.■
Hype Cycle for Edge Computing ■
Innovation Insight for Federated Machine Learning ■
Emerging Technologies Impact Radar: Edge AI ■
Distributed-cloud solutions ■
Edge operating systems ■
Ruggedized servers ■
G00765954 Page 12 of 26There are a wide array of requirements for solutions in this category, depending upon the
expected location placement and speciﬁc feature needs. This range of requirements may
include ruggedization and zero-touch manageability, as well as hardware and software
security. This results in a spectrum of provider offerings from limited-function appliances
to more general-purpose/multiuse products. It is also quite common among this diverse
group of providers to offer defeatured, low-end solutions intended to meet target price
points.
Micro data center providers in this category enable the housing of compute and storage
infrastructure in small modular enclosures. These enclosures provide the required power,
cooling and connectivity support for that infrastructure and are typically less than 75
square meters in size, down to sizes that resemble a single-rack optimized server.
Distributed cloud solutions are offered by public cloud providers to bring public cloud
infrastructure on-premises or to an edge location.
Overall, edge computing server solutions providers are focused on the infrastructure itself
and not the broader aspects of an edge solution that incorporates colocation facilities and
applications. This means that whether they are server, intelligent gateway, HCI or micro
data center providers, they seek partners to be a part of the more comprehensive edge
solution.
I&O leaders with knowledge of edge integration with main data centers and/or public
clouds may consider one or more of these offerings when they are looking for speciﬁc
edge computing “point products” in terms of edge computing server solutions.
Edge Communication Infrastructure
Analysis by Tim Zimmerman, Mohini Dukes, Sylvain Fabre
The edge communication infrastructure component of the edge computing platform is
broken into the following elements:Hyperconverged infrastructure ■
Edge gateways ■
Micro data centers ■
G00765954 Page 13 of 26The physical communication, either upstream or downstream, can use the same or
different mediums. For example:
Data Center and Content Delivery Networks Edge Services
Analysis by Bob GillCommunication between the platform and the edge (IoT) devices or CDN and
communication from the platform, which may reside anywhere between the device
and the application in a colocation data center, private cloud or public cloud. This
element may also be called the Edge WAN communication infrastructure.■
Edge communication services. These services may include management and
network analytics, and may reside on the edge computing platform to control,
manage or report on downstream edge devices and the edge communication
infrastructure that connects them to the edge computing platform. Additionally,
these services may control or manage one or more upstream communication paths
(e.g., WAN management) to the application, if needed.■
A low-power wide-area network (LPWAN) or narrowband IoT can be used to connect
IoT sensors or IoT devices at a remote site to an edge compute platform. The long-
distance connectivity is designed for infrequent transmission and optimized for low
bit rates, and battery power consumption. However, from a local area context,
connectivity between the IoT endpoints and edge compute platform involves Wi-Fi,
Bluetooth or Zigbee.■
An environment that consists of mobile endpoints such as drones, robots or
automated guided vehicles, with more-frequent transmissions, at high data rates,
beneﬁts from 4G or 5G connectivity with the edge compute platform.■
A corporate site requiring connectivity to a CDN provider’s edge location can use a
ﬁxed or 5G/LTE mobile broadband access in the wide area. Bandwidth reliability and
optimization are important requirements to satisfy the user experience in a
collaborative environment.■
G00765954 Page 14 of 26It is important to remember that edge computing is foremost a factor of “topology,” or
where workloads and data are located and processed, rather than just speciﬁc
technologies. The “near edge,” closer to the core, includes regional data centers, carrier-
neutral data centers, telco locations and cellular towers. The “far edge” extends to the
locations where physical and digital worlds actually meet. In many cases, it isn’t practical
or necessary to extend physical infrastructure to the “far edge.” Two viable alternatives
include: a physical approach, with distributed data centers, and a hybrid physical and
logical, or software-based approach, as offered by the CDN providers.
Colocation/Data Center Providers
These providers allow:
CDNs as Edge Platforms
For the past two decades, CDNs have optimized internet bandwidth and improved web
viewing and application performance. They do this by distributing and caching content,
from an origin point outward to regionally located cache servers, which in turn act as
intermediaries. This serves to ofﬂoad the origin and allows “seeding” of the distributed
locations with copies of the content closer to potential users. While this appears to be
similar in philosophy to edge computing, it has often been largely conﬁned to an
outbound distribution of prepared content, much like a broadcast network. CDN offerings
have been evolving, however, now allowing the placement of not only outbound content,
but enterprise-developed interactive applications at the CDN’s distributed points of
presence, in essence turning a simplex broadcast model into a bidirectional distributed
compute model.
The CDN-as-edge platform model provides:The location of enterprise IT systems to be external to the traditional enterprise data
center, establishing distributed “points of presence” wherever the provider has a data
center.■
The interconnection of data centers, which helps the enterprise to create a “mesh of
infrastructure,” perhaps closer to concentrations of users or customers.■
Lower latency, lower bandwidth costs, greater resilience, or the fulﬁllment of
regulatory requirements on data placement, based on topology alone, without
signiﬁcant changes in application architecture or the need for complex distributed
computing models.■
G00765954 Page 15 of 26Edge Vertical Industry Solutions
Analysis by Sandeep Unni & Scot Kim
We view vertical expansion as the next evolution of edge computing solutions. There are
examples of edge computing deployments in all vertical industry segments, but
investment strides in industrial IoT in manufacturing and store transformation in retail
have seen these verticals gain early adopter momentum with edge computing for vertical-
speciﬁc workloads.
Edge Systems Integrators in Manufacturing
While any systems integrator can build edge computing solutions for any vertical,
solutions with industry specialization, like smart factory strategies in manufacturing, have
created market differentiation among the vendor landscape. Over the past ﬁve years,
manufacturers have shown increasing rates of adoption of industrial IoT platform
solutions. At the same time, over the past three years, 82% of respondents to the 2021
Gartner IT/OT Alignment and Integration Survey report that CIOs have become responsible
for OT systems. Furthermore, over three-fourths of respondents say CIOs have either
signiﬁcant or complete responsibility over various levels of OT-systems-related decisions
(see Survey Analysis: IT/OT Alignment and Integration).
Today’s manufacturing automation is all about the cloud; tomorrow’s manufacturing
hyperautomation is about an edge-in and cloud-out architecture. By coupling AI software,
edge processors and IoT, edge AI is bringing intelligence to the machine level.  
To facilitate a cloud-out/edge-in architecture, manufacturers require manufacturing-
experienced systems integrators to understand the intricacies of the shop ﬂoor problems,
like unplanned downtime, anomaly detection, asset health monitoring and predictive
maintenance. Some notable edge use cases for manufacturing are:A ready-made, highly distributed infrastructure, managed and operated by the CDN
provider, with state-of-the-art networking, security and orchestration already in place.■
Application development platforms and tools in place, supporting familiar
approaches such as VMs, containers and serverless.■
IT/OT convergence and system integration ■
Connectivity of industrial assets and product lines ■
Contextualization of asset telemetry data ■
G00765954 Page 16 of 26System integrations that are adept to the smart factory and factory-of-the-future vision
and initiatives will empower manufacturing CIOs to propagate an edge architecture
throughout all lines of businesses within the manufacturing value network.
Industry Specialists in Retail
The retail industry continues to witness tremendous transformation of the physical store.
To survive, retailers must use data-driven, intelligent in-store execution to increase
operational visibility and efﬁciency, as well as to improve customer engagement and the
associate experience. Retailers have struggled with gaining real-time visibility and
intelligence about store operations, thereby limiting their ability to bridge the gap between
the physical store and digital channels. Furthermore, on-shelf availability of stock is an
urgent business driver, not only to facilitate customer satisfaction and store revenue, but
also to enable store-based fulﬁllment of online orders.
Edge architectures facilitate processing of large volumes of data across a number of IoT
technologies getting deployed at the “store edge.” This includes technologies such as
smart shelves, smart check-out, smart robots and RFID, as well as traditional store
systems like point of sale. Edge computer vision is also being deployed to power real-time
video analytics to track and reduce shrinkage in the store. Shrinkage is an enormous pain
point in the industry for inventory loss and spiraling operational costs. The National Retail
Federation estimates that shrinkage totaled more than $60 billion in FY20.
Beyond in-store execution and operations, critical functions like merchandising and
demand forecasting can beneﬁt from real-time feedback on inventory status, as well as
customer preferences from the stores, resulting in signiﬁcant efﬁciencies in retailers’
balance sheets. Edge computing solutions continue to gain wider application in retail
along with in-store IoT solution deployments, while also providing the means to extend the
efﬁciency and life cycle of traditional systems.
Representative Vendors
The vendors listed in this Market Guide do not imply an exhaustive list. This section is
intended to provide more understanding of the market and its offerings.Extraction of insights of asset telemetry data ■
G00765954 Page 17 of 26Market Introduction
A list of representative vendors (see Note 1) is provided in the categories described below
(see Tables 1 through 8). This is not, nor is it intended to be, a list of all the vendors or
offerings in edge computing. It is also not, nor is it intended to be, a competitive analysis
of the vendors discussed.
Several vendors provide products in more than one edge computing submarket. However,
each vendor is listed only once in what Gartner considers to be its predominant category.
Table 1: Representative Vendors in Edge Management and Orchestration
Source: Gartner (October 2022) Avassa Control Tower, Edge Enforcer
 Platform9 Managed Kubernetes, Managed KubeVirt,
Managed Bare Metal
 Pratexo Pratexo Studio, Pratexo Micro Clouds
 Scale Computing SC//Fleet Manager
 Sunlight Sunlight Infrastructure Manager (SIM)
 Veea Veea Control Center, VeeaHub Manager
 ZEDEDA ZEDEDAVendor Product, Service or Solution Name
G00765954 Page 18 of 26Table 2: Representative Vendors in IoT Platforms
Source: Gartner (October 2022)
Table 3: Representative Vendors in Edge Data Management
Source: Gartner (October 2022) ABB ABB Ability Genix
 Envision Digital EnOS AIoT Platform
 Exosite Murano IoT Platform
 Hitachi Lumada IIoT
 Litmus Litmus Edge
 Microsoft Azure IoT
 PTC ThingWorx IIoT
 Software AG Cumulocity IoT, thin-edge.ioVendor Product, Service or Solution Name
 FairCom FairCom EDGE, FairCom EDGE HUB
 IBM IBM Cloud Pak for Data, IBM Cloud Satellite
 Macrometa Macrometa Global Data Network
 ObjectBox ObjectBoxVendor Product, Service or Solution Name
G00765954 Page 19 of 26Table 4: Representative Vendors in Edge Analytics and Machine Learning
Source: Gartner (October 2022) Aizip Aizip Intelligent Audio, Aizip Intelligent
Module, Aizip Intelligent Time-Series, Aizip
Intelligent Vision
 Arundo Arundo Edge, Arundo Marathon
 Crosser Crosser Node
 Edge Impulse Edge Impulse
 Falkonry Falkonry Analyzer
 Hailo Hailo-8, HailoRT, Hailo TAPPAS
 OctoML OctoML
 Swim Swim Continuum, SwimOSVendor Product, Service or Solution Name
G00765954 Page 20 of 26Table 5: Representative Vendors in Edge Computing Server Solutions
(Enlarged table in Appendix)
G00765954 Page 21 of 26Table 6: Edge Communications Infrastructure
Source: Gartner (October 2022)Gateways:
 ADLINK MXE-210 Series
 Eurotech DynaGATE, DuraCOR, ReliaGATE
 Schneider Electric FactoryCast HMI Web Gateway
Cellular/LPWA (4G/5G/Private 5G):
 Ericsson
 NTT Communications
 VerizonVendor Product, Service or Solution Name
G00765954 Page 22 of 26Table 7: Representative Vendors in Data Center and CDN Edge Services
Source: Gartner (October 2022) Akamai
 Cloudflare
 Cyxtera
 Digital Realty
 Equinix
 Fastly
 Lumen
 StackPathVendor
G00765954 Page 23 of 26Table 8: Representative Vendors in Edge Vertical Industry Solutions
Source: Gartner (October 2022)
Market Recommendations
Be strategic. It’s important to start small, but think big with edge computing. Too often,
enterprises focus tactically on the requirements of a single edge computing use case, but
then build a dead-end solution that doesn’t enable extensibility as their needs grow.
Alternatively, it allows a vendor to insert a platform or framework into the enterprise that
isn’t optimal for long-term enterprise requirements. From the very beginning, it’s important
to plan strategically, consider future extensibility, start choosing frameworks and
standards, ask vendors what a future roadmap looks like and consider how vendors can
help you on your strategy. Of course, this doesn’t preclude starting small, on a speciﬁc use
case, and evolving over time. Accenture Manufacturing
 Acumera Retail
 Amazon Web Services (AWS) Manufacturing and Retail (Just Walk Out)
 Deep North Retail
 Kalypso Manufacturing
 NCR Retail
 Siemens ManufacturingVendor Representative Vertical Industry
G00765954 Page 24 of 26Align requirements and expectations. In addition to vendors, edge computing involves a
lot of stakeholders in the enterprise — across business units, IT, security and OT staff
(especially in an enterprise that already has decades of OT, OT processes and ownership).
Requirements, capabilities, expectations, roles and responsibilities all need to be clearly
communicated. Edge computing is not a well-understood concept, and it’s easy to have
misunderstandings about what is being created, and even what different terms actually
mean. Focus on clear communication, understanding and alignment with vendors, but
also across the business and IT.
Plan to be adaptable. The eight submarkets discussed in this market guide all have
overlap (and potential gaps). Vendors are jockeying for position with existing or brand
new technologies, ﬁlling gaps in their portfolios and creating new partnerships. Some very
important new technologies will emerge from startups that may become important
vendors in the market, or may be acquired by other vendors. Other startups will struggle in
a market that has the attention of so many major IT vendors. As the market matures,
vendor offerings will change. Vendors that are laser-focused on speciﬁc market
opportunities are less likely to be viable over time than those that have broad applicability.
Choose partners that have broad market potential, and maintain a fallback migration
strategy.
Choose resilient ecosystems, not just vendors. The diversity of edge computing use
cases and verticals requires that vendors partner to address different market opportunities
and different customer use cases. Rarely will a single vendor dominate. Some
partnerships will be resilient, while others might not last past a single customer
engagement. Choose ecosystems of partners who can help you on your overall edge
computing journey and help accelerate your innovation. Capabilities within a vertical
industry are much more important than capabilities for a single use case. A strong, ﬂexible
ecosystem is much more important than a single, strong vendor.
Note 1
Representative Vendor Selection
The lists of vendors and offerings are representative only. Vendors are explicitly only listed
once, even if they compete in several submarkets of edge computing. Representative
vendors were selected on the basis of one or both of the following:
Client interest via searches on the Gartner website and client inquiries ■
Vendor offerings that focused on edge computing ■
G00765954 Page 25 of 26Note 2
Gartner’s Initial Market Coverage
This Market Guide provides Gartner’s initial coverage of the market and focuses on the
market’s deﬁnition, rationale and dynamics.
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Infographic: Understanding Edge Computing
Predicts 2022: The Distributed Enterprise Drives Computing to the Edge
Hype Cycle for Edge Computing, 2022
Create an Optimal IoT Architecture Using 5 Common Design Patterns
Get Ready For Data Management at the Edge: Key Considerations and Actions
Top Trends in Data and Analytics, 2022
Building an Edge Computing Strategy
Innovation Insight for Federated Machine LearningVendors that are representative of a particular submarket or speciﬁc capabilities in a
submarket■
G00765954 Page 26 of 26© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
G00765954 Page 1A of 8ATable 1: Representative Vendors in Edge Management and Orchestration
Source: Gartner (October 2022) Avassa Control Tower, Edge Enforcer
 Platform9 Managed Kubernetes, Managed KubeVirt, Managed Bare Metal
 Pratexo Pratexo Studio, Pratexo Micro Clouds
 Scale Computing SC//Fleet Manager
 Sunlight Sunlight Infrastructure Manager (SIM)
 Veea Veea Control Center, VeeaHub Manager
 ZEDEDA ZEDEDAVendor Product, Service or Solution Name
G00765954 Page 2A of 8ATable 2: Representative Vendors in IoT Platforms
Source: Gartner (October 2022) ABB ABB Ability Genix
 Envision Digital EnOS AIoT Platform
 Exosite Murano IoT Platform
 Hitachi Lumada IIoT
 Litmus Litmus Edge
 Microsoft Azure IoT
 PTC ThingWorx IIoT
 Software AG Cumulocity IoT, thin-edge.ioVendor Product, Service or Solution Name
G00765954 Page 3A of 8ATable 3: Representative Vendors in Edge Data Management
Source: Gartner (October 2022) FairCom FairCom EDGE, FairCom EDGE HUB
 IBM IBM Cloud Pak for Data, IBM Cloud Satellite
 Macrometa Macrometa Global Data Network
 ObjectBox ObjectBoxVendor Product, Service or Solution Name
G00765954 Page 4A of 8ATable 4: Representative Vendors in Edge Analytics and Machine Learning
Source: Gartner (October 2022) Aizip Aizip Intelligent Audio, Aizip Intelligent Module, Aizip Intelligent Time-Series,
Aizip Intelligent Vision
 Arundo Arundo Edge, Arundo Marathon
 Crosser Crosser Node
 Edge Impulse Edge Impulse
 Falkonry Falkonry Analyzer
 Hailo Hailo-8, HailoRT, Hailo TAPPAS
 OctoML OctoML
 Swim Swim Continuum, SwimOSVendor Product, Service or Solution Name
G00765954 Page 5A of 8ATable 5: Representative Vendors in Edge Computing Server Solutions
Distributed Cloud:
 Google Anthos
Edge Servers:
 Cisco Cisco UCS E-Series Servers
 Hewlett Packard Enterprise (HPE) HPE Edgeline
 Inspur Inspur EIS200 Edge Microserver
 Lenovo Lenovo ThinkEdge
Hyperconverged Infrastructure:
 Dell Technologies VxRail Satellite Nodes
 Nutanix Nutanix AOS
 StorMagic StorMagic SvSAN
 VMware VMware vSAN
Edge Operating Systems:
 Wind River VxWorks
Micro Data Centers:Vendor Product, Service or Solution Name
G00765954 Page 6A of 8ASource: Gartner (October 2022)
Table 6: Edge Communications Infrastructure
Source: Gartner (October 2022) Vapor IO
 Zella DCVendor Product, Service or Solution Name
Gateways:
 ADLINK MXE-210 Series
 Eurotech DynaGATE, DuraCOR, ReliaGATE
 Schneider Electric FactoryCast HMI Web Gateway
Cellular/LPWA (4G/5G/Private 5G):
 Ericsson
 NTT Communications
 VerizonVendor Product, Service or Solution Name
G00765954 Page 7A of 8ATable 7: Representative Vendors in Data Center and CDN Edge Services
Source: Gartner (October 2022) Akamai
 Cloudflare
 Cyxtera
 Digital Realty
 Equinix
 Fastly
 Lumen
 StackPathVendor
G00765954 Page 8A of 8ATable 8: Representative Vendors in Edge Vertical Industry Solutions
Source: Gartner (October 2022) Accenture Manufacturing
 Acumera Retail
 Amazon Web Services (AWS) Manufacturing and Retail (Just Walk Out)
 Deep North Retail
 Kalypso Manufacturing
 NCR Retail
 Siemens ManufacturingVendor Representative Vertical Industry
G00757917 Page 1 of 17Predicts 2022: The Distributed Enterprise Drives
Computing to the Edge
Published 20 October 2021 - ID G00757917 - 19 min read
By Analyst(s): Thomas Bittman, Bob Gill, Tim Zimmerman, Ted Friedman, Neil
MacDonald, Karen Brown
Initiatives:I&O Platforms
Digital transformation is pushing enterprises to be more distributed
and expand to the edge. I&O leaders must plan for the growth of
IoT and related data at the edge, protect their enterprise from edge
growing pains in security and connectivity, and prepare for
changes in edge computing use cases.
Additional Perspectives
Summary Translation + Localization: Predicts 2022: The Distributed Enterprise
Drives Computing to the Edge
(29 December 2021)■
Invest Implications: Predicts 2022: The Distributed Enterprise Drives Computing to
the Edge
(26 October 2021)■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 2 of 17Overview
Key Findings
Recommendations
I&O leaders responsible for edge computing should:Digital business solutions increasingly demand faster data distribution and reduced
latency, requiring more data management and processing outside of cloud and
traditional data center environments.■
Edge solutions have historically been managed by the line of business, but the
responsibility is shifting to IT for operations and to optimize cost.■
5G is considered a major enabler of edge computing, but low-band 5G is the
foundation of carrier 5G offerings and has similar performance to 4G LTE.■
Bandwidth and bandwidth costs will improve, but more slowly than the cost of
computing and the rate of data growth.■
Deep learning at the edge positions the edge device as an algorithmic or model-
based intelligent data capture mechanism for further upstream processing and
analytics.■
Maximize edge deployment success by extending the organization’s data
management strategy, modernizing data management capabilities and allocating
personnel to address edge data requirements.■
Actively discover and identify all devices connecting to the edge network by
monitoring connectivity. Implement a governance policy that places all unknown
devices into quarantine.■
Mandate that the carrier has guaranteed 5G coverage and bandwidth by deﬁning the
use case requirements and creating a minimum SLA with penalties.■
Manage edge complexity by identifying technologies and vendors that can assist in
managing on-location edge computing nodes (e.g., as a service, remote
management).■
Ensure efﬁcient and effective deep learning at the edge by including edge in any
overall enterprise artiﬁcial intelligence (AI) effort — and share experience, skills and
models.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 3 of 17Strategic Planning Assumptions
By 2025, more than 50% of enterprise-managed data will be created and processed
outside the data center or cloud.
By 2025, as enterprises continue to converge OT/IT, 25% of edge networks will be
breached, up from less than 1% in 2021.
Through 2025, 80% of edge devices using 5G cellular as the primary connectivity option
will be decommissioned or utilize differing technology.
By 2025, bandwidth cost will be the primary driver for new edge computing deployments,
versus latency in 2021.
By 2027, machine learning (ML) in the form of deep learning (DL) will be included in over
65% of edge use cases, up from less than 10% in 2021.
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 4 of 17Analysis
What You Need to Know
Edge solutions are a nascent trend that will continue growing to enable digital
transformation in every industry, from “Industry 4.0” in manufacturing to smart buildings
to immersive classrooms. Digital transformation necessitates extending and distributing
the digital enterprise to the edge — where customers and employees, and buildings and
enterprise assets are located — and connecting everything and everyone digitally. Over the
next several years, edge solutions will experience growth, growing pains and evolution in
the trend itself. Our predictions cover those three topics.
Growth of devices and data. Gartner forecasts that the number of Internet of Things (IoT)
devices will triple from 2020 to 2030 (see Figure 1), growing at a compound annual
growth rate (CAGR) of 11%. The fastest growing segments will be manufacturing and
natural resources (18% CAGR), healthcare providers (13% CAGR), and smart buildings
(12% CAGR). In addition, the data these devices produce will grow even faster, due to
larger data streams and increases in video resolution.
Security and connectivity growing pains. The growth of IoT and the shift from air-gapped
operational technology (OT) to networked edge devices will create a massive challenge to
extend enterprise security to newly connected edge devices. Many enterprises will fail to
do so. Connectivity itself will also be a challenge. Expectations that cellular connectivity
(and 5G) will meet enterprise requirements are set far too high.
Edge computing evolution. There are several factors driving edge computing, but latency
has been the primary driver to date. The rapid increase in data production at the edge will
shift the balance to reducing bandwidth costs by bringing more compute power closer to
the point of data production. Likewise, data and compute power will increase the use of
deep learning across a broad array of edge use cases.
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 5 of 17Figure 1. Enterprise and Automotive IoT Units 2020-2030
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 6 of 17Strategic Planning Assumptions
Strategic Planning Assumption: By 2025, more than 50% of enterprise-managed data will
be created and processed outside the data center or cloud.
Analysis by: Ted Friedman
Key Findings:
Market Implications:
Diverse use cases are driving the interest in edge capabilities for data and analytics.
These range from supporting real-time event analytics for system automation, control and
maintenance, to immersive experiences, to enabling autonomous behavior of “things.” An
increasingly popular use case is computer vision, where the capture of streaming data
and inferencing on it often must occur at or very near distributed assets. With more
digitalization of interactions and more instrumentation of assets at distributed customer
touchpoints, the volume of data created and processed at the edge is exploding. Unlike
traditional data centers and enterprise applications, edge computing digitizes everything
— and lots of this edge data is ephemeral. It has a very short half-life of value, and may
never leave the edge. It will be created and possibly stored, processed, analyzed and
destroyed without ever being collected in a data center or the cloud.Edge-generated data is growing dramatically in terms of volume and diversity. Since
it is often undesirable or impossible to centralize this data, organizations need to
support distributed data persistence models.■
Digital business solutions increasingly demand faster data distribution and reduced
latency, requiring more data management and processing outside of cloud and
traditional data center environments.■
Data sovereignty and solution reliability concerns are generating interest for data to
be stored “locally” in edge environments, as well as driving demand for techniques to
reduce data size and complexity while retaining ﬁdelity.■
Both I&O and D&A teams have limited knowledge and skills in edge computing
concepts and technologies, with resource-constrained device edges representing the
most substantial gap.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 7 of 17These requirements mean that enterprises must extend their data
management strategies, modernize data management capabilities
and allocate personnel to address edge data requirements.
By using distributed computing resources and spreading the load across the ecosystem,
the enterprise can more broadly scale its capabilities and extend the impact into more
areas of the business. This includes use cases and outcomes traditionally managed only
via operational technology teams, such as those managing equipment in industrial
settings. Pushing data management capabilities toward edge environments can also
bring beneﬁts in the form of greater fault tolerance and autonomous behavior. If edge
environments do not require centralized resources, then issues with connectivity to or
unplanned downtime of those centralized resources don’t disrupt processes that rely on
local edge capabilities.
The distribution and complexity of edge environments bring greater challenges from a
data and analytics governance perspective. It will be critical for teams to extend the reach
of their governance practices to include edge-resident data workloads and to push
controls on data quality, security, privacy, life cycle management and deﬁnitions/models
into edge environments.
Recommendations:
Provide support for data persistence in edge environments by including edge-
resident IT-oriented relational and nonrelational database management systems
(DBMSs), as well as small-footprint-embedded databases for storage and
processing of data closer to edge devices.■
Optimize distributed data architectures for their use cases by balancing the latency
requirements against the need for data consistency between cloud/data center and
edge, as well as across edge environments.■
Use edge topology to address data sovereignty and availability requirements while
minimizing risk by extending data and analytics governance capabilities to edge
environments and providing visibility through active metadata.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 8 of 17Related Research:
Get Ready for Data Management at the Edge: Key Considerations and Actions
Top Trends in Data and Analytics for 2021: Data and Analytics at the Edge
 Hype Cycle for Data Management, 2021
Strategic Planning Assumption: By 2025, as enterprises continue to converge OT/IT, 25%
of edge networks will be breached, up from less than 1% in 2021.
Analysis by: Tim Zimmerman, Neil MacDonald
Key Findings:
Market Implications:Maximize the success of edge computing solutions by investing in skills
development and ensuring collaboration between OT teams and data management
roles in IT.■
Network and device security are not core competencies of most edge device
manufacturers.■
Edge solutions have historically been managed by the line of business, but the
responsibility is shifting to IT, and organizations are utilizing IT resources to optimize
cost.■
Edge systems have traditionally been air-gapped, but this is no longer the case due
to automation and remote data gathering requirements.■
Edge locations are rarely highly secure, so device theft and compromise must be
assumed.■
Some edge devices incorporate their own wireless connectivity zones separate from
enterprise connectivity, enabling bridging onto the enterprise network from
unauthorized or compromised devices.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 9 of 17The responsibility for managing and securing edge solutions is shifting. Historically, these
solutions have been the responsibility of line of business leaders. As air-gapped solutions,
they had their own engineering and support resources. But a recent Gartner IT-OT survey
shows that over 75% of CIOs are now responsible for IoT devices at the edge. Securing IoT
edge devices and the data they hold, in fact any device that connects to the enterprise
network, is paramount.
For many edge network devices, enterprise security is not a priority. For example, many
devices that try to connect to the network by wired or wireless means don’t turn off Wi-Fi
once they authenticate to the wired network. Additionally, devices are incorporating
multiple radios where one Wi-Fi radio is used for enterprise communication and the other
Wi-Fi radio (or Bluetooth low energy [BLE]) is used for creating an underlay network or “hot
spot” for accessories such as keyboards or other input/output devices. This increases the
attack surface area. The problem is exacerbated by dated wireless intrusion detection
system (WIDS) and wireless intrusion prevention system (WIPS) algorithms/protection,
leaving new ways to access and attack the network. As edge solutions continue to evolve,
security must take the forefront in the architecture process.
Visibility must be the cornerstone of any edge security strategy. IT must discover all
devices on the network, identify them, and apply the appropriate security policy and,
optionally, quarantine them. Zero-trust security postures must assume the edge network
and indeed the device itself will be compromised, and security protection must be
designed for this. Edge network devices will also have vulnerabilities discovered and
patches as well as updates will be required. IT’s (and speciﬁcally, I&O’s) scope of
responsibility for patching must now extend to all components of edge solutions. Zero-
trust access policies must be in place and enforced so that even authenticated users are
governed to use only the required resources. All trafﬁc to/from edge locations should be
encrypted, and all data at rest should be stored encrypted.
Recommendations:
Actively discover and identify all devices connecting to the edge network by
monitoring connectivity. Implement a governance policy that places all unknown
devices into quarantine.■
Segment the edge network, and restrict communications to/from edge locations by
using ﬁrewalling capabilities with in-line threat-detection capabilities.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 10 of 17Related Research:
Segmentation or Isolation: Implementing Best Practices for Connecting ‘All’ Devices
The 6 Principles of Successful Network Segmentation Strategies
Three Styles of Identity-Based Segmentation
Strategic Planning Assumption: Through 2025, 80% of edge devices using 5G cellular as
the primary connectivity option will be decommissioned or utilize differing technology.
Analysis by: Tim Zimmerman, Karen Brown
Key Findings:Secure and defend in-depth by extending network monitoring to edge locations to
identify trafﬁc patterns indicative of a threat (see Market Guide for Network
Detection and Response).■
Improve security protection by extending enterprise patching processes to edge
systems where possible, designing compensating controls when devices cannot be
patched.■
Strengthen the procurement process for edge systems and software by including
speciﬁc questions on device security, edge data security and secure network
connectivity capabilities.■
Reduce the attack surface by mandating a security policy that requires turning off
unused network connectivity (whether it is switch ports, connectivity ports on
gateways or any wireless connectivity).■
Improve security oversight by extending security operations center (SOC) visibility
into edge locations and telemetry.■
Protect data at remote locations with limited physical security by assuming the
device will be stolen and encrypting all data at rest with the key protected in a trusted
platform module (TPM) or other strong hardware-based mechanism.■
Low-band 5G is the foundation of carrier 5G offerings and has similar performance
to 4G LTE.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 11 of 17Market Implications:
At the moment, there is a lot of marketing hype about 5G. Carriers offer many 5G products
such as 5GE, 5G and 5G Plus, Ultra Capacity 5G or Extended Range 5G, which differ in the
spectrum band that is being used and the functionality provided. Some offerings, such as
5GE, are not actually 5G.
There are three different spectrum bands that are used to provide 5G; low (600-850 MHz),
mid (1-6 GHz) and high (25-39 GHz, also known as mmwave). The higher the frequency,
the shorter the wavelength and higher the performance (mmwave has been tested as high
as 3 Gbps). However, for mmwave, the shorter wavelength makes it susceptible to
absorption from rain (also called rain fade) as well as absorption from walls and
reﬂection off of windows, which will affect not only the performance but also the range.
In addition to the low, mid and mmwave bands, there is also a difference in which version
of the 3GPP speciﬁcation has been implemented. While 5G has arrived, it will keep arriving
for the foreseeable future. Most 5G networks have implemented version 15 or 16 of the
3GPP speciﬁcation. Version 17, which is scheduled for 1Q22, provides important features
such as networking slicing and multiple input, multiple output (MIMO) capabilities that are
needed to provide network capacity. However, mission-critical applications will need
ultrareliable low-latency communications (URLLC) and massive machine type
communications (mMTC) to connect a large number of devices in the service area to
achieve many business cases.
All carriers use a combination of low, mid and high (mmwave) bands to describe their 5G
offering, but high band is typically available in parts of major cities or speciﬁc locations
such as stadiums. We expect low-band and midband 5G offerings that perform similarly
to 4G LTE to provide 80% of global coverage in the next 10 years, while high-band
solutions will only be reaching 50% coverage in the same time frame.
Recommendations:Lower frequencies are less susceptible to interference, meaning fewer, but larger,
towers need to be built to cover an area than mmwave, which are currently limited to
parts of cities.■
There are multiple versions of the 3GPP speciﬁcation that are implemented in carrier
5G offerings.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 12 of 17Related Research:
Assessing 5G Mobile Technology for Organizations
Strategic Planning Assumption: By 2025, reducing bandwidth cost will be the primary
driver for new edge computing deployments, versus latency in 2021.
Analysis by: Tom Bittman and Ted Friedman
Key Findings:Mandate that the carrier has guaranteed 5G coverage and bandwidth by deﬁning the
use case requirements and creating a minimum SLA with penalties.■
Standardize the implementation of 5G by deﬁning high band (mmwave) as a
minimum capability and minimum 3GPP speciﬁcation version associated with
coverage in the deployment location.■
Optimize investments by comparing the cost of 5G implementation to other
technologies such as 4G LTE, LTE-M, private cellular including CBRS and IEEE
802.11be for indoor and outdoor projects.■
Edge computing is deployed for use cases that: ■
Require low-latency responses ■
Create large amounts of data that can’t affordably be sent to a data center ■
Need to continue operating when their connection goes down ■
Require data to remain local for data privacy and sovereignty requirements ■
In 2021, low latency is the primary driver for edge computing deployments, and it will
remain an important edge computing driver in the future as low-latency use cases
grow.■
IoT deployments at the edge are growing 11% annually, and data production at the
edge will continue to grow even faster (e.g., higher-resolution video cameras, sensors
in connected “smart” products and autonomous ﬂeets of vehicles).■
Bandwidth and bandwidth costs will improve, but more slowly than the cost of
computing and the rate of data growth.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 13 of 17Market Implications:
In 2021, much of the focus on edge computing has been on reducing latency and
improving bandwidth. 5G promises both, at least for certain locations and over time.
Hyperscale cloud providers and data center providers are also investing in new edge
locations to extend their reach and provide local data center solutions close to major
population areas. Together, these will help address many edge computing requirements in
certain locations where the use case requirement is low latency.
However, as IoT deployments grow, the production and consumption of data will also
grow. Gartner forecasts IoT units to grow with an 11% CAGR through 2030. However, data-
heavy verticals will grow even faster (e.g., IoT unit growth in manufacturing and natural
resources will grow with a CAGR of 18% through 2030).
Three factors will make bandwidth cost the top driver for edge computing:
In addition, use cases will continue to grow at the edge, including interactions between
things and people at the edge. A typical location, whether a factory, a store or a home, will
have a mix of use cases that have a variety of requirements for data collection,
processing and analytics.
The modern data architecture will include the edge, and
enterprises need to make plans to capture the opportunities and
prepare for the challenges of managing data in edge
environments.A rapid and continuous growth of data at the edge ■
Continuous reduction in the cost of computing (Moore’s Law) ■
Maturing technologies to manage edge compute endpoints with zero touch ■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 14 of 17If an edge node or some other form of existing distributed computing is already available
on-site, it may also be used for use cases that are latency sensitive. The implication is that
edge computing will be addressed at all layers of the topology (between the cloud and the
device edge), but edge computing on-location (embedded, edge gateways, edge servers)
will remain a growing paradigm.
Recommendations:
Related Research:
Get Ready for Data Management at the Edge: Key Considerations and Actions
Building an Edge Computing Strategy
Infographic: Understanding Edge Computing
Strategic Planning Assumption: By 2027, ML in the form of deep learning will be included
in over 65% of edge use cases, up from less than 10% in 2021.
Analysis by: Bob Gill
Key Findings:Build an edge computing strategy by making explicit the need for some edge
computing capability on-location long-term.■
Manage edge complexity by identifying technologies and vendors that can assist in
managing on-location edge computing nodes (e.g., as a service, remote
management).■
Position the enterprise to capitalize on edge computing opportunities by building
distributed data management skills and extending the data management strategy to
include edge data persistence, integration, governance and analytics requirements.■
DL at the edge positions the edge device as an algorithmic or model-based
intelligent data capture mechanism for further upstream processing and analytics.■
DL at the edge can learn or uncover patterns otherwise lost in data compression,
such as which physical measurements combine to predict failure of a mechanical
device.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 15 of 17Market Implications:
To date, edge computing has been used because of physics and economics problems,
such as minimizing latency, optimizing bandwidth usage and ensuring
resiliency/autonomy of the systems. Most edge applications have relied on traditional
“hard coded” logic that tells the edge system explicitly what to do and under which
circumstances. In contrast, inferencing at the edge can use far richer data and perform
rapid classiﬁcation of inputs for appropriate processing, such as the conﬂuence of events
that predict a failure. It can bring a far greater degree of responsiveness and analysis to a
stream of data or sensor inputs, including images, audio and video.
Until recently, performing inferencing anywhere has implied the need for GPUs or high-
performance CPUs with enough horsepower to run the models. This drove electrical power
needs and costs to levels that made such implementations infeasible at the device edge.
Rather than relying on ever more power-efﬁcient and powerful hardware, which is being
developed, models such as TensorFlow Lite and TensorFlow Lite Micro are attacking the
problem from the mathematics and model side. The intent is to wring greater DL
efﬁciency and optimization out of existing MCUs and sensors.
These advancements in model optimization already allow true inferencing to take place in
devices as common and low cost/low power as Arduino (and compatible MCUs). When
we combine the rapid advancement in model optimization with the signiﬁcantly more
powerful silicon being developed, the multiplicative effect has the potential to bring
dramatically greater capabilities to embedded ML implementations. In other words, we
cannot only feasibly run DL inference truly at the edge, but widespread edge-based
training is probable within the next several years.Specialized ML models such as TensorFlow Lite Micro continue to advance in their
ability to run on lower and lower-powered devices, such as microcontroller units
(MCUs) or the sensors themselves. This lessens concerns with heat and power
consumption that resource-intensive systems created in the past.■
Emerging neuromorphic technologies promise signiﬁcant improvement in inference
speed and power consumption at the edge.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 16 of 17Commercial offerings that incorporate these advancements and simplify the process of
training and creating models, bringing data science results to nondata scientists, are
coming to market in 4Q21. These “simpliﬁcation interfaces” will be required to take
advantage of the rapid advances in models and hardware. “Democratizing” ML for
solution builders is critical, as there are simply not enough data scientists to handle the
increases in volume of devices and projects.
Drivers include:
Data at the edge and in IoT devices represents a volume problem, a latency problem, a
bandwidth problem and a cost problem. Distributing intelligence further out along the
edge computing continuum will reduce these issues.
Recommendations:
Related Research:
Tech Providers 2025: The Future of AI Is on the Edge
 Hype Cycle for Edge ComputingThe volume of data at the edge is massive and growing, requiring organizations to
address it where it lives rather than send it all back to the core data center or cloud.■
There is a need to extract and preserve greater value from sensor data at the edge.
Today, 99% of raw sensor data is discarded. By sensing and inferring at the edge, we
can curate summaries of previously lost patterns, classiﬁcations and anomalies, for
example; in the past, we threw them away.■
Ensure efﬁcient and effective DL at the edge by including edge in any overall
enterprise AI effort — and share experience, skills and models.■
Promote DL use where appropriate by tasking several members already involved
with edge to investigate DL capabilities to see what beneﬁts it can bring to existing
and planned edge use cases and applications.■
Choose DL solutions effectively by investigating the technical feasibility of
deploying edge DL and considering some of the emerging vendor offerings that
marry simpliﬁed interfaces to powerful capabilities.■
This research note is restricted to the personal use of .Gartner, Inc. | G00757917 Page 17 of 17Emerging Technologies: Critical Insights on AI Semiconductors for Endpoint and Edge
Computing
A Look Back
In response to your requests, we are taking a look back at some key predictions from
previous years. We have intentionally selected predictions from opposite ends of the scale
— one where we were wholly or largely on target, as well as one we missed.
This topic area is too new to have on-target or missed predictions.
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Get Ready For Data Management at the Edge: Key Considerations and Actions
Segmentation or Isolation: Implementing Best Practices for Connecting ‘All’ Devices
Assessing 5G Mobile Technology for Organizations
Building an Edge Computing Strategy
Tech Providers 2025: The Future of AI Is on the Edge
© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."
This research note is restricted to the personal use of .Gartner, Inc. | G00779926 Page 1 of 15Predicts 2023: Edge Computing Delivery and
Control Options Extend Functionality
Published 2 December 2022 - ID G00779926 - 16 min read
By Analyst(s): Jeffrey Hewitt, Sid Nag, Tim Zimmerman, Arun Chandrasekaran, Sylvain
Fabre, Eric Goodness, Jeff Vogel
Initiatives:Cloud and Edge Infrastructure
New development methods and control options lead edge
computing evolution. I&O leaders must get their timing right to
maximize business value from these ongoing changes.
Additional Perspectives
Overview
Key Findings
Recommendations
I&O leaders responsible for edge computing should:Summary Translation: Predicts 2023: Edge Computing Delivery and Control Options
Extend Functionality
(17 January 2023)■
Organizations are increasingly interested in using the same programming models,
APIs and management systems as public cloud in their edge computing systems —
particularly for infrastructure and platform layers.■
Beyond the public clouds, serverless functions-as-a-service (FaaS) are now being
used at the network’s edge to achieve just-in-time, personalized user experiences, and
for rapid processing of event data closer to the point of collection.■
4G/5G cellular connectivity to a local edge server is a more cost-effective closed
loop solution than DSL or satellite connectivity to the cloud.■
Edge compute is increasingly being consumed as a service to provide high levels of
ﬂexibility and to lower IT operating costs.■
G00779926 Page 2 of 15Strategic Planning Assumptions
By the close of 2027, nearly 10% of industrial control systems across North America,
Europe and China will combine IoT and ML-deep learning at the edge as a feedback
mechanism to closed loop control systems to automate processes and asset
performance, up from less than 1% in 2022.
By 2025, more than 70% of organizations will deploy hyperscale cloud edge solutions for
at least one of their edge computing systems in conjunction with their cloud deployment,
up from less than 15% in 2022.
By 2025, 15% of serverless functions will run on the edge, up from less than 5% today.
By 2025, 20% of 4G/5G Private Mobile networks will deploy edge computing platforms to
optimize bandwidth, reduce latency and provide better security, up from 5% in 2022.
By 2026, consumption-based storage pricing initiatives will replace most of traditional IT
budgeting, sourcing and hardware administration activities at the data center edge.Align with OPEX spending approaches by leveraging consumption-based pricing
from providers.■
Achieve time-to-deployment advantage by utilizing cloud providers for edge
computing solutions when appropriate.■
Improve edge computing efﬁciencies through the use of serverless computing
functions at the edge.■
Architect the right technologies needed for edge connectivity by identifying the
bandwidth, latency, security and business outcome requirements.■
G00779926 Page 3 of 15Analysis
What You Need to Know
Edge computing is a distributed paradigm that places information processing closer to
the physical location, where people and things connect to the networked digital world (see
Infographic: Understanding Edge Computing). Edge computing reduces latency and can
be delivered as a hardware and software solution or as a service. Edge computing enables
addressing a variety of business problems and use cases (see Infographic: Identify the
Business Challenges That I&O Can Solve With Edge Computing), and as a market, has a
myriad of providers who offer an array of edge-computing-related solutions (see Market
Guide for Edge Computing).
While currently nascent and heterogeneous, the overall edge computing market is
projected to grow signiﬁcantly over the next four years. One measure of this is the end-
user spending on the hardware infrastructure to support edge computing that
encompasses both what end users and service providers will consume. That spending is
forecast to grow at a compound annual growth rate of 20.9% to reach $18.8 billion by
2026 (see Forecast Analysis: Edge Hardware Infrastructure, Worldwide). If we look at
where infrastructure hardware spending is this year and compare that by use case to its
projected growth, it can be graphically represented this way (see Figure 1).
G00779926 Page 4 of 15Figure 1. Edge Computing Relative Spending and Growth by Use Case
The edge computing space is poised to advance both its deployment and control options
in a number of ways which will support this growth over the next four years.
G00779926 Page 5 of 15Strategic Planning Assumptions
Strategic Planning Assumption: By 2027, 15% of industrial control systems will utilize
edge IoT and ML-deep learning as a closed-loop feedback mechanism to automate
processes and asset performance, up from less than 2% in 2022.
Analysis by: Eric Goodness and Simon Jacobson
Key Findings:
Market Implications:
Smart factories are the future of production. The smart factory combines modern
technologies and standard work to innovate how factories operate.
Smart factory investments offer an opportunity to leverage different technologies — such
as cloud and edge computing, IoT and AI — to minimize risk, increase competitiveness
and innovate production processes and human interactions. Smart factories also play a
role in sustainability initiatives by using technology and data to improve automation for
resource efﬁciencies or lower carbon emissions.
In a 2022 Gartner Digital Business Impact on Supply Chain survey, users cited IoT and AI
as the most strategic technologies to enable digital strategies. There are three core
building blocks for a smart factory:Smart factories are a foundational element of smart manufacturing, which is part of
the broader digital supply chain and Industrie 4.0 initiatives. Smart factories create a
data-driven environment where workers, and technology interact in an open,
connected and coordinated fashion.■
Manufacturers have created edge-driven use cases and technology enablers to build
smart factories. Increasingly, edge AI-ML combined with IIoT design patterns for
enhanced automation of asset performance and process control has captured
increasing R&D investment and is being operationalized in some factories —
especially in life sciences where eliminating variability eliminates substantial risk
and cost.■
Edge computing and edge AI investments not aligned with digital enterprise
objectives can quickly morph into well-intentioned, stranded technology projects at
sites that create excess costs and constraints to executing broader digital initiatives.■
Industrial IoT to augment incumbent OT (e.g., equipment, PLCs) ■
G00779926 Page 6 of 15Critical to Success:
Factories have the precursors/foundations in place through legacy, thick automation
investments. The economic case for rip and replace is not materializing. More and more,
these foundations will increasingly depend on edge-based architectures. Bringing
intelligence and inference closer to these legacy OT systems, production equipment and
workers enhances real-time process control data for visibility and condition monitoring.
This allows for better intervention and variability elimination.
Recommendations:
Related Research:
Emerging Tech Impact Radar: Edge AI
Emerging Technologies: Edge Technologies Offer Strong Area of Opportunity — Adopter
Survey Findings
Emerging Tech Impact Radar: Edge Computing
Cloud-Out and Edge-In: How Cloud Service Providers Can Leverage the Two Edge
Computing ArchitecturesAnalytics to visualize production data ■
Autonomous things to automate targeted methods (e.g., cobots for packaging lines) ■
Avoid isolated technology projects by promoting edge AI investments for smart
factories as part of an agile and automated system designed to respond to
manufacturing demand.■
Ensure adequate governance of edge AI-enabled smart factory solutions by creating
hybrid teams that blend stakeholders across IT, OT, ET, supply chain and HR
functions. Such groups foster cross-enterprise communications and assure timely
and necessary upskilling.■
Safeguard scalability potential by integrating edge AI innovation operationalization
with continuous improvement and production systems. This reduces any
misalignment of objectives by providing clarity across strategy, KPIs, culture and
technology to ensure site readiness. It also will ensure the value creation in the
factory matches the value created for the customer.■
G00779926 Page 7 of 15Strategic Planning Assumption: By 2025, more than 70% of organizations will deploy
hyperscale cloud edge solutions for at least one of their edge computing systems in
conjunction with their cloud deployment, up from less than 15% in 2022.
Analysis by: Sid Nag
Key Findings:
Market Implications:
Hyperscale edge compute (HEC) is a public cloud-out to edge solution based on a
distributed cloud platform. It is managed and controlled through a hyperscale public cloud
service in which data storage and processing are placed close to the things or people that
produce and/or consume that information. Drawing from the concepts of mesh
networking and distributed computing, hyperscale edge computing strives to keep trafﬁc
and processing local and off the center of the network. HEC balances latency
requirements — often with a 5G mobile technology — and the bandwidth required for an
application, allows for autonomous operation, and enables the placement of workloads
and data that satisﬁes regulatory/security demands.Organizations are increasingly interested in using the same programming models,
APIs and management systems as public cloud in their edge computing systems,
particularly for infrastructure and platform layers.■
Hyperscale cloud providers compete in the edge computing market and are
expanding their suite of offerings in the market to complete and complement their
cloud offerings.■
Organizations are looking beyond cloud offerings and combining edge and 5G to
solve complex business use cases.■
G00779926 Page 8 of 15The advent of 5G connectivity from major telco providers could open up a new world of
edge solutions and offerings that we call the “superedge.” The superedge, unlike the
traditional or legacy edge, is built on the platform that hyperscale cloud providers offer as
part of their distributed cloud solutions, such as Amazon Web Services (AWS) Wavelength,
which can be a great aggregator for general purpose capabilities. These superedges have
cloud stacks that are identical in form, factor and functionality as on-premises private
cloud offerings. The solution is coupled with 5G connectivity to the cloud data center, as
well as the legacy edges on either side. This allows for solving latency, the traditional
challenge of edge technology. The high-speed, low-latency connectivity of 5G thus
ofﬂoads many functions to these superedges, as opposed to relying on cloud data centers
for certain hypercompute and distributed workloads.
Hyperscalers’ superedge offerings are also architected with open APIs, thereby creating an
ecosystem of developer communities that can leverage these distributed superedge
architectures to write applications, such as gaming, video/live streaming and high-
performance computing.
Recommendations:
Related Research:
Emerging Tech Impact Radar: Edge Computing
Emerging Tech: Hyperscale Edge Enables Integrated Edge Infrastructure and Platform
ServicesOrganizations must examine beyond their existing cloud provider offerings. They
must then focus on adopting edge offerings that capitalize on emerging
opportunities in the cloud-edge market by bringing cloud computing resources closer
to the physical location, where data and business activities happen while retaining
the power of centralized management from the public cloud.■
Demand edge offerings from cloud providers that include vendors that already have
relationships at the edge (e.g., industrial equipment, workplaces, automotive, etc.)
thereby supporting ecosystems that can help deliver complete solutions, as opposed
to discrete and stand-alone offerings.■
Deploy solutions that leverage the combination of cloud, edge and 5G technologies
to solve complex industry use cases, such as smart cities, gaming and high-
performance computing.■
G00779926 Page 9 of 15Cloud-Out and Edge-In: How Cloud Service Providers Can Leverage the Two Edge
Computing Architectures
Competitive Landscape: Hyperscale Edge Solution Providers
Strategic Planning Assumption: By 2025, 15% of serverless functions will run on the edge,
up from less than 5% today.
Analysis by: Arun Chandrasekaran
Key Findings:
Market Implications:
Serverless edge FaaS in beneﬁcial to developers in several ways:Beyond the public clouds, serverless functions-as-a-service (FaaS) are now being
used at the network’s edge to achieve just-in-time, personalized user experiences and
for rapid processing of event data closer to the point of collection.■
Serverless FaaS on the edge carry many of the beneﬁts of Serverless architectures —
developer productivity, elastic and on-demand compute resources with the added
beneﬁt of lower latency.■
Serverless FaaS is being reimagined for edge environments with a focus on
minimizing the impact of cold starts, offering stateful services and vendors
leveraging their global content delivery network (CDN) to execute functions from
edge locations that are closest to the end user.■
Public cloud providers are extending their FaaS to the edge while CDN providers,
such as Cloudﬂare, Fastly and Akamai are innovating by offering edge optimized
FaaS and expanding their suite of developer services.■
It provides them the ability to execute serverless functions — not in a speciﬁc cloud
region, but in the location closest to the user’s request. This allows developers to
deliver content to users with a fast and personalized experience.■
Serverless FaaS abstracts the compute and application runtime environment from
developers, enabling them to be more productive and focus on application
architecture and code.■
G00779926 Page 10 of 15Cloud providers, such as AWS and Microsoft Azure, are extending their Serverless
functions to the edge. AWS offers lambda@edge, Google provides Google Cloud
Functions and Microsoft offers Azure Functions for IoT Edge. Cloudﬂare has been a
pioneer in the Serverless edge FaaS space. The company offers Serverless workers aimed
at developers — recently open sourced its core engine, Workers — enabling developers to
do local testing and development and, in addition, offers a suite of storage services, such
as a key value storage, durable objects and a SQLite data store. Fastly offers
WebAssesmbly as the application isolation technology, enabling developers to create a
portable compilation target for Web applications. All of these vendors are striving to
deliver a uniﬁed edge FaaS solution that provides developer workﬂow tools and
operational features, such as observability and embedded security.
Edge serverless FaaS can enable organizations to build more real-time digital services and
improve customer experience for a variety of use cases, such as video streaming, content
stitching and delivery, targeted ads and API acceleration.
Recommendations:
Related Research:
A CTO’s Guide to Serverless Computing
Compute Evolution: VMs, Containers, Serverless — Which to Use When?Serverless FaaS is available on an elastic, on-demand basis, which positively
impacts the time to market and elasticity needs of digital services, while providing
an economical way to run them on an as needed basis.■
Identify use cases where leveraging Edge serverless FaaS can boost application
performance and deliver superior customer outcomes.■
Start with event-driven applications that are inherently “bursty” or highly
parallelizable to maximize the beneﬁts of the pricing model.■
Prepare I&O teams to rethink IT operations from infrastructure management to
application governance, with an emphasis on monitoring, cost management and
ensuring that application SLAs are being met.■
Run extended POCs with prospective vendors and evaluate them on performance,
cost, developer workﬂow tooling and operational excellence.■
G00779926 Page 11 of 15Infographic: An Overview of Serverless FaaS
Strategic Planning Assumption: By 2025, 20% of 4G/5G private mobile networks will
deploy edge computing platforms to optimize bandwidth, reduce latency and provide
better security, up from 5% in 2022.
Analysis by: Tim Zimmerman and Sylvain Fabre
Key Findings:
Market Implications:
Collecting data from remote sensors has historically been difﬁcult, but the ability to
deploy 4G/5G private cellular networks directly where they are needed — as well as the
ﬂexibility to install edge computing platforms in the ﬁeld wherever they are needed —
provides a cost-effective architectural option that is also agile and scalable.Cellular provides multiple options for connecting not only devices to the edge
computing platform, which provides location ﬂexibility, but also from the platform to
the location of the application (e.g., the cloud or colo).■
Outdoor storage locations, utilities and agriculture are a few organizations that have
large geographic coverage that require data collection from sensors, but have no
network connectivity. 4G/5G cellular connectivity to a local edge server is a more
cost-effective closed loop solution than DSL or satellite connectivity to the cloud.■
Some use cases — such as presence detection of people, things or activities that use
the raw footage from remote security cameras — have high performance
requirements and must utilize edge computing platforms with analytics applications
to process and act faster in time-sensitive use cases to avoid transporting the raw
data to the cloud.■
G00779926 Page 12 of 15Developers deploying edge computing platforms as part of their remote data collection
architecture must be aware of the data requirements to select the right cellular technology
from the ﬁve plus options (i.e., LTE, 4G, CBRS, 5G low-band or possibly 5G midband)
available and the ability to deploy it privately. The implementation will be important since
the available bandwidth on a private cellular network can vary widely, from 10 Mbps to
approximately 100 Mbps or more, depending on the technology. Additionally,
communication from edge devices to the edge computing platform may require different
performance, and therefore different technology, than from the edge computing platform
to the upstream application (e.g., in the cloud). This is because the edge compute
platform may analyze one or multiple data streams and aggregate the data, and send it to
one or multiple upstream applications.
This means that business problems that were once limited to satellite connectivity can
now be architected by organizations without necessarily depending on carriers (although
there may still be a need to connect the PMN and its edge to a public or private cloud, a
managed service provider or another corporate site). They can also meet application
needs by distributing processing closer to data collection using edge computing platforms
and meet the SLA that is needed by one or more upstream business applications.
Recommendations:
Related Research:
Market Guide for 4G and 5G Private Mobile Networks
Market Guide for Edge Computing
Infographic: Understanding Edge ComputingDocument the use case including the bandwidth, latency and security requirements
— as well as the expected business outcome to architect the connectivity that
utilizes the right technology or technologies.■
Implement ﬂexibility and agility in the solution by aggregating and analyzing data
from multiple sources to the edge computing platform that can be connected
upstream to one or more applications.■
Test the private 4G/5G network and edge compute platform placement, as well as
the aggregation/analysis to evaluate/ensure that the SLA connectivity metrics are
being met.■
G00779926 Page 13 of 15Building an Edge Computing Strategy
Strategic Planning Assumption: By 2026, consumption-based storage pricing initiatives
will replace most of traditional IT budgeting, sourcing and hardware administration
activities at the data center edge.
Analysis by: Jeff Vogel
Key Findings:
Market Implications:
I&O leaders are beginning to grapple with issues related to the expanding data estate at
the edge. I&O leaders realize that a signiﬁcant part of the infrastructure services will likely
remain on-premises and, as a result, require an edge compute and storage data services
strategy. Edge storage is an enabling technology and architecture that is responsible for
the creation, analysis, processing and delivery of data services at — or close to — the
location where the data is generated or consumed, rather than in a centralized
environment. In most use cases, edge storage enables processing at the data source to
minimize transmission time, reduce or eliminate bandwidth consumption and data
movement costs and to improve processing performance.
Many data centers are emerging or transforming themselves into cloud-connected edge
data centers. This is driving I&O leaders to change their role from the providers of
infrastructure to providers of data services everywhere. I&O leaders are reexamining their
storage strategies and vendor partnerships for data center cloud operating model
infrastructure, focusing on a metric-based platform services consumption model strategy
to effectively and efﬁciently handle data at the edge.The data center is no longer at the center with infrastructure and data services
expanding to edge locations to support data intensive, low latency workloads.■
Edge compute and storage is increasingly being consumed as a service to provide
high levels of ﬂexibility and to lower IT operating costs.■
Edge storage and compute are enabling decentralization of data to provide local
analytics, ability to process in real time and provide collaborative access from any
global location.■
The diversity of use cases and workloads at the edge introduces the potential for
issues in system management, costs, broadened threat radius and data governance.■
G00779926 Page 14 of 15Recommendations:
Related Research:
Innovation Insight: Rethink Your Enterprise Storage and Cloud Data Services Strategies for
the Edge Awakening
Market Guide for Consumption-Based Models for Data Center Infrastructure
Infographic: Identify the Business Challenges That I&O Can Solve With Edge Computing
A Look Back
In response to your requests, we are taking a look back at some key predictions from
previous years. We have intentionally selected predictions from opposite ends of the scale
— one where we were wholly or largely on target, as well as one we missed.
This topic area is too new to have on-target or missed predictions.
Recommended by the Authors
Some documents may not be available as part of your current Gartner subscription.
Market Guide for Edge Computing
Infographic: Identify the Business Challenges That I&O Can Solve With Edge Computing
2022 Strategic Roadmap for Edge (IoT) NetworkingCreate an edge storage platform strategy and key initiatives by identifying edge-
centric workloads, use cases and data service management methods.■
Investigate the use of storage as a service consumption as a cost-effective strategy
by working with ﬁnance and architects to create a business case.■
Choose an edge storage topology and platform approach by addressing unique
workload requirements that are self-healing, software-deﬁned, power-efﬁcient and
can scale cost-effectively to a wide range of data volume and data type
requirements.■
Identify what actions should be taken now to plan for optimizing IT operating
models to mitigate risks and avoid pitfalls that may jeopardize efforts at the edge.■
G00779926 Page 15 of 15© 2023 Gartner, Inc. and/or its afﬁliates. All rights reserved. Gartner is a registered trademark of
Gartner, Inc. and its afﬁliates. This publication may not be reproduced or distributed in any form
without Gartner's prior written permission. It consists of the opinions of Gartner's research
organization, which should not be construed as statements of fact. While the information contained in
this publication has been obtained from sources believed to be reliable, Gartner disclaims all warranties
as to the accuracy, completeness or adequacy of such information. Although Gartner research may
address legal and ﬁnancial issues, Gartner does not provide legal or investment advice and its research
should not be construed or used as such. Your access and use of this publication are governed by
Gartner’s Usage Policy. Gartner prides itself on its reputation for independence and objectivity. Its
research is produced independently by its research organization without input or inﬂuence from any
third party. For further information, see "Guiding Principles on Independence and Objectivity."



hello everyone a very good morning to  all of you  wonderful to have you with us on this  journey where we explore everything on  cloud computer on sorry edge computing  my name is diksha nirukan and i lead  nascom's cloud advocacy program  this webinar is a collaboration between  nascom insights and the nascom cloud  advocacy program  nasscom insights is the research arm of  nasscom and we publish multiple reports  every month on various technology topics  the most recent one that we published  just about last week  was around the air adoption index and  the ai game changers 2022 which is a  compendium of ai innovation stories  these reports and many other reports are  available on our community website i'll  share the link on the chat window  sometime soon  please feel free to explore this and the  other reports that we release  the nascom cloud advocacy program is a  platform that aims to bring together the  entire cloud ecosystem for continuous  conversations on cloud computing and the  value that it can bring to businesses  throughout the year we'll be conducting  multiple programs events webinars  roundtable discussions leader talks and  more to highlight different aspects of  cloud computing  covering the technology side and the end  user enterprise side as well  today's webinar on edge computing is one  such program  we visited this topic three years ago  when edge computing was just taking off  and we are revisiting it today to better  understand what has changed especially  since the pandemic how mature is edge  computing and sorry edge adoption today  what are some of the emerging  opportunities and what are some of the  important challenges that need to be  addressed  before we begin here are some  housekeeping rules  all of you will be on you through the  duration of the webinar should you have  any questions please please put these up  only on the question and answer panel  please not on the chat panel  if you wish to direct any question to a  specific panelist please mention their  name as well  i will try and take few of these  questions after their discussion and if  time permits if not then i will work  with the panelists to share their inputs  with all of you at later date  i will also share links to our reports  on edge computing that we did in 2019  and 2020 please feel free to  let's begin the downl  to share the  perspectives i have today for subject  matter experts from different aspects of  edge computing system integrators  solution providers and communications to  start with i have ajay paltry associate  vice president of india technologies  welcome ajay  uh thank you diksha for the introduction  good morning  next we have karthik sabasin business  consultant from tcs  hi katy good morning  good morning thanks  rajesh  vice president and global head of  managed hosting and cloud services data  communications  hi diksha and happy to be part of this  discussion  and finally sanjeev  vice president technology and  architecture global logic hi good  morning everyone and i'm very excited to  be here i'm sure that we will have a lot  of fun in this discussion  keep keeping the questions  thanks  thanks to all the panelists  uh i want to kick start this discussion  with the quest few questions around you  know overview of what has been happening  like i said in edge computing  uh so what are some of the broad trends  that you are saying uh seeing in this  space how is the adoption of edge  computing accelerating especially since  the pandemic i want to start with  karthik i think if you can first just  briefly introduce yourself and then take  the question  yeah sure direction  myself tech  i am i'm part of  tcs i'm working as a business consultant  uh currently i am leading  uh edge computing and ml operation coe  uh under cognitive business operation  unit for these things  i have overall like 15 plus experience  mostly in telecom domain and other  domains  to start with uh  when it comes to broad trends right the  edges  wide  is wider  so we have seen organizations uh  shifting uh towards cloud services uh in  past  where it is almost a homogeneous or a  centralized and scalable environment  past pandemic  a new trend of cloud adoption is getting  emerged  like more of like hybrid and multi-code  kind of an environment  it is becoming a de facto standard for  organizations uh to go towards  multi-cloud and hybrid cloud  when i say hybrid uh it is no more  a cloud private cloud or public cloud or  a  data center hosted by the organization  it is getting extended towards the  infrastructure at the edge  so  when i say edge or age computing it is  it is more about bringing the compute  storage and network uh  near to the data source or where the  data is getting generated  uh so we see like edge is going to be a  driving force  for the  in future for the compute world  so we see a lot of adoptions of edge  computing especially in  manufacturing retail  utility automobiles and agriculture and  especially in mobility sectors  where telcos are  bringing their capabilities or  network-centric capabilities especially  in  multi-access or make kind of a  infrastructure into this domain  though there are a lot of uh  benefits that edge brings right some of  the benefits are latency low latency  high bandwidth  uh kind of uh  and many more  latency is seen as the  major benefit for enterprises to adopt  edge computing  uh for their use cases  uh and the typical workloads what we see  that has been into production  is mostly on a a kind of a workload and  some kind of a control logic  workloads but  we are seeing a lot of pocs and proof of  values uh kind of uh use cases  uh in ar via spaces uh in mobility  spaces of drones or automated guided  vehicles  uh so  so in a nutshell we can tell like uh  currently the total compute of the  cloud uh  public cloud service is higher but  sooner or later the total compute that  edge computing technology is going to  provide  will go over uh overrule the total  compute of cloud services  so that is my taker  thank you  very much  yeah hi um so i'm rajesh avasthi i head  the cloud and hosting business for  product communication  i've been in industry for 30 plus years  in various roles from technology  delivery  technology  architecting  sales business development and product  management functions  uh happy to be part of this discussion  i've seen industry evolving over a  period of time  and most of the things that we have seen  are driven by the use cases which  various industry verticals are looking  for  and in that sense what we have seen  as the data volumes have grown  where we see the  velocity of data increasing we see the  veracity of data coming in from various  sources  uh the requirement is also becoming  important to see that we have  models where you process data closer to  where it is getting generated and with  industrial iot  becoming pervasive we will see this  going manifold further and that's where  we will see that more and more things  will be computed  near to the  location where the data is getting  generated  and there would be use cases which would  uh require low latency or a lower round  trip  accessibility those use cases need to be  closer to where the data is getting  consumed and that's where we see that  distributed cloud model is going to  become a pervasive  solution deployment from that we've seen  with hyperscalers like aws azure and  google things are getting consolidated  to the central sides um but certain use  cases which would need this kind of  capability which i was talking about i  think  edge is going to become very relevant in  that sense and we see that happening in  various industry verticals where  customers are looking for use cases  where they want to deploy compute nearer  to where they want to consume that  and data which is very important and has  to be  kept for historical reasons or for to be  processed on a central site is something  which would be sent to the cloud  environment so that's going to be a  technology trend that we will see over a  period of time  we see with cloud  getting the flexibility and the scale  that you wanted was uh becoming  important uh but with edge uh getting  added to that we will see a distributed  cloud deployment and that's something  which we have seen in some of the  industry verticals like communication  retail and other particles as well  so those are some of the things which i  believe are going to become very  important in the course of time  i'll touch upon the  distributed part of it later but ajay  over to you  yeah uh thank you diksha for giving me a  chance to express my thoughts on this  platform uh just introducing myself in  short uh my name is ajay palker and i'm  working as an avp in winter technologies  from last three plus years  uh winged has been in in this industry  from 18 plus years of having worked in  fintech mobility solutions iot  aiml successfully delivering blockchain  applications and methods  uh i personally have 11 plus years of  experience and uh  you know in this interest and i've  worked in crm market research bfss  segment and more  so uh  talking about the broad trends and you  know adoption of edge computing all  together  uh you know firstly you need to  understand that you know it's computing  market growth uh if you compare from uh  you know 2017 to 21 uh it has grown to a  compound annual growth rate of more than  35 percent approximately  uh around 10 percent of enterprise uh  generated data is created  and processed outside the traditional uh  you know centralized data center or  cloud  and uh gartner predicts that by 2025 uh  this figure would reach out you know up  to 75 percent uh as well so uh key  driving factors are the you know advent  of uh industrial  ideas rajesh actually mentioned  the physio networks and  you know increase  in the number of intelligent  applications  and uh you know growing load of that on  cloud infrastructure all together so  this this all signs indicate that 2022  uh you know will be a private year for  edge computing altogether so uh  also talking about broad trends  uh you know that we are seeing uh we can  see a good growth in industrial sectors  uh manufacturing  uh industrial process monitoring  uh  you know predictive maintenance all  together uh autonomous vehicles uh even  the smallest things like charging  stations uh there's transportation uh  healthcare smart homes smart cities  smart buildings uh and you know a lot  more things going on  can you bring me to my second question  uh related question is  uh in terms of use cases  how are the use cases of edge deployment  evolving  again from a pandemic perspective right  if you look at cloud computing we have  seen a lot of acceleration in cloud  adoption during the pandemic so are you  seeing that reflected in edge  deployments as well  and which are some of the verticals that  are leading the adoption of  edge computing and if you can you know  give us examples both from  indian enterprise perspective and the  global perspective that will be good and  for this question i want to start with  sanjeev  sure  again good morning everyone i'm sanjeev  i had the architecture practice uh in  global logic in addition to the  architecture practice i also  uh lead the uh the metaverse coe and the  low code notebooks you as well in global  logic so that's about me uh yes so  you're right so the as far as the uh the  uh the in the edge computing adoption  and the the things are changing into the  vertical uh automotive is one of the  great example that we can tell and uh  and if we talk about the india right now  the mahindra has actually already  uh put the a das feature into the xuv  700 right that's a classic example  though the level the maturity may not be  that level that like we are talking for  the autonomous car uh but that is the  that's a the the the start of that you  know innovation is happening in india as  well  so uh typically if we take the  automotive as an example right so  in the past typically we have 50 to 60  uh you know sensors on on on a car on a  connected car which is now increased to  100 plus and now it is keep more adding  more more and more uh sensor on board  onto the on to the on to the car itself  right which may generate huge amount of  data now in terms of connected cars if  it is connecting to the  uh to the to the central cloud and uh  you know sending huge amount of data  latency would be a big problem so how  the and this is where the edge computing  can  rightly fit into that where  the uh the data can be collected at the  source itself the decision can be uh can  be uh can be made at the at the vehicle  level especially from that uh the the  context of your information if i'm  driving a car and i need to know that  who is  going uh your nearer far from me and i  need to make the decision faster and if  you go everything for the central cloud  it would be it would be mess right and  that is where the classic example and  i'm sure that in the next three to five  years we will see lot of innovation in  this area and india would be leading  that because innovation  happens at a scale in india i am very  very positive that that will happen and  the intelligent solutions like  the equalization avoidance traffic  routing uh io flow detection for example  right and then  giving indication understanding the  behavior of the driver for example right  the those kind of  aspects of the use cases can be built  because now we have  exponential ai ml exposure we have a  compute we have a very uh oe uh from the  infrastructure standpoint people are  building lightweight uh battery  optimized uh  the the hardware devices which is on  boarded onto the on to the connected  cars or the autonomous so i think a lot  of things are happening and that's why  cloud uh  being a  distributed nature of the the edge  computing is is making it uh making it  uh possible as well as  5g is also coming so that we the vehicle  can be connected to the  mobile uh edge computing and the they  can process the data they don't need to  have everything on board as well so a  lot of innovations are happening in this  area  your inputs in terms of  edge deployments and also if the  panelists can also indicate  what is the  maturity in terms of how much of the  edge deployment sustained poc versus how  much of it is  commercialization  and that would also give an indication  of how mature this industry is overall  yeah sure  from my perspective like the pandemic  right the pandemic has accelerated a lot  of transformations uh digital as well as  in technology  uh  the lot of use cases that was uh built  specific to address the challenges of  pandemic  is going to be permanent in nature as  the customers as well as enterprise sees  value in it  uh it is going to be permanent and  previously those use cases were driven  by cloud services with limited  components on it  but uh slowly we are seeing the shift  many organizations are shifting their  focus towards edge native use cases uh  complemented by the cloud services right  especially in pharma manufacturing we  know a lot of changes has happened  in manufacturing specifically uh with  industry 4.0 and digital factory kind of  an initiative  we see a lot of use cases that are  enabled by edge  the itot integration is the main  priority for any manufacturing global  view of assets  whatever happening in the  on the real-time monitoring of their  assets  preventive maintenance a lot of  analytical use cases data gathering itot  integrations are happening at the  manufacturing level  from manufacturing perspective we have  seen lot of  ai kind of use cases and data analytical  use cases getting deployed and they have  moved beyond pocs and they are into  productions  still certain use cases on mobility like  agvs and any computer vision use cases  are in  mvp stages  but the organization intention we can  see that uh it has to be productionized  not only for a single factory  uh from a global strategy perspective  they want to scale it up to multiple  factories so they are looking for an  infrastructure uh from that perspective  not a piecemeal kind of an approach to  for a particular use case  at a global level how they can leverage  their data analytics and other stuffs to  bring value to the production  efficiency and the documentation  [Music]  yeah see  again various verticals are in different  stages when you talk about from an  adoption perspective if i go to your  question  specifically from india perspective  where are we whether we are in the poc  stage or are we in actual commercial  deployment so it depends on industry  vertical to vertical and also it depends  on the use case that you are picking up  okay um i'll possibly take example of  two industry verticals which i see uh  happening and yes there are others also  if you talk about uh telecom industry uh  telecom industry if you look at it and  i'm talking of b2b telecom not b2c  telephone  industry that we look at  uh two areas where we would see  adoption of edge computing being adopted  one area is when  5g is going to be  becoming an enabler for the enterprises  when they would deploy private 5g not  the 5g in the telco world  um i see that edge computing is going to  play a big role in that the  devices which need to be everything is  becoming a software defined today  so these would be deployed on the edge  nodes which would would be deployed in  the enterprise private 5g  deployments the other area that we will  see  and which is already happening  is on the sd-wan space  where most of the application or the  controllers are deployed on these  standardized compute so you no longer  have appliances uh being deployed there  and these are deployed on appliances on  which you can have various workloads  also so you see  sd van deployment and in in conjunction  with that you also see  uh secure access on the service edge  that you would create so sasi is going  to be deployed  we've already seen production  deployments of cdn network on the  network side where content is getting  delivered so these are production  systems which are very much in use  and are having the commercial  deployments also if i go to retail  specifically in retail if you look at it  i think in india specifically we are in  a poc stage or a stage where people are  looking at use cases  one one such use cases people are  talking about is the use of here we are  for  looking at the customer behavior and  then deciding how they would want to  kind of take it  forward from that and that's where the  edge deployment use cases are being  considered pocs are being done by some  of the retail customers  looking at inventory management is going  to be enabled by the edge devices  looking at the volumes that you are  talking of so those are some of the use  cases which i see in these two verticals  if i have to talk about  where we see  adoption of edge computing being taken  into account but one of the things which  we have seen is because when you are  talking about rolling out this in a  large scale  uh manageability from a central console  both from orchestration perspective  and monitoring and management  perspective is going to become very key  enabler for this technology to be used  for those kind of use cases because if  you're talking of retail and if you have  a retail company which has  thousands of stores or maybe thousands  or tens of thousands of store uh having  the workforce to manage this kind of a  deployment and monitor and manage that  is going to become a big challenge for  uh the  industry so having the central control  is going to become very important in my  view and that's something which is uh  which will be the enabler for  this getting utilized more and more by  other industry verticals as well  thanks  and i'll touch upon even the 5g aspect  and the  because those two i think together will  be a big boost for its computing and  adoption yeah yeah sure go ahead sanji  yeah one thing actually i just wanted to  add uh is that like now  uh there is very interested uh  interesting actually concept is now  implemented in the vehicle to everything  right so instead of on boarding the  devices onto the onto the on the onto  the vehicle itself you can actually have  that environment so basically your road  your your roads are actually equipped  with those devices  and i believe that some prototype is  happening uh i think uh mr gutkari has  mentioned something uh in one of his  speech that right now they are doing a  poc where instead of onboarding all the  devices uh sensors on the on the vehicle  itself uh the road is equipped with that  so you you don't need to have a huge  investment on that one onto the vehicle  so you can leverage that edge nodes all  together which is connecting to your  vehicle so vehicle to vehicle vehicle to  environment vehicle to road vehicle to  traffic and that kind of innovation is  happening so there's a very big concept  the the  the revolutionized concept it is  happening so i i highly recommend that  audience to  go and explore that and global logic has  recently done some innovation area uh  some pocs as well uh not in india but  outside uh so that's a very uh good  concept for us to explore as well we do  v2x  the smart roads concept in a way  are they over to you  yeah um so see um kovid19 will push more  companies to the edge we all know that  so we all know that you know cloud  computing has been receiving the line  share of attention throughout the global  pandemic and as we are heading toward uh  you know the third year there's a shift  from edge computing to its cloud at this  moment so uh talking about you know the  industrial verticals and uh you know  different uh you know approaches that  has been taken right from uh  you know we have been uh in this uh  pandemic working with a lot of different  industrial segments right from uh you  know manufacturing  uh  in bfsi as well uh in uh you know  construction industry and we've also  deployed certain things in farming and  you know  greenhouse uh you know verticals all  together so uh  when when when i say uh you know bfsi we  have done certain pocs as well as we  have deployed uh you know certain uh  projects as well so edge computing has  helped you know the customers to  uh you know fight against the issues at  the transaction side or you know  recognize the user while onboarding uh  this is what i'm talking specifically  into the retail banking new banking or  you know the fintech space as well so um  i'm currently working on retail uh  real-time uh you know uh facial  recognition for smooth customer  onboarding  uh fraud detection and prevention all  together so uh we are using face  liveness check uh you know comparison uh  uh we're doing with the captured  lightness picture  uh with the identification documents uh  at the atms uh you know uh in the cctv  you can uh use the facial recognition uh  in real time to detect potential fraud  uh uh you know you can  block down the atms you can alert the  customers the banks uh and uh you know  even the cops uh to prevent the crime  all together  uh  in manufacturing uh specifically uh we  are solving certain challenges like uh  you know incorporating internal  information uh uh including workflow uh  energy uh you know expertise data as  well as external contextual logistics  like uh you know geolocation  uh  the partner information the supply chain  uh and environmental inputs as well so  we connect manufacturers to both  structured and unstructured data uh  generated on the factory flows uh acting  on insights closer to where the data is  created uh which improves the production  quality enhances the operations uh  boost the kpi performance and  you know you can you know proactively  make certain decisions  i want to  now touch upon what rajesh  spoke about which is 5g  and bring in the aspect of matter was  also to understand  how are these two factors influencing  each computer rajesh i know you touched  upon it briefly  but if you can give us a more broader  view about  private 5g and if i can call it public  5g how is that evolving again  both from a global perspective we know  in the u.s there's a lot of uh advances  in private fighting that's happening and  acceleration is also increasing but what  about the rest of the world and what  about specifically  for india  how are these two panning out and how  will they play a key role in  edge computing and also the role of  metaphors  because i see that being very integral  to  i mean edge being very integral to  metals  so rajesh  5g is going to become an enabler for a  lot of things when we are talking of  things which would have augmented  reality or virtual reality and that's  where you're talking about meta what's  also coming into picture and when you  talk of here we are  you're talking about uh  an experience which should be a  lifelike experience and when you're  talking of life like experience  what you would want is that the  capabilities are such that it gives you  performance which is more real life like  and that's where i think  both edge computing  and 5g is going to be an enabler because  5g would give us the speed at which you  will be able to consume data from  various and i'm talking from a general  not even on only from a private 5g  perspective but also otherwise also but  there won't be certain workloads which  would need computing and if you're  talking about gaming as one such example  where people want real life like  experience today  then it has to be closer to where it is  getting consumed otherwise you'll be  getting data from or  sending data to a centralized location  which could be a cloud and that's where  you see  edge becoming a very pervasive part of  that solution  so in my view both edge and 5g is going  to be an enabler 5g from a perspective  that it would allow huge amounts of data  to be accessible at a speed which would  give real life like experience  uh edge from the perspective whatever  can be  localized and can be customized for a  certain segment of people or a  demography  it can be done  in on the edge computing side but all  this has to be  basically controlled from a central  location and that's something which is  going to make it more pervasive  so what in my view one of the challenges  which would be there when we want to  make it more  accessible for various use cases how do  you make sure that the entire ecosystem  becomes an enabler  and you have the right skill set to  manage that ecosystem that you have to  enable to create a use case  in my view edge is going to be more  driven by use cases than by technology  technology would be used for a specific  use case like what sanjay was talking  about if you're talking of a moving  devices like cars or automotive industry  use case is very different whereas when  you go to the retail where a customer  comes into a store which is a fixed  store the use case is very different  that you're talking about so uh use  cases will  uh decide what technology components and  how they would be integrated and that's  how we will see so 5g and edge computing  in that sense is going to work hand in  hand so they would be working in  conjunction to create that use cases for  our customers and  a better  user experience for our customers  and the method was rajesh how  i spoke about augmented reality is going  to become a part of metaverse because  you're talking of  a verse which is going to be  more artificial you would want  artificial intelligence to be part of  that  you would want more people talking about  that  and and using that in a business context  is going to  be something which would be coming over  a period of time i think today metaverse  is being spoken about in certain  industry vertical areas only it has more  of a  i would say it is in a stage where  people are excited about it uh gaming is  one area obviously where metaverse is  going to play a big this thing but  that's more of a b2c  how does it pan out in b2b scenario is  something which we would have to see i  see  uh some usage of that in form of what we  started off as the chatbots  i think they would become more  enabled on the  on the video side and artificial  intelligence would play a big role in  that particular space so it would evolve  over a period of time but at this point  in time in my view uh the use cases are  more b to c than p to b we'll see more  of that coming into b to b space  i'm exploring doing another either a  panel discussion on metaverse and  online gaming and cloud so hopefully  that should also happen sometimes  yeah um so see uh the role of 5g uh in  edge computing uh as rajesh has  you know explained that it goes  uh hand in hand uh you know the 5g  deployment and the age computing  altogether so uh i was just reading an  article from erickson digital uh so as  far as 5g is concerned uh it does  forecasted that by 2023 uh you know more  than 1 billion 5g devices will be  connected worldwide all together  so organizations uh you know are  investing in 5g to streamline you know a  lot of things like data collection  power the iot devices  enhance digital experiences all together  so under optimal conditions 5g  you know is about 10 times faster uh as  we have seen so but  while fighting can provide fast local  connectivity  uh the technology doesn't account for  commute or storage at this time so that  is why uh you know uh the cloud  computing uh it's basically uh you know  h2 cloud and 5g becomes a mediator there  uh most business still run on supporting  functions uh you know through a central  uh data center at this moment and which  creates latency and reduces network  performances all together  so 5g obviously is going to be a boost  there uh  looking forward you know more companies  will augment 5g deployments with edge  computing this approach can reduce  latency and help companies generate  stronger roi  uh from these rollouts uh the edge  computing also  uh helps uh with use cases like virtual  and uh augmented uh reality  uh altogether and uh as uh you know  rightly said self-driving cars uh which  require uh you know processing of  heavy amounts of data  uh in real time and uh you know  including the minimal latency altogether  so yeah this is uh the take on 5g  when it comes to edge computing  okay so moving on to the second thing  that we identified which is the cloud  and edge continuum right and again i  want to start with rajesh because you  touched upon orchestration  and  security uh how do you see cloud  computing evolved with edge computing  right basically the question of  centralized to decentralize and what are  the possible implications in terms of  you know architecture complexity like  you said orchestration access and  security data processing etc  rajesh can you  share your insights  sure  see again um as i told you edge is going  to be driven more by use cases  uh and as we are talking about data  getting generated from various sources  and being processed closer to the source  by the edge devices or the edge  compute or a cloud setup that you may  have closer to  the data source  there would still be need to have a  consolidated view of data as well  either for getting insights in a broader  sense  or also making sure that you have  historical data also which is there so  you will see  uh data and and i think  in analytics world people have been  using  that which is the etl function which is  you extract data you transform data and  then you use it for a specific use case  in my view edge is going to become more  like an enabler from an extraction and  transformation perspective and use it in  some form localized in that area but  eventually if you want to get a larger  or a broader view from the data which is  coming from various edge sources you  will have to have a consolidated uh  deployment also and that's where you see  most of the cloud providers are today  talking about edge computing being an  extension of the cloud setups which they  have uh and and that's where you see  they are saying that  we'll be able to give the same  technology which you have consumed in  your cloud environment  and be deployed closer to the source of  data for creating the use case which you  want that is one  one of the things which we have seen and  karthik can add more from a technology  perspective  i see kubernetes is becoming a de facto  standard when you're talking about  orchestration across environments  more and more applications are getting  modernized and when we are talking about  these use cases  uh on the edge computing in my view  these are going to be cloud native use  cases that means they would be using  some of the technologies which are being  used in the cloud arena  so orchestrating them through kubernetes  or k8s  is going to become the standard and  that's where i think you see most of the  hyperscalers are also talking about  their edge footprint being orchestrated  through kubernetes  uh making it easier for people to manage  it and also make sure that there is some  amount of standardization although there  is no standardization as of now  on these worlds that's where we see both  both areas kind of merging into this and  that's where we why i was saying that it  is going to become an  extension of distributed cloud so  earlier what we did was we had data  centers we consolidated those data  centers into cloud deployments now we  are saying for certain use cases you may  want to use the same technology closer  to the source of data and process it and  get an outcome of that from that on a  real-time basis  and that's something which would be  enabled by the edge cloud computing area  and that's where we see distributed  systems coming up  yeah  us rajasthani said  edges  is growing in many forms every time  starting from device uh to near age  forage regional age or towards the  footage to cloud architecture is  becoming is a complex thing  so the intelligent orchestration or  automation is going to be critical in  managing edge devices  so  we see lot of  technologies on self  diagnosing  or self monitoring and self healing kind  of capabilities  in order to manage such large scale of  devices  where they should not incline towards  any manual interventions uh kind of a  thing  apart from that like uh uh the security  is going to be a critical aspect uh it  has uh edges and layered architecture  right it may be top to bottom  uh the  security itself is evolving in different  domains they have a different trust from  these threat vectors  uh  there are different approaches by  different organization for security may  be a monitoring alert or vulnerability  assessment  risk mitigation kind of an approach  so those complexities  are there  in a short like the heterogeneity or  heterogeneous environment is going to  define edge in the future  so any centralized or unified  orchestration or a management plan you  should be able to handle  such heterogeneous it may start with a  small  gateway device or a small sensor device  to a hyper converged infrastructure kind  of a device  uh again in application perspective it  may be as registered it may be a  containerized or a physical or it can be  a  docker base or a kubernetes based  applications so the management plane  should handle such complexities in  managing monitoring  securely onboarding those devices  rollout of devices patches over the air  update  there are multiple complexities that  should be considered while deploying it  yeah  so see uh i'll rightly jump on to the  edge containers that we are actually  discussing at this moment  uh  so you know looking uh it's it's one of  the trends uh which is going on uh  so  uh  software containers which are deployed  at the age of the network uh along with  uh orchestration platforms like  kubernetes uh containerization uh you  know involves packaging applications you  know in a way  that uh you know isolate software and  its dependencies from other components  and containers are highly portable uh  lightweight which make them easy to  deploy and maintain all together so you  can use them in public private or you  know hybrid cloud environments uh uh you  know all together uh itself so  uh you know an organization uh you know  i'll just take an example uh generate  data from internet of thing uh uh  internet iot devices uh smart sensors  and other devices on the edge of the  network so this data must be collected  stored and processed so in order to  extract business insights from this data  it must flow seamlessly between edges  clouds data centers  and uh using a wide variety of work  locations and involvement altogether uh  so we need to  uh you know take care of uh you know  architecture right from uh the inflow of  of the data load balancing and uh  obviously from the security aspects  itself  uh the devices and sensors are where  information is collected process  or maybe both of them is done they have  just enough bandwidth uh you know you  need to see that memory the processing  capability all together so uh we'll also  need to take care of uh scaled scale  you know if you can scale down  uh on premises edge servers or data  centers and which can be easily moved  and fit into a comparatively you know  smaller remote locations all together so  flexibility and scalability are  necessities uh uh you know at this  moment what enterprise needs uh in order  to change and  [Music]  uh you spoke about  uh  security orchestration kubernetes  containers etc and that brings me to one  of the most important points from an  edge computing perspective which is  talent right so what kind of skills  would need to be developed  for an edge environment and that's  mainly because we are seeing a  convergence of  edge cloud telecom networking hardware  software applications right  so are the skills mutually exclusive  when you form a team or are there a fair  bit of cross killing that is required  what is your approach to talent  development when it comes to  edge computing  i think let us continue with you  sure  so uh see the first thing to understand  uh uh you know is that edge computing  has not uh you know evolved uh in a  vacuum all together so  uh it does not refer to a single uh  technology uh uh you know at this moment  so uh it's an  overall architectural concept  uh related to a  larger paradigm you can call it as  distributed computing altogether uh in  which uh you know you the computing  resources are spread throughout a system  rather than a centralized uh in a sort  of a master controller or an application  you can say so among leading skills that  are required i would broadly uh you know  categorize into different segments uh  first it comes  you know as a part of a system design so  you need someone uh you know as a skill  who can understand how everything fits  together  uh the edge devices the uh you know the  networks uh the software systems it's  helpful to understand you know some  common data exchange formats like uh you  know json and how to process and  organize data efficiently in  collaboration between many devices  uh second i would uh bring in networking  so when working with edge computing  system if engineer understands how to  measure and manage  the  performance of you know computer  networks  uh implementing efficient communication  protocols  uh there's mqtt  which can further reduce uh you know  bandwidth demand  so  that is one of the skills now uh you  know as you're moving uh towards um you  know  uh the importance of data storage and  analysis uh you'll also uh you know the  third thing that i'll bring in is a need  of database expertise all together uh  you know as a skill set  uh  another thing which i would highlight is  from the security aspect so you know  physical  uh and cyber security cannot be taken  for granted uh you know when the data is  stored uh  when the data is stored accessed or you  know managed all together so uh we need  to understand the function of  implementation of network security uh  you know devices like firewalls um  or maybe you know data protection  features like uh authentication uh  encryption and certifications all  together  so uh yeah yeah you know we have uh  people of all the backgrounds to see  themselves represented and included uh  you know in our work all together when  we work in venture so we actively seek  to diversify our team uh into the broad  spectrums that i've introduced and  bring more voices to the tables all  together  when it comes to easy  sanji  yeah sure  so uh thanks i think rj has uh brought  up very good point on the three or four  areas the core skills so i would divide  it into two pieces altogether one is the  foundational skill and second is the  add-on screen so when it comes to the  foundation sales  the the edge computing is all about  distributed paradigm right so how  actually you can design the system and  that require a different dimension your  communication uh point to point  connectivity how your mess what are the  messaging protocol that you're using so  typically that you need a broader  spectrum of that someone who understand  the system design or into the  distributed environment right and it  covers all the aspects like  cap theorem and decap theorem and all  the stuff right that is one area uh the  second area again the again the core  piece is the network and you understand  the protocols for example right so uh  there are very uh good uh you know areas  to explore in terms of how actually  communication works actually right how  computers work sold together and these  are the by the way foundational skills  right typically we have seen that many  people  you know miss these kind of foundational  skills which is typically part of your  software engineering uh you know a  courseware right and that is second area  third is because uh it's all about  generating data and and  you should have a analytical mindset you  know what the data should be processed  how the data should be optimized what  are different dimensions we need to look  into the data that is third all right  and the horizontal would be more of a  security at the end you know  other than that these are the foundation  skills which aj has already touched upon  that other than that  uh two more areas that i would highly  recommend to be uh to build that skill  is that number is the the edge native  which is based on the cloud native  principle site  so you should have a good understanding  of the cloud native architecture or the  edge native architecture your devops  skills for example which is again  because like uh you know  rajesh mentioned that the orchestration  deploying manageability of the the  distributed ecosystem is very very  complex area and the that's why the the  capabilities like sre site reliability  engineering those capabilities are  evolved and that's where you the  operation architecture standpoint you  need to consider those factors other  than that the add-on skill would be like  we have a linux foundation uh you know  there's an organization actually who are  working on many iot iot uh kind of edge  frameworks uh that is that is good  interesting a good area to explore  because that  that is is it is a amalgamation of your  micro service architecture your uh you  know devops architecture your uh cross  cutting concerns what are the core  capabilities required for a distributed  application scalable flexible kind of  architecture altogether right so uh so  in in nutshell it's uh the core skills  that the that a person should have all  these verticals like the distributed uh  system design the networking security  data that we talked about and then we  have the add-on service like we should  be familiar with that what the industry  has has already invented and we should  leverage instead of reinventing the  wheel so if we if we mix the two that  will really help us to make a a full  stack kind of edge edge engineer edge  architect to build the solution  altogether and because right now finding  these kind of skills is not possible uh  from the industry and that's why most of  the organization especially the our  company actually has some organic growth  of our people so we have we are running  some academies academies actually we  have a  consolidated curriculum we are building  those skills all together right so i  think this is where  i would highly recommend that if someone  would like to start their career into  the edge computing uh the the start with  the frameworks like edge like we have a  lot of  linux foundation and the cncf these are  the two areas that people can start with  plus they should revisit to their  courseware the distributed computing  fundamentals networking essentials right  those respects has to be taken care of  together  in fact i'm actually in talks with the  linux foundation to see if we can do  something together absolutely  i'll take the setup  and and i just wanted to add we also  have explored all the uh frameworks  using uh the the lf we have also built  some kind of accelerators the iot at the  mla at the edge so like again the other  panel is mentioned about the deployment  issues so we have a mldx you can deploy  your solution at the edge at the real  time without it's a kind of zero down  time so that you can uh you know  continues learning at the central place  and distributing or delivering the new  updated upgraded model onto the edge  itself so that's kind of capabilities  are essential for nait digital twins or  the edge computing and all this stuff  anything  karthik and rajesh you want to add over  and above what ajay and sanjiva already  tested from a talent perspective  i think i don't want to dwell into on  deeper but i think  a role of an enterprise architect is  going to become very important because  here you have to combine everything  because when you're talking of an edge  basically the resources are going to be  limited so how well you  utilize those resources for the  application or the use case which is  getting built is going to become very  important  so a person having knowledge about the  various tools and i think people have  spoken about ci cd is going to become  important devops is going to become  important so having capabilities on  those kind of tools which would enable  those is going to become important but  in my view enterprise architect to  understand the use case and look at an  architecture in a holistic view is going  to become a very important  skill set to have to make these  successful these use cases successful  so i come to the final section uh  very quickly uh i wanted to hear from  karthik and sanjeev on the emerging  opportunities  uh from edge and  how are companies monetizing these  opportunities  so  cutting do you want to go first  [Music]  yeah  see as we have seen a lot of  opportunities in edge  uh coming together as we said earlier  like 5g  iot and  bringing lot of innovations in the  industry perspective  there are a lot of exciting use cases we  see in  as sanjay was telling in autonomous  vehicles  in drones as well as in metals as we  touched earlier but  the metabolism is going to be huge uh  already the digital economy started  growing in metabolism we see a lot of  people started creating  buying selling uh digital assets in a  metals  is one area can we can focus uh we see a  lot of purpose built hardwares for a  tower's perspective  at least for initial use cases what they  claim and  [Music]  out over the time uh  it's it seems to be growing  and one of the thing is they are trying  to offload that compute capabilities  that what the devices require  uh to the near edge so that the device  itself will become lightweight uh  it can be easily adaptable cheap  uh those kind of stuff we are seeing  and from mobility perspective as i  sent you has already told more on like  roadside units to  offload those compute from autonomous  vehicles  those are also getting explored  a lot of researchers are going on with  agriculture and ugly use cases these are  the areas of trends we are seeing in  each community  okay yeah so of course uh the metaverse  is definitely though i was uh i was  waiting for my call when rajesh was  talking about metaverse right i'm very  excited to talk about the metaverse yes  uh because uh the  like rajesh mentioned that the edge and  the 5g are the backbone for me towers  right because augmented reality uh  required lot of data to process at the  edge itself and and uh especially let's  say if you view a headset for example  like oculus for example right that do  some processing and but it is connecting  to your facebook account or your central  server right now think about that if  there is a latency and if i'm talking to  someone else as a let's say virtual  workplace or maybe let's say agile  stand-up meeting is happening and i'm a  digital world for example right that  latency latency will kill that  experience and metaverse is all about  experience right nothing else so if we  if we have a you know if we uh see that  metaverse and the adoption of 5g and the  edge computing that will make it as a  real happen and right now metaverse  seems to be in fancy or any more more of  a prototype reason being is that because  right now we don't have the sufficient  infrastructure in place so uh especially  the csps the communication service  providers need to upgrade their backbone  they need to have the edge-enabled  infrastructure in place so that we have  a mobile edge computing we so that you  know our devices can be connected to the  closure to us and we can have more rich  experience more real experience all  together right and that that is that  that is the reason that though  technology wise uh the uh right now we  are more of a technological stuff but  not on the use case stuff because you  know i am conducting a workshop better i  can i can personally visit and and meet  my people not in a digital world because  that will be overhead as well for me but  if we have that experience the the zero  downtime  or maybe a  very minimal intensive example right i  can have that experience and the uh the  the situations like pandemic uh i can  actually have that experience i can meet  and one of the example i tell you that  in recent one of the customer actually  are working is that we are  thinking of a green kind of airports for  example right we can have a virtual  meetup so people especially the  corporate travelers they can meet their  families in a virtual zone and those  kind of use cases where monetization can  happen right so monetization i see into  two pieces one is that okay how can we  can optimize the cost so if i send lo a  small set of data to the central server  for processing right i can optimize my  course that is the monetization way of  looking how can optimize infrastructure  optimal you know operation as well  second is that how can i cross sell so  if i have some uh through the gaming  let's say i have a ar vr and through  that here we are i'm setting selling a  air vr or a maybe uh air kind of  applications but in addition to that  because i'm i'm gaining more attention  on the experience side i can have a  closer so the opportunities are endless  and i i believe that the the prediction  is 1.5 trillion billion dollar by 2029  into the meat hours and that's why  recently we have established the coe and  i'm sure that this is in the next two  years we will have a lot of  improvements and upgradation in the  metaverse area and a lot of monetization  capitals  and finally since we are running out of  time i'll be in the last question but it  is equally important it is in terms of  edge computing strategy right any  company that is looking at  deploying uh edge computing what is the  strategy  uh that they need to building what are  the factors that they need to take into  account in building  you know edge computing  strategy rajesh can we go with you on  that booth i i think strategy is a very  big word i would say i think something  what you define should have a clear use  case and what is the outcome that you  are looking for  if you are able to achieve that in my  view your age strategy works it will be  driven more by edge  by the use cases that you're talking  about but yes when you're talking of a  large enterprise deploying this you will  have to bring in some amount of  standardization you can't have for every  use case a new way of doing things so  you will have to create a framework  that you what you would want to deploy  and how you would want to deploy and how  you want to monitor and manage it from  an overall use cases perspective so  create a framework  and then look at use case clearly define  what is the outcome that you want from  that use case and then deploy it  according to that  simple not too many  uh things to be taken into  is that so there are five or six  actually attributes to be considered one  is that do you really need a load  tendency application or not do you  really need the offline and online  connectivity the smart connectivity or  not right do you really need to have the  intelligence  for the critical decision making all  right so there are five or six factors  actually we have that that will be  decision factor uh whether we go so  instead of going with a technological  dream and go with the business case  given that is the right way too  yes as  i just told rightly defining edge is  going to be critical for any  organization  the proper definition of edge should  come  with what kind of workload what latency  they require  what kind of hardware they are going to  use what kind of a network connectivity  they are they have to define each  property  the second thing should be the adoption  of hybrid mindset  that is so one factor that we see like  the organization should have a mindset  to adopt hybrid  and bring a unified control plan or a  management plane for both on-prem uh and  as well as circle  as edges having a different kind of an  elasticity and scalability choosing the  right components is going to be critical  otherwise  uh not only competent services and  integrations is also critical otherwise  the value that edge use case generate uh  they may not be able to see that roi  what they are trying to deploy  uh apart from that like uh when when it  comes to enterprise applications uh if  they try to  uh deploy it on edge or if they if they  try to they may not be suitable or  modular portable enough for each  computing environment  the consideration has to be taken like  whether they have to build those  capabilities into their existing  applications are completely those kind  of a  balanced decision has to be taken prior  just uh  just adopting an h2  cloud kind of a situation  then  the culture itot  teams as we see a lot of organization  the field the service team is little bit  different compared to that idt  especially in manufacturing the ot has  their own functionality and safety  concerns and other stuff  so the both the teams has to work  together uh the people has to come  together to  see like what kind of data has to be  pulled out what benefits they can bring  out  these are certain considerations uh they  should take prior  jumping so it should be a phased manner  from my perspective uh they have to have  some discovery gap analysis to define  edge and then they can build a portfolio  like a single factory portfolio use case  portfolio or a retail store portfolio  then they can scale it too many and many  to all player  just a last note from my side as well so  uh a great uh you know feedback so  you have to define a good framework  that's that's for sure uh you have to go  with the use case uh instead of  technology uh so all the panelists have  added that i just like to add one thing  on to it uh  which is you know you'll need to build  an edge which  makes uh for the best user experience so  this is the third aspect that we're  already working on from last four five  years uh we have uh done something uh  you know on the iot sense uh which is  our full-scale iot platform uh you know  it provides real-time analytics  rule-based actions and interim  communication that is there  we have predict sense uh which is an  aiml based platform we have vision sense  uh so we are trying to build in a good  user experience all together while  enabling uh you know framework and a  good roi uh you know that we are seeking  out  for  so yeah  i think at the end of the day what is  the one  requirement i feel is that there's a  fail-proof connectivity infrastructure  for all these to work out  otherwise i think it might be a bit of a  unpleasant user experience if i may call  it that but  we've run out of time i want to thank  the panelists thank you so much it was a  great great interaction  i've got a lot of points which i can  pick up and explore further so i will  reach out to you again  i will keep this conversation going i  also want to thank the participants for  you know joining in  there are some questions but  unfortunately since we have run out of  time we will not be able to take it  but i'll  connect with the audience later and you  know  share the responses to that question as  well but thank you very much and have a  great day everyone thank you thank you  hey everybody I'm Stephen foskett uh  publisher of Gestalt it and uh organizer  Tech field day and we are here at Tech  Field Day in Santa Clara uh where we  have been discussing uh the evolution of  computing uh we talked about cxl and uh  the emerging uh composable  infrastructure we talked about the cloud  we talked about the data center and the  changing emphasis between cloud and data  center and Edge and that's one of the  things that came up uh again and again  and again here at Tech field day is the  world of edge Computing now we actually  did a round table discussion uh about  this uh previously  um and frankly uh it's questionable  exactly what the edge is is it anything  that's not in the data center is does  the cloud count does a mobile device  count I don't want to necessarily have  that argument but what I do want to talk  about is the fact that more and more  devices are being moved out of the  confines of the data center and into  other locations whether it is retail or  industrial iot or increasingly mobile  ships military all sorts of things uh  heck even the moon as we as we talked  about in Mars I mean you know the  cubesats around Mars I guess that's Edge  Computing too so um the question is uh  first off for the delegates how real is  this or is this just another marketing  Trend are things really being moved to  the edge who wants to start with that  well if I can start very quickly I mean  we always had Hedge  you know and so remote offices so  everything that was outside the data  center  was at maybe we didn't have ads as a  ward years ago  I mean most of my  customers at that time when I was you  know  really building infrastructure so when I  had this real job and and  they had you know remote offices all  over the work connected with the mpls's  or you know other other stuff but yeah  we used to call it Robo right yeah but  essentially what's happening now is  there's just more capabilities that what  we can bring to these remote locations  that you know instead of calling  removable we're calling them Edge but  all of a sudden there's more devices  more capabilities more bandwidth  whatever right and so capabilities have  enhanced and so we there's a lot of  things that used to be centralized that  can be decentralized uh in a good way  that will help out the business that  just weren't previously possible yeah  yeah we always had Edge but it's just  getting better and so we need a better  name for it I guess I don't know what  what happened to the DMZ  you still have it but it can be I mean  it can be kind of anywhere right but uh  that's a different problem I think I I  just like looked up uh Windows domain  controllers and when the read-only  domain controllers is really designed as  like a remote Office Solution it was  introduced in Windows 2008 and now  Microsoft basically says don't use them  but that's a different story uh yeah I  think it's it's just kind of always been  there it's just evolved into what we're  doing and and what we see  I'm expanding I mean so a few years ago  the remote workers were less and now you  know  especially after the pandemic everybody  could be a remote worker so many  Enterprises adopted technology SAS  software in many  in many circumstances that you know and  it was not the case  you know before the pandemic for example  and so now you are this continuous  expansion uh to you know towards the far  Edge and it's changing everything even  in you know security for some companies  I mean some were already there but  actually for others this Paradigm change  also change the the way they have to  think security so do we drive that down  that rabbit hole now uh of what is Edge  like does the distributive Workforce  count as an edge or is it more along the  lines of a location with a you know a  closet where a bunch of gear goes yeah  I've also wanted to jump in and stir the  pot a little bit because I remember  um before the cloud was a thing right it  was like someone read it in CIO magazine  at the time I worked in a very very  large data center it was my company I  worked for was one of the top five  biggest consumers of VMware in the world  right sorry to start in cloud cloud  we're all looking at each other like  we've been we've been doing this stuff  for like a long time right so we had the  cloud Trend and I kind of feel like  the cloud is commonplace now right  there's no more Cloud Trend it's no more  it's everywhere there's no more like  this is the bud word this is the hot new  thing someone says Cloud at this point  your eye is kind of plays over and  Stephen I want to jump back to something  we talked about on other round Table  Right uh the whole 2020 thing kind of  faced um pushed some changes in the  Computing World in general so this is  like the next evolution of stuff right  everybody went into the cloud we're good  with the cloud well what's the next  what's the next big thing we're going to  talk about right is people didn't get  into their data center so maybe they  started doing stuff other places and you  know robo's been around forever I used  to do a bunch of Robo stuff  um too with like manufacturing and  things like that so I kind of feel like  this is like the next hot Trend which  kind of ties into well how do you define  it this goes back to the early days of  the cloud where we're just like well how  how do you define the cloud anywhere  like what is it how do you put a name on  it it's like the same thing over again  because we need that next big thing I  guess is a  technological Society to gravitate  towards  so it sounds like you're skeptical uh  yeah pretty much  okay well let's let's go in that  direction uh  skepticism well I actually I mean if you  look at even the vendors okay so a few  years ago they didn't have hedge real  Edge servers I mean yeah  think about the rotate shop okay to the  you I mean at least I saw a lot of times  PCS used as servers and you know cabling  that didn't really make any sense so  other stuff that you you can't really  think of if you have an Enterprise  solution even if it's a retail shop and  the other day I saw a major vendor  proposing a you know server designed for  for the edge which which is smaller  which is uh you know uh with a different  uh funds to to not you know to to avoid  too much noise uh a form factor that can  stay actually in the advertisement they  showed it in a broom closet  because this is where this stuff is okay  so it's more rugged put it this way and  and so they they are they are finally  understanding that users are are asking  for it I mean asking for this kind of  devices and and they were uh let's say  Niche uh vendors that were building this  stuff now you know even the primary  vendors are are finally understanding  and building stuff that makes sense for  for this kind of applications so it's  not really  a trend or a you know it's not just a  fashion it's something and and users are  asking for it now the management part  could be a totally different story  because on one end you know now finally  we have the hardware software comes  always later you know what we call  usually Fleet Management okay so how do  you manage uh devices that are deployed  in the middle of nowhere are you you  know install patches install everything  and you know the first day uh the easier  install of this stuff because you have  the clerk you you have somebody  that doesn't know anything about  computers in this retail shop and they  have to plug it in you know attach the  network the machine for the first time  so all the management is well there are  solutions but they are not as common as  we may think they're not coming so it's  like the bailing wire and duct tape that  used to hold together our data center  infrastructure forever is now just  moving out to these all these different  locations and we're getting by as best  we can until uh the space matures right  like you said there's Solutions but  they're not really complete or  comprehensive will the space ever mature  in terms of like management and location  you're not going to build a data center  on your remote site and you're always  going to have to put no matter what the  form factor of the computer is you're  always going to have to put it somewhere  uh-huh and and it's going to get Dusty  and  fans will clog up and things like that  what do you think yeah that's where they  they start to look I think uh from a  management perspective a lot more like a  like a corporate laptop is yeah  um where they're much more disposable  than the hardware that you've got in the  data center something goes wrong with it  um you know what we'll we'll just  re-image it completely that doesn't work  we'll just ship you another one and and  Slot it in  um that that it's you're not going to be  dispatching someone with a lot of  technical skill to go and resolve them  yeah well you do a lot of work with like  Wi-Fi and like places where access  points don't get touched by Wi-Fi  Engineers after after they're dropped  off uh mostly how does that kind of  support work because that's a a pretty  good example of edge in a different  context that's a really good point  because I mean basically uh all those  all those APS are Edge Computing yeah  but at their core they're designed from  the start to be centrally managed  um so they're they're for the most part  phoning home to a controller or a Cloud  solution and you're monitoring them that  way um if they've got mechanisms within  the software that hey this radio is  stuck we're going to bring that down and  come back up and that's just that's just  part of it at its core  um I mean there's  also the mechanism within design that  we're doing some level of overlapping  coverage where if you lose one maybe  it's not ideal there's still secondary  coverage for you to connect to and it'll  still work in that area  um so there's a there's a lot that that  solution has going for it but it was  designed from the start to be  distributed yeah it's interesting sorry  Henrico no problem one of the things  that I love about the tech Field Day  events is we can get people from  different disciplines together and we  can learn from each other and I remember  when sd-wan was coming and the wi-fi  people were like hey you know we've  already been talking about you know  separating management management plane  or control plane from data plane we've  already been talking about Remote device  management and all this stuff for years  and years and here you are as well  saying the same thing again because out  of necessity you've had to do exactly  what you just said you've had to figure  out out of box experience you've had to  figure out uh how to have  um  non-uh you know inexperienced installers  difficult  locations I mean you know you think that  it's hard to have a rack of servers in a  in a in a warehouse how about uh all the  different places access points have to  go underneath the seats at a stadium  Outdoors or something so I think that we  can learn so much from the world of  Wi-Fi when we're trying to think about  Edge Computing right yeah  um yeah absolutely and I think one of  the things that when we talk about these  servers that are out at the edge  um a very common workflow that I have is  that if there's an AP that isn't really  doing what it needs to do it's a factory  reset it comes up Auto Discovery happens  it phones home and this entire  configuration is pushed back to it that  that auto discovery that ztp  um the further away you're going to get  from the data center and your your core  team of people that are deploying it the  more important that's going to end up  being  don't you think of that you know but to  the point of you know having these  management planes and separated by the  data planes and everything so the the  the future of the energy is hyper  convergence anyway so from that from the  perspective of management Simplicity  having you know uh uh everything that  looks  similar somehow in the way you manage it  so this is probably you know the most  interesting Evolution for HCI I mean so  the Fleet Management part everything and  oh and very at the moment very few  vendors are taking you know the  necessary step to build the the  the software to manage at scale this  kind of environments yeah and and for  that I actually want to call out one  company in particular that's talking  about this which is scale Computing yes  they are 100 working on basically HCI  for Edge and it is it is it's a natural  fit I think that they didn't even maybe  know that it was a Fit until people  started buying it and using it that way  because you don't want to be in the  situation that so many Edge Computing or  remote offices or warehouses or whatever  were in previously where you had all  this different equipment all these  cables all the specialized stuff that no  one could touch that was you know buried  under something in the warehouse  right yeah if you can just plug in power  and ethernet you're done uh and it's  also disposable to the point where it's  easy to replace meaning uh to your point  I don't everything's centrally managed I  just Swap and replace if something fails  uh much easier to maintain than the more  complex solution with all these  different moving parts that you know are  managed separately but but you need the  hardware needs to be simple as well  because right not like a data center  where you're going to have you're going  to have this rack of rack of storage  rack of compute  um there if you're going if it's going  to be at a remote office or something  you want that all together in one box  where it is literally just power and one  ethernet that goes in so and I think  that's the point he's making is that  that's the fit for hyper convergence  right you've got you've got a plug for  you for ethernet for the uh you know for  the actual communication maybe you have  another one for out of band you have  power and you maybe have two or three of  these uh in a shelter on a rack and  they're swappable basically they just  cluster together and they handle all  that yeah and consider that most of the  time we are talking about in this agile  location you have to or five vehicle  machines not more than that right and so  these are really really small clusters  which is you know introduces a lot of  other issues including that the the you  know the hypervisors itself  uses too much resources for this so so  you need an optimization level that is  really uh really important so both  hardware and software has to be designed  to in a different way well I think it's  it's all the old challenges are still  present and for a robo institution I  mean you do you want to go deploy a  three-tier complex architecture at every  site for your Edge your new Edge it's  going to be a nightmare otherwise I  think there needs to be Innovation for  solid persistent performant storage  those type of things have to be part of  the solution  I mean I think they mostly are in Modern  Edge devices right like you have most of  the Modern Edge devices all show up with  ssds shoot modern laptops almost all  ship with ssds I think it's pretty hard  to find a spinning disc but it's is that  that may be durable but is it replicate  does it have you know other features  that you expect for modern data center  storage but on the other hand is it  necessary  yeah so if you're if you're just rolling  out a couple at that location that's not  a high demand on it um and and it's just  sort of servicing the need for low  latency at a specific location or in a  specific region and you've got an  application that supports going back to  your centralized data center as a backup  if that's storage fails is that okay and  that's going to be use case and  application driven I think we're talking  about this the other day hopefully the  idea is that maybe there's a lot of data  being created at the edge and maybe it's  being processed to a degree but then  it's you know offloaded somewhere else  and if if you lose capabilities there  then you just start a new you put in the  new hardware you boot it up you  configure it and everything's pulled  down and nothing is lost and I think  that would be the hope that's feasible  for most Edge locations these days I  feel like as network connectivity is  proliferated through all kinds of new  stuff whether it's sd-wan or 5G or  whatever uh and the need to basically  have some kind of resilient data store  at The Edge is not as important as it  was in the past and we were pretty much  a three tier everywhere right yeah I  mean what if your use cases you want an  a domain controller that's not an Azure  domain controller on your Edge device  yeah have you have you ever thought  about the edge as you know also the the  next room to you for some technologies I  mean think about decentralized storage  peer-to-peer storage that failed so many  times because because you know the why  it is designed but actually if you have  1000 shops and you have you know the  connectivity that is now available  practically everywhere why not build a  huge Object Store out of this 1000 nodes  yeah I mean they are all under your  control so you you have a certain level  of security you have resources there to  run two for virtual machines and maybe  you have a couple of hard drives or  better ssds doing nothing for you know  99 of the time so you can you know save  huge amounts of money and this is you  know disaster recovery it's all included  yeah for free form factor aside you're  focusing on the functionality like that  I mean it it could be a win so Edge Edge  has a lot it's still it's it's  undiscovered a lot of potential yeah  absolutely and I think that that's  exciting uh whether it's cloud or  whether it's you know sd-wan or Wi-Fi or  whatever topic to see how these  Technologies can be brought to bear I  love that Enrico because you're so right  that so many of these um distributed and  peer-to-peer uh approaches have been  attempted in places that they really  kind of didn't belong but suddenly maybe  it's a natural fit and I think that's  why it's so great to like have all these  different perspectives here because you  know I think that that people from  different Industries are going to be  able to say wait wait wait what about  this you know the out of box experience  the fact that you know you just opened  up a brand new piece of piece of  hardware and you don't know what to do  with it and it and it kind of phones  home and configures itself uh you know  this whole taxonomy of of of management  versus operational control versus  production  um hyper convergence all these things  are  you know make a lot of sense in this in  this environment you know what other  Technologies can you think of I mean one  of the things that just jumps to my mind  is that some of the uh data center  products that have been developed to  control uh temperature and uh and so on  those could be deployed very much at the  edge too I've seen some interesting  sealed racks with their own cooling  systems that you know maybe in a data  center okay I can see why that's  important but in the edge yeah I can see  why that's extremely important  yeah I mean some some of the devices you  see are more tolerant to a range of  temperatures and can in atmospheric  conditions but at the same time if  you're going to have a rack of devices  you're probably going to want that  temperature control to be localized  because that means you don't have to  build a special room and count on  somebody to run it and and all of those  things  any other Technologies you can think of  I mean connectivity Technologies  um I know that there's a lot of  companies that we've seen at Tech field  day I mean I you know zero tier for  example punching through firewalls yeah  I I don't want to say the other word but  Docker containers uh are pretty useful I  think in terms of being able to run like  get higher levels of density uh in a  sense of virtualization uh to run a lot  of workloads so especially workloads  that can support it uh it's that can be  a good approach to to get more out of  the hardware that's that's there well  and and you know after this round table  we are going to set it right so think  about all the I mean there are some very  coordinate use cases but but you have  thousands of CPUs  at this point you have a super computer  do you have any workload that can run  maybe during the night I mean when you  are not using your virtual machines for  for your store for I don't want to mine  Bitcoin yeah you can buy but maybe we  can search for extraterrestrial  intelligence yeah I mean no but but  maybe you have some some AI workload so  you need just to do a major recognition  on thousands and thousand million of I  don't know photos something like that  and you can use these I mean you can  distribute some of these images to the  to these remote locations and then you  during the night you don't have anything  to do right and maybe you can you can  use it to uh I don't know I mean so you  can have a very huge distributed  database of some sort so that's why  you've you've mentioned the idea of  a mass metric Matrix of edge Computing  devices together you have the network  you have the the resources they are  distributed I mean it's the secondary  we're going to see a great example of  that this afternoon  for a long time or like the first thing  at home like it's all it's using  distributed Hardware to run the numbers  and then back that's a common data  pattern why I'm the architecture where  you do some analysis or do some analysis  and alerting locally and then everything  gets ultimately uploaded to some larger  centralized system that does batch  processing I think it's in deeper  analysis yeah yeah and and I'll just  chime in about industrial iot is  definitely trending in that direction  where there's sort of this tiered  approach where you do data collection  um on-site then you have machine  learning that's processing that data on  site in Edge Computing resources and  then you pass up the up the stack the  most interesting or relevant metrics to  the next level and I think that that's  uh absolutely something that we're going  to see in Edge Computing so that means  that this environment is not going to  necessarily be a hyper-converged  standard systems because it's going to  need processing it's going to need ml  processing power and so on it may be a  bigger system than we think that is  existing in these in these Edge sites I  mean you know let's talk you know retail  um you know if if you're going to have a  whole bunch of cameras and sensors and  so on uh you're going to need some  pretty some pretty strong computation  capabilities to deal with all that data  on site in order before you pass it up  the stack or before you pass it to  distributed  um and and so I could see that these  systems may not be as simple as we think  they may be  or am I wrong nothing ever is so uh once  we started playing this stuff more in  the field then we're gonna realize all  the things we've forgotten about most  likely as happens Time After Time the  oversight's kick in they always do well  and again so they are they are powerful  but so if you think about a phone today  as you know uh specialized uh tensor  uh CPUs or whatever they are called so  they have accelerators for so why not in  your small  Edge server and yes you do video  analysis to analyze the customers  behaviors so that you can position  better your your products in your rotate  shop why not I mean so and you just pass  to the to the cloud or to the core just  the relevant information so you can  collect and see if there are you know  patterns that are similar in other  locations I mean we are we can think  about everything and so you you have you  have a again a lot of resources  distributed you can use them locally and  then distribute things uh sorry and then  send to the core or actually you can  have something at the core distribute to  the to the remote location to something  and then return the result I mean  potentially  it's huge  yeah I I think one of the uh sort of  more approachable near-term applications  of this is is that you've got options  for resiliency of existing workloads and  existing applications is that you've got  manufacturing facilities that that  they've got some centralized like job  data  um that can get pushed down to  um to the equipment all right maybe you  can centralize all of that and just have  a local option here is today's job data  that's going to be on site we lose  access to that centralized you're still  going for the day before before it  becomes a problem  um so I think from an application  perspective you can  um there's a lot of options for existing  things that you're doing now that you're  just moving some of that compute and  some of that storage to on-site uh for  near term  maybe in the future the age definition  will be changed because every discipline  has its own definition regarding to the  age for example you said that iot  devices yes there are lots of iot  devices in the world and they are  getting lots of their producing lots of  data in every hour in every second it's  a huge data Big Data  yeah absolutely and and I think that you  know as Enrico mentions you know more  and more processors are going to have  tensor cores in them and that's going to  drive this because I think that's the  interesting thing is that when you  deploy Solutions uh to solve one problem  you often create your next problem and  so if we deploy more and more tensor  cores at the edge then we're more likely  to collect more data which means that we  haven't actually solved the problem  we've just added more data to process  and then we have to process that and I  think that this is really why  um well I guess this is my question my  final question for you in my mind the  proliferation of networking Technologies  like 5G hyper convergence distributed  storage tensor cores and Industrial iot  and all of this means that edge is maybe  a little different than the old use case  of remote office because where once it  was sort of something you had to  grudgingly do because of poor  communication connections or you know  now it's more of an opportunity and less  of a penalty Am I Wrong am I right  I think you're right so I I say all the  time that like basically it moves in  waves of centralization and  decentralization this is the latest wave  of decentralization  um there's a lot of new stuff we're  getting excited about maybe a little  over enthusiastic about and we'll  probably learn some lessons over the  next few years about what is appropriate  to put in the edge and what's not but by  and large we're going to be seeing far  more capabilities uh going to the edge  than we did previously and we'll stay  there and there might be a few things  that we pull back in when we realize  that wasn't really a good fit for the  edge but most of the stuff we stick out  there will be there to stay yeah yeah I  think it's it's just more a broader  societal thing of measuring more things  precisely right so we're capturing more  data from various devices and in various  things to to be more accurate in our  processes of All Sorts whether they be  industrial or even cooking uh I I think  that's something that's uh  leads to this yeah I think that's really  great insight and I think you're  completely right Ken then there is a a  constant shift between these things but  what happens is is once we've once it  gets its hooks in  um it's going to be there and I think  that's that's what I'm seeing is that  this is not going to go away it's only  going it's going to evolve it's going to  be refined but it's only going to get  more and more well it makes sense  there's a proliferation of data center  Technologies why not apply it at the  edge and make that world more robust  more performance more more productive  well I think that this is uh the  conversation that we're going to be  having now starting now so uh  this is uh the New Frontier or a new  frontier for Tech um this is certainly  something that we're going to be talking  about and uh we at Tech field day are  probably going to be doing an edge Field  Day in uh q1 of 2023. so if you've  listened to this and you've said this is  interesting this is new this is real  this is relevant um stay tuned at Tech  field day maybe subscribe to our  newsletter so you get an announcement  when that happens uh maybe submit  yourself as a presenter or a delegate if  this is your topic because we I think  have to have this conversation because  so many of the technologies that we've  been working with are going to the edge  so many of the drivers of new technology  development are coming from the edge  and I think that that's inevitable and I  think uh you know as as all of you are  pointing out the you know we can't get  carried away with the hype but we have  to reflect on the reality of of what  this new use case needs and how  technology can serve that and that's  pretty much what we do uh here at Tech  field day so uh please keep an eye out  for Edge field day and please do  continue this conversation  um if you go to techfieldday.com you'll  see the names of the delegates around  this table uh maybe in the notes for  this video as well and you can continue  the conversation with them and with us  here at Gestalt it  hello everyone and welcome to the iot  for all podcast I'm Ryan Chacon and on  this episode we are going to get into  some really great insights into Edge  Computing and Industrial iot forecast  for 2023 and I'll be talking with Jim  White the CTO of iotec they are in Edge  Computing and management software  company there's a ton of value here so  if you are watching this on YouTube we  truly appreciate a like And subscribe  and be sure to hit that Bell icon so you  get the latest episodes as soon as they  are out but uh all right before we get  into it there's a quick word from our  sponsor leverage any of you out there  are looking to enter the fast growing  and profitable iot Market but don't know  where to start check out our sponsor  leverage leverages iot Solutions  development platform provides everything  you need to create TurnKey iot products  that you can white label and resell  under your own brand to learn more go to  iot changes everything.com that's iot  changes everything.com and without  further Ado please enjoy this episode of  the iot for all podcast welcome Jim to  the iot for all podcast thanks for being  here this week  Ryan thanks for having us really a  pleasure to be with you and thanks uh  for allowing us to uh talk with your  with your guests absolutely yeah so um  let's kick this off by having to give a  quick introduction about yourself and uh  the company you're with sure so uh I am  the CTO of iotek systems uh it's a  company based out of the UK as you can  probably tell by my accent I am not uh  British I'm trying to do my best but  um our company is actually a global  company focused on iot and Edge software  um my background is one where I I came  from Dell Computing where we were  building iot Edge gateways and I was the  guy tapped to help  um co-found an open source project  that's known as egex Foundry so the  leading open source Edge software  provider today and Ajax Foundry became a  big part of iotek systems and so that's  what attracted me to IO Tech and that's  been my business now for the last five  years  fantastic  um very exciting conversation that I  know you and I have planned here I'm um  and we're going to be talking about a  couple different areas that are really  interesting and kind of been very  popular as of late for a lot of our  audience one is is The Edge Computing  world and the other is industrial iot  for a bit I wanted to kind of set the  stage for that by asking you to kind of  give me a quick overview from your  perspective of the current state of edge  Computing and the current state of  industrial iot if you want to recap kind  of 2022 and what kind of happened then  that's fantastic but um just kind of  high level it for our audience so we can  kind of set the stage that way yeah I  sure can Ryan so I think um a lot of  complexity in those questions I I always  have to begin any conversation I have  with almost any customer or partner or  what have you to find Edge for me  because everybody's Edge is a little bit  different all right so from an iotex  perspective we are largely focused on  Industrial in fact that's where we got  our start is trying to provide what we  called industrial iot solutions to  companies which meant really trying to  focus on connect things in industrial  spaces into the Enterprise  that has changed  um a bit there's still a large Port of  our of our customer base a large part of  the people we talk to that are  industrial based today but we're seeing  more and more needs emerge from this  what people are generally calling Edge  and the reason we usually have to talk a  little bit about what is your Edge  versus my Edge there's different types  right you've got Edge Computing from the  standpoint of a of a Telco where you and  I are carrying around our cell phones  and we're coming close to environments  where that phone can be used as a device  to alert people on you know that might  I'm present and and I could use some  information all the way down to you know  a home Edge we're talking about things  that are happening inside your own home  space  um industrial Edge which is an extension  of iot so all that is is is coming to  play and the reason I say things have  started to change a little bit is we're  getting into environments now which even  from an Iowa Tech perspective we didn't  perceive originally that these are going  to be large Edge players that's right a  couple of examples  um you know you look at something like  um retail environment retail store and  they need iot space but no they don't  necessarily need iot but they are  starting to bring together all sorts of  different sensing environment and  Equipment into things like the point of  sale where you and I do our checkout  right you know we don't think of that as  industrial right that's not a sensor on  some sort of factory line but lo and  behold that is now very valuable  information that those retailers need to  make all sorts of decisions and to help  customers so Edge is expanding in a lot  of different ways so I'm not sure if I  completely answered the question I took  my best shot no that's great fantastic  um yeah it's a very exciting space a lot  is happening there Edge Computing has  become even more popular to to power a  lot of these Solutions help you drive  down drive down costs and improve kind  of performance across the board so as  we're now into to 2023 and we're kind of  kicking things off what is how do you  kind of view things going into this year  and I guess um you know what we were  hoping to do today is talk about kind of  what the forecast looks like when it  comes to Edge Computing and the  industrial space whether that's together  a little bit separate's fine but I  wanted to kind of talk through a number  of different kind of points that I know  that are on your mind for us to go  through but if you want to maybe high  level that for us and talk about what  the forecast kind of looks like  yeah yeah I I guess I've become infamous  over some of the predictions I've made  over the last few years some of them I  think are are turning true others or not  are taking time  um so what what I see in in the  marketplace right now first of all a lot  of things as we all have seen in almost  all of our daily and professional lives  have changed over the last few years  with covert right it has impacted a lot  of the ways we see ourselves see our  environments see our businesses  um that is changing things considerably  um where where a few years ago we were  all going to an office and we were  looking to provide iot and educative  billion office not so much anymore but  maybe we need it more now in our  personal space so there are changes in  where we live occupy our time and and  need space so that impacts things and  then there are a lot of other uh  exciting things going on just the way  that the technology is always being  applied to things like environmental  concerns right we are now very much  concerned about saving our planet and  saving resources I'm not going to say  that those weren't important two years  ago but I think it's become more front  and center is our world gets hotter as  our world gets more resource constrained  right Edge in iot Computing can really  help in those spaces so the the impact  of the last few years economically  socially economically I mean  environmentally all those things are are  bringing some changes about in our our  world that are putting iot and Edge  capabilities front and center to to help  solve those problems right right right  what do you what do you think about  um kind of you know the view  then perception people have had about  what Edge Computing is how it works and  what the value can provide to what  companies really need so it sounds like  a lot of companies really want complete  Solutions right  um so how is kind of edge playing into  that or how are things transitioning  from maybe the way Edge was thought  about and being utilized or purchased  into what really people need and want  and how it's kind of working  great question and um we've seen this in  the evolution of our business uh when we  were starting  um  uh faced a lot of early adopters who  were looking at what I call a piece of  parts right help me connect to this  sensor help me get the data to This  Cloud so they were looking they were  early adopters they were figuring out a  lot of it themselves fast forward now a  few years companies are they're done  with that I the way I I read about and  talk about it is we are done playing  games time to get real time to get  serious here I don't want pieces and  parts what I want is a solution we start  to see an evolution of that here in the  last couple of years but now it's  getting real serious where companies are  moving  um or transitioning from pocs and Pilots  right to learn and to experiment and  figure out what's working and now  they're coming to companies are a car to  say pilot PLC I may want to do that just  to prove something out but I need to do  that very quickly it goes in hmm you  know 12 months I'm being asked to  deliver solutions that are going to  either go to our customers or go to our  employees or what have you and these  things have to work so they've gotten  out of the play time and now it is go  time and and that's I think changing the  marketplace considerably it's going to  put some pressure on the marketplace you  know we're going to see some companies  come and go uh bigger companies are  going to make Acquisitions things of  that nature  yeah absolutely  um what about kind of when it comes to  the edge about what I guess as it  relates to security as well so you know  that's becoming a pretty important topic  so like OT Edge security has become you  know something real what what do you  think about that as as we're kind of  looking at this year yeah um so I used  to joke with uh with my going into  customers that would talk about uh  obviously security is a concern you know  it's going to be concern with everybody  anything uh it has to do with the  information flow today  and I would ask companies so tell me  what are your security concerns what  tell me what you're trying to protect  against  and I would joke because it was actually  an answer I got from one of the CEOs I  talked to I don't exactly know what  security is all about in the space all I  do know is I want you to keep me off the  cover of the Wall Street Journal so  security to them was  protect me from making big mistakes but  I don't know what those potential  threats or mistakes are that's also  changing right there's still that  concern don't get me wrong I don't think  any CEO wants to be on the cover of the  Wall Street Journal and certainly we've  seen in the Press  reports of of people coming in and  attacking things at the weakest points  but we have to go a little bit farther  today we actually have to have answers  and importantly I as a CTO and a company  like iotek that's providing Solutions we  have to start to really understand the  real threats right not not just say yeah  we're going to try and help you keep Off  the Wall Street Journal we have to know  where those weakest links are and try to  provide Solutions into that space so  companies are getting more wise leaders  are becoming more wise about what some  of those threats are but now it's  becoming more specific and they need  products that address those specific  needs fantastic absolutely  um so kind of Shifting topics here for a  second if we're taught and but still  talking about kind of this forecast how  have hyperscalers really kind of played  a role in this in in your mind kind of  like what what disruption have they  brought to the market  um and just just kind of overall from  your perspective there to be blunt they  haven't they haven't really played yet  and they haven't really been impactful  yet some of them uh uh you know when we  talk about let's talk about the the  cloud providers uh to be specific you  know  um and we can identify them by name it's  not like everybody doesn't know them  right the Googles the Amazons and  microsofts of the world  they took a very cloud-centric approach  that is we want all the data we want  your data and we want to help you  facilitate to get it all up into our  cloud  and they didn't really understand  OT very well okay and then companies who  bought into that approach started to  look at it and they'd start to look at  their bills and realize this is not  going to work right my my Amazon bill of  transporting all the edge data to the  cloud and then working on it from there  is not going to scale because it's just  an immense Bill and it didn't really add  a lot of value  I think that's got to change and I think  it is changing we're seeing where the  cloud providers in particular is as you  know a cross-section of the hyperscalers  are looking at this saying okay so what  we're going to need to do is we're going  to need to enter this space very much  like uh we'll look at at some of the the  hyperscalers and what their business was  and then how they adapted to something  like mobile technology or uh you know  Cloud SAS I'm sorry SAS or our past time  move of Technologies they entered it  into a big way trying to learn what it  is they can provide is value where it is  they need to hand things off to you but  where they actually provided value  they're starting to learn those lessons  they're learning the lessons that  they're not OT providers right they  don't know operational technology sure  but there's a place they can play  because they do know scale and this is a  big scale problem uh Edge Computing is a  big scale problem so they know scale how  can they apply what they do best into  these environments and bring in the  right expertise to help them where they  they maybe not are specialist and by the  way probably don't belong not all of  your data belongs in the cloud from The  Edge it makes no sense to shovel it up  into the cloud so they're learning okay  great uh what about Ai and ml how do you  think that's like from a forecast  standpoint because obviously that's you  know a big topic and how that needs to  be in there but does everybody really  does everything I guess need need that  uh yeah great great question  um again another kind of a joke and a  side I have with people is you know  Ai and ml is part of the solution I  don't know what the problem is but we're  going to make it part of the solution  that was kind of our approach you know  is AI and ml became big and oh my gosh  it is impactful right it can do so many  things  what we're starting to learn is uh you  don't necessarily need that level of  sophistication that level of uh compute  and and analytics capability to do some  really important critical and return on  investment things at the edge  Simple Rules engines simple little  analytics capability can return some big  Investments so if you're going to say I  need ainl AIML know where and how that's  applied especially when we're talking  about Edge Computing and then think  about that a little bit because it does  put some extra requirements on you you  need some Hefty Duty compute so I I see  an evolution in Ai and ml in that there  are specific use cases or specific  environments where Ai and ml is going to  become very important in Edge Computing  example  visual inference at the edge right all  these all these Edge compute use cases  have cameras all over everything  being able to detect what's going on at  the edge and get an understanding of  that from an image usually takes I Ai  and ml but to determine that something  is a little bit out of alignment you  know your your sensors can do a lot of  that and then you need simple if then  health conditions and hey this value is  not in range let's do something about  that that's not AIML that's what I  learned in my first days of  undergraduate Computing way back when  and that doesn't cost me the type of  funding I need to put together an AIML  model let alone the type of compute  resources I mean right at the edge so  apply it with some uh some understanding  and some constraints and figure out what  it is you really need and how to apply  it just don't say we need AIML at the  edge right right right okay fantastic  last thing I want to ask you and this is  just something I don't know a lot of  people know about but we're talking  about  um kubernetes we haven't actually really  talked about that much at all but it's  it's a pretty popular  um  uh term around a lot of  um you know Tech groups and developer  groups I wanted to talk about kind of  how that plays into to all this  um and you know it's very popular uh  tool for people to be using and but just  from your thoughts how does this kind of  play into what we're talking about here  from a forecast standpoint yeah right so  um once you've developed some sort of  solution for your Edge the next question  is how do we get it out to the edge how  do we deploy org Street manage it  monitor it how do we know it's up and  running  kubernetes has been fantastic in the  Enterprise let's even make it more  General Cloud native Computing cncf  Community has been fantastic at getting  those kinds of problems solved in the  Enterprise deploy orchestrate manage  right but the cloud and the Enterprise  is not the edge  we are resource constrained in many ways  we have needs that are not the same as  the Enterprise for example uh you know  the Enterprise never has to connect that  modbus sensor  the Enterprise doesn't have to have  dedicated you know communication  security channels that say the edge does  so a kubernetes solution is applying an  Enterprise technology and trying to  bring that down the edge can that work  in some cases maybe yep take a look at  it but don't try to apply Enterprise  technology wholesale without really  looking at some of your requirements and  that's the problem that we've seen in  iotek is it's okay to want to bring your  Enterprise technology and thoughts into  the edge to see if they might apply but  don't expect them to solve all needs  there are unique needs in the  orchestration deployment management that  kubernetes or cncf in general don't  handle now that's going to change right  the cncf community just like we talked  about the clouds right how they're  adapting same thing's going to happen  right the cncf community is a huge group  of very Brilliant Minds and they're  trying to figure it out and we see a lot  of of things in that space you know  there's microcates there's k3s there's a  lot of technology in this space  I in my opinion I haven't hit The Sweet  Spot yet but they're they're working  towards it so we're going to see some  things help us in this space  but as a user as somebody who's applying  technology be aware that the Enterprise  tools that you have don't always apply  and you're going to probably have to  look at some Alternatives at the same  time yeah fair enough okay great  um fantastic conversation I really  appreciate you taking the time last  thing I want to ask you before we go is  for audience out there who um I guess  two things I want to ask you one is for  audience out there who wants to learn  more about what you all have going on at  iotek and and just kind of generally  follow up maybe learn more about some of  these these topics what's the best way  they can engage reach out and that kind  of thing like that  yeah I appreciate the opportunity right  so um iotexys.com I always visit the  companies like ours website we provide a  lot of information out there we're part  of lfnge Linux Foundation Edge Computing  and The edgex Foundry so from an open  source uh participation you know we're  very active in those communities and so  we actually find that a lot of people  find us through those sources as well so  we'd love to chat with folks on those  channels as well so  lfedge.orges foundry.org  send me a note love to hear from you  fantastic and last before I let you go  what are you most excited about going  into this year I mean we talked about a  lot of forecast stuff on the edge  industrial side but just generally  speaking something we didn't touch on  anything that people should really keep  their eye out for yeah you know you know  it's um I'm in the UK right now um I'm  based in the US but in the UK right now  boy isn't it nice that we can actually  finally get a chance to meet face to  face talk to clients face to face  there's an element of that that's  wonderful  um but it short Cycles a lot of our our  um work as well I'm hoping to see  acceleration in this market space not  just because we meet in person but  because I think people are excited to  finally get back and into a Groove which  feels a little bit more normal than what  we've experienced over the last two  years absolutely I couldn't agree more I  was just a Cs and um  you know there's I think I said about  115 000 people attended it was people  just loved being back in person uh even  more it's defense you know CS might not  be the best gauge because it's a huge  thing but I'm curious to see how more of  the smaller vertical specific and more  Niche events come back over this year as  opposed to last year so  um so I agree with you it's exciting  Zoom has been our godsend for the last  couple years but I think we could all  use a less um you're still muted and  more of let me shake your hand yeah well  I totally agree with you yeah you get a  lot done I think it's a different kind  of relationship too so you get to a job  absolutely thanks so much for your time  really appreciate it and uh look forward  to getting this out to our audience  Ryan appreciate your time and thanks for  allowing us to speak with you and your  audience absolutely all right everyone  thanks again for watching that episode  of the I2 for all podcast if you enjoyed  the episode please click the Thumbs Up  Button subscribe to our Channel and be  sure to hit the Bell notification so you  get the latest episodes as soon as they  become available other than that thanks  again for watching and we'll see you  next time  [Music]  basically  um I wanted to talk to you about  what's going on this year with with Edge  Computing last year we were talking with  One-Stop systems about the AI  transportable concept and where those  systems fit in the overall Edge  Computing landscape  now among the use cases  you were describing last year for data  centers at the edge where aircraft and  Military applications and autonomous  vehicles things like heavy duty  equipment that are in mines how focused  are you going forward on expanding and  do new markets like that or targeting  opportunities like the transportation  industry  yeah so the the AI transportable  strategy that we put in place identified  actually 16 different verticals that we  would approach uh eight in the military  and eight in the commercial industrial  area  and what we're seeing happen is certain  ones are blossoming  and we're getting a lot of activity and  Traction in and for example one of them  is an autonomous trucks on the  commercial industrial side  and that is uh something that where we  appear to be the market leader  um and is a market segment that could  result in revenues to the company of  over a billion dollars so it is  significant it's a very large  fast-growing Market  and is that billion dollar figure like a  five-year  Market plan or or longer  yeah the the actuals on that is based on  third-party data for autonomous trucks  in 2025 that the total opportunity for  products that we build is somewhere  between a half a billion and a billion  dollars and at 2030  it is between uh five and ten billion  dollars  okay so we're talking addressable Market  well so speaking of of money and  earnings you mentioned on an earnings  call recently that the Centauri storage  accelerator  which is a complement to the rigel edge  supercomputer project is is doing well  and that there's a role that that  product has played in four new program  wins in q1 tell us in brief what is a  storage accelerator what is its role in  the autonomous Transportation use case  yeah using the autonomous truck which is  its main uh area it's winning currently  but it's going to be applicable to  others what it basically does is that  for if back up a little bit autonomous  truck when it's driving and they've  driven hundreds of thousands of miles  autonomously with our equipment on board  it is collecting vast amounts of data  terabytes upon terabytes and and it's  doing some processing using our compute  on board so our storage and compute but  it also comes into a hub  and what they want to do is remove that  memory as fast as they can and plug in  another set of memory so historically  what they did is they went into the back  where the sleeping cabinet used to be  where this the the computer and the  storage is they would remove the hard  drives put new hard drives in what we've  done for them to find Rhythm has built  this accelerator that sits on the side  of the truck so somebody can walk up to  the truck open a hatch and slide a hot  swappable canister that has many  terabytes of data and slide in a new one  and the truck can go on and so this  accelerator storage interfaces via PCI  Express cable to the main system and  that way they can get the memory off  which then they dump it into a server in  the hub and it uploads over a course of  weeks or days all that data there is no  network there's no Airway to dump that  data quickly because it's so large so  you have to physically move the memory  so not only when you when a driver gets  to their destination they have to  offload their  their palettes but they're also  offloading their their data now correct  kind of an interesting new right is that  something that you would see the drivers  themselves doing or somebody in in uh  with with the transportation company  itself yeah it's definitely not the  drivers it's uh there there are these  Hub locations that these people reside  um and so they would be coming and  addressing that there is at currently in  these autonomous trucks driving around  there is a person in the truck sitting  in the driver's seat but 99.9 percent of  the time  he is not driving the truck he's only  there as a backup as these trucks are  learning and getting better and better  at their autonomous function so that  person wouldn't be the one coming out  and do it it'd be somebody at the Hub  well speaking of autonomous vehicles uh  you know we say that word and I think a  lot of  people think of things like Teslas and  other futuristic vehicles that are  driving themselves around without driver  intervention  describe for us basically what you are  seeing in the autonomous vehicle Market  you know in in comparison to things that  people are familiar with which is  Tesla's driving around autonomously  without a person in the vehicle you know  what are you seeing in the autonomous  vehicle market and and where are you  playing the role right now yeah so um  one of the interesting facts that most  people are not aware of unless you've  studied this Market is that autonomous  trucks are much further along and be in  reality than autonomous cars  um and one of the Dynamics is that the  autonomous truck has an economic pull  that is enormous because of the 80 000  shorter 80 000 driver shortage that's  going to go to 1 million over time and  the fact that you're buying more on  Amazon than you ever have and you're  going to require even more trucks so the  demand is an economic one and an  autonomous truck  can produce an Roi twice as fast as a  regular truck so the Amazons of the  world the UPS's the fedexes the hunt  Foods they want this so that's what's  driving it to happen sooner  as a result there's money more money  going at it and they're doing it a much  more professional level and in fact the  bad rep is that you know Tesla for  example have had accidents it is not an  autonomous car it's not ready for prime  time it's a nice feature whereas  autonomous truck guys they've had no  accidents there was a report just out  uh three or four days ago from the  safety board uh citing zero accidents  caused by a semi truck over this period  of time  so again I want to repeat that this is a  market happening before autonomous cars  um and uh and as a result there's many  trucks on the road already  I can imagine that  things like the pandemic which changed  our work habits a lot are also going to  change our interest in autonomous  driving Teslas right if I don't really  need to spend three hours driving to  work into the San Francisco Bay Area  anymore  uh you know the demand for that kind of  technology is going to be slower as  you're noting than that for trucks now  what else are you seeing in terms of the  transportation Market I think you were  remarking that a lot of the the training  so to speak of of these vehicles is you  know been taking place on open highways  uh is that starting to  work its way into the city systems or is  it still largely uh autonomous on on the  open Highway  yeah great question one of the reasons  this Market is being able to develop is  that the economics are so strong that  it's willing not to have the perfect  solution  and if you think of what a truck does  and the real value is being able to get  a truck to go across coast to coast in  two days rather than four days and  because 99  99.99 of the time would be on a highway  if they can solve the problem just on  the highway that's a big benefit so  that's exactly what they're doing they  call it Hub to HUB and so the truck in  an autonomous fashion will only drive  the highway miles which is about  one-third the complexity of doing a city  miles so you can imagine a truck driving  along and if it needs refueling it would  pull into a hub which is right off the  freeway  it would refuel once it got to its  destination  um it would empty its its cargo or  trailer and a regular truck would take  it to the Walmart or the Final  Destination through the city and it  would load on another one to  autonomously go back on the highway  so that that is the dynamic that's  happening there  and I guess the other thing that people  are often conflating is again because  Teslas are all electric that maybe this  autonomous vehicle future will also be  all electric but I don't think that's  what you're actually seeing in the  transportation Market currently right I  mean are we seeing uh installation of of  edge Computing into existing Vehicles  right internal combustion engine  vehicles  correct  um so you know in fact in my slide deck  I show this picture of this future look  future looking uh truck and what I point  out to people that's not what's  happening we're not waiting for that so  we're taking regular trucks built by  Peterbilt they're diesel trucks and  adding basically an option to them that  will give it this autonomous  capabilities everything else is normal  in the truck it gets the autonomous  capabilities by adding lidar radar  cameras around the perimeter and other  sensors those feed into our our compute  and storage systems and work with the  software of the system to control the  vehicle so um  that is what's happening so the electric  portion versus the Diesels totally  independent so that doesn't mean it  can't be an electric truck it just means  they're not tied to each other and the  first atomic the first autonomous truck  will probably be primarily diesel  so another aspect of what you're working  on is that you're serving markets where  there are obviously harsh conditions  there's vibration there's extreme  temperatures cold and hot and that's  Central to your company's product  development efforts while customers on  the one hand are wanting more compute  and storage power to deal with data for  applications such as autonomous vehicles  you have to balance that against power  consumption and cooling how are you  addressing some of these challenges or  any areas of research you can mention in  terms of Technology such as liquid  cooling  yeah you know first of all what is so  exciting for us in a lot of these market  segments we're talking about including  autonomous trucks their position is give  me as much performance as you can  and my environment's harsh  and that's our sweet spot that's what we  do we take the latest and greatest  technology from the data center  and we bring it into these harsh  environments and it survives and we're  really kind of stand alone on that  um but what it does also mean is if  we're going to be valuable to the  customer we have to solve some of the  other problems which are the ones you've  listed which includes you know getting  enough power to it and the cooling For  example none of these trucks none of  these cars none of these autonomous  Vehicles were designed to have a 3000  watt computer sitting in them and so  there's work that goes along with these  autonomous truck companies uh to help  with that and do conversion of the power  you know maybe it's 24 volt or 48 volt  into something the computer can work  with and then the second is cooling  which you mentioned and so we're making  investments in cooling that's optimized  for an autonomous truck and working with  people to do that  liquid cooling that is  which is always an interesting challenge  obviously data center providers are  looking at ways to use things like  immersion cooling or  some other methods of using liquids to  take heat away from chips  um you know I guess when you get into a  vehicle  having liquid sloshing around is it's  part of the package but when you're  doing it around Electronics it's it's a  bit of a challenge you know is that  something that you feel like your team  is going to handle in-house or do you  have some suppliers that are working  with you on on some of those challenges  yeah so we have suppliers and partners  working with us on two different aspects  of liquid cooling one is the um the type  of uh plate cooling where there's  actually a plate that sits on like the  CPU and the gpus and liquids flowing  through the system and then we have  somebody else that's working with us on  full immersion and uh we're working with  one of the leaders there we have a close  relationship with them and we're we're  creating some very disruptive Innovative  products for the truck space and others  where the system would be fully immersed  and what helps here is that we can do  this in a very compact form factor  um we were able to get rid of the Heat  and also the liquid we're working with  it to actually help the ruggedization of  the system so  um we're doing a lot of work there we  haven't shared a lot of information on  it but it's a big focus and we believe  it's a door opener in a lot of these AI  transportable markets  I guess I have to have to be a little  bit amused that we're a long time ago  when I had my own pickup truck I had to  change the radiator and the radiator  fluid right and now we're maybe talking  about uh every hundred thousand miles do  I need to change the fluid on my  computer  exactly exactly it's a different world  well so you mentioned also uh I don't  know if it was you or one of the other  Executives who mentioned on the recent  earnings call that uh you might be  looking at not only Partnerships but  perhaps uh some acquisitions  I know as a public company you can't say  this is my particular Target but what  are some areas of interest in terms of  Technologies where you could see down  the road uh maybe an acquisition might  make sense  yeah so we're focused primarily in that  first of all the the opportunity on an  organic level with the stuff we're  working on is so great that it would be  a mistake if we got too distracted by m  a but as a result of things like the  autonomous truck space which is so large  what we're looking for is Acquisitions  down the road that potentially would  make us stronger  more value to the customer and basically  hard to replace and so that tends to  drift into the area potentially cooling  power supplies  additional software layers we have a  software team and we're adding software  layers but doing more on that area it  could drift into safety functions you  know if you think of a truck there's  gonna be a lot of safety built into that  thing before it hits the road with  nobody in it so being able to add more  value on that front so that's what we  think will drive our m a strategy  um as opposed to what we're not really  focused on is rolling up other  HPC companies  um we think we do something really well  and we can add value by you know  branching out and adding technology to  it rather than just have more of the  same okay makes sense yeah I didn't know  if you had any other final thoughts you  would like to maybe add in uh something  that I perhaps missed in in our  conversation so far  yeah I would say that  um you know we're interested in space we  already are designed in working with  three of the top players  and I'm not aware of any other company  that's got three of these guys we have  other people that we know of that have  one uh we knew somebody had two but we  bumped one of them out  um and we look like we have a leadership  position in this area so it's exciting  um  and uh you know we see not only the  production where they're doing training  I mean the development but also being a  player long term in the production  that's our intent and that's what we're  working with and you know focused on as  a company  and so if we were going to come back and  talk to you and say another year and  then another five years what percentage  of your overall business do you think  this would be or is that getting again  too much into the stock knowledge I'll  tell you what I tell investors and that  is that these three uh autonomous truck  companies will be in our top ten this  year  next year and there's probably an  inflection point  um where  uh they could end up being one two and  three  um if not in 23 that could easily happen  in 24 because the the opportunity is so  large and then if we layer off layer on  additional  um players in this market that would  only enhance that so this is this is big  business  um and the other thing I tell investors  is that we're growing somewhere between  15 and 25 a year  uh this growth that could come from this  Market would be an inflection point that  would be much greater than that it's  just a matter when the volumes go from  being 25 systems at a time to hundreds  of systems to thousands of systems  one of the other things I would just add  is something that's pretty interesting  is that since these trucks have been on  the road  or hundreds of thousands of miles  there's a good chance if you live in the  Southwest United States you have passed  one and you never knew it and that's  kind of a scary thought to a lot of  people and they are driving autonomously  but they do have that person for now in  there but eventually that person goes  away  so so who's going to honk the horn when  my kid waves from the window now right  is it going to be the computer vision  system  absolutely  well you know actually since you  mentioned uh growth I just thought what  percentage of your business is  international and is that an opportunity  as well going forward or is there sounds  like there's plenty going on in the  North American Market  yeah so our main business is North  America and Europe  um a much smaller degree in Asia and  right now we're primarily focused on  those two areas but there's  opportunities for this Market segment  and others in Asia so the autonomous  truck is the biggest opportunity right  now is in North America but we're also  pursuing it in Europe and uh some of our  customers are pursuing that in Europe  also as far as AI transportables in  general same same situation we're very  involved on the military front with the  U.S but we're also starting to get  engaged with NATO  um and uh look forward to that because  they're looking for some of the same  things the U.S armed forces are looking  for  I would another comment I'd make is that  I started this off by saying there were  these 16 verticals you know in the  military it's things like a submarine's  one a planes one  uh tank is one in the case of the Indus  the industrial commercial segment some  of the other ones are autonomous farming  tractors  autonomous mining equipment  autonomous bus buses and all of those  have a potential and there are in early  phases of that also or they're using  less sophisticated technology but are an  opportunity to leverage the same things  we're doing in autonomous trucks  [Music]  so welcome to edge industry review  fireside chat i'm jim davis editor of  edge industry review today's topic is  securing networks in the edge computing  era  maybe you wouldn't be surprised anymore  to hear of a startup raising over 300  million dollars but you might be curious  about why a company would develop a new  chip and software to tackle the  networking market dominated by companies  like cisco the company's name pensando  pinsondo describes its products as  enabling a new edge services model of  enterprise and cloud computing their  technology enables the programming of  stateful services integrated into the  network including security services like  firewalls and encryption  we're talking today with scott stevens  cto at pinsondo about the company and  how a chip is going to enable a more  secure world for edge and cloud  computing  scott welcome  thanks jim  so briefly describe the company and what  it is offering  so i think you did a pretty good job of  the intro talking a little bit about  what pensando does but i guess the  simplest answer to that is  we build  next generation silicon software and  orchestration platforms  um to deliver all of those  infrastructure services required for  compute data center build apps whether  we're talking edge compute or large data  centers or hyperscalers so we we package  that in the form factor of either a dpu  so it plugs into a server and we can  offload all those infrastructure  services to the very server edge or uh  with aruba as a top of rec switch we're  at the other end of the cable now we're  at the data center fabric edge and we  can deliver most of those same services  at the edge of the data center so it's a  it's a very useful set of functions for  anybody who's building out compute nodes  larger nodes small nodes edge nodes  wherever they might be  but for the first time  allows anybody in the industry to get  access to the same sort of technology  that's been reserved only for the  hyperscalers until now so really kind of  democratizing  access  to this  fourth generation of how we build out  data center or cloud fabrics where the  services are now part of the fabric  they're not bolted on  on the outside of the fabric any longer  literally bolted as in  a number of different appliances  before i go on though what is a dpu and  why is it going to be useful  in uh our view of the world edge  computing  so if you look at the evolution so edge  computing is based on compute  and for the longest time compute was  based on a cpu  and as as a lot of the the functionality  required for compute became  more complex the gpu market evolved so  if it was graphics intensive or  um ai intensive the cpu would offload to  a gpu and that same compute platform so  now we kind of had two different  processors one doing general purpose  processing and all the application  hosting and everything you might need  and another that could be optimized for  ai or video or  other other data intensive compute  functions  in the last couple of years  the dpu has evolved as the third piece  of this evolved compute platform and the  dpu is the data processing unit and so  what it is is a highly morphed version  of the old nick  where the old nick would just take  something from ethernet and hand it off  to the pci bus  in the server what the dpu has done is  it's essentially  built its own compute platform in that  form factor so the dpu has its own own  processors and its own functions  to deliver as we talked about in the  beginning all those infrastructure  services those stateful things that you  have to do in a fabric so connectivity  you know the sdn stack vxlan that whole  bit but as you mentioned security or  encryption or load balancing or  naps or all these other storage offloads  all of those things can now occur in the  dpu  with no cost to the cpu or the gpu and  so it's just the further optimization of  the compute platform but this time  encompassing the fabric aspects not just  the compute aspects  so given the founders background in the  network industry and the incorporation  of  your technology into switches and line  cards  it's easy for folks to think of pensando  as a network technology provider what  role do you see the company playing in  security  yeah so the founders definitely have a  long history  in in network technologies right the  the founders here  developed most of the data center  innovations for cisco over the last 25  years  so they very much understand how to  build scalable data center fabric  those fabrics and the services  associated with them they've also built  large storage architectures and other  technologies as well  my personal background is actually  security and so there are many of us  here at pinsando that have a security  background as well  and so  if you think about  that this world of how do we further  scale and accelerate networking and  security and storage and all those other  infrastructure services as we build all  this computing environments  we've kind of gone through several  generations and the folks here helped  build the first three and so if you  think about how data centers have  evolved or compute environments have  evolved the first generation back in the  90s was all layer two fabrics and  because there was so many protocols  those ip ipx deck net and all these  other protocols going on and so layer  two was a common way to build them in  the 2000s in the second generation we  focused on ip as the dominant protocol  so we moved to layer three and the  fabrics became not just switching but  routing right so today we think of  fabrics as layer three not just layer  two or both  in the 2010s  as applications started to decompose the  fabric that supported them had to evolve  with that and so again these teams that  you know the team that i'm here with  figured out how to build architectures  and others in the industry of course  figured out architectures like dx lan  and how do we build overlay  architectures in the data center as what  used to be a single application  decomposed into 100 different nodes and  had to interconnect with itself so the  fabric architecture's evolved but  through all three of those generations  any services that you mentioned in the  beginning those appliances or those  other things that we need for security  or or load balancing encryption were  bolted on they were just plugged in  someplace else and we tried to route  packets through them if we needed to  deliver that service to support the  workload and that's really we're  bringing the fourth generation to market  in you know the dpu form factor or the  tor form factor where we're no longer  bolting on those services those services  become part of the fabric the fabric is  no longer stateless it's stateful  and and so we don't have to build all  that complexity that we've had to build  over the years  we were able to simplify the fabric and  so the people who figured out how to do  this for the last you know 30 years  are the same ones that are figuring out  how to take us to the next step and  security obviously is a big part of  doing that correctly  so there is a general concurrence that  data volume at the network edge is  growing so fast that more data  processing should be done  where that's being generated that edge  may be at a factory with  enterprise-owned ip infrastructure  it may be at a regional data center or  even a public cloud provider  interconnected with a wireless network  provider so we think of services like  aws wavelength for example  how should enterprises start to think of  network and application security in this  era of distributed computing  yeah  it's funny we  we call edge a lot of things these days  don't we but  um where is your edge  well and so our edge when we think about  the edge we're really thinking about the  edge between compute and the fabric in  any one of those distributed compute  environments that you mentioned right so  we could talk about a mech out at a 5g  edge we could talk about a data center  in the factory we could talk about the  hyperscaler back in the cloud  and so  from a networking perspective the edge  is as close to the user as possible from  a compute perspective the edge is right  right next to that server right you know  the dpu or the tor at the edge where the  fabric interconnects to the workload  itself  and so our goal our focus obviously is  regardless of which one of those edge  architectures you're pursuing the mac  the data center and the factory or the  hyperscaler  we want to make sure that you have a  consistent way on how you build it i  don't believe that an edge compute  should be different than a data center  compute it should be different from a  cloud compute  the the more we can make them common the  more we can simplify them the better we  are but we know regardless of  which of those compute environments  you're using  the the the traffic in that compute node  in that data center footprint uh is  going to be about 80 east-west and only  20 north south and so the requests  coming in and the answer is going back  out versus all the communication that's  going to happen between those workloads  within the data center about 80 of it is  going to occur in the fabric  and that's been one of the bigger  challenges in how we build these  distributed compute or centralized  compute or hyperscaler architectures  is how do we make sure that all of the  infrastructure services required within  the fabric not just getting in and out  of the fabric  are available they're consistent and  they're simple so how do we do it  without cannibalizing cpu resources on  x86 or putting appliances all over the  place  and how do i give you a common  architecture  so that  scaling and moving more edge compute  nodes further out is just copy paste  because it's exactly the same  architecture that you're using more  centralized  so i'm going to diverge a little into  the east-west traffic notion  i think most of the time when we're  thinking of security we're worried about  being attacked from an outside service  uh however of course a lot of  architecture these days involves  api calls they're not just to our own  resources but it's going to be to a  partner so within a data center  uh there might be an api call to  you know a sas provider for instance  that would be east-west traffic within  the data center right  so  is that what you're talking about when  you're talking about protecting  east-west traffic  there's several ways to look at that  you're absolutely right um but when you  when you think about  let's let's let's focus first on those  decomposed applications  and so or you can do you know web tier  app tier database tier right there's all  these different ways that we build out  these compute environments  but  when when a request comes in and we're  worried about an attack this whole  concept of zero trust sort of emerges as  this conversation what do we do  within that data center fabric  to ensure  that all the communication that's  occurring is what we want to occur and  nothing malicious is happening and so if  you think about it in the context of  zero trust  the  the first job and sort of that zero  trust tenant is instead of building a  perimeter remote around your environment  and assuming anything on this side of  the moat is safe  zero trust says no  we don't trust any communication from  the outside in nor do we trust any  communication within  that fabric and we need to inspect and  ensure that only the communication that  we want to take place happens so that's  really the first focus around east west  is how do we provide security between  those workloads within the fabric so  that how they interact with each other  is only what we want to be happening and  if an attack had occurred and somebody  had gained access into the data center  they're going to start doing malicious  things east west  if we're not able to secure east west if  we're not able to inspect east west  they've got a free-for-all they've got  all the time they want and so again back  to the tenant of zero trust the first  thing we have to do with zero trust is  east-west security you build out from  the the place where your data resides is  where you begin and then you sort of  build out from that towards where the  user sits and think about how do we  control all those interactions  so  the company has made inroads with  centralized  public cloud services  you're integrated into switches  and other technologies  where else will we see pensando another  way of asking might be you know which  edge locations  you know an edge server gateway  inside the factory or some other  enterprise data center  what are some viable markets for the  technology  i think the nice thing about our  technology is it's sort of foundational  and so if we're talking  about a hyperscaler architecture where  we become the dpu  that's sitting in the server edge within  the hyperscalers environment but that  same architecture  is completely relevant for an enterprise  customer that's building on a large data  center and so we can be the dpu for them  um or if they're not ready for a  dpu-based architecture we can be the  stateful tour for them the other end of  that cable right from the server to the  top of rack  and so  those those kind of work everywhere if  you really think about it so whether i'm  a hyperscaler i'm a data center i'm on a  factory floor or i'm in a macro far  distributed edge  those foundational elements are required  to make these things work right i need  to have you know connectivity i need to  have networking i need to have the  ability to secure we make those things  happen regardless of that footprint so i  think the two form factors we have today  are very interesting right as you know  the server edge and the network edge  and i think that's going to be where  you're going to see us for a good amount  of time  and it's really up to the customer in  terms of which form factor makes sense  for them in in the problems they're  trying to solve and the journey there on  to reduce complexity and simplify and  frankly you know if you really want to  we can talk about zero trust on a lot of  things but  in the end people need to figure out how  to better automate what they do within  their data center infrastructure  automation gets rid of of clicking and  manual work automation requires machine  learning machine learning requires  telemetry  we get to see all of the traffic within  the data center architecture we can feed  ml engines from security ml engines to  net performance management ml engines to  to ddos mitigation ml engines and so  being able to provide telemetry in  either of those form factors  to help deal with this need for better  automation  you've got to feed ml ml creates  feedback loop and if they figure out  something they change our posture they  update us through our open apis  to change our security posture or  whatever based on what that ml engine  just found  so it's it's really an interesting space  but i think the form factors are  pretty consistent regardless of where  you need that to be  i'm not sure where  where this question might lead us but  what are the implications of placing  encryption and policy management  capabilities and other  programmed security services outside the  traditional cloud and enterprise data  center environments  most of what we're talking about so far  i think uh you know we would still be  placing security in these places we're  used to putting them  um if i'm thinking of a company like an  akamai or a cloudflare that's using  servers  you know hypothetically with pensando  technology  what might be some of the interesting  possibilities  of placing  dpu-powered  security services in a more distributed  fashion  i think  so you said two interesting things there  so let me stay with you later one um  first which is what's sort of the value  in these distributed computes and these  you know more cdns or or scaling of just  of connectivity to users  um i think the ability to decouple  things like  security or encrypt the crypts  and take them off of x86  means now our servers scale far better  so i get greater efficiency out of my  server because for a lot of those  companies they're using appliances  in front of those servers or they're  using  software running on x86 on that server  and so if i can offload that to the  compute engine of the dpu  i now have  all of that focused on a compute engine  that's optimized for solving that  problem which is associated with those  services and i o and encrypted crypto  security whatever it is we're talking  about  and my x86 environment and x86 isn't  doing moore's law anymore it's not  scaling you know as fast as it used to  and so my x86 environment now gets to  scale far more effectively for the  service i'm delivering so that would be  one of the answers i would give you  there  the other  in the first part of your question if i  understood you correctly  i think when i think about the services  we've been talking about and things like  zero trust versus things like encryption  um zero trust is about inspection of  flows encryption is about obfuscation of  traffic right hiding what it is as it  transits a network and so i think  they're both very relevant they're  different conversations and and we  obviously have the ability to support in  both of those those use cases so they'd  be able to the ability to encrypt and  decrypt at line rate without impacting  the performance of whatever it is i'm  doing  is really powerful  especially in those distributed  environments that you're talking about  but even after i decrypt i still want to  inspect i still want to make sure i know  just because it's encrypted doesn't mean  it's trusted  the other end of the encrypted flow  might be infected and so i still need to  inspect and make sure that  the traffic that i'm getting is what i  am supposed to get and it's coming the  way it's supposed to come to me  and you were talking about scaling  operations  the  speed at which you're doing things like  encrypting and decrypting  isn't  impacting the power budget  as much as one might think right  tell me a little bit about the power  consumption of the chips that we're  adding in here  yeah you know depending on the  generation of silicon we're talking  about here it's you know 20 to  kind of 45 watts of power consumption  per server if we're in the server  footprint and similarly of course if  we're within the tour so it's really low  power draw  um for the kinds of things that we're  talking about and so be being able to  well if you if you do the math  so we add you know a normal nic will run  obviously fewer watts but let's say  we're running 45 watts in a server but  we're replacing a bunch of appliances  that we're probably running a couple  hundred watts each  in what we're able to do  and so the overall efficiency of that  compute environment wherever it is goes  way up because we're not consuming as  much power we're not generating as much  heat we don't need as much cooling  and so there's a lot of advantages that  you get but you get a operationally we  get a simpler environment  fewer operating systems to worry about  fewer things to manage fewer you know  fewer people i need in my op staff to  try to manage the spaghetti of  everything that i've got there um we  start to bring that same paradigm that  the hyperscalers have they don't have  that complexity they don't have those  appliances they have dpus in the cloud  environment  we bring that out to everybody so you  get that same power efficiency  operational efficiency and simplicity in  your compute environment that the rest  are having  and so you've been mentioning the  hyperscalers a lot  obviously folks like aws have been busy  more and more these days designing chips  of their own whether it's for ai or  other functions  um what then is your target market in  terms of a cloud provider  perhaps one that's providing some sort  of a distributed edge cloud for instance  you know is aws your target customer or  is it somebody else i think all the  hyperscalers are our target and as we  talked about in the beginning our  technology is sort of foundational  regardless i mentioned the hyperscalers  a lot because they figured out this dpu  thing  years ago  right and so they're they're the canary  if you will they're the direction  they're the true north like that is how  everybody will build their compute  environments moving forward and so  they've learned how to do it at massive  size and great simplicity and that's the  only way a cloud works right you have to  have massive scale but it has to be  simple and fully  automated to make it happen  there's no reason that shouldn't be  available to everybody else and so any  hyperscaler is a target for what we  build and i think we've got some of the  best silicon and software in the  industry right in terms of how we can do  that for them but  you know any enterprise customer who's  building out a data center framework  wants that same simplicity a service  provider who's building out you know  mech architectures and distributed  compute and centralized compute same  problem right how do i simplify that how  do i reduce the amount of moving parts  and the operational overhead so that i  don't have to necessarily outsource it  to a cloud i can deliver it myself and i  can make that happen so  again what i really like about the  technology that we've got here is it's  foundational regardless of your size or  scope  but it simplifies what we're trying to  do operationally and  it's sort of the you know we're skating  to where the puck is going to go that  that reduction in complexity and that  reduction of power consumption is going  to be key for everybody who continues to  build compute environments whoever that  may be  and of course uh that's really really  important for security to have that  automation and simplification because  lord knows you know even simple things  like securing a bucket of storage you  know those are things that people are  still kind of struggling with  so uh  but having it fully distributed across  the entire data center fabric  it scales now it hasn't scaled within  the fabric before north south we scale  fine but within the fabric east-west and  between all of those like you said the  distributed compute and distributed  storage you need to have a fully  distributed set of security services in  order to make that work without  impacting those services  alexa scott thanks very much for taking  the time to speak with us today i really  appreciate it  jim was a good time thank you very much  [Music]  hi i'm james mcguire and on today's  e-speaks we're talking about edge  computing we're taking a look at where  edge is here in 2022  along the way we're looking about we're  talking about what it means to build  distributed applications and execute  code at the edge some pretty complicated  stuff to discuss that i'm joined by  lakshmi sharma chief product officer at  fastly lakshmi thank you for being with  us today  thank you so much james looking forward  to our conversation today  you know i think that uh edge computing  is having a very big year in the year  2022. we've had a lot of headlines about  cloud all these years but it feels like  this year is the year the edge is really  taking off  uh as you survey the edge computing  market what are a couple of trends you  see driving us forward this year  yeah um  very excited to be talking about this  subject very close to my heart um so the  trends that have been evolving over like  say 10 plus years um the first trend i  would say digital transformation right  as like whether it was retail and banks  and  uh say hospitality like you know any  industries into that as all of them  start to go from like so this you know  in-person in-store  uh real-time experience right let's use  that word real-time for people getting  into the stores getting into the  buildings like for banks or atms so all  of those were like experiences that i  know thanks to actually retail industry  which started to kind of transform the  first because they had the need to do  that first right so the digital  transformation and the certain specific  industries like e-commerce retail and  then banks they transitioning into this  digital experience right so and then  making that real time because if once  you go from store to something omni  channel the words like omni channel  became really kind of prevalent if you  remember from those days right 10 years  ago omni channel means store experience  and digital experience so the digital  transformation was the first trend and  that i would say led into a lot of other  trends that happen and around the same  time happen cloud right the cloud  migration if you want to build a digital  experience of the mobile devices and  then you know  omni channel experiences or it means  basically all kinds of digital endpoint  devices then cloud started to play a  role oh we can offer that right i can be  the point the central point which will  take away your like you know  infrastructure needs and like uh you  know the needs that you don't need to be  worrying about so that you can do  business so the cloud transformation  cloud migration happened so then the  third trend that happened is when you're  building these clouds and putting focus  on the digital basically you're putting  everything onto internet right and then  each of these kind of industries right  retail e-commerce and then banking and  transportation they all needed their own  experience personalized experience and  then you don't get a personalized  experience by just putting things into  centralized cloud right you need to  bring things to the edge  to the retail edge to the e-commerce  site to the bank edge to the you know  any edge that you call transportation  edge so you need to bring it that means  that you need developers  or the code that can be personalized to  those needs right so the personalized  edge needs developers so then that's the  third trend is like the developers kind  of and the developments and then people  who are operationalizing that that is  devops right so the number one digital  transformation second is the cloud  migration and then third is the one in  order to accelerate and support those  towards the you know the software  becoming a key leader into that  transformation then the fourth one is in  order to operationalize that gigantic  personalized edges at each of those  industries you needed things that can  you know that can roll out services  faster that can operationalize things  faster  but then imagine those dev then the  devops kind of you know trend started  instead of going from it ops legacy i.t  to the dev you know the fast and agile  i.t which is devops right and then all  the security teams are coming and say oh  cso teams will come say oh no you're  going too fast for my policies right and  then like still slow slow slow so they  were becoming kind of blocker into the  devops that they needed right and then  they say okay let's just kind of but  then what happened then the number of  vulnerabilities that were coming from  code grew more than csos had to become a  part of the day-to-day thing they're  like hey this phenomena called devsecops  happen right so then the fourth trend is  like devsecops right so the four things  starting from digital transformation  number two is cloud migration and then  third is like the software enabling  everything edge personalized edge  programmable edge and then the devsoc  ops actually making and putting all of  that into the business and the  operations and trying to match the  previous world does it make sense to you  so what are you doing what have you seen  well i i mean i think it's  it's almost like the edge and and uh and  cloud are sort of working together in  many ways  sometimes i think they're they're slowly  in competition i think there's one  really interesting point you said that i  that i want to zone in on and that is  the idea of the personalized edge  um  what exactly do you mean by the  personalization can you give me an  example of that  yeah  so um personalize like you know when i  say i'm i'm talking from the enterprise  experience right let's say i'm a  retailer right or i'm a bank so if  um if i need to kind of i'm in a bra i'm  a bank which is a global bank right so  my requirements for a branch in say u.s  would be different from you know there  would be certain components which are  common but there are the my user profile  the experience that i need to deliver  digitally to users in the say you us or  even within same different parts of us  midwest user versus like you know west  user or east user so you're talking  about the digital experience that you  deliver to different users would be  different the speed that you need to  deliver the workflow that you need to  have the data governance and the data  kind of you know rules that you have all  of them need to be catered to the first  number one industry that you're in so if  i'm retail e-commerce or i you know a  software technology company versus a  bank or a you know health care your  requirements of delivering how you  deliver their digital experience are  different so that's that's what i meant  by like you know the speed is different  the governance is different the  workflows are different and also if  you're you know global company then you  then you want to be giving like very  specialized experience to the users with  one country versus one language and  different that's what i meant the  industry experience the location  experience and then the governance  experience that makes perfect sense  so let's talk about challenges i mean i  know there are plenty of challenges  involved with edge let's look at the  challenges involved with building  distributed applications of the edge i  mean any advice you would give companies  yeah um and i would like your input too  uh you know this is kind of you have  suffered and you have been talking to a  lot of people  so um so the what we see right so  and having been in a cloud company  before right and having talked to  customers um so the trend that is  happening is um is that um from a  personalization perspective going back  to my personalized edge kind of you know  notion so when you want to deliver  something  uh for you know for a particular  location or a particular region  um if if you as a developer i don't want  to be worried about like where is my  region where is my you know zone what  what kind of vm do i need what kind of  because my for my focus should be on the  logic business logic right right truly  hannah vicker and  i'm going to be on internet right i'm  putting this application for internet  public internet so then if i have to so  there are two things i want to avoid one  is  i don't if i'm on internet i don't want  to be figuring out what is my you know  the  am i going to be attacked by bot what  are the signals that i'm going to get  what kind of expressions do i have those  are distractions yeah exactly right so  as an application developer do i need to  become the full stack security developer  and then second is that do i need to  become the full stack infrastructure  developer by figuring out what region  what compute what storage  that's that's the challenge because then  it's it delays mine right from an  infrastructure perspective if i need to  worry about all of them like you know i  need to understand become an  infrastructure like distinguished  engineer maybe to write that application  so so that delay is like the speed that  is so your challenges are  innovation versus speed  and where do you deploy that speed  because as i talked about the location  and the global kind of where are you  deploying the experience personalized  experience are different from location  for user second is that on the security  side right you're worried all the time  being on internet that how do i keep my  application secure so if the velocity  the security  of requirements for delivering an  application  uh for at the edge for your industry for  your personal industry require a lot  more knowledge  if we don't really make it easy for  developers to kind of you know abstract  those information away and just keep  those tools as part of the application  stack does it make sense it really does  and i i think  the other issue that i think about on  the edge is the concept of  where the numbers are getting crunched  where the data analytics is happening  and that  these applications at the edge create a  certain amount and actually an ocean of  data  and then and how do we handle that data  do we crunch it there at the end of the  end point do we move it back to the  headquarters to crunch it  uh and it seems like as time goes on  more and more of that data is getting  handled at the endpoint instead of  needing to go back to headquarters to  get processed that is fantastic that is  fantastic point  uh like you know when you talk about  these you know say reuse cases like say  in automotive right so you're talking  about autonomous cars you're not going  to kind of continue to process those  billions of devices in a car itself  right or maybe hundreds of thousands of  devices you don't have any time this is  the real-time life you're talking about  right at the edge i agree with you so  how do you  um how do you make that decision you  know and  in real time for the data that you're  collecting at edge what do you process  how much of processing power do you need  and then how much of that you kind of  take away for your let's say historic  information or auditing purpose that's  actually this this is a critical need  for  so that's where the data is and  real-time view into the data i come back  to real time all the time because these  applications as you said like so more  and more data is coming like happening  all the time right so the data  processing is important and also when  when you talk about data there is also  that i know that point about how much uh  can i have access to the data if i am  the one kind of providing the ad then  access and you know and policies for who  can access what amount of data what do  you keep at the edge are you allowed to  keep something else like which goes back  to the governance question too i think  there is a huge um direction i have to  say there is a huge  market first of all of how do we kind of  deal with that but also certain  challenges that we need to be kind of  focused on in what data to kind of keep  what data to send and what data goes  back to the customer itself because we  don't have access to the data and we  should not have access to the personal  data  and and what about the idea of machine  learning and artificial intelligence at  the edge and the algorithms learn more  and more to a certain extent they're  operating independently on those  endpoints sometimes i think we're  creating this world where  there's this extreme network it's it's  out there in many ways it's operating by  itself as as the ai and the ml operate  by itself so it's  i'm not sure what my question is how  this thing is fascinating that we put  more machine learning at the edge and  the machine learning gets more  independent so it's almost running  itself out there in those end points  um  yes does it make any sense at all or am  i  am i making that up  no you are not making it up and i i know  that sometimes folks think that i've  heard from some people ah you know  uh aiml everyone has to just talk about  it but that's a reality like you know  when people talk about like  mirrors or the aiml that's real that  happens in life like you know every day  you can we can really automate things  um at edge so that and again everything  is going back to like helping people  right we have businesses so that you  help people and in order to help people  if you can take away  um those routine tasks right if you can  take away that information that is not  really helping them add value to their  life or add more value to a business  right so then that's where ai and ml can  to be honest really help  fast kind of automate things that you  don't really but then if you have to  keep writing automation yourself versus  and imagining the data point that you  talked about if you keep automating kind  of you know if if an engineering is  automating every day oh i have this  better point of data then let me analyze  automate then automate more instead you  can really use ml right ml  start collecting and putting the  automation self-automating that's what  really ml is about and then you can  start to find trends on the data which  is coming right at the edge in order to  deliver value in real time right so the  edge is helping personalize real-time  experiences so aiml becomes very  important to take away the task  that you don't want to and plus like  edge is not centralized data center you  won't have like infinite compute there  you won't have infinite data there so if  you don't really start to kind of take  away things you know in an automated  manner through ml sooner and dumb cert  is something not dumb maybe send certain  things in the centralized place and  decide that sooner  that what is most important to be sent  back to the end user in you know then  then uh each of those edges will become  the gigantic data center so that's for  ai and ml and with tying that to the you  know the question that you had asked  about data i think that becomes really  important  hmm  how are you thinking about it  well i think it's really the interesting  thing i think is that um  those sensors at the edge  i think are still in terms of being able  to handle data and crunch data  are still relatively primitive by  comparison of this the to the data  crunching you know processes back at the  headquarters but  i think as as the years go on and not  that far in the future  the compute power at the edge is going  to get you know greater and greater and  greater so it's like  it  the edges is really is it's going to be  powering our world  what do you think about like tpus gpus  like where do you think they will sit in  the story  well this is a good question i mean i  think that the way the gpu has i think  so revolutionized enterprise i t and we  looked at the gpu in terms of  uh really driving artificial  intelligence forward  uh i'm not sure what i see you know gpus  on the edge are gps gpus living out  there in the edge now  um no i would not i would not want that  that's like too much i mean it's talking  about too much of compute happening at  the edge  we're not ready for that yeah i don't  think gpus but like i would see that  there are like smaller form factors with  tpus that we have seen like you know uh  and which industry has talked about i  think that could be a potential and  that's where like because they're also  optimized for ml and data models so i  think that can be a place and we have  seen industry kind of using them but not  so much gpus at the edge yeah  do you think there will come a time  though when a gpu is at the edge  um  i think that would be too much of space  power and then that would not fit into  the current designs of the  pops and then mini pops and many data  centers that we have i think that will  that will then make it a centralized  data center but at least looking at the  current form factors of the power and  you know the the power uh that these uh  units are taking so until that is kind  of you know changed but that may really  mean that it is really uh optimized to  do something and tpu seem like that  version of that optimal version of the  gpus to me  let's talk about  fastly itself i mean how is fastly  addressing the cdn and the edge  computing needs of its clients  what's the fastly advantage  yeah fastly as i've been saying uh that  the trends that we talk about right the  trends of digital transformation putting  software programmable edge immersive  edge and immersive real-time experiences  fascists have been leading this you know  transformation or being leader into this  uh programming bill edge real-time  immersion applications through its  lowest latency platform you know so we  have been always like a developer's  first company we have always focused on  how do you reduce latency and when i say  latency latency means couple of things  to me latency is that when you configure  uh configure something like latency is  the time to process that's how i see and  while if i was a network engineer many  years ago i would think of latency as  like how much time does it take for a  packet to go from bit to go from one to  another right but then the newer word  like latency to me is that how much time  do you take to  you know push a configuration down how  much time do you take for an application  to come up how much time do you take for  something to roll out right so  at fastly is programmable edge that's  where this company found was founded on  that and edge platform or compute edge  platform has always been software you  know developers first and then we have  we have kept our value around  reducing the latency the amount of time  it takes the first use case had been  like cdn so the way to think about is  that we we have been building an edge  experience platform  built for developers from day zero our  use cases had been cdn  caching you know being the best purging  algorithm so that you get the best cash  hit ratio for any kind of object static  object dynamic objects so we kept  building and keep making it so that we  and then we added more use cases we are  like  you know the our customers were building  like vaf our customers were building  like you know image optimization so we  think of this as that a company that  kept building like a marketplace of the  services that  we found our customers doing because our  customers started to be developers and  devops and those people and then we  acquired six hikes you know signal  science which is a signal based like  security company we also integrated that  back into the edge right so then the  same same people like you know who had  challenges about you know i talked about  like the csos had these challenges like  how do i stay on speed with my devops  folks so we started to also go out to  and then work with those people like in  the devsecops so  we were seen as a cdn company but the  way kind of but we were actually always  helping  a programmable edge company which was  built in the foundation of delivering  everything fast whether it is config  fast deploy fast roll out fast and  policy  so we you know i think so  uh the whole ex you know the  programmable edge personalized  experience at edge we we have been doing  pretty well and cdn gave us that  stepping stone because that's the that's  where the area where we proved our value  the first and then we continue to add on  to that  so is this is the fastly network does it  is it built on or does it interoperate  with the the major cloud  vendors how does it work in that world  uh we um we think of this like we we  kind of are complementary to cloud i  know because cloud started kind of you  know central journey and then there are  you would see almost all the cloud  providers are going towards the edge but  we started from edge right so so we are  complementing and then the way this the  techniques that we use for example we  use like was on which is web apple you  know web assembly uh  web assembly way of putting the modules  together or services together service  chaining at edge and those are like web  requests we are talking literally  services so we had always been web  service first  at the edge while cloud providers were  coming from iis and pairs and then  containers right so  we it's kind of a very good mix or  complementary so we application service  first at edge while cloud providers  started from the cloud centralized model  first we are literally location-less  service anywhere anytime and then clouds  are like you know okay start from  centralized service build a you know  kubernetes cluster or build a you know  service mesh so we are a very good  complimentary uh we can actually have  any platform running on us so we can be  an integrator into anything i would say  aha okay  well given you given how close you are  to the market i'd love to get a sense of  what you think about the future of edge  computing uh what what are some key  milestones you think you see looking  ahead  and i would like your opinion too  because you talk to a lot of people  i please i'll i'll i'll trade you so  what what is your future of what's the  future of edge is as you see it so um so  the edge is just happening i would i  would pick some numbers like we i picked  up from gartner so  uh gartner number says that enterprises  had like five percent of enterprise  workloads were close to  in 2019 were moving to edge you know do  you say five percent five percent in  2019 small and it will be yeah it will  be 40 in 2024 wow okay it's like massive  right yeah yeah if you look at like that  that transformation of the compute  workload that are there but then as we  talked about like earlier then certain  industry kind of went into that sooner  like e-commerce and you know technology  companies others are still exploring the  potential  and like say financial right and other  companies so it's really kind of what  i'm getting to is that it's really use  case driven edge is opening  opportunities that people couldn't  imagine before  right so it's like going back to the  data point point that you mentioned data  governance at the edge and then like you  know keeping like making decisions which  are like real-time telemetry real-time  visibility  real-time in real time did not happen  with cloud and other technologies they  were like  lag if you needed information you would  get samples not full information right  if you needed visibility you will get  like five seconds or so certain interval  visibility not real time so the true  real time personalized happen at the  edge right and then you and there are  technologies now like wasm i talked  about where you can get full isolation  of one you know one customer from  another one organization to another  right at the edge and then you can  really  do things like update and maintenance  everything like you know in your own  isolated environment which is not  possible with other technologies so the  technologies have evolved right  technologies have evolved and that's why  it is opening up newer use cases so  that's where i see that it is just  started and like even if you look at the  numbers that i just talked about and the  technologies that i talked about like  wasm like you know being able to do web  logic and then web assembly not  necessarily web logic that's another  term but web assembly instead of kind of  doing bottoms up infrastructure location  aware to location less logic so  technologies are there  software is evolved and newer use cases  are just coming  and that will kind of create and your  industries are adopting it as i said  like from retail media media and gaming  that that was a kind of our first one  media gaming to retail to e-commerce to  now uh financial and other companies so  i think it's just it just started  hmm yeah i totally agree that that edge  has a very bright future and i don't  think people even realize the extent to  which edge is going to grow and play a  big role in our life i think it's still  sort of an underdog in terms of its  profile  i think one of the issues involved is  uh artificial intelligence and data  because you you with ai you've got the  algorithm but and that's really powerful  but the reality is that algorithm  doesn't do much without a real an ocean  flowing a notion of data flowing into  that algorithm you need to get a lot of  the data to really make that algorithm  powerful so the question is  how do we get that level of data to  those edge and end points and i think  when we solve those kind of problems  with network flow it's going to become  ever more powerful  that's that's 100 right um  data requires a powerful network right  and data requires like um you know once  we have figured out all the data tagging  and data isolation and then you know the  data level problems that what data  required comes with it but then you need  this infrastructure and the network to  actually deal with that you know how  much and then uh there is there would  always be this kind of you know the uh  the return of investment and like the  point that we that  return of investment on the edge what we  have to keep making those decisions and  what  what is the size of the edge right and  how close and personalized do you want  it to be so if you want more and more  personalized then you it needs to stay  nimble it needs to stay small form  factor wise too so that the you know  going back to the cost and all those  kind of um  agility and the cost and everything that  comes with the size and the location  that needs to stay  uh you know that that need that's very  important and then  um that's why like and coming back to  your aiml point  uh if you like you know if i want to  build model if i want to automate more  right at the edge then we need to start  to think about where do you act also  keep these models right because these  models also take kind of that they're  like configurations the model needs  space the model needs storage and then  what models do you actually train better  to stay at the edge versus versus what  models do you kind of get you know move  to the centralized cloud i think that's  something that we need to continue to  work but i i think that would require  all these edge cloud providers and all  of us to work and build a marketplace i  kind of call it i've seen that in my  previous company that  where people kind of you know we you  test a lot of models across different  companies or different customers i mean  and then you take the most relevant one  most used one and you make them like you  know  you offer them to all the customers as  part of your api right so driving a data  model  uh driving a data model or an ai model  machine learning model for your most  personalized use cases delivering that  as an api  because you know most of the customers  would use it and you know bringing more  and more these edge companies into one  place and saying that let's keep adding  more and more  you know the ai models or the data  models to the edge as an api  and i think that will drive a lot of  success  hmm  watch me i i think you said it it's a  lot of fascinating stuff it's going to  be a super interesting secular following  the years ahead and i i greatly  appreciate you sharing your insight  today thank you thank you so much it was  it went again fast like  you and your session it just feels like  very very important which technology  technology itself moves pretty fast  right that is true yeah yeah  thank you thank you  you  hello and welcome this is eric sofci  moderating for mit horizon thank you all  for joining us today we're lucky to have  with us wei seong shi for a discussion  about the future of edge computing  wei song xi is a professor of computer  science and the associate dean for  research and graduate studies at wayne  state university where he directs the  mobile and internet systems laboratory  and the connected and autonomous driving  laboratory  he's also a a distinguished scientist in  the association for computing machinery  an ieee fellow  and in 2019 he was lead editor for ieee  special issue on edge computing  so thanks for joining us weizong  and let's get started  so to  start us off i wanted to first  see if we can establish exactly what  edge computing is  so how do you define it and why are  organizations interested in it right now  thank you eric yeah thank you very much  for having me it's it's an honor to have  this opportunity to share with the  community about the edge computing and  the future so go through your first  question i know there is a very um  general understanding from from the uh  community is that you know we process  the data at the uh as close as possible  you know uh to the data sources but here  the um i would like to give a sort of a  official you know the the definition  which is originally coming from the  paper i wrote in 2016.  so here  we define edge computing as the enabling  technologies  that allow computation to be performed  at the edge of the network  here edge refers to any computing  resources such as cpu network storage  resources along the path from the data  sources to the cloud data centers so it  is a continuum it's not like a wine  device you know as some people  are expected here the computation  involves two directions so including the  downstream  that uh the computation  on behalf of the cloud services  you know and also the upstream uh from  the you know internet of things the data  and then all the way on behalf of the  iot services so i think it's very  important to know that edge computing is  not just one direction actually it's two  directions  yeah um that was the sort of the  definition i would like to share share  here and the you asking why the  organizations are interested in this  that's actually perfectly matching to  that definition i i shared just now for  example as a cloud provider you know  amazon microsoft and google they will be  very interesting to push their services  to the edge uh as close as possible to  the end users and where the data  generates for example amazon you know  lambda computer number computing and etc  and then at the same time you know as  the upstream you know you have a lot of  data collected at the field you know  either field a smart home or maybe in  the manufacturer  and etc in this case you want to do a  lot of computation as close as possible  you know to provide some extra services  uh you know in this case like a smart  manufacturer they are not really coming  from cloud but they come from we have  data we want to do some local processing  here so that that's why there's a lot of  organizations are using this and from  both directions  okay so so it seems like obviously cloud  and edge are uh very related there's a  lot of overlap  um  in in a best case scenario  uh do you think edge computing would  support cloud computing or would it  replace it essentially again only in a  given application obviously not  replacing the entire cloud  yeah  i think that you know from the beginning  when edge computing was  first proposed uh you know that  around 2015  um then people always ask this question  is this going to be something new going  to be replace the cloud uh i don't think  so you know that azure computing is a  extension of cloud  and it complemented with cloud very well  you know as you can see that the cloud  the the beauty of cloud is they have a  global view  imagine you you have an organization  it's a global organization so the cloud  has a global view of everything what's  going on with the multiple factors for  example picking a manufacturing example  they have the global view that they may  they can make some macro kind of  decisions you know some data  machine learning models and etc however  the edge can help them to say well i  want to do some localized you know  processing i also want to have some  customize the design and this so this  would be a perfect example to running  this on the edge  so  in some cases one of the benefits of  edge computing is you increase the  reliability or availability you know per  se because  if you purely rely on one data center  sometimes if there is a you know  networking issue or maybe there's a you  know server down and then the whole the  whole system you know the service will  be interrupted so with multiple ads you  know deployed in the different  geographically for example then at least  you can have some sort of you know  services still available even when the  whole cloud you know is out so i think  in this case they actually um i would  say edge is an extension of cloud but  that makes it much more you know the  the panel traded to our uh to our life  okay and and is there sort of strictly  speaking  are there examples of edge that don't  rely on the cloud at all  yeah yeah i i think that that that is it  is possible for you for example  um  if you want to say completely don't rely  on that it's possible uh for example in  some of this larger warehouse which i'm  aware of for example i'm not sure if  amazon is doing that because amazon  obviously they have a cloud but in some  of this like  jin dom you know jd is one of the bigger  online in china  they have there's a bigger warehouse and  then within the warehouse they're  putting a edge computing there to  coordinate like a 400 you know  autonomous robots  which is sitting there to coordinate  them in this case they don't really need  to rely on cloud you know they can  perfectly to execute themselves improve  the efficiency for that purposes  right it seems like oh i'm sorry go  ahead  sure yeah yeah i think that's that's the  case and also in in some scenarios like  in the uh disaster you know in a  disaster areas and then you're sending a  lot of vehicles coming in um probably  you even you wonder using a cloud but  there's a it's not available at that  time so in some of these scenarios  it is yeah okay yeah it seems like in  that example of the warehouse robots  using the cloud could almost be a  problem because there'd be more of a  delay in you know uh  sending information and receiving it  whereas the edge seems like that's one  of the main advantages right is that  speed of processing and sort of  less uh or data being sent over a  shorter distance right  right  okay the um  so okay you mentioned amazon  one of the things that we've been  looking at with this topic is  this notion of um companies like amazon  and verizon other sort of partnerships  where companies are creating what we're  calling sort of public edge  infrastructure so you might have edge  servers that are placed  in or near a 5g tower or other places  where sort of  nearby users can access them  do you think that sort of approach is  is  eventually going to become the default  for how most people sort of interact  with edge resources or is that just sort  of one part of a larger picture  yeah i i think that is  probably just uh  one part of the larger picture because  as uh as you know that uh to answer  another question let's first start with  look at the in the future where are the  data uh you know where is this data  generated you know according to the  gardener by 2025 75 of the data  generated on this planet will be outside  the  cloud data center  so this then means the cloud center  some people asking oh are you going to  move in the data from cloud to the edge  that's not the case so cloud will be  still going to be increasing however at  the edge of the of the network you're  going to generate  the path you know the pace to generate  the data is much faster than the growing  on the data center so because of this  reason  that uh you can imagine that this is a  huge market here  right because of 75 of the data needs to  be processed  that's why there's a lot of  um potential players gonna be in the  energy computing field the two you  mentioned you know amazon which  represents the cloud providers obviously  uh they have their incentive you know  they want to continuously grow their  business they want to deploy these  things as much as possible and then for  the you also mentioned the verizon att  this telecom you know globally  that all these telecoms including uh  vodafone uk telephonica you know in uh  in spain and china mobile and et cetera  you know in china so  those those  companies there are they view this as a  great opportunity for them because  this so that they can deploy those edges  servers so that uh serving as a  resources  all right so that is uh from their point  of view and now we already see that you  know at t  uh in china unicom they have been  starting deploying these things however  there i i noticed that there is uh uh  that there are some other  you know potential players recently  jumping for example that  the what we in u.s we called  um  for example dt energy you know like a  smart grid  and  yeah especially in in china state grid  because their view is very simple um  where's the data where's this data  generated basically it's uh those data  is generated  by most of the things and they have to  have a power correct  so once you have a power that means they  already have a computing resources there  you know most of the power controllers  that they can add a little bit of  computing resources that will be a  perfect places for for the people to do  that and you know to put the edge  computing resources here so for example  in china what happened is because china  is very similar to new york you know you  have a lot of these  high-rise apartments and right  for you know one of the divisions you  could have like a 30 000 residents in  this area and then they're building  these computing resources next to the uh  you know the the power entries of of the  i i don't know how to describe that that  you know but you know  what i mean here is you know you have to  have some sort of a switch right  then you put in the computing resources  there and uh can serve there's a whole  you know area so that is another  potential player and actually state grid  in china is invested heavily in this i  haven't seen too too much in in the u.s  side probably is because of the density  here is not as dense as as they used to  see in europe or in china scenario so  that's why i my answer to your question  here is on telecom and the cloud  obviously they want to do that  but there's other potential there's  another potential player here is the  city government you know maybe through  some uh some of the private factors of  private sector  the reason here is in addition to the  central tower we also see a lot of  traffic and eyes  you know one of the work that now  recently we deployed at the wayne state  campus is we put in the edge server on  our five intersect intersections  connected to the traffic signal and then  to support our campus uh you know that  uh  we we call this as a connected vehicle  edge computing uh live test bed because  the students your your phone you  you know you can through anyway  wednesday the campus already have a  wi-fi coverage you know even the home  the middle town area at the in detroit  so you can connect it to this edge of  servers and then you can send your  warning about the project you know for  pedestrians a warning for the traffic  and etc so in this case you can see that  this is not necessarily deployed by  either cloud service or you know telecom  this is actually coming maybe coming  from  uh you know traffic transportation area  right  so those are all these people all these  different industry sectors they see  there's a huge potential here so i view  this is that eventually you're going to  see a variety of agents you know  providers and then that's good for our  customers because when you was driving  uh going you know walking through the  city you have an opportunity to to reach  a different type of what is uh you know  the  resources that's it's  it's interesting because it seems like  the common thread there in what you're  describing if you have sort of uh  traffic infrastructure  um  electric or power infrastructure energy  infrastructure and then also the the the  sort of telecom infrastructure it seems  like maybe more than even cloud which  edge is sort of again kind of uh  compared to cloud a lot cloud the  infrastructure issue is handled by  big providers often who just  buy like a large you know plot of land  and and build a huge facility  in this case it seems like we're up  against this infrastructure issue right  where it's where who has the real estate  who has the sort of right-of-way you  know who can who can place edge  servers edge computers in a place where  people can get to them but do you think  i guess i'm curious that obviously  infrastructure issues  pose their own challenges  are there other challenges to or guess  what are the other main challenges to  sort of increased development of edge  computing  yeah this is a great question because  it's the nature of follow-up with that  since we have a potential a variety of  the infrastructure providers here and  then the key things the main obstacle  even today is about how can i develop a  program can can use it  you know when i develop a programming  for example we are computer scientists  here so i want the writing application  today let's say if i want to write an  application to running on my phone  connected with a one of the edge server  here  i don't have a  immediately easy way to do that right so  i think those are something that right  now i view that probably potentially in  the next  um you know five years is going to be  see a  a big improvement here right now  we have seen that  amazon introduce their their own you  know lambda programming uh interface and  then you know some open source info open  source foundation such as openstack you  know originally was a very success in  the cloud now they're moving to  supporting edge and then a linux  foundation rfh also comes with this  adriano kind of uh programming framework  is trying to define several scenarios  how to connect it and and you know  connect it from the device to the edge  and then to the cloud so i think once we  see  um some of this you know programming  interface is clearly defined and then  once we see those uh infrastructure has  been deployed there this is exactly what  we plan to do in the in this year 2022  because we have infrastructures already  put there now we we want to choose one  of the programming infrastructure you  know programming framework there so that  we can allow the students to write in a  code to this so i think oh sorry about  that that's what's the worst thing you  know  yeah normally you don't have this phone  call at home  sorry about that um yeah once once this  barrier has been uh you know we can pass  and then you will see a blue sum of the  applications going to be deployed here i  think that will really enter the you  know edge computing era okay that makes  sense the um so in terms of the  industries that you think should be most  interested in edge computing right now i  mean it sounds like  maybe five or ten years in the future or  at some point  there won't really be a choice for a lot  of of organizations it's just just like  with the cloud you're sort of using it  without thinking about it but are there  are there industries that you think  where organizations in those industries  should actually be paying close  attention to  edge specifically and maybe even sort of  investing or preparing for it again in a  specific way not just in the sense of  knowing that it'll arrive eventually and  become more common  sure you know  uh you know i come from academia so i  maybe and also  uh that uh have a little bit of bios on  this you know just  a warning at the beginning because my  own research is on connected autonomous  vehicle the reason i started working on  this uh back to three years ago because  i view this is an area you know that  they will have to rely on edge computing  you know moving to the next level  you know the the reason is that  traditionally the vehicle  even the business model if we're  thinking bigger you know in the last  century because i come from detroit you  know ford gm is out here if you think  about in last century vehicle itself is  only like a transportation tour you know  what the business model is you and i go  to a dealership buy a vehicle that's it  period  that's how the way they make money right  but now if you're looking for in the  next century the whole the things here  is changing significantly because now  how do you gonna make a money uh you  know the business model behind the  future vehicles as you can see tesla  recently increased their price for the  autopilot from ten thousand to twelve  thousand one that is a very good  indication here is in the future  you all the oems you're getting a  vehicle all right let's say you get a  vehicle at a price x that's just the  beginning of the story so that  continuously they will give you more and  more software-based options and then  that you know the vehicle is all  connected and then that they will  continuously need to update their  vehicle up to your software and you know  either add the new function you you want  to specifically want to you know add or  maybe they need to update the current  soft the software already uh you know  for some certain functions you already  purchased so in these cases the whole  business model is completely  changed and that's also putting a huge  challenge for the infrastructure  you know auto industry  before they don't really care about the  infrastructure you know there was the  only one is the dealership i guess and  now there's very different all the oems  have really think about how can i  maintain this connect connect  connections services with all of the  vehicles that i own for example toyota  alone you know they might have like  definitely close to several uh i don't  know exactly the number but they  probably have like a 50 million vehicles  right actively running on the road right  now so they they need to maintain this  type of  you know connectivities and etc so for  them edge is a very important you know  infrastructure i'm so glad that actually  toyota took the leader created this  so-called aecc uh stands  for uh  automotive edge computing consortium so  right now is a big uh you know  consortium is pushing that the how edge  computing gonna help with the automotive  industry to moving forward you know all  this is a major hardware telecom  companies are part of that uh consortium  so this is one of them i see that  they definitely need to pay attention to  that  what do you think is the main advantage  uh  or the main benefit that edge can  provide uh specifically to automobiles  right so  the advantages does actually apply to  almost the uh all the benefits that the  azure could have potentially brought the  reason is one  uh let's pick an example if you if oem  want to maintain you know  keeping connect collecting data from  this uh  10 million vehicles and then the one  thing is who gonna pay for this data  collection  it's a cost cost for them is very big if  you imagine that  you know you and i have a vehicle and we  need to pay 50 bucks a month just for  them to collecting data no we were not  going to do that right so they have to  pay it but this for them is extra cost  so the first thing is what the edge can  do is as you can significantly reduce  those traffic you know the  the bandwidth requirement because you  can you can do some process on board on  a vehicle and that's sending any very  limited kind of data you know to to the  uh you know sending back to them and  then they probably can cut these costs  too dramatically that's that's the first  one now second is about the scalability  issue because  if you  imagine that each vehicle every five  seconds you collect all the engine  entire and and many other you know  sensor data that itself  is a huge amount of this in terms of  connections so  none of them is able to do that so they  have to rely purely rely on today for  example  with some cloud providers to help them  to store the data you know to stop  collecting the data and storing the  storage and et cetera so those are the  things that they also are going to be  have a huge you know benefit the other  thing is about  so far we are talking about is what i  mentioned earlier today is upstream you  know you're collecting data  so i want to use this as another example  uh it's for the downstream why because  also you know  as i mentioned those softwares because  now we call the software definer  vehicles all the software needed to be  updated so that means you have one  software now you need to update in 10  million vehicles so what are you going  to do so this energy is a very good  example as you can help to  distribute it and that's the  original idea of the content the  delivery network right so it's  you deliver the software in uh  in vehicle side they're called over the  air you know ota or the over-the-air  kind of software update so all these  issues will be they could potentially  benefit from the energy computing  okay that's that's interesting  one thing that you mentioned there was  this idea of  using edge to sort of  lower the amount of bandwidth needed for  you know lots of transfers  and it seems like in part the idea for  edge is that you'll have certain  applications like the ones you mentioned  with cars but other ones maybe for  security or other things where you  handle a lot of the data locally with  edge processors  and then you only send you know a  smaller amount uh out to the cloud or  elsewhere but that it seems like one of  the advantages there is could be  increased privacy right because you  could have an organization that sort of  has better control over the data of  their users and what they're um sending  out what they just destroy after using  it all that stuff just more control  which seems like an increased privacy  i'm curious about  and this is kind of a future sort of  notional issue still but it seems like  it's come up with experts  that there is this potential risk for  security where  if you have these public edge  infrastructure  resources you have them in street lights  or  5g towers or other places places that  unlike a cloud  data center you could maybe get to more  easily you know there's no it's not the  same level of security and all that  stuff that you'd have at like an amazon  uh cloud data center  do you think that it's true that we have  to that this is a genuine sort of  concern going forward that it's it  having sort of  edge infrastructure that's out in the  world and more reliance on edge  could lead to more sort of security  risks like more attacks where a hacker  or whoever could literally just break  into a sort of public box and have  physical access to edge infrastructure  is that a real concern or experts kind  of just reaching  well i'm not a security expert but then  we are trying to understand this um as  we know that the edge uh first i agree  with that you know that uh  there's a potential  that uh  uh particularly the physical  security threat right because now you  hang on your traffic at night you know  people even you they can physically  attack it but we also need to notice  that you know attack on you know one of  the edge devices compared with attacking  a cloud sensor the impact is  completely different imagine you can  even you can destroy the one of the  traffic lights so what's the impact it's  only in this small region right so you  need you know the cost to do this attack  and the impact of this attack is also  dramatically you know reduced  so say if you want you and i want a  physical attack how many how many edge  servers we can attack per day you know  you physically destroy them you are not  going to generate a huge impact here so  i'm not really too worried about that  giving that hey you put in this with  this uh you know traffic at night  so so why we happen to see so many  people attack a trafficking night so  this  other similar things there so i'm not  really worried about that that's too  much in terms of  physical security yeah god okay that's a  great point um so i think the last thing  i wanted to ask you about is about the  the the  future of edge when we look maybe five  years 10 years out are there  sort of major developments or major sort  of changes that you see on the horizon  or is it just a matter of well it'll  become more accessible because there'll  be sort of more infrastructure available  is there anything more dramatic or  anything like a trend or again a  development you think is worth keeping  an eye on  oh yes i i think that uh  you know edge computing since we've seen  is already been here a particular  academic  academia that they already been talking  about this for about five years so i  think now the really a good trend is uh  keep an eye on your own industry you  know sector because as i as i mentioned  earlier auto industry right now seems  like already realized this is very  important so i'm sure that maybe five  years later they will have some really  good progress and then in some sectors  they just realized that for example in  the health domain again they're because  of the privacy issues also the data  cannot moving out of the organization so  health domain also starting look at this  you know i've been interviewed a couple  of times by their health magazines they  also believe that the ads are going to  change them significantly so those are  the factors uh the interesting thing is  i don't expect that all these industry  sectors will be moving at the same path  you know same it you know it's dependent  on the way how uh how the  how they realize it is important for for  their  business right i think for them business  number one and but i do see all two  manufacturers and those and then  particularly smart home you know  in this areas that there is a lot of you  know  uh  ongoing things a smart home you already  see we it's gonna be a penetration to  our life you didn't really realize that  for example many of these  uh the smart home gateways today are  doing edge computing for you now but you  just haven't realized that so yeah  that's great  all right well i guess we know we could  talk all day but we're um  we're out of time now so thanks again we  sangshi for all of your time today and  thanks to everyone who's uh joined us uh  make sure you give us feedback in our  survey and visit us at horizon.mit  for all the latest on emerging  technologies  and we hope to see you at the next event  thanks again thank you for having me  yeah absolutely our pleasure  [Music]  you  so  [Music]  my  let's get into the big topic of open  source something that we actually  cultured  what  let's say has is kubernetes ecosystem  really boom  [Music]  welcome to another episode of in the  cloud i am your host stu miniman and we  could say that this episode is going to  be out of this world yes we are going to  be talking about a special edge  computing project uh on the  international space station something  that ibm and red hat work together on uh  obviously uh many of us in tech are are  space enthusiasts uh so uh i  for one am really excited to be able to  talk about it it's a really a great  example of what ikt can do when when  teams have reliable infrastructure uh in  the software to go along with it uh  there's actually there's a reason why  red hat has a new brand campaign so uh  if you watch television especially if  you watch sports you might see uh this  new advertisement that we are  going to share with you here uh quick  30-second ad and then we will be back uh  with the program so uh please enjoy  this advertise  test commencing  good code should work anywhere even in  outer space  water safety test commencing in five  when we know our apps will work in any  environment my team can focus on our  code  we did an extra round of debugging right  yep the code is clean okay here we go  everybody  houston  water analysis was a success  [Music]  this is what connecting your clouds  feels like  awesome so uh you know as i mentioned uh  before that uh you know space activity  uh when i look at cloud computing what  we've been doing at red hat with  openshift uh we talk about having our  code run anywhere edge computing of  course has been uh a big effort to be  able to  make the footprint fit and talk about uh  you know some of the some of the  challenges you know edge computing we're  going to the far edge when we go to  something like the international station  so i'm really happy to welcome to the  program our guest for today is naim  altif he is an ibm distinguished  engineer and he's also the chief  technology officer for space tech naeem  thank you so much for joining us and uh  gotta say i i love this the space setup  that you've got behind uh next to you  there with you know the rockets and  satellites and the likes uh thanks so  much for joining us  thank you so much for having me all  right so so name uh you're a  distinguished engineer at ibm so you  know distinguished engineers and fellows  you know these are people that have you  know broad experiences and done a lot um  tell us a little bit about your journey  to space tech i mean you you weren't you  know somebody started out you're for  more of the software side so help us  understand a little bit your journey as  to how you got to today  sure so like you mentioned like uh we  all in the you know technology sector  weird space enthusiast many of us are  so i think few years ago i got the  opportunity to work at the seti  institute as a volunteer we were working  on some interesting projects  and from there i got this you know  visibility where as you know most of the  industry today is going software defined  so like you said i don't have a you know  aerospace background i'm a computer  scientist uh so it's like okay you know  programming is everywhere code is  everywhere so it means that we can also  be in this uh in this sector so that's  how i got interested and started  learning about what are the different  segments in this space where vs software  developers can play in  and that's how you know all these ideas  came into picture and we'll discuss more  about these things  yeah so uh you're at ibm of course ibm  deals with companies not only around the  globe but now now you're dealing with  solutions in space so bring us  help set the table for us you know  working with nasa what was nasa looking  to do how did ibm get involved what were  some of the you know the challenges that  that that they pulled  you and the team uh into  sure  so uh  when we started looking at uh you know  what interesting things we can do in in  space so right now the buzz in the  industry as you know from we went from  cloud computing now in in the era of  edge computing and the idea was how can  we extend this edge computing into the  orbit in space and uh the target was  international space station  international space station as you know  is a laboratory for many many  experiments happening there  and we we can judge  we know we have we know the  communications we have on terrestrial  networks if you are at some forward  location you have latency you know  bandwidth challenges if you're in space  those magnify those challenges so we  reached out to space station national  labs and nasa and we like okay what kind  of projects you're running there which  are  heavy on compute  heavy on data  so you can do some match computing for  you  and the project which they mentioned was  one of the projects was dna sequencing  and they mentioned that you know it  creates almost half a terabyte of data  on each one and now that data has to be  you know brought down to the uh to the  ground through through the ground  stations and the way  you know just a bit of background uh  when you are in the low earth orbit  which where the space station is around  i think 250 miles up or kilometers up  you are moving at 17 000 miles per hour  so your touch point with a ground  station any given ground station is only  seven minutes and you're talking about a  radio frequency so you know there are  several bands but you're not talking  about gigabits of transmission  so in space station in the any satellite  in the lower earth orbit if it wants a  much more frequent uplink downlinks it  actually goes up to the geostationary  flap satellites and nasa has a  constellation of them for the data relay  so the data goes from the space station  up in the geo stationary orbit and from  there it comes down to the ground  station  and the reason for that is if you have  dish at home like dish network or drink  whatever you know you have  you probably have noticed once you put  the dish at your home you don't have to  go out and keep on moving the dish  because earth is rotating satellites in  geostationary orbit the satellites are  in mo in exact motion of how the earth  is revolving so that's why you have  these dishes which can point to your  telecommunication satellites so that's  how it works from the space station goes  up and comes down  so while we were uh  working with them so we started how  about if we uh  perform this computation on the space  station so we work with the uh you know  with the nasa try to understand you know  what needs to be done and it's a it's a  bit complex uh  uh you know  uh  problem because uh when you're talking  about dna sequencing there are multiple  steps involved you have base scaling you  have the demultiplexing you have the  alignment and then you have a general  report so we took that code we  containerized that code and we wanted to  get the openshift platform we want to  put the containers in space so at that  time two years ago we had the code ready  container from the uh from red hat  openshift which could fit on the single  node so we took that code  we containerized all the code and we  didn't put this uh it actually went up  last year in april  uh so and since then we have like three  or four runs so basically it's running  in production now and and again the goal  was  minimize the downlink you don't want to  downlink half a terabyte now for example  when the report is generated  it's literally one meg file  so so again so you did all this  computation there and because the  principal investigators who are  requesting  this run  they just want a resulting file they  don't care about all this raw data and  right so and and  and and the genesis of this project was  uh  they're looking for  microbes inside the space station so  basically the astronauts they take a  sample from the surface of the space  station and they run through this device  which generates all the data and we pick  it up from there and generate the final  report  so these has huge implications for the  future for example  you know we are hearing a lot about you  know probably watching the rtms program  going back to the moon the sls is  already on the pad to go up right  uh for the uh  and then you're going to the mars and  and you know for all these deep space  missions  imagine you have these robots and you  have these uh dna sequencer these  devices so these robots just like how  mars does a mars robot does right rover  it goes and it pokes into the ground to  pick up the sample so what if it would  pick the sample and it has a dna  sequencer there and it can run through  the sequencing and  it maybe it can find life somewhere  so that's the whole idea about edge  computing and edge computing in space  yeah no it's it's it's fascinating uh  when you look at this uh it's  one of the patterns we look at for  artificial intelligence often  you might do  your training at a centralized location  like often the cloud is where you need  to kind of build the models but you need  to be able to deploy  where i'm at the data source is so the  example we've used in the industry for a  long time is  you know if you look at cars  if i have a car and it has a lidar i  can't wait to go back to a public cloud  because i will have hit  you know  the thing before i get the data back  space  you know absolutely is is a phenomenal  example of that um as you say just on  the iss there's limited bandwidth but  right if i'm going to the moon if i'm  going to mars if i need to be able to  analyze data or you know maybe look at  certain safety concerns where i would be  testing things that would be so  important to be able to do that locally  um question for you you talk about the  maturity of kubernetes you worked with  our team at red hat um  we spent a lot of time over the last  couple of years  shrinking the footprint and the amount  of resources there so  maybe you could talk to a little bit the  intersection of the requirements that  they had and uh the the maturation of  the technology to reach uh you know the  successful solution  sure no so that was a and it's a great  uh partnership with the redhead and uh  and shout out to the team you know at  the crc and we'll talk about the micro  shift as well so there so the goal was  to reduce the  enterprise uh open shift version to a  single node two years ago which i know  it exists today  so so that was the initial goal  then from there the next step was okay  how can we shrink even further because  when we are talking about edge computing  devices we are talking about devices  like uh  raspberry pi  stuff like that can we have a version of  an open shift which can fit on this  device and we can you know  and also beyond you know besides these  uh versions of open shift  we also have as you know apartment to  run the containers so we are using a  combination of we how it fits where we  use portman what is the lighter weight  of the uh you know the uh open shift so  that's how we know we work together  because the uh the target needs for we  need a  lightweight platform because the  footprint of hardware is not as big as  you can you know expect on the  industrial networks so that drove all  these uh requirements with the team and  uh it was a great experience working  with other team  all right one of the other concerns also  is you know do i have the skill set to  be able to take care of things so how  many red hat and ibm people did we send  to the iss to be able to set up you know  configure and manage this whole  environment  this  one  did we actually have uh have an employee  from uh uh up on the iss or you know  what was it somebody that was trained  no no so we have the uh actually we have  access to the systems from the ground so  we were able to like ssh into the  systems and basically the flight systems  were built here in alabama so we  uploaded everything over there wrapped  up system and once it went up and it  took like couple of months to go live at  that point we were able to go and you  know start the uh the whole sequence  so uh but at this point i mean minimal  right i mean the initially with ibm  engineer redhead engineer i'm making  sure the system is all prepped before it  launches and after that it's just a  matter of running the script  yeah um  yeah that's phenomenal and that that's  the you want it to be able to manage  things you know out of that but you you  don't want to have to have the resource  touch there automation needs to be  involved you know remote management all  that sort of sort of stuff um what else  anything else we should know about you  know the kubernetes solution uh on the  space station  no so like you hit the right point  fully automated that's the key thing  because you you have like  at any given time between three to four  astronauts on the on the space station  and there  every minute is scheduled i mean i have  seen those uh schedules i was in the iss  uh headquarters in dc and i was just  surprised to see up to a minute that  they will do this they will they will  sleep for this minute they will eat now  it's like everything is scheduled for  weeks  so so we had to make sure that the  platform we put in can fully because  like i mentioned there are multiple  steps involved it's a complicated  process so we need a platform which can  take care of which is resilient and you  know in case of any failure it can go  back and start so we shouldn't have to  ask somebody  whether on this space station or on the  ground station at the nasa to go and  trigger this again a fully automated  process  that's awesome um you mentioned uh  microshift so uh for those that aren't  familiar micro shift is currently a open  source project uh to be able to take uh  you know when we had  open shift uh we can do a single node um  of of openshift so kubernetes on a  single server but  there's still you know a minimum  requirement for how much resources are  available microshift actually isn't full  kubernetes there's some things we need  to strip out we need to be able to make  it to fit on a smaller footprint so  where's  where's microshift intersecting with uh  with what you're looking at  on the space programs  so so that's a good good question so  our goal is to  introduce a  containerized platform for the space  industry and as you mentioned right  there there are a lot of uh restraints  here because of the the footprint of the  compute  so actually uh  there's a there's a very uh  i think the only conference for the  small set category which happens every  year in utah and we just submitted which  got accepted so this summer we will be  actually presented presenting microshift  to the developer community of the  satellite and it's a huge community and  we want to introduce them that as you  are building schools universities and  the small set programs take a look at  this platform which we will talk about  what we are doing in future with this  platform on our mission but uh but  that's the idea right i mean uh make it  available to the developers  and let them play with it and see how  they can you know orchestrate from their  different kinds of workloads because  the thing thing about this are  the  the data centers we have on the ground  today  these data centers are planning to move  in the orbit yeah it sounds like little  bit but but that is true they want to  move these data centers in the orbit so  you will have basically a cloud will be  running in the orbit so all the things  which we care about the multi-tenancy  right how can the platforms handle  multiple workloads the uh sas kind of  solutions they all will be going into  the space we just have to be  we can't just go over there physically  and do some things you have to keep that  in mind while we are developing these  systems and solutions  yeah i i had too many years uh you know  watching the data center space the thing  that comes to mind is like well cooling  should be less of an issue in space  because we should be able to  you know vent out heat rather than uh  we're worrying about uh other things  there um yeah but maybe we speak a  little bit about the endurance mission  and the cube satellites and where people  can learn more about that  sure  so uh when we started uh looking into  the uh you know into the space area one  was one thing was to uh  how can we also launch our satellite and  this is purely for research and  educational purposes  and uh so so we started working on that  and we wanted to use the latest and  greatest technology available  among us ibm and red hat and at that  time micro shift was still in very it's  a it was a thought process right so  we've worked with the team over the year  and finally we have open source version  of micro shift actually this will be  launching on june 1st net 30 which means  you know because of the weather it can  get delayed few days but the target is  the june 1st the largest cubesat which  will be uh  which will be running on raspberry pi  and the idea over here is  as i was going through the process of  how can we put a satellite if you do a  look for a checklist  it is huge and for right reasons because  you know it you have to go through so  many steps and approvals and i mean if  you you guys can just google nasa 101  cubesat and you will see this like 200  page book guidelines which tells you you  have to follow all these steps so i was  like okay if if we're living in these uh  uh you know modern part of the world it  is so hard imagine the developing  nations or underdeveloped countries  across the globe it will be almost next  to impossible for them to launch  something or have access so the goal was  okay we will launch this thing and we  will make it available to every school  kid on the planet it will be available  so uh so right now we are starting with  ibm ptek program which is the schools  they work with all over the globe and  i think the uh the link is shared so so  schools can register and we will be we  will give them the apis everything they  can write their code in python and they  will be able to submit the code and the  code will actually go on the uh on the  cubesat execute there  and and come back there are variety of  sensors on this cubesat from  you know from all the positioning  there's like i think 20 different  metrics for positioning vector you have  the uh all the metrics played for the  battery for operational point of view  you have the metrics for the solar  panels you have the the heat information  then you have the  compute over there where we can use  commands like you know like nmon we will  have podman there we will have  microshift there we have ai binaries  there so we can  detect you know because you have a  camera on the cubesat so it can go and  detect stuff so we have variety of  sensors variety of metrics which your  code can play with and do some edge  computing computation  and then once you one last thing  once you uh execute the code and the  results come back we will issue you a  certificate which will be uh created by  basically the stamped by quantum  computer you know the number you get  will be quantum community generated  yeah naima you know i i know i was  excited when uh  you know i could have my name uh you  know on something that was like  going up in a rocket and like nasa send  you a little certificate but what you  described there you know so many home  enthusiasts  you grab a raspberry pi it has centers i  can do that but  you're removing the hardware and it's in  space so how cool is that the kids get  to play with this they get there i i  would hope that this will you know just  increase people's interest in stem uh  you know we we need you know globally to  be looking at uh you know the the the  the opportunities of space so uh that is  that is so awesome uh to see the  activity um you know something we expect  you know a company like ibm to be  involved with so it's great to get back  on that um  you know  you talk about how everything gets all  planned out um you know i i watched  lately it's like up well you know we  have the rough date as to when the iss  will you know be decommissioned and come  down so um what can you tell us about  you know future activities you know  are we planning for future space  stations are there other you know  projects that that you can share that uh  you know ibm and red hat are involved in  like cool  yes so i mean space station has been  amazing right and it's coming end of  life at 20 30. and actually you know  there is a there is a graveyard selected  for where it will go in the ocean it's  the west of australia right that was  really cool i learned recently about  that so so now as space station is you  know uh  coming coming down our end of life there  are news companies who are emerging  because this is what has happened in  last couple of years and uh  and a huge you know uh stuff which uh  the industry which has been changed is  by of course by spacex what they have  done so basically the  the commercial and the private sector is  coming together with the government  sector right they're opening up and you  will see there is a  there is a flight going up tomorrow  right uh ax1  for the private astronauts to go on  space station then there is the crew 4  going end of this month so i mean again  going to the space station  but now since the uh the private sector  has  you know come forward there are  many companies like i think there are  three or four who got the awards to put  these space stations the next nursing  space station  so if you think about the space station  if you think about insight it you know  it has lots of sensors it has lots of  research equipment inside where they're  doing research for microgravity for  you know looking growing plants the food  stuff i mean watching the health of the  astronauts and doing you know a lot of  imagery from on the planet all that kind  of stuff but inside to do all those  things you need you need id  infrastructure you need compute storage  and network and that's what these  companies are you know we are in  discussions with few and talking about  okay what will be the  next generation  platform which is the whole stack right  your uh your infrastructure your  platform your tasks your sas the whole  stack how will that fit in that uh space  station  and then you know the rules will be a  bit different how you interact with  basically the idea is the space stations  will become because they want to  commercialize this stuff so they will  become like labs right so you have  different kinds of experiments you want  to run so you're on the ground and you  know a cloud of your choice and you know  from their cloud you say you know i want  to access something i want to run some  experiment on that given space station  so that also brings in the picture of  this ai sec  devops you can add so many  acronyms right so the whole pipeline  basically so that is a huge uh  discussion okay how can we build these  pipelines where users can build their  code and that's where you know the uh  the cool thing about openshift is you  you write once you can push your code  anywhere and that's our messaging is  that we have a  we have a platform whether you're in  your data center or you are in some  somewhere in the cloud or you are at the  edge somewhere on the test network or  your edge happens to be in space your  underlying platform is the same so me as  a developer while i'm  writing my code when i push my code it  doesn't matter whether it's called in  space edge on the ground because the  platform underlying is the same so so  that's one area the other area is how do  we build these pipelines to push and  then the transmission  there are challenges the network you  know the disruption between  in the orbit how do we make sure that  it's resilient that whatever i pushed to  this far edge  it's the same thing if it did not got  compromised especially when we are  talking about the uh  machine learning models it's very very  sensitive you know because you're  relying on the output of those models so  you want to make sure the model which  you developed is the same model which is  running over there  so yeah these are all the different  things i mean you will think about the  platforms for software development the  your data pipelines how you're going to  push security is huge  area how you're going to secure the  communications  so and this is basically data center is  now going from the terrestrial into the  space yeah and even i i think about you  know public cloud architecture name when  you talked about you know the  geosynchronous orbit you know do you  expect to see a public cloud announcing  a region uh you know  on the geosynchronous satellites because  it's probably still going to be a little  bit while uh before they have an  availability zone on on the moon so you  know with the geosynchronous orbit be  the first place that you know i might  have certain cloud services available  there  most probably in the low earth orbit and  the reason is the companies are talking  about putting a constellation so you  have a ring of let's say you put six or  eight of these  mini data centers  and they will be connected through  optical connections in a ring and then  based on that you can write information  among each other and push down whoever  has the best site to get to the ground  station so i think definitely we will  see these uh edge locations in space  in few years  wow um it was interesting i attended uh  back in 2019 when we still had physical  events amazon held a conference that was  uh you know space was a big piece of it  and i i geeked out i went to a whole  bunch of sessions and i talked to some  friends and i was like oh i wish i was  in the space industry and they're like  stu compared to the i.t industry the  space industry actually moves kind of  slow so i'm wondering if you know you've  got a great viewpoint of this you talk  about all the planning you talk about  the 200 pages you know  today's day you know i i want to be able  to respond fast spin things up do  instant so  where do you see kind of space and i.t  intersecting when it comes to  the path of innovation  i think on the  on the on the uh development of physical  infrastructure it's going to be slow  because you only have one shot  and that's why you know i mean in space  industry one launch failure you're  almost  you're out of luck your company is done  so that part is hard to  move faster but the things which will  move is faster is the software defined  and that's what we are trying to focus  okay you know yes you you guys are  expert in building these spacecraft  satellites we are expert in the you know  on the software side how can we come  together to create these software  defined satellites  again think about it the the challenge  of solving data on the on the ground i  think this is far more advanced than the  type of data you will have in space i  mean think about you know all the social  media the amount of traffic flows  through networks every day you don't  have that much traffic so i think we  have pretty good idea of how we can  build these kind of platforms which are  resilient now the challenge is how we  can fit those in a smaller tighter space  that's the challenge and this and the  industry is moving towards courts course  means you know compute off the shelf so  before it used to be proprietary fpgas  stuff like that but now it's opening up  because there are companies who are  building these uh enclosures which can  protect from radiation radiation is the  biggest thing  to protect your computer from so so they  will be like radiation hardened  enclosures where a compute can your  regular compute can sit in and it can do  stuff  right so uh so these are some challenges  but i think that the aspect of software  defined for the space industry that will  speed up things  yeah um  i i'm curious you you live down down in  the austin area and we've seen you know  texas just seems to be the hotbed uh for  you know space uh you know these days um  yet in the last two years a lot of the  stuff has gone remote so  you know speak a little bit if you could  too you know what's happening in texas  versus you know the global participation  uh in all of the space activity like  your team is it you know is it a  globally uh dispersed group or you know  or is is there a core team you know  in one of the ibm locations  so these days you know everybody is in  remote locations because of you know the  pandemic or last two three years so the  core is team is in austin texas but we  have participants from almost all over  the world because you know we we have a  thing called in ibm uh the academy of  technology where you go and create  initiatives and people worldwide join  try to help you to build those different  kinds of solutions so it's a family and  we also had participants from the red  hat of course so we all working whoever  can contribute from wherever they are in  their capacity  so that's where uh you know how we are  working now if you talk about you know  what's happening in this region right so  you know we have uh  boca chica right has a very cool name in  in south texas over here we have  starship right and probably guys saw in  last year when they did their test with  starship so i think that is uh  we are very looking forward to that when  actually the starship launch happens  and a huge and a lot of things are  happening  you know they're talking about building  the uh the next generation of spaceport  here in the uh  in the south but that will also bring  modernization of your command and  control data centers and that's where  again our platforms our technology will  come in so there's a lot of room as  software developers  uh with little or no space background  all week all we want is your uh  enthusiasm and your you know  your curiosity about space i think  there's plenty of room well  i guess that that that's a great setup  for my my final question is you know  where what are the skill sets that that  people are looking for um for those  people that work in tech that are  enthused you know  what are people hiring for what what are  the areas where uh you know that there's  real need to be able to uh you know i  might not be the person getting on the  rocket um but i can help uh you know  build the technologies around uh what's  happening there  sure  again let's i mean just just to look at  the uh the recent examples right so look  at how the  the booster comes down that's pure  programming how it's coming down you  know  that that's just you know amazing to  watch it's sometimes it looks like  you're watching a movie and then you saw  when the uh the uh dragon which took the  first  uh crew to the space station you saw the  consoles right the linux space or  windows based consoles inside so yeah as  long as you know hardcore programmers  there is plenty of opportunities and  then if you have you know again if you  want to be in this aerospace or space  industry you need to understand a bit  more about how the dynamics works here  in the space you need to have some ideas  about  and but if you are a good programmer i  don't see you know how you can fit here  because they need lots and lots of  programmers  awesome well name thank you so much for  sharing uh we we definitely uh we we've  got a chat group here at red hat that  you know watches all the space flights  and uh really is interested and you know  a great collaboration between ibm and  red hat to make these solutions work so  thank you so much for joining us  thank you so much my pleasure  all right so uh hope you enjoyed this  special uh session uh about space you  know definitely a topic uh i i'd love to  hear uh hear plenty about uh just  programming note uh we will not have the  program two weeks from now we will be  back uh in four weeks we've got a lot  coming out uh if if you look at red hat  red hat summit uh is happening there in  may so look for lots of announcements um  you can join the event it is a hybrid  event and registration is open so uh  thank you so much for joining us uh for  your journey in the clouds i'm stu  miniman we'll see you back in four weeks  [Music]  [Music]  [Music]  [Music]  you  
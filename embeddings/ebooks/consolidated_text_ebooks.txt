Machine Learning for 
Edge ComputingEdge AI in Future Computing
Series Editors 
Arun Kumar Sangaiah
SCOPE, VIT University, Tamil Nadu
Mamta Mittal
G. B. Pant Government Engineering College, Okhla, New Delhi
Soft Computing Techniques in Engineering, Health,  
Mathematical and Social Sciences
Pradip Debnath and S. A. Mohiuddine
Machine Learning for Edge Computing: Frameworks,  
Patterns and Best Practices
Amitoj Singh, Vinay Kukreja, and Taghi Javdani Gandomani
Internet of Things: Frameworks for Enabling and Emerging Technologies
Bharat Bhushan, Sudhir Kumar Sharma, Bhuvan Unhelkar,  
Muhammad Fazal Ijaz, and Lamia Karim
For more information about this series, please visit: https://www.routledge.com/
Edge-AI-in-Future-Computing/book-series/EAIFCMachine Learning for 
Edge Computing
Frameworks, Patterns  
and Best Practices
Edited by
Amitoj Singh, Vinay Kukreja,  
and Taghi Javdani GandomaniFirst edition published 2023
by CRC Press6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC© 2023 selection and editorial matter, Amitoj Singh, Vinay Kukreja, Taghi Javdani Gandomani;  
individual chapters, the contributorsReasonable efforts have been made to publish reliable data and information, but the author and pub -
lisher cannot assume responsibility for the validity of all materials or the consequences of their use. 
The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or here-after invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com 
or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923,  
978-750-8400. For works that are not available on CCC please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are 
used only for identification and explanation without intent to infringe.
ISBN: 978-0-367-69432-6 (hbk)
ISBN: 978-0-367-69833-1 (pbk)ISBN: 978-1-003-14346-8 (ebk)
DOI: 10.1201/9781003143468
Typeset in Times
by KnowledgeWorks Global Ltd.v
Contents
Editors  ...................................................................................................................... vii
List of Contributors  ................................................................................................... ix
Chapter 1  Fog Computing and Its Security Challenges  ........................................ 1
Kamali Gupta, Deepali Gupta, Vinay Kukreja, and Vipul Kaushik
Chapter 2 An Elucidation for Machine Learning Algorithms  
Used in Healthcare  ............................................................................. 25
Veerpal Kaur and Rajpal Kaur
Chapter 3 Tea Vending Machine from Extracts of Natural Tea Leaves and Other Ingredients: IoT and Artificial Intelligence Enabled ................ 37
Neha Sharma, Ram Kumar Ketti Ramachandran, Huma Naz, and Rishabh Sharma
Chapter 
4 Recent Trends in OCR Systems: A Review  ....................................... 53
Aditi Moudgil, Saravjeet Singh, and Vinay Gautam
Chapter 5 A Novel Approach for Data Security Using DNA Cryptography with Artificial Bee Colony Algorithm in Cloud Computing .............. 69
Manisha Rani, Madhavi Popli, and Gagandeep
Chapter 
6 Various Techniques for the Consensus Mechanism in Blockchain  ....83
Shivani Wadhwa and Gagandeep
Chapter 7 IoT-Inspired Smart Healthcare Service for Diagnosing Remote Patients with Diabetes
 ........................................................................ 97
Huma Naz, Rishabh Sharma, Neha Sharma, and Sachin Ahuja
Chapter 8 Segmentation of Deep Learning Models  ......................................... 115
Prabhjot Kaur and Anand Muni Mishra
Chapter 9 Alzheimer’s Disease Classification  .................................................. 127
Monika Sethi, Sachin Ahuja, and Vinay Kukrejavi Contents
Chapter 10  Deep Learning Applications on Edge Computing  ........................... 143
Naresh Kumar Trivedi, Abhineet Anand,  
Umesh Kumar Lilhore, and Kalpna Guleria
Chapter 11 Designing an Efficient Network-Based Intrusion Detection System Using an Artificial Bee Colony and ADASYN Oversampling Approach
 .................................................................. 169
Manisha Rani, Gunreet Kaur, and Gagandeep
Index  ...................................................................................................................... 187vii
Editors
Dr. Amitoj Singh  is an Associate Professor in the School of Sciences and Emerging 
Technologies at Jagat Guru Nanak Dev Punjab State Open University, Punjab, India. 
He received his Master of Computer Application (MCA) with Distinction from 
Punjabi University, Patiala and PhD in Computer Science from Punjabi University, 
Patiala. He has more than 15 years of teaching/research experience; has published 
more than 30 national/international papers in reputed journals, more than 6 books, 
and has presented more than 30 papers at national/international conferences. He 
has supervised 2 PhD students and 15 master’s students. He has filed more than 7 
Indian patents. His areas of interest include machine learning, software development 
methodologies, and assistive technologies. He has been granted a Major Research 
project by IEEE SIGHT to develop language resources for auditory impaired per -
sons. Braille slate developed by him for blind children has won many national and 
international hackathons and has presented the slate at United Nations headquarters, 
New York in 2016.
Vinay Kukreja  earned his Master’s Degree in Computer Science from Punjabi 
University, Patiala and PhD in Computer Science & Engineering from Chitkara 
University, Punjab, India. He has been teaching for more than 14 years. He is pres -
ently working as an Associate Professor at Chitkara Institute of Engineering and 
Technology, Chitkara University, Punjab, India. Presently, he is guiding PhD schol -
ars and master’s of engineering (ME) scholars. He has also filed patents. He won first 
prize in the SIH Hackathon (2018) under the flagship of the Ministry of Housing & 
Urban Affairs, India. His areas of research interest mainly include machine learning, 
deep learning, agile software development, Natural Language Processing (NLP), 
data analysis, and structural equation modeling.
Taghi Javdani Gandomani  (IEEE senior member) received a PhD in Software 
Engineering from Universiti Putra Malaysia (UPM), Malaysia, in 2014. He cur -
rently serves as an Assistant Professor at Shahrekord University, Shahrekord, Iran. 
He has about 20 years of work experience in industry and academic institutions. His 
research interests include software methodologies, software processes, and software 
improvement.Taylor & Francis Taylor & Francis Group 
http://taylorandfrancis.com ix
List of Contributors
Sachin Ahuja
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Abhineet Anand
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Gagandeep
Department of Computer Science
Punjabi University
Punjab, India
Vinay Gautam
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Kalpna Guleria
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Deepali Gupta
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, IndiaKamali Gupta
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Gunreet Kaur
Department of Computer Science and 
Engineering
Thapar Institute of Engineering and 
Technology
Punjab, India
Prabhjot Kaur
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Rajpal Kaur
Department of Commerce and  
Education
Maharaja Ganga Singh University 
Rajasthan, India
Veerpal Kaur
School of Computer Science and 
Engineering
Lovely Professional University
Punjab, India
Vipul Kaushik
ADDVAL Pvt. Ltd.
Punjab, Indiax List of Contributors
Vinay Kukreja
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Umesh Kumar Lilhore
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Anand Muni Mishra
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Aditi Moudgil
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Huma Naz
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Madhavi Popli
Department of Computer Science
Punjabi University
Punjab, India
Ram Kumar Ketti Ramachandran
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, IndiaManisha Rani
Research Scholar
Department of Computer Science
Punjabi University
Punjab, India
Monika Sethi
Chitkara University Institute of 
Engineering & Technology
Chitkara University 
Punjab, India
Neha Sharma
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Rishabh Sharma
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Saravjeet Singh
Chitkara University Institute of 
Engineering and Technology
Chitkara University
Punjab, India
Naresh Kumar Trivedi
Chitkara University Institute of 
Engineering & Technology
Chitkara University
Punjab, India
Shivani Wadhwa
Department of Computer Science
Punjabi University
Punjab, India1 DOI: 10.1201/9781003143468-1
Fog Computing and Its 
Security Challenges
Kamali Gupta, Deepali Gupta, and Vinay Kukreja 
Chitkara University Institute of Engineering &  
Technology, Chitkara University  Punjab, India
Vipul Kaushik
ADDVAL Pvt. Ltd  Punjab, India1
CONTENTS
1.1 Introduction to Fog Computing  ........................................................................ 2
1.1.1  Introduction  .......................................................................................... 2
1.1.2  Architecture  .......................................................................................... 2
1.1.3  Characteristics  ...................................................................................... 2
1.1.4  Applications  .......................................................................................... 2
1.1.5  Advantages of Fog Computing  ............................................................. 4
1.1.6  Workings of the Fog Model  .................................................................. 5
1.2 Security Challenges in the Fog Model  ............................................................. 5
1.2.1  Trust and Authentication  ...................................................................... 6
1.2.1.1  Trust  ....................................................................................... 6
1.2.1.2  Authentication  ........................................................................ 8
1.2.2  Privacy or Confidentiality Preservation................................................91.2.2.1
 Data Privacy Preservation  .................................................... 12
1.2.2.2  Usage Privacy Preservation  ................................................. 12
1.2.2.3  Location Privacy Preservation  ............................................. 13
1.2.3  Network Security  ................................................................................ 13
1.2.3.1  Secure Data Storage  ............................................................. 15
1.2.3.2  Secure Data Computation  .................................................... 15
1.2.3.3  Secure Data Communication  ............................................... 15
1.2.4  Others .................................................................................................. 16
1.2.4.1  Other Security Solutions  ...................................................... 17
1.3 Areas Requiring Security Attention in Fog  .................................................... 19
1.4 Conclusion  ...................................................................................................... 19
1.4.1  Current Observations and Future Security Prospects in Fog  ............. 21
References  ................................................................................................................ 222 Machine Learning for Edge Computing
1.1 INTRODUCTION TO FOG COMPUTING
1.1.1 Introduct Ion
The word fog originates from the term “clouds at the edge,” [ 1] which is symbolized 
as services offered to users near to the ground. Fog [ 2] is predominately envisaged to 
services of cloud through a unified Application Programming Interface.
The paradigm of fog [ 3] is that it is a distributed and vulnerable environment that 
is well equipped with provisions of storage facilities, computational mechanisms, 
and networking methodologies and facilitators.
1.1.2 ArchItecture
The architecture of fog is hierarchical and bi-directional in its service offerings. 
Figure 1.1  presents an architecture of the fog model that briefly discusses that data 
from devices and sensors gets generated in large quantities and is further routed to the fog node, where it is reduced to eradicate unnecessary data. The nodes then send this data to cloud for the purpose of decision-making. Therefore, the fog layer acts as a filter that removes unwanted patterns specific to the application/request in hand. Thus, the three-layer architecture is a robust process-management system that controls and manages data for its efficacious use [ 4, 5]. Figure 1.2  illustrates the 
composition/components of these three layers, wherein the sensors are deployed at edge of the network. Cloudlets are at the Fog layer and Cloud is at the top-most level.
The next section embarks on the characteristics of fog model.
1.1.3 c hArActer IstIcs
Fog as an emerging paradigm is able to extend a number of advantages in com -
parison to cloud, which is a decentralized network. The parametric representation of characteristics of cloud and fog presented in Table 1.1  is a clear depiction of its 
popularity in a number of real-life applications that are discussed subsequently.
1.1.4 ApplIcAtIons
Although the cloud model has captured the market in its entirety, it still suffers from 
increased latency due to massive traffic. This has led to fog playing a major role in 
FIGURE 1.1  Wide variety of things with large velocity of massive data generation.3 Fog Computing and Its Security Challenges
catering to the requirements of fog applications in the real world  [7], such as smart 
cities, smart grids, smart homes, healthcare, oil and gas, connected vehicles, agricul -
ture, energy management, retail transportation, wireless sensor networks, software 
defined networks, Internet of Things (IoT) and cyber-crime physical systems, mobile computing systems, and many more.
This chapter has been organized considering the fog as an emerging technology 
and the benefits it proffers in the paradigm of computing. Efforts have been made to expound on the existing fog offerings pertaining to its definition, characteristics, advantages, workings, security challenges, and security solutions. A summary based on a huge, dedicated survey has been drawn as a conclusion, and observations and future directions are further suggested. The glimpse of the organizational structure 
of this chapter is illustrated in Figure 1.3 .
FIGURE 1.2  Fog computing: A depiction of three layers.
TABLE 1.1
Characteristic Parameters of Fog Computing [6]
S.No. Parameter/Characteristic Cloud Computing Fog Computing
1. Latency High Low
2. Delay jitter High Very low
3. Location of service Within the internet At the edge of local 
network
4. Distance between client and  
the serverMultiple hops One hop
5. Security Undefined Can be defined
6. Attack on data en route High probability Very low probability
7. Geo-distribution Centralized Distributed
8. Support for mobility Limited Supported
9. Real-time interactions Supported Supported4 Machine Learning for Edge Computing
1.1.5 AdvAntAges of fog comput Ing
The advantages of the fog model can be understood by analyzing its applications such 
as smart architectures, which include buildings, grids, Software-Defined Networking 
(SDN), and cities that completely exploit all its services for better service offerings [ 8].
The prime advantages of the fog model include a better response time with the sys -
tem, reduced/filtered traffic directed to the cloud platform, a reduced usage of bandwidth, mobility support, greater scalability and security, portability, the handling of heterogene -
ity from all dimensions, localized control management of massive data, and so on.
Thus, looking at the advantages offered by the fog paradigm in the world of com -
puting, several technology enablers are shifting to its methodologies. A glimpse of such vendors appears in Figure 1.4  [9].
The next section expounds on the working methodology of fog computing.
FIGURE 1.3  Organization of this chapter.5 Fog Computing and Its Security Challenges
1.1.6 WorkIngs of the fog model
The fog acts as a library for implementing cloud services and is written in Ruby 
language. The steps that embark on its working methodology are presented below:
• At the outset, the cloud center is set up to fix up resource specifications such 
as the internet, storage, computing, networking, and so on. The cloud server is switched on in the cloud environment.
• Subsequently, the fog layer is configured and resides at the edge by switch -
ing on the fog server using the fog file as it has been stated in working methodologies.
• The user submits his request compositions for service acquisition.
• The compute model runs with respect to the technology enabler for which the request has been generated using technology employed in the back end of the fog model.
The above steps outline the general workings of the fog model and are acceptable to modification as per application-specific requirements. Though the fog model caters to eradicate the effects of latency in the network, it still gets exposed to security threats. Therefore, challenges related to fog security are discussed in the next sec -
tion, along with their anticipated solutions.
1.2 SECURITY CHALLENGES IN THE FOG MODEL
The data that is being routed to different machines while traversing from fog to cloud faces huge security challenges. It is imperative that these security challenges are han -
dled and security solutions be customized as per the threats encountered. Figure 1.5   
illustrates some of the security challenges being faced by the fog model.
Each challenge is described in and its categorization is presented in the following 
sections.
FIGURE 1.4  Technology enablers for fog computing.6 Machine Learning for Edge Computing
1.2.1 t rust And Authent IcAtIon
1.2.1.1  Trust
The networking capabilities [ 10] offered over fog are expected to be reliable and 
secure. This can be ensured if the participating devices maintain a degree of trust 
with one another. Authentication is a secure way to validate the identity of commu -
nicating devices by establishing a relation between the fog network nodes and IoT 
devices. Though the authentication parameter is beneficial in permitting authentic 
users, the network is still susceptible to malfunction due to malicious attacks. A 
possible solution to the security issue can be offered by the trust as it can foster 
relations on the basis of previous interactions. It comprehends solutions by validat -
ing if the fog node offering service to the IoT device is genuine. A fog node is the 
most crucial component as it is responsible for ensuring privacy in the network. 
Further, the IoT devices aiming to communicate with the intended fog nodes con -
firm the security of the data and request flow prior to their communication. The 
research works presented in [ 10–12] address the issues pertaining to trust in net -
work communications.
The issues encountered in the implementation of trust on the fog model can 
be understood more clearly by looking at some questions and their relevant  
answers:
• How do you recognize the attributes and definition of trust with respect to 
the fog environment?
Solution: Feedback of the previous runs.
FIGURE 1.5  Security challenges in fog computing.7 Fog Computing and Its Security Challenges
It is imperative to recognize the attributes and definition of trust with respect to 
the fog model, though a number of solutions have already been comprehended in the 
cloud environment. But since they have been derived to provide viable solutions at 
centralized levels, so to cope up with mobility related issues, techniques need to be 
proposed that lend feasible solutions in the fog model. One such research has been 
carried out in this direction and is available in [ 13]. The focus of this research has 
been directed toward evaluating the main attributes that can comprehend the defini -
tion and mechanism used to measure trust in a fog model. The researcher sets up an 
experimental test bed comprising 18 service providers to measure their trust value in 
terms of their previous service offerings. The research first evaluates the performance 
of service providers in terms of trust before assigning them any request appearing in 
the fog environment. The researcher has presented a hybrid mode that can cater to the 
issue of measuring trust to evaluate the performance of cloud providers.
The model implementation has been realized by first collecting and aggregat -
ing the feedback from the service providers of previous runs. The aggregated value 
is termed as trust value and is oriented toward choosing the best service provider 
among a set of available ones for the submitted user request. The research work 
shows a complete mapping of the request to service fulfillment as a series of steps 
incepted from the setup of the cloud and fog center to the evaluation of service pro -
viders by the evaluator module. The research work contributes to the maintenance of 
service level agreement (SLA) between customers and service providers. Similarly, 
the solution needs to be derived for the networks that are being built arbitrarily. This 
issue is discussed in the question below:
• How do you ensure trust in services offered by vendors capable of building 
fog networks capriciously?
Solution: Reputation based trust model.
Besides, as communicated in [ 14], the fog model and the cloud model use differ -
ent approaches in the ownership of data centers. The fog model offers flexibility in 
deployment choices to vendors capable of offering a particular service type while 
the cloud model is implemented as a usual mechanism a data center is owned by 
a service provider. The fog model categorizes the vendors building their own fog 
networks as follows:
• Internet Service Providers:  They are also termed as wireless carriers as 
they have been actively involved in controlling cellular base stations or 
home gateways and can build their own networks with the existing infra -
structural resources.
• Cloud Service Providers:  They comprise a group of people concerned 
with expanding their IT services to the edge of networks by establishing 
their own fog networks.
• End Users:  This constitutes a group of people who want to lease their spare 
resources by converting their local private networks into typical fog models 
in order to decrease the cost of ownership.8 Machine Learning for Edge Computing
The flexibility of making fog networks capriciously complicates the implemen -
tation of trust in fog. In order to inculcate trust level into communications that are 
peer-to-peer (P2P) centric, involve e-commerce, and are based on user reviews and meant for social networking, robust reputation systems are efficacious. In this direc-tion, a robust study has been carried out in [15], wherein the researcher has proposed a robust reputation system for implementing the task of resource selection in P2P networks. A distributed polling algorithm has been developed that measures the reli -
ability of a resource prior to its usage on internet. Efforts have been put forth to address issues related to distinct and unique identity, differentiation, and the handling of intended and accidental access and redemption and castigation of reputation. The author has tried to implement a self-regulating P2P system that can isolate any node depicting any illegal behavior in the system.
1.2.1.2
 Authentication
The authentication process is the first step of security implementation in a fog envi -
ronment. It keeps check on the entities entering the cloud/fog model to extract or 
offer services in the form of users and providers, respectively. Besides implementing security in cloud, the authentication process also addresses the issue of resource acquisition and ensures uninterrupted service to the user. The research work pre -
sented in [10] addresses the following two issues more clearly:
1.
 Since massive traffic lands to fog to extract the services limited in quan-tity, it is imperative to restrict the entry of unauthorized nodes aspiring to acquire the resources from entering to the fog.
2.
 The dynamism property of cloud helping the user acquire resources as needed offers flexibility in frequently leaving and joining the environment, which poses interrupted services to the users who are registered in fog. Some measures need to be adopted that can enforce the complexity in the registration and re-authentication process for users logging in frequently within a framed period of time.
Solutions:  Traditional techniques of authentication involving Public Key 
Infra
 structure (PKI) and certificates experience scalability and resource 
constraints. In today’s scenario, with increasing traffic demands to estab -
lish communication among a number of people, achieving a desired level 
of security and privacy raises a major concern as it becomes difficult to assess the intentions of users entering into cloud. Several studies have been conducted in this context and are discussed as follows:
•
 Public Key Infrastructure:  A PKI-based authentication protocol has been 
proposed in [16] that utilizes the technique of multicast authentication for enabling secure communications.
•
 Multicast Authentication Protocol:  In order to cater to security issues 
originating in multicast communications, a study has been explicated in [17]. The research paper elucidates a new multicast authentication scheme 9 Fog Computing and Its Security Challenges
for real-time applications using an advanced encryption standard algorithm 
by assigning a new index number to an entrusted member whenever a block of packet is being sent in a multicast group.
• Intermediary Certifying Authority:  Another solution to put a constraint 
on the service requests arriving from malicious and compromised nodes by restricting unauthorized nodes to become a part of the fog network can be assured by involving an intermediary certifying authority that can authenticate the requesting devices aspiring to obtain services of storage and processes.
• Fine ‑Grained Access Control:  Authenticity can also be achieved by 
employing fine-grained access control mechanisms, as illustrated in [ 18]. 
Cryptographic techniques are employed here to assign an individual access control scheme to each data item owner. These access control policies are kept secret as the attributes specific to control policies defined by the data owners responsible for associating mapping between the attributes and their keys are not revealed to the holders of the credential keys. Similarly, bio-metric authentication has been emerging as a good solution in mobile and cloud computing. Research is being done to implement fingerprint, face, touch-based, or keystroke-based authentications in the area of fog comput -
ing to enhance data privacy.
Some more models explored by research work carried out in [ 10] are expounded in 
Table 1.2 .
1.2.2 p rIvAcy or confIdent IAlIty preserv AtIon
When the services are accessed over cloud by massive traffic, they generate a threat 
to access and leak the information overshadowing the security models implemented in cloud. The data residing and exchanged between fog clients is more prone to unauthorized access as the fog nodes are closer in vicinity to end users compared to remote cloud nodes [ 14].
To implement high levels of security in fog, privacy needs to be addressed at 
various levels. There is a dire need to look into the following questions to ensure  
privacy:
• How can data privacy be ensured in a typical fog model?
• How can a fog service usage pattern be composed for a fog client in order to ensure maximum privacy?
• How can location privacy be ensured at the time a fog node offloads tasks to its nearest neighbor while assuming other nodes to be distant?
The above questions pose needs to fog model implementers to derive feasible solu -
tions that maintain data, usage, and location privacy. A brief layout of the content covered under this section is presented in Figure 1.6 . The techniques are discussed 
in detail later in this chapter.10 Machine Learning for Edge ComputingTABLE 1.2
Existing Authentication Schemes for Fog Computing
System ModelAuthentication and  
Privacy Models Countermeasure Performances ( +) and Limitations (-)
Fog computing with face 
identification and resolution 
application• Confidentiality
• Integrity
• Availability• Authentication and session key  
agreement
• Advanced Encryption Standard (AES)  
symmetric key encryption mechanism based  
on session key
• Secure Hash Algorithms (SHA-I)  
algorithm(+) Response time for different face 
databases
(+) Can detect Man-in-the-Middle (MITM) 
attack and identify forgery
(-) Increased computation and 
communication overhead a bit
Fog computing-enhanced IoT • Privacy of individual IoT  
device data
• Integrity• Chinese reminder theorem
• Homomorphic Paillier encryption
• One-way hash chain(+) Computational cost and communication 
overhead compared to the aggregation
(+) Can resist against the false data injection 
attack
(+) Fault tolerance
(-) Traceability is not considered
Vehicular crowd sensing  
using fog computing• Confidentiality
• Mutual authenticity
• Integrity
• Privacy
• AnonymityCertificate less aggregate (+) Computational cost and communication 
overhead
(+) Key escrow resilience
(+) Provide anonymity compared to the 
scheme
(-) Location privacy is not considered
Fog storage architecture with  
the three system entities,  
including the cloud, fog,  
and end user• Data privacy
• Forward secrecy• Printing-based cryptographic
• Merkle tree
• User-level key management and  
update mechanisms(+) Secure user-level key management
(+) Efficient in terms of computation, 
communication, and storage as compared 
to the scheme
(-) Adversary’s model is limited
(Continued )11 Fog Computing and Its Security Challenges
Vehicular ad  
hoc network (V ANET) using  
fog computing• Identity privacy
• Location privacy
• Authenticity• Location based encryption (LBE) scheme
• Cryptographic puzzle
• SHA-I algorithm(+) Average delay to solve a puzzle(+) Average delay to verify the proofs(+) Defending denial-of-service attacks(-) Anonymity is not considered compared 
to the scheme
(-) Adversary’s model is limited
Location-based fog computing •
 Location privacy • Position based cryptography (e.g., position- based key exchange)
•
 Position-based multi-party computation,  position-based public key infrastructure(+) Without using additional computation 
overhead
(-) Integrity is not considered
Fog data centers consist of four 
renewable fog nodes•
 Differential privacy • Query function
• Laplacian mechanism(+) Efficient in terms of execution 
efficiency, private preserving, quality, data utility, and energy consumption
(+) Resist node and edge recognition attack(-) Adversary’s model is limitedTABLE 1.2  (Continued )
Existing Authentication Schemes for Fog Computing
System ModelAuthentication and  Privacy Models Countermeasure Performances (+) and Limitations (-)12 Machine Learning for Edge Computing
1.2.2.1 Data Privacy Preservation
Though the fog networks have a number of privacy-preserving algorithms residing 
between the cloud and the fog model that have restricted access to resources lying 
at the edge, these algorithms can still collect data sensitive in nature produced by 
sensors and other end devices. The solution to this stated problem could be to adopt 
techniques of homomorphic encryption  and differential privacy.
Homomorphic encryption, a popular encryption technique, allows computations 
to be performed on ciphertexts, creating an illusion that the data is still in its original 
form [ 19, 20]. It lets the user perform complex mathematical operations on encrypted 
data, which subsequently generates an encrypted result that, when further decrypted, 
is similar to the result of operations when performed on plain text. It carries all the 
operations efficaciously without compromising the encryption applied. The term has 
derived its meaning from the Greek word “same structure.” It is typically being used 
in systems such as collision-resistant hash functions, secure voting systems, private-
set interactions, predictive analysis in healthcare, and many more. 
Differential privacy refers to non-revelation of privacy by any single entry belong -
ing to any dataset when performing statistical queries [ 21]. It is a statistical technique 
that optimizes the performance of query evaluation in statistical databases. The 
technique was developed by cryptographers and can be explained by this example: 
Differential policy when applied on two identical databases, with the former contain -
ing some information pertaining to the user not contained in the later one, ensures 
that any statistical query fired on both will produce nearly same result irrespective 
of the database from which the query has been fired.
1.2.2.2  Usage Privacy Preservation
This privacy can reveal information pertaining to user service usage patterns. Although 
usage information is needed to match the SLA requirements, it can be harmful in dis -
closing information sensitive to other user aspects. For example, with a smart grid, the 
readings in a smart meter not only tell the required electrical usage information but 
also can breach user privacy by disclosing information such as the time a user is not at 
home, and so on. In order to take care of such issues, solutions can be provided using 
FIGURE 1.6  Privacy preservation challenges and solutions.13 Fog Computing and Its Security Challenges
a trusted third party that can employ a mechanism to ensure only information relevant 
to the applied application is revealed. This can be achieved by creating dummy tasks 
by the fog client and offloading them to multiple assorted fog nodes, thereby hiding the real clubbed tasks. Though it offers a good solution because each fog node is concerned only with its stated task, it can waste resources and increase the financial burden on the fog client. Therefore, a smart mechanism to partition the applications can be defined to ensure resource offloading without any disclosure of sensitive infor -
mation. The technique is recommended for future research.
1.2.2.3  Location Privacy Preservation
The resources available over the fog/cloud model are limited in number, which raises the provider’s need for context-aware sharing among devices that are geographically closer in order to increase resource utilization. Such sharing can be prone to a Man-in-the-Middle (MITM) attack that can access and disclose massive amount of data sensitive in terms of location, identity pertaining to fog nodes, and so on, as per [ 22, 
23]. The location privacy can be preserved by identity obfuscation . It indicates that 
even though the communicating fog nodes are neighbors, identity verification is still needed in order to maintain and establish their communication. Further, practices such as offloading onto some other fog client on the basis of not only location, but also latency, load balance, and reputation can create a feasible solution, as the node desiring to offload will not have a precise idea about the location of the desired fog node, restricting the communication in intersection of their locations.
Though addressing solutions to these three stated areas can provide some good pri -
vacy implementations, there are other security issues related to privacy as proposed by the researchers in [ 10]. The issues along with various questions to be addressed 
are presented in Figure 1.7 . The colored part in the figure resembles the attributes 
pertaining to privacy challenges in security, and the question to be resolved is stated under the attribute.
1.2.3 n etWork secur Ity
The notion [ 14] of fog computing has realized its implementation with the emergence 
of wireless technology. This has raised major concerns for implementers with wire -
less network security as it may be affected due to attacks such as jamming attacks, sniffer attacks, and so on. The virtuous practice of network management is isolating regular data traffic from network management traffic [ 24]. As the fog nodes reside 
at the edge of the network, it muddles up the network management process due to an increase in the cost of managing huge scale servers distributed in the network in its entirety with reduced access for maintenance. The problem can be eradicated by employing SDN  technique. SDN is a Software-Defined Networking technique 
capable enough to manage an entire network by isolating network control functions from forwarding functions of a network [ 7], that is, SDN is a networking paradigm 
that segregates the task of data control from the data communication layers [ 25].  
The control functions can include the task of routing, application execution and related work, policy definitions, and so on. The decision regarding routing traf -
fic is solely deputed to a virtual network control plane while SDN abstracts the 14 Machine Learning for Edge Computing
physical networking resources. Thus, SDN plays a major role in network manage -
ment, increases scalability, and reduces cost. SDN in association with fog computing 
[26] is providing solutions in implementations of vehicular networks, resolving 
collisions and high-packet loss rates, and intermittent connectivity by augmenting vehicle-to-infrastructure and vehicle-to-vehicle communications.
The traits of network security can be understood and justified while looking at 
and answering the following questions:
• Is the data stored in the fog environment secure?
• Are the computations performed on this stored data secure?
• How is data that is in transit of its flow secured?
Therefore, dealing with the issue of network security necessitates to deal with the underlying issues of secure data storage, secure data computation, and secure data communication. These issues are briefly discussed here.
FIGURE 1.7  Security and privacy issues in fog computing.15 Fog Computing and Its Security Challenges
1.2.3.1 Secure Data Storage
In fog networking, the outsourcing of user data is done as the user’s control over fog 
data gets delegated to fog node. This introduces the issue of ensuring data integrity 
as the outsourced data may get tailored accidentally or may get lost. Additionally, 
the data could be misused by unapproved parties for a third person. To address these 
problems, auditable data storage services  are suggested. The providers of services 
can opt for homomorphic and searchable encryption  in combination to inculcate 
confidentiality, integrity, and verifiability for data residing on untrusted servers. 
Further, a privacy-preserving public auditing mechanism  can be employed for data 
that relies on a Third- Party Auditor (TPA) for using the techniques of a homomor -
phic authenticator or random mask method, as suggested in [ 27].
1.2.3.2 Secure Data Computation
The aim of fog computing [ 7] is to minimize the traffic deviating to cloud for the 
acquisition of services in the form of resources for its computational needs. This has 
posed concerns dealing with node heterogeneity in computation, automatic trouble -
shooting, automatic deployment, service discovery, intelligent configurations, and so 
on. Dealing with such outsourcing of computation has led to the discovery of pos -
sible solutions such as verifiable computing  and data search . Verifiable computing 
aims at instilling user confidence in the computations being delegated to fog nodes 
as the user can verify the correctness of the computations. It allows computations to 
be offloaded to untrusted servers despite maintaining verifiable results. The servers 
other than the offloaded one evaluate the process and revert with a proof that the 
result is correct. In this context, the research work presented in [ 28] complements 
a significant verifiable computing protocol capable of returning a computationally-
sound proof that can be verified by the client with no additional cost. Similarly, the 
work presented in [ 29] provides an absolute demonstration of verifiable computing 
by producing a model, named “Pinocchio,” which allows the client to create a public 
evaluation key that evaluates general computations performed by a server taking into 
consideration cryptographic assumptions. To ensure data privacy, the prerequisite 
for outsourcing sensitive data is an encryption that checks for data utilization and 
deployment services. The champion of all these techniques is the keyword search  
technique that works on patterns in the form of keywords searching encrypted files. 
The research work conducted in [ 30] elucidates schemes that perform searches on 
encoded data for provable secrecy by query isolation, provision for hidden query, 
controlled searching, and so on.
1.2.3.3 Secure Data Communication
Though security implementations [ 10] are required in a network while data is in phase 
of storage and computation, data is prone to exposure while in the communication 
phase. IoT devices are required to have some security mechanisms to provide secu -
rity as data cannot be offloaded to fog devices lying in its vicinity. Communication is 
established in a fog network for the purpose of offloading either a storage or a com -
putation request. The fog nodes interact with each other to discuss issues pertaining 
to resource management or network management. Thus, the interactions in a typical 16 Machine Learning for Edge Computing
fog model are either established between IoT devices and fog nodes or between vari -
ous fog nodes. In order to ensure secure data communication, the communication 
enablers are required to look at these three perspectives:
• How can the communication between fog nodes and IoT devices be secured?
• How can the communication between various fog nodes be secured?
• How can the overhead of beckon and other such type of messages be reduced within resource constrained networks?
An IoT device aspiring for a service is sometimes not aware of the existence of any fog network. This may create an issue of unsecure communication as symmetric cryptographic techniques may not be able to offer good protection. Asymmetric key cryptography and implementing PKI techniques to foster security in the network have their own challenges. Additionally, network users also suffer from the bogus messages circulated in the network by attackers. Solutions to such an issue have been provided in the research work presented in [ 31]. An intermittent and flexible 
security mechanism has been devised that can cater to the problem of unreliable network connections by establishing security configurations particular to application requirements. Further, techniques of homomorphic encryption  can be utilized that 
use pseudonyms to guarantee data secrecy by protecting the identity of devices lying at the edge of the network [ 32]. To maintain the integrity of data, some kind of mask -
ing algorithm or light-weight encryption  technique can be used. Such techniques 
make use of lightweight block ciphers, stream ciphers, hash functions, and similar methods to measure security without compromising limited resources.
A summary of various perspectives of secure data communication, along with 
their solution technique, is presented in Table 1.3 . In addition, there are some other 
challenges that are presented in Section 1.2.4.
1.2.4 o thers
The security challenges that also affect the performance of a fog model are presented 
as follows:
a. Access Control:  The outsourcing of data and services in cloud requires a pre-
check on access control and can be implemented using cryptographic tech -
niques. Methodologies such as fine-grained access control and policy-based 
TABLE 1.3
Perspectives and Techniques for Secure Data Communication
Perspective for Secure Data Communication Technique That Can Foster a Good Solution
Secure data storage Homomorphic encryption, privacy preserving public auditing
Secure data computation Verifiable computing, data search
Secure data communication Homomorphic encryption, light-weight encryption17 Fog Computing and Its Security Challenges
access control can be used for secure operations between heterogeneous fog 
clients and IoT devices. Several approaches can be used to implement better access control in a fog environment and are presented as follows [ 33]:
• Behavior Profiling : It deals with protecting the data from unauthorized 
access. It involves behavior profiling a user via decoying information technology methodologies to compare the behavior of an unauthorized user to an authorized one.
• Attribute Encryption : To ensure the security of data, advanced encryp -
tion techniques such as the advanced encryption standard algorithm, the Rivest Shamir Adleman algorithm, and the ciphertext attribute-based encryption algorithm can be used. Such attribute-based approaches guarantee confidentiality and fine-grained access control.
• Certifying Authority and Policy-based Access Control : For enhanced 
security in fog-based networks, certificate-based revocation and infor -
mation distribution need to be supported by fog nodes. It involves enti -
ties such as a back-end cloud, certificate authority, IoT devices, and fog nodes. Extensible access control mark-up language can be used to for -
malize network policy specifications for supporting secure collabora-tions and interoperability among the resources heterogeneous in nature.
b. Intrusion Detection:  The task of these systems [ 10] is to mitigate attacks 
like Denial of Service (DoS) attacks, scanning attacks, insider attacks, MITM attacks, and so on. These detection methods are applicable to sys -
tems such as cloud, smart grid, supervisory control and data acquisition (SCADA), and so on. Fog computing, as discussed earlier, follows a three-tier architecture. It is imperative to apply these detection techniques to all the three layers of a fog model for analyzing and monitoring traffic and the attributes of end devices, fog nodes, and cloud servers. In such multi -
layered architectures, implementing security in one layer does not verify the propagation of malware from a vulnerable node to others. There is a dire need to deploy perimeter Intrusion Detection Systems (IDSs) to coor -
dinate assorted detection components for improvised security implemen -
tations. These can better assist in dealing with challenges such as alarm parallelization, real-time notifications, correct responses, and false alarm control.
In cloud computing, IDS [ 14] plays a vital role in detecting attacks such as flood-
ing attacks, insider attacks, port scanning, attacks on hypervisors, and so on. In fog computing, IDSs are used to check log files and user login information, control poli -
cies, and detect DoS attacks. Though IDSs are well-implemented in fog and cloud architectures, a lot of efforts still need to be made with mobility, massive traffic, geo-distribution, latency minimization, and so on.
1.2.4.1  Other Security Solutions
Section 1.2.4 discussed in detail possible threats to data when it is in various phases residing in a fog along with some possible solutions. In addition, solutions that can be extended to resolve any challenge along with their benefit are presented in Table 1.4 .18 Machine Learning for Edge Computing
TABLE 1.4
Efficacious Solutions to Security Challenges [34]
Solution Category Security Challenge Resolved Benefit Offered
Data encryption • Malicious insiders
• Data loss
• Spyware/malicious processes
• Insufficient due diligence
• Data breachIt will protect data from 
unauthorized access even though 
the data is breached either at the 
time of computation, rest, or 
transit.
Network monitoring • Advanced persistent threats
• Denial of service attack
• Access control issues
• Abuse and nefarious use of 
resources
• Malicious insiders
• Data breaches
• Insufficient due diligence
• Attack detection• Logging of nasty events for later 
analysis
• Indicate behavioral attributes of 
system with respect to security 
parameters
• Block apprehensive incoming/
outgoing traffic
• Instant notification about ongoing 
attack
Malware protection • Insecure API
• Account hijacking
• Data corruption/damage risks
• Service and application 
vulnerabilities
• Performance degradation
• Shared technology issuesProvisions for scanning and 
exclusion of malicious applications 
in real-time
Wireless security • Access control issues
• Advance persistent threats
• Data breach
• Illegal bandwidth consumption
• Eavesdropping attacksAllows scalability in connection of 
fog devices as per their availability 
in a better secure manner
Secured multi-tenancy • Account hijacking
• Access control issues
• Insecure APIs
• Abuse and nefarious use of 
resources
• Malicious insiders
• Segregation issues
• Data breaches• Efficacious utilization of fog 
resources
• Avoidance of hopping attacks
• Secure data associations between 
authorized users
Backup & recovery • Data unavailability issues
• Data loss
• Insufficient due diligence
• Data integrity issues
• Malware infectionThe availability and integrity of data 
is preserved even at the time of 
disasters
Securing vehicular  
networks• Access control issues
• Advance persistent threats
• Account/session hijacking
• User identity protection
• Denial of service attacksIt preserves the identity of the user 
and location to increase the road 
safety by preserving data in 
communication19 Fog Computing and Its Security Challenges
1.3 AREAS REQUIRING SECURITY ATTENTION IN FOG
In addition to the security challenges discussed previously, the areas that require 
security to be implemented are presented in Figure 1.8 , and illustrated in the survey 
carried out in [ 34, 35].
The entire domain of the computing paradigm depends on robust software and 
the processes adopted for its development [ 36]. Additionally, when developing any 
agile project that is secured in its operation, structural equation modeling is used that can help facilitate a timely completion of projects. So, from the point of view of how submitted requests in the computing environment are better served besides being secured, agility is a major component.
Areas desiring security attention fog are presented in Table 1.5 .
Table 1.5  presents various areas in a typical fog paradigm where implications 
of security necessitate having better mechanisms that can cope with the possible threats. The table also explicates possible solutions in this direction along with the impact of the threat if any lapse occurs in security implementation. Section 1.4 pres -
ents the conclusion on the conducted survey.
1.4 CONCLUSION
This chapter analyzes fog paradigm security flaws along with their existing solu -
tions. It was observed that research practices adopted by various researchers focus on improving fog in terms of its functionality, and a few research works focus on fog security parameter. Consequently, the whole research work expli -
cates various security challenges, possible solutions to them, IoT areas requiring security attention in fog, and some of the generalized solution techniques with their benefits. The research presented in this chapter helps readers understand fog security measures in depth while envisioning the architectures of novel fog models.
FIGURE 1.8  Areas desiring security attention in fog.20 Machine Learning for Edge Computing
TABLE 1.5
Areas Desiring Security in Fog Computing [34]
Fog Security Area Possible Threats Possible Solutions Impact
Virtualization • Hypervisor attacks
• Side channel attacks
• Privilege escalation
• Service abuse
• Inefficient resource  
policies
• Privilege escalation  
attacks
• Virtual machine (VM)-
based attacks• Multi-factor 
Authentication
• IDS
• User data isolation
• Role-based access 
control model
• Attribute or identity-
based encryption
• User-based permissions 
model
• Process isolationEach computation is 
taking place in a 
virtualized 
environment, so any 
lapse in security 
implementation may 
lead to adverse effect 
on every fog service.
Wireless security • Message replay attacks
• Message distortion issues
• Data loss/breach
• Illegal resource 
consumption
• Sniffing attacks
• Active impersonation• Encrypted 
communication
• Authentication
• Secure routing
• Wireless security 
protocols
• Key management  
service
• Private networkAny lapse in security 
implementation can 
compromise 
consistency, 
availability, accuracy, 
trustworthiness, and 
privacy.
Internal/external 
communication• MITM attack
• Poor access control
• Inefficient rules/policies
• Insecure APIs and  
services
• Session/account hijacking
• Single point of failure
• Application vulnerabilities• Encrypted 
communication
• Partial encryption
• Mutual/multi-factor 
authentication
• Isolating compromised 
nodes
• Transport layer security
• Limiting number of 
connectionsAny lapse in its 
implementation could 
lead to eavesdropping 
by an attacker to 
access constrained 
resources.
Web security • Cross-site scripting/  
request forgery
• SQL injection
• Insecure direct object 
references
• Session/account  
hijacking
• Drive-by attacks
• Malicious redirections• Secure code
• Regular software  
updates
• Find and patch 
vulnerabilities
• Periodic auditing
• Intrusion prevention 
system
• Firewall/anti-virus 
protectionAny lapse in 
implementing security 
can enable the attacker 
to get into the system 
and install malicious 
application.
Malware  
protection• Virus
• Worms
• Trojans
• Spyware
• Performance reduction• Anti-malware programs
• IDS
• Rigorous data backups
• System restore points
• Patching vulnerabilitiesAny lapse could result 
in damage of data 
permanently and 
degradation of system 
performance.
(Continued )21 Fog Computing and Its Security Challenges
1.4.1 c urrent  observ AtIons And future  secur Ity prospects  In fog
Based on the study conducted here, this chapter presents the following observations 
and future recommendations in the area of fog security:
1. Data encryption is used to ensure data confidentiality, so its potential mech -
anisms can be used at different stages for securing data. For example, the 
Advanced Encryption Standard (AES) algorithm can be applied for data 
at rest, and the Secure Socket Layer (SSL) protocol can be used for data in 
transit. Integrity checks are to be mandated prior to and after the communi -
cations. Similarly, it is important to distinguish between the sensitive data 
and archival data such as public streaming videos. When encryption tech -
niques are applied to such archived data, it may hamper the overall system 
performance due to constrained resources. So, strategies could be made in 
this direction in future work.
2. The frequently used data being stored in cache is prone to cache attacks, 
such as exposing cryptographic keys that might leak sensitive information. 
Therefore, strategies need to be devised involving hardware and software 
modifications that can prevent cache interference attacks.
3. A network is formed by connection of small devices. The data generated by 
one device may be small, but in a network with a number of communicat -
ing devices, the data becomes massive in quantity, creating difficulties in 
detecting anomalous activities as filtering each data packet can instigate 
more resource consumption. Anti-viruses, IDS, and firewalls can be used 
for efficacious network monitoring. In future research work, communica -
tions taking place at multiple levels can be monitored using rule matching 
patterns in artificial neural networks. Virtual private networks can also be 
established to isolate networks from external attacks.
4. The existing malware attacks, such as spyware, trojans, viruses, and worms 
may spread unwanted infections on sensitive data in the network. This 
highlights the need to implement an efficient cross-storage, light-weight 
detection service that can defend against these threats without compromis -
ing system performance.Data security • Data altering and  
erasing attacks
• Data replication and  
sharing
• Data ownership issues
• Illegal data access
• Low attack tolerance
• Multi-tenancy issues
• DoS attacks
• Malicious insiders• Network monitoring
• Security inside design 
architecture
• Policy enforcement
• Secure key management
• Data masking
• Encryption
• Obfuscation
• Data classificationThe user and the fog 
systems data may get 
compromised due to 
illegal file and 
database access if 
security parameters 
are not considered.TABLE 1.5 (Continued )
Areas Desiring Security in Fog Computing [34] 
Fog Security Area Possible Threats Possible Solutions Impact22 Machine Learning for Edge Computing
5.The fog paradigm has been widely implemented using wireless sensors and
IoT devices. Techniques need to be developed that can provide solutionsto packet sniffing and similar challenges. The mobility in these devicesfurther complicates the implementation of security in communication asattackers get unprecedented freedom to intercept sensitive data. Therefore,more advanced Wi-Fi security algorithms, such as Wi-Fi protected access,wireless protocols such as 802.11a and 802.11g, and IDS for guardingin 5G heterogeneous mobile networks should be implemented for futurecorrespondence.
Taking the threats posted in this chapter into consideration, along with directions for future contributions, a systematic system or model must be derived to provide secured storage, and computation and communication of data within constraints of resources. Such a model is envisaged to protect networks from potential damage and can avoid the occurrence of proactive vulnerabilities.
Table 1.6  presents the abbreviations used in this chapter.
REFERENCES
1.What is Fog Computing? - Definition from IoTAgenda. IoT Agenda. (2021). Retrieved 
7 A
pril 2021, from https://internetofthingsagenda.techtarget.com/definition/fog-computing-  
fogging .
2.Fog: A Powerful “Cloud Services” Gem. Rubyinside.com. (2021). Retrieved 7 April
2021, from http://www.rubyinside.com/fog-a-powerful-cloud-services-gem-3375.
ht
ml.
3.Ni, L., Zhang, J., Jiang, C., Yan, C., & Yu, K. (2017). Resource Allocation Strategy in
Fog Computing Based on Priced Timed Petri Nets. IEEE Internet of Things Journal,
4(5), 1216–1228. https://doi.org/10.1109/jiot.2017.2709814 .
4.Salamone, S. (2021). Why Edge Computing Can Help IoT Reach Full Potential -
RTInsights. RTInsights. Retrieved 3 February 2021, from https://www.rtinsights.com/
why-edge-computing-can-help-iot-reach-full-potential /.TABLE 1.6
Abbreviations
Abbreviation Description
AES Advanced encryption standard
CPU Central processing unit
DoS Denial of service
HTTP Hyper Text Transfer Protocol
IDS Intrusion detection system
IoT Internet of Things
P2P Peer-to-peer
PKI Public key infrastructure
SDN Software defined network
SSL Secure socket layer
TPA Third party auditor23 Fog Computing and Its Security Challenges
 5. B ittencourt, L., Diaz-Montes, J., Buyya, R., Rana, O., & Parashar, M. (2017). Mobility-
Aware Application Scheduling in Fog Computing. IEEE Cloud Computing, 4(2),  
26–35. https://doi.org/10.1109/mcc.2017.27 .
 6.  B akker, R., Oppenheimer, P., Bakker, R., Story, B., Spade, J., & Bakker, R. et al. (2021). 
Perspectives - Page 27 of 47 - Cisco Blogs. Cisco Blogs. Retrieved 10 January 2021, 
from https://blogs.cisco.com/perspectives/page/27 .
 7. W asim Akram, S., Rajesh, P., & Shama, S. (2018). A Review Report on Challenges 
and Opportunities of Edge, Fog and Cloud Computing by Employing IoT Technology. International Journal of Engineering & Technology, 7(3.29), 263. https://doi.org/10.  
14419/ijet.v7i3.29.18808 .
 8. F og - The Ruby Cloud Services Library. Fog.io. (2020). Retrieved 10 September 2020, 
from http://fog.io /.
 9.  M ukherjee, M., Shu, L., & Wang, D. (2018). Survey of Fog Computing: Fundamental, 
Network Applications, and Research Challenges. IEEE Communications Surveys & Tutorials, 20(3), 1826–1857. https://doi.org/10.1109/comst.2018.2814571 .
 10. M ukherjee, M., Matam, R., Shu, L., Maglaras, L., Ferrag, M., Choudhury, N., & 
Kumar, V. (2017). Security and Privacy in Fog Computing: Challenges. IEEE Access, 5, 19293–19304. https://doi.org/10.1109/access.2017.2749422 .
 11.  K o, R., Jagadpramana, P., Mowbray, M., Pearson, S., Kirchberg, M., Liang, Q., & Lee, 
B. (2011). TrustCloud: A Framework for Accountability and Trust in Cloud Computing. 2011 IEEE World Congress on Services. https://doi.org/10.1109/services.2011.91.
 12. K han, K., & Malluhi, Q. (2010). Establishing Trust in Cloud Computing. IT Professional, 
12(5), 20–27. https://doi.org/10.1109/mitp.2010.128 .
 13. G upta, K. (2019). Shodhganga@INFLIBNET: Browsing Shodhganga. Shodhganga.inflib -
net.ac.in. Retrieved 13 October 2020, from https://shodhganga.inflibnet.ac.in/browse .
 14. Y i, S., Qin, Z., & Li, Q. (2015). Security and Privacy Issues of Fog Computing: A 
Survey. Wireless Algorithms, Systems, and Applications, 685–695. https://doi.org/  
10.1007/978-3-319-21837-3_67 .
 15. D amiani, E., di Vimercati, D., Paraboschi, S., Samarati, P., & Violante, F. (2002). 
A Reputation-Based Approach for Choosing Reliable Resources in Peer-to-Peer Networks. Proceedings of the 9th ACM Conference on Computer and Communications Security - CCS ‘02. https://doi.org/10.1145/586110.586138 .
 16. L aw, Y. W., Palaniswami, M., Kounga, G., & Lo, A. (2013). WAKE: Key Management 
Scheme for Wide-Area Measurement Systems in Smart Grid. IEEE Communications Magazine, 51(1), 34–41. https://doi.org/10.1109/mcom.2013.6400436 .
 17. A bouhogail, R. (2011). New Multicast Authentication Protocol for Entrusted Members 
Using Advanced Encryption Standard. The Egyptian Journal of Remote Sensing and Space Science, 14(2), 121–128. https://doi.org/10.1016/j.ejrs.2011.11.003 .
 18. Y e, X., & Khoussainov, B. (2013). Fine-Grained Access Control for Cloud Computing. 
International Journal of Grid and Utility Computing, 4(2/3), 160. https://doi.org/10.  
1504/ijguc.2013.056252 .
 19. H omomorphic Encryption - Wikipedia. En.wikipedia.org . (2020). Retrieved 10 April 
2020, from https://en.wikipedia.org/wiki/Homomorphic_encryption . Last edited on  
23 May 2022.
 20. W hat Is Homomorphic Encryption? - Definition from WhatIs.com. SearchSecurity. 
(2020). Retrieved 10 April 2020, from https://www.techtarget.com/searchsecurity/
definition/homomorphic-encryption .
 21. M atthew Green. (2020). A Few Thoughts on Cryptographic Engineering. Retrieved 
10 April 2020, from https://blog.cryptographyengineering.com/author/matthewdgreen/
page/12 /.
 22. P erera, C., Zaslavsky, A., Christen, P., & Georgakopoulos, D. (2014). Context Aware 
Computing for the Internet of Things: A Survey. IEEE Communications Surveys & Tutorials, 16(1), 414–454. https://doi.org/10.1109/surv.2013.042313.00197 .24 Machine Learning for Edge Computing
 23. Saeed, W., & Hussain, R. (2014). Service Based Model Using Context Awareness for 
Ubiquitous Computing. International Journal of Computer Applications, 97(6), 21–22. 
https://doi.org/10.5120/17011-7286 .
 24. Tsugawa, M., Matsunaga, A., & Fortes, J. (2013). Cloud Computing Security: What 
Changes with Software-Defined Networking? Secure Cloud Computing, 77–93. https://
doi.org/10.1007/978-1-4614-9278-8_4 .
 25. Stojmenovic, I., & Wen, S. (2014). The Fog Computing Paradigm: Scenarios and 
Security Issues. Annals of Computer Science and Information Systems. https://doi.
org/10.15439/2014f503 .
 26. Liu, K., Ng, J., Lee, V., Son, S., & Stojmenovic, I. (2016). Cooperative Data Scheduling 
in Hybrid Vehicular Ad Hoc Networks: VANET as a Software Defined Network. 
IEEE/ACM Transactions on Networking, 24(3), 1759–1773. https://doi.org/10.1109/
tnet.2015.2432804 .
 27. Fakeeh, K.A. (2016). Privacy and Security Problems in Fog Computing. Communications 
on Applied Electronics, 4(6), 1–7. https://doi.org/10.5120/cae2016652088 .
 28. Gennaro, R., Gentry, C., & Parno, B. (2010). Non-interactive Verifiable Computing: 
Outsourcing Computation to Untrusted Workers. Advances in Cryptology – CRYPTO 
2010, 465–482. https://doi.org/10.1007/978-3-642-14623-7_25 .
 29. Parno, B., Howell, J., Gentry, C., & Raykova, M. (2013). Pinocchio: Nearly Practical 
Verifiable Computation.  2013 IEEE Symposium on Security and Privacy. https://  
doi.org/10.1109/sp.2013.47 .
 30. Song, D. X., Wagner, D., & Perrig, A. (2000). Practical Techniques for Searches on 
Encrypted Data. Proceeding 2000 IEEE Symposium on Security and Privacy. S&P 
2000. https://doi.org/10.1109/secpri.2000.848445 .
 31. Mukherjee, B., Neupane, R., & Calyam, P. (2017). End-to-End IoT Security 
Middleware for Cloud-Fog Communication. 2017 IEEE 4th International Conference 
on Cyber Security and Cloud Computing (Cscloud). https://doi.org/10.1109/cscloud.  
2017.62 .
 32. Wang, H., Wang, Z., & Domingo-Ferrer, J. (2018). Anonymous and Secure Aggregation 
Scheme in Fog-based Public Cloud Computing. Future Generation Computer Systems, 
78, 712–719. https://doi.org/10.1016/j.future.2017.02.032 .
 33. Zhang, P., Zhou, M., & Fortino, G. (2018). Security and Trust Issues in Fog Computing: 
A Survey. Future Generation Computer Systems, 88, 16–27. https://doi.org/10.1016/j.
future.2018.05.008 .
 34. Khan, S., Parkinson, S., & Qin, Y. (2017). Fog Computing Security: A Review of 
Current Applications and Security Solutions. Journal of Cloud Computing, 6(1). https://
doi.org/10.1186/s13677-017-0090-3 .
 35. Qiu, M., Kung, S., & Gai, K. (2020). Intelligent Security and Optimization in Edge/
Fog Computing. Future Generation Computer Systems, 107, 1140–1142. https://doi.
org/10.1016/j.future.2019.06.002 .
 36. Kukreja, V., Ahuja, S., & Singh, A. (2018). Measurement and Structural Model of Agile 
Software Development Critical Success Factors. International Journal of Engineering 
& Technology, 7(3), 1236. https://doi.org/10.14419/ijet.v7i3.12776 .25 DOI: 10.1201/9781003143468-2
An Elucidation for 
Machine Learning Algorithms Used in Healthcare
Veerpal Kaur
School of Computer Science and Engineering, 
Lovely Professional UniversityPunjab, India 
Rajpal Kaur
Department of Commerce and Education, Maharaja Ganga Singh UniversityRajasthan, India2
CONTENTS
2.1 Introduction  .................................................................................................... 26
2.1.1  Supervised Learning  ........................................................................... 26
2.1.2  Semi-supervised Learning  .................................................................. 26
2.1.3  Unsupervised Learning  ...................................................................... 26
2.1.4  Reinforcement Learning  ..................................................................... 27
2.2 Decision Making in Healthcare  ...................................................................... 27
2.2.1  Decision Making  ................................................................................ 27
2.2.2  Decision-making Architecture in Healthcare  ..................................... 27
2.3 Machine Learning in Healthcare  .................................................................... 27
2.4 Machine Learning Scope in Healthcare  ......................................................... 28
2.5 Machine Learning Algorithms for Healthcare  ............................................... 28
2.5.1  Support Vector Machine  ..................................................................... 28
2.5.2  Naive Bayes Classification  .................................................................. 28
2.5.3  Decision Tree  ...................................................................................... 28
2.5.4  K-nearest Neighbor  ............................................................................. 28
2.5.5  Fuzzy Logic  ........................................................................................ 29
2.5.6  CART  .................................................................................................. 29
2.6 Predicting Several Diseases Using Machine Learning  .................................. 29
2.6.1  Machine Learning in Heart Diagnosis  ............................................... 29
2.6.2  Machine Learning in Diabetes Diagnosis  .......................................... 29
2.6.3  Machine Learning in Cancer Diagnosis  ............................................. 2926 Machine Learning for Edge Computing
2.6.4  Machine Learning in Thyroid Diagnosis  ........................................... 30
2.7 Related Work  .................................................................................................. 30
2.8 Conclusion  ...................................................................................................... 30
References  ................................................................................................................ 34
2.1 INTRODUCTION
Machine learning is the technology to make machines learn by training them with the 
available data, and once trained, the machines are tested for desirable results [1]. No doubt a machine can’t cope with all the real-world data shortcomings, but it shows astonishing results when used in different application areas. The use of machine learning to train the systems has proven to be a successful technique that can be implemented by various other fields except computer science, such as earth science, applied sciences, healthcare, and any other field where there is need for data introspection. Machine learning is divided into four categories (cf Figure 2.1 ) [2], which are discussed in the subsequent sections.
2.1.1 s uperv Ised leArnIng
The supervised learning algorithms are the ones that learn first and then they imple -
ment the results on the provided dataset. In a simple way, it can be said that the model is first learning itself and then on the basis of learnt skills, it processes the information provided. Examples of such algorithms are regression and classification algorithms.
2.1.2 s emI-superv Ised leArnIng
Semi-supervised learning, also known as pseudo learning, is a machine learning 
technique that uses the labeling method to train the model. It is a mixture of super -
vised and unsupervised machine learning techniques.
2.1.3 u nsuperv Ised leArnIng
In this learning technique, the system does not have any labels to learn from, and 
hence, it is called unsupervised. It is used to detect hidden patterns available in the dataset and create data groups. Examples of such algorithms are clustering algorithms.
FIGURE 2.1  Listing of different machine learning algorithms.27 Machine Learning Algorithms Used in Healthcare
2.1.4 r eInforcement  leArnIng
This is the learning technique in which the system is allowed to use hit and trial and 
the system learns from its own experiences and errors. Examples of such learning 
techniques are Q-learning algorithms.
2.2 D ECISION MAKING IN HEALTHCARE
Decision making is the most crucial part of healthcare system. If the decision-  
making system fails, fatal results may occur. Machine learning algorithms help medical staff to view these things far better. With prediction techniques, one can foresee the effect of disease; with classification, one can classify the different factors behind a disease occurrence; with clustering, one can group the people with same symptoms; and all these techniques help a research team to reach the ultimate stage of finding a cure to the disease.
2.2.1 d ecIsIon mAkIng
Decision making is the procedure in which the doctor reaches a conclusion after 
going through the steps of meticulously analyzing data. Machines can greatly help doctors in this process by displaying visualization results of the huge data available.
2.2.2 d ecIsIon-mAkIng ArchItecture  In heAlthc Are
Architecture should be followed to make effective use of the machines available by 
maintaining the records of the patients, accounts, medicines, and many other so as to support the whole architecture [ 7]. The data saved from previous patients will 
definitely aid doctors in making decisions to detect and diagnose a disease or other issues, and guide them on which medicines to prescribe to patients. So, the whole system will be influenced by the machines present in the diagnosis centers.
2.3 M ACHINE LEARNING IN HEALTHCARE
The role of machine learning is much powerful than any other fields. Machine learning algorithms can be applied in studying and analyzing images, patient data retrieval, and predicting cases of a particular disease so that the support system will be ready for any kind of emergency. Without analyzing the forecasting image of the disease casualties, the whole healthcare system will be disrupted [ 8]. Machine learn -
ing helps foresee the impacts prior to their occurrence, and based on the estimates, the disease can be cured accordingly. The healthcare system is highly dependent on computer-aided machines that help diagnose and visualize the data interpretations to get a view of the data and its hidden patterns. Based on that, the patient will be prescribed specific medical procurements.
Currently, the healthcare system has collected immense information that has been 
compiled from hospitals directly and the databases of local pharmacies. From minor headache symptoms to major heart issues, all have been recorded and can be used to 28 Machine Learning for Edge Computing
draw crucial and statistical analysis that can help to diagnose the disease and even 
generate instances of various other ailments to be cured.
2.4 M ACHINE LEARNING SCOPE IN HEALTHCARE
The thorough study of hidden patterns, image diagnosis, and predictions from the data available makes using machine learning algorithms a must in the field of medi -
cal science. The treatment of many health issues can possibly be done using machine learning strategies. Given the algorithms used in machine learning are highly accu -
rate and able to generate various visualizations for the given dataset, they can be put to use to get better results and a clear view of the current scenario for the disease diagnosis.
Patients looking for medical care have a trust level established by hospitals that 
have a proven track record of accurately diagnosing symptoms, and hence patients go for help to the same doctor again and again. So, in order to protect, and even cure, the patients, doctors have the responsibility to have a clear view of the patient’s health. Using machine learning at different levels of the healthcare system makes the whole architecture relevant and reliable.
2.5 M ACHINE LEARNING ALGORITHMS FOR HEALTHCARE
2.5.1 s upport  vector  mAchIne
The Support Vector Machine (SVM) was developed in the late 1990s. The SVM 
algorithm is used to perform sampling of bulky datasets and split the given dataset into variable groups. The SVM is mostly used to implement regression and classifi -
cation problems [ 13].
2.5.2 n AIve bAyes clAssIfIcAtIon
Based on the Bayesian theorem of probability, the Naive Bayes classification algo -
rithm is used to characterize the data into different labels, and the given instances can be verified by classifying new data points into different groups. The Naive Bayes is used to classify a huge dataset [ 14].
2.5.3 d ecIsIon tree
Decision tree is one of the finest and most used algorithms for the purpose of clas -
sification of data points. Labeling is used for the classification of data, and the root of the tree is the explicit condition that initializes the process and is further bifurcated into labeled classes that satisfy or dissatisfy the condition of the root node [ 15].
2.5.4 k -neArest neIghbor
K-nearest neighbor (KNN) is a supervised classification algorithm that is quite easy 
to deploy and is simple to understand. In KNN, a centroid is taken and then the data 29 Machine Learning Algorithms Used in Healthcare
points are calculated for the distance between the data point and centroid; the data 
point that is closest is grouped into the centroid set of points and the process is fol -
lowed for all the data points. Hence, for a class labeled with a centroid, condition and distance is measured using Euclidean and Manhattan methods [16].
2.5.5 f uzzy logIc
Fuzzy logic has been proved to be the best algorithm that uses fuzzy sets having val -
ues between 0 and 1. It is one of the most famous methods in artificial intelligence, and hence, has good accuracy [17].
2.5.6  cArt
The Classification and Regression Tree is known as CART. In this strategy, the target is to get categorical and continuous values. These values are used to find data points in a tree, and the tree can be a regression tree or a classification  
tree [18].
2.6 PREDICTING SEVERAL DISEASES USING MACHINE LEARNING
2.6.1 m AchIne leArnIng In heArt dIAgnos Is
A huge intake of unhealthy food, like junk food, that has more cholesterol and other 
fats harms the heart to a great extent. The precision is of utmost importance in the process of analyzing heart-related issues. The data presented in the Table 2.1 has been taken from various sources and precisely reflects the machine learning usage and its impacts. The SVM algorithm has been compared with J48 and Naive Bayes, and the results reflected 94.60% precision with SVM and 74% precision with Naive Bayes [18].
2.6.2 m AchIne leArnIng In dIAbetes dIAgnos Is
Diabetic issues have been profound among old age people nowadays; hence, viable 
machine learning approaches need to be among the methods used to diagnose dia -
betes among patients. The dataset implemented the Naive Bayes and Decision Tree algorithms get the precision percentage. A higher proportion of accuracy is reflected by Naive Bayes than decision tree [21].
2.6.3 m AchIne leArnIng In cAncer dIAgnos Is
The dataset used from University of California Irvine (UCI) repository identifies 
a better algorithm for the diagnosis of cancer. The J48 algorithm seems quite good at its job with a 94.2% precision rate in comparison to Naive Bayes with 83% [23]. Various other algorithms have also been proposed for breast cancer treatment, such as KNN, decision tree, and SVM. Among these, SVM has stolen the show by getting the highest accuracy rate of 96% [24].30 Machine Learning for Edge Computing
2.6.4 m AchIne leArnIng In thyro Id dIAgnos Is
Fuzzy logic, SVM, and decision tree classification algorithms were deployed onto 
the dataset taken from UCI repository to get a clear view of the accuracy of the cho -
sen algorithms. The implementation results reflect that fuzzy logic has been the best 
among the category [26].
Table 2.1  depicts the diagnosis approaches using machine learning techniques 
with their precision rate.
2.7 RELATED WORK
A rigorous literature review has been conducted to dive deep into the work already done, and a systematic tabular representation of it appears in Table 2.2 . The table 
includes reference paper numbers with their titles and advantages, as well as their future scopes.
2.8 CONCLUSION
To conclude, this chapter reveals machine learning approaches that are being used in healthcare and that can be used in near future to facilitate the decision-making system in the medical science field. Machine learning approaches are prominently known for their accuracy; hence, they can be incorporated into healthcare to classify patient disease categories and can provide a better visualization of the methods that can be followed to cure diseases in a better way. Image recognition and their hidden patterns can be studied using machine learning algorithms, and an amalgamation of machine learning with healthcare technological advancements will lead to medical science that is par excellence. The future of healthcare is predicted to be a brighter one with the use of fifth-generation techniques, and hence will surely lead to a new world of healthy living.TABLE 2.1
Use of Machine Learning in Healthcare
Machine Learning  Model UsedFor Disease Category Dataset Taken from Precision
SVMJ48 [27]Heart UCI 85.03%84.35%
Naive Bayes [28] Heart Diabetic Research Institute, Chennai 86.41%
SVM [29] Diabetes UCI 78%
Naive Bayes [30] Diabetes- type2 Different sectors of society in India 95%
J48 [31] Breast cancer UCI 98.14%
Decision tree [32] Breast cancer Swami Vivekananda Diagnostic Centre 97%
Decision tree (DT) + SVM [33] Breast cancer UCI 91%
CART Breast cancer University of Wisconsin Hospital 92.42%97.42% Naive Bayes [34]31 Machine Learning Algorithms Used in Healthcare
TABLE 2.2
Literature Review of Machine Learning in Healthcare
Reference No. Title of the Study Benefits Future Work
35 Applications of Machine  
Learning in the Field of  Medical CareExcelled in the previous system for proper medical distribution and 
resources. The paper presented the techniques of computer sciences  such as artificial intelligence to explore the benefits of the current technology into health care.
The paper also dived deep into the historical study of machine learning  
and its applications in medicinal field.The study will focus on the human power 
and machine power to create a balance between both of them.
36 Breast Cancer Detection Using 
Machine Learning AlgorithmsPrediction approaches using machine learning are presented in the  
paper for breast cancer.
With the best accuracy, precision, and F1 score, KNN has been proven  
to be the best algorithm for the said purpose in comparison to other algorithms.Other supervised machine learning 
algorithms are yet to be explored for breast cancer early diagnosis and prognosis.
37 A Literature Review on Machine 
Learning Based Medical Information Retrieval SystemsThe paper presents a deep exploration into artificial intelligence used in 
medical field. The information retrieval (IR) algorithms for managing  the big data related to medical field are also discussed.The need for big data still exists to train 
the machine learning model used for medical information retrieval.
38 Medical Imaging Using Machine 
Learning and Deep Learning Algorithms: A ReviewA standard dataset has been used to predict the required diseases using  
the images. Both supervised and unsupervised techniques have been  used for the purpose. The search approach for the best algorithm in  terms of accuracy is proposed in the paper.Future work will look for such 
algorithms that may help the medical image inferences make crucial decisions.
39 Machine Learning for Improved 
Diagnosis and Prognosis in HealthcareAn inference system using the Bayesian approach has been  
proposed for diagnosing Alzheimer’s disease,The search continues for a better 
classification algorithm for classifying cell images to detect the breast cancer at an early stage.
Significant work needs to be done to 
arrange huge datasets rich in figures for the optimum accuracy of diagnosis.
(Continued)32 Machine Learning for Edge Computing40 Heart Disease Identification 
Method Using Machine  
Learning Classification in E-HealthcareTo solve the feature selection issues, a fast, conditional, novel feature 
selection algorithm has been proposed.
The proposed system ensures the accuracy and reduction in execution  
time for the given classification algorithm. The proposed approach  is working optimally with the Support Vector Machine (SVM)  classifier to detect heart diseases.Future work of the study will focus on 
the optimization of the proposed approach.
41 Prediction of Diabetes Using 
Machine Learning  Algorithms in HealthcareThe presented work focuses on the prediction algorithms in machine 
learning that best suit the prediction of diabetes.To get better accuracy, the future work 
will emphasize the integration of other machine learning algorithms with the proposed approach. Testing the proposed approach using big datasets that are pre-processed will help geta clearer view of the accuracy of the proposed approach.
42 Deep Learning for Health 
Informatics: Recent Trends  
and Future DirectionsThe paper studies the deep learning domain of artificial intelligence and  
its application in helping the healthcare sector with decision making.
43 Comparative Study of Machine 
Learning Algorithms  
for Breast Cancer Detection  and DiagnosisThe study presents the comparison between three prominent machine 
learning algorithms for detecting and diagnosing breast cancer in  patients. The algorithms used in the study are SVM, Random Forest (RF), and Bayesian Network (BN).
The study shows that SVM has the highest prediction of accuracy as  
well as precision.
44 Architecture of Smart Health  
Care System Using Artificial IntelligenceThe proposed system has the capability to find hidden patterns in the data 
given to the model, and hence helps medical officials get better insight into the data and help them make crucial decisions, saving time and effort.TABLE 2.2  (Continued )
Literature Review of Machine Learning in Healthcare
Reference No. Title of the Study Benefits Future Work
(Continued)33 Machine Learning Algorithms Used in Healthcare
45 Prediction of Cardiovascular 
Disease Using Machine  
Learning AlgorithmsThe paper presents the prediction of heart-related issues in the given 
patient’s data. It helps in the diagnosis steps of the process.The future study will focus on the hybrid 
of the algorithms discussed to get better performance in most areas of evaluation.
46 Blockchain and Machine  
Learning in Health Care and ManagementThe study focuses on the blockchain technology along with machine 
learning techniques that can be used in the healthcare sector to help manage data more securely and effectively.The future work of the study will discuss 
implementing Internet of Things (IoT), along with artificial intelligence, in a healthcare system.
47 Predictive Analytics in Health 
Care Using Machine Learning Tools and TechniquesThe paper focuses on the electronic maintenance of patient-related  
data, the prediction and diagnosis of various diseases, and the need  of a suitable machine learning model.Highly unstructured distributed and 
constantly changing datasets are the key challenges yet to be focused.
48 Applying Best Machine  
Learning Algorithms for  
Breast Cancer Prediction and ClassificationThe proposed study has implemented four different machine learning 
algorithms to predict breast cancer in patients.More rigorous studies on machine 
learning techniques to get a hybrid approach with deep learning algorithms will be interesting to see.
49 Machine Learning Model for 
Breast Cancer PredictionThe convolution neural network used to detect breast cancer has been 
presented in the study. The results show better accuracy rates of the proposed approach.
A systematic calculation has also been implemented to draw the results 
from the proposed technique.
50 Machine Learning Based  
System for Prediction of Breast Cancer SeverityArtificial Neural Network (ANN), KNN, Binary Support Vector  
Machine (Binary SVM), and Decision Tree (DT) are the analyzed algorithms for getting a better computer-aided diagnosis for predicting cancer in patients and helping reduce death tolls.The optimization of the proposed work is 
yet to be addressed.
51 Regression Analysis of  
COVID-19 Using Machine Learning AlgorithmsThe study presents a deep analysis of COVID-19 among various states 
using the regression approach of machine learning.Predicting the disease using a more 
accurate and precision-efficient algorithm from machine learning domain is to be addressed.TABLE 2.2  (Continued )
Literature Review of Machine Learning in Healthcare
Reference No. Title of the Study Benefits Future Work34 Machine Learning for Edge Computing
REFERENCES
 1.  T . Mitchell, “Machine Learning,” McGraw Hill. p. 2, 1997.
 2.  A . Mishra, A. Shukla, “From Machine Learning to Deep Learning Trends and 
Challenges,” CSI Communications, December 2018.
 3.  N . M. Allix, “Epistemology and Knowledge Management Concepts and Practices.” 
Journal of Knowledge Management Practice, vol. 4(1), pp. 1–24, 2003.
 4.  A . Mathur, G. P. Moschis, “Socialization Influences on Preparation for Later Life.” 
Journal of Marketing Practice: Applied Marketing Science, vol. 5, pp. 163–176, 2007.
 5.  L . P. Kaelbling, M. L. Littman, A. W. Moore, “Reinforcement Learning: A Survey,” 
Journal of Artificial Intelligence Research, vol. 4, pp. 237–285, 1996.
 6 .  T . G. Thompson, D. J. Brailer, “The Decade of Health Information Technology: Delivering 
Consumer-Centric and Information-Rich Health Care,” US Department of Health and 
Human Services, 2004.
 7.  K . Rajalakshmi, S. C. Mohan, S. D. Babu, “Decision Support System in Healthcare 
Industry,” International Journal of Computer Applications, vol. 26(9), pp. 42–44, 2013.
 8.  R . Bhardwaj, A. R. Nambiar, “A Study of Machine Learning in Healthcare,” IEEE 41st 
Annual Computer Software and Applications Conference, 2017.
 9.  M . Hauskrecht, S. Visweswaran, G. Cooper, G. Clermont, “Clinical Alerting of 
Unusual Care That Is Based on Machine Learning from Past EMR Data,” 2015.
 10.  M . Kohn, “Real World Data and Clinical Decision Support,” Elsevier, 2009.
 11.  J . Sukanya, “Applications of Big Data Analytics and Machine Learning Techniques in 
Health Care Sectors,” International Journal of Engineering and Computer Science, vol. 6, pp. 21963–21967, 2017.
 12.  K . P. Murphy, “Machine Learning: A Probabilistic Perspective,” The MIT Press, 2012.
 13.  A . Hazra, S. K. Mandal, A. Gupta, “Study and Analysis of Breast Cancer Cell 
Detection Using Naïve Bayes, SVM and Ensemble Algorithms,” International Journal of Computer Applications, vol. 145(2), pp. 39–45, 2016.
 14.  P . Sharma, A. P. R. Bhartiya, “Implementation of Decision Tree Algorithm to Analysis 
the Performance,” vol. 10, pp. 861–864, 2012.
 15.  C . M. Bishop, “Neural Networks for Pattern Recognition,” Oxford University, 1995.
 16.  H . Zimmermann, “Fuzzy Set Theory and Its Applications,” Kluwer Academic 
Publishers, 2001.
 17.  A . Hazra, S. K. Mandal, A. Gupta, A. Mukherjee, A. Mukherjee, “Heart Disease 
Diagnosis and Prediction Using Machine Learning and Data Mining Techniques: A Review,” Advances in Computational Sciences and Technology, vol. 10, pp. 2137–2159, 2017.
 18.  G . Parthiban, S. K. Srivatsa, “Applying Machine Learning Methods in Diagnosing 
Heart Disease for Diabetic Patients,” International Journal of Applied Information Systems, vol. 3, pp. 25–30, 2012.
 19.  A . F. Otoom, E. E. Abdallah, Y. Kilani, A. Kefaye, “Effective Diagnosis and Monitoring 
of Heart Disease,” International Journal of Software Engineering and Its Applications, vol. 9, pp. 143–156, 2015.
 20.  A . Iyer, S. Jeyalatha, R. Sumbaly, “Diagnosis of Diabetes Using Classification Mining 
Techniques,” International Journal of Data Mining & Knowledge Management Process (IJDKP), vol. 5, pp. 1–14, 2015.
 21.  S . K. Sen, S. Dash, “Application of Meta Learning Algorithms for the Prediction of 
Diabetes Disease,” International Journal of Advance Research in Computer Science and Management Studies, vol. 2, pp. 396–401, 2014.
 22.  K . Williams,  P. A. Idowu,  J. A. Balogun,  A. I. Oluwaranti, “Breast Cancer Risk 
Prediction Using Data Mining Classification Techniques,” Transactions on Networks and Communications, vol. 2, pp. 1–11, 2015.35 Machine Learning Algorithms Used in Healthcare
 23.  Z . K. Senturk,  R. Kara, “Breast Cancer Diagnosis via Data Mining: Performance 
Analysis of Seven Different Algorithms,” Computer Science & Engineering, vol. 1,  
pp. 1–10, 2014.
 24.  J . Majali, R. Niranjan, V. Phatak, O. Tadakhe, “Data Mining Techniques for Diagnosis 
and Prognosis of Cancer,” International Journal of Advanced Research in Computer 
and Communication Engineering, vol. 3, pp. 613–616, 2015.
 25.  E . I. Papageorgiou, N. I. Papandrianos, D. J. Apostolopoulos, P. J. Vassilakos, “Fuzzy 
Cognitive Map Based Decision Support System for Thyroid Diagnosis Management,” International Conference on Fuzzy Systems, pp. 1204–1211, 2008.
 26.  V . Chaurasia, S. Pal, “Data Mining Approach to Detect Heart Disease,” International 
Journal of Advanced Computer Science and Information Technology, vol. 2,  
pp. 56–66, 2018.
 27.  K . Vembandasamy, R. Sasipriya, E. Deepa, “Heart Diseases Detection Using Naive 
Bayes Algorithm,” vol. 2, pp. 441–444, 2015.
 28.  V . A. Kumari, R. Chitra, “Classification of Diabetes Disease Using Support Vector 
Machine,” International Journal of Engineering Research and Applications, vol. 3,  
pp. 1797–1801, 2013.
 29.  A . Sarwar, V. Sharma, “Intelligent Naïve Bayes Approach to Diagnose Diabetes Type-
2. Special Issue,” International Journal of Computer Applications and Challenges in Networking, Intelligence and Computing Technologies, vol. 3, pp. 14–16, 2012.
 30.  S . S. Shrivastavat, A. Sant, “An Overview on Data Mining Approach on Breast Cancer 
Data,” International Journal of Advanced Computer Research, vol. 3, pp. 256–262, 2013.
 31.  E . Venkatesan, T. Velmurugan, “Performance Analysis of Decision Tree Algorithms 
for Breast Cancer Classification,” Indian Journal of Science and Technology, vol. 8,  
pp. 1–8, 2015.
 32.  K . Sivakami, N. Saraswathi, “Mining Big Data: Breast Cancer Prediction Using DT- 
SVM Hybrid Model,” International Journal of Scientific Engineering and Applied Science, vol. 1, pp. 418–429, 2015.
 33.  S . S. Shajahaan, S. Shanthi, V. ManoChitra, “Application of Data Mining Techniques 
to Model Breast Cancer Data,” International Journal of Emerging Technology and Advanced Engineering, vol. 3, pp. 362–369, 2013.
 34.  M . R. N. Kousarrizi, F. Seiti, M. Teshnehlab, “An Experimental Comparative Study 
on Thyroid Disease Diagnosis Based on Feature Subset Selection and Classification,” International Journal of Electrical & Computer Sciences, vol. 1, pp. 13–19, 2012.
 35.  H . Dou, “Applications of Machine Learning in the Field of Medical Care,” 34th Youth 
Academic Annual Conference of Chinese Association of Automation (YAC), 2019.
 36.  S . Sharma, A. Aggarwal, T. Choudhury, “Breast Cancer Detection Using Machine 
Learning Algorithms,” International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS), 2018.
 37.  A . Gudivada, N. Tabrizi, “A Literature Review on Machine Learning Based Medical 
Information Retrieval Systems,” IEEE Symposium Series on Computational Intelligence (SSCI), 2018.
 38.  J . Latif, C. Xiao, A. Imran, S. Tu, “Medical Imaging Using Machine Learning and 
Deep Learning Algorithms: A Review,” International Conference on Computing, Mathematics and Engineering Technologies, IEEE, 2019.
 39.  N . G. Maity, Dr. S. Das, “Machine Learning for Improved Diagnosis and Prognosis in 
Healthcare,” IEEE Aerospace Conference, 2017.
 40.  J . P. Li, A. UlHaq, S. UdDin, J. Khan, A. Khan, A. Saboor, “Heart Disease Identifica -
tion Method Using Machine Learning Classification in E-Healthcare,” IEEE Access, vol. 8, pp. 107562–107582, 2020.36 Machine Learning for Edge Computing
 41.  M . A. Sarwar, N. Kamal, W. Hamid, M. A. Shah, “Prediction of Diabetes Using 
Machine Learning Algorithms in Healthcare,” 24th International Conference on 
Automation and Computing (ICAC), IEEE, 2018.
 42.  S . Srivastava, S. Soman, A. Rai, P. K. Srivastava, “Deep Learning for Health 
Informatics: Recent Trends and Future Directions,” International Conference on Advances in Computing, Communications and Informatics, IEEE, 2017.
 43.  D . Bazazeh, R. Shubair, “Comparative Study of Machine Learning Algorithms for 
Breast Cancer Detection and Diagnosis,” 5th International Conference on Electronic Devices, Systems and Applications, IEEE, 2016.
 44.  M . M. Kamruzzaman, “Architecture of Smart Health Care System Using Artificial 
Intelligence,” IEEE International Conference on Multimedia & Expo Workshops, 2020.
 45.  D . G. Kumar, K. Arumugaraj, V. Mareeswari, “Prediction of Cardiovascular Disease 
Using Machine Learning Algorithms,” International Conference on Current Trends towards Converging Technologies, IEEE, 2018.
 46.  S . Jain, A. Anand, A. Gupta, K. Awasthi, S. Gujrati, J. Channegowda, “Blockchain 
and Machine Learning in Health Care and Management,” International Conference on Mainstreaming Block Chain Implementation, IEEE, 2020.
 47.  B . Nithya, Dr. V. Ilango, “Predictive Analytics in Health Care Using Machine Learning 
Tools and Techniques,” International Conference on Intelligent Computing and Control Systems, IEEE, 2017.
 48.  Y . Khourdifi, M. Bahaj, “Applying Best Machine Learning Algorithms for Breast 
Cancer Prediction and Classification,” International Conference on Electronics, Control, Optimization and Computer Science, IEEE, 2018.
 49.  A . Gupta, D. Kaushik, M. Garg, A. Verma, “Machine Learning model for Breast 
Cancer Prediction,” Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud), IEEE, 2020.
 50.  S . Laghmati, A. Tmiri, B. Cherradi, “Machine Learning Based System for Prediction 
of Breast Cancer Severity,” International Conference on Wireless Networks and Mobile Communications, IEEE, 2019.
 51.  E . Gambhir, R. Jain, A. Gupta, “Regression Analysis of COVID-19 Using Machine 
Learning Algorithms,” International Conference on Smart Electronics and Communication, 2020.37 DOI: 10.1201/9781003143468-3
Tea Vending Machine 
from Extracts of Natural Tea Leaves and Other Ingredients
IoT and Artificial Intelligence Enabled
Neha Sharma, Ram Kumar Ketti Ramachandran, Huma Naz, and Rishabh Sharma
Chitkara University Institute of Engineering &  
Technology, Chitkara University  Punjab, India3
CONTENTS
3.1 Introduction  .................................................................................................... 38
3.2 Methodology  ................................................................................................... 39
3.3 Existing System and Mechanism  .................................................................... 40
3.4 Biography of the Vending Machine  ................................................................ 40
3.5 Existing System  .............................................................................................. 40
3.6 Literature Survey  ............................................................................................ 41
3.7 Objectives  ....................................................................................................... 44
3.8 Architecture  .................................................................................................... 44
3.9 Complete System  ............................................................................................ 46
3.10 Working Units  ................................................................................................. 46
3.11 Machine Components  ..................................................................................... 48
3.11.1  Servo Motor  ........................................................................................ 48
3.11.2  Arduino Uno Microcontroller  ............................................................. 48
3.11.3  Water Heating Unit  ............................................................................. 48
3.11.4  Container  ............................................................................................ 48
3.11.5  Appliance Body  .................................................................................. 48
3.12 Innovation and Specific Functions Performed ................................................ 49
3.13 Future Enhancement  ....................................................................................... 49
3.14 Conclusion  ...................................................................................................... 50
References  ................................................................................................................ 5038 Machine Learning for Edge Computing
3.1 INTRODUCTION
As technology advances every day, vending machines provide simple as well as 
complex services to users according to their needs. The design, automation, and 
technology of vending machines are changed as per the users’ needs. Various prod -
ucts and services are offered by these automated machines, and they all fall under 
vending machines [ 1]. The very first vending machine was developed only for indi -
vidual product delivery, such as for newspapers, cigarettes, beverages, and so on. 
Other types of services, such as transportation tickets and vehicle washing [ 2], are 
categorized as a payment for one service. These machines are commonly installed 
in crowded or busy places, such as bus/train stations, schools, airports, companies, 
shopping malls, and universities [ 3]. Users can purchase products without wast -
ing time and can quickly make cashless payments [ 4, 5]. Some places offer 24 × 7 
service, which explains why vending machines have gained so much of popularity 
and have led to large profits for vending machine owners [ 6]. All around the world, 
beverages like tea and coffee have become a part of life as well as an everyday rou -
tine. Hence, it is a very big market on a daily basis. The global intelligent vending 
machine market already reached a market value of more than $5,000 million in 2017 
and is expected to reach more than $15 billion by 2025. It is estimated to grow at a 
compound annual growth rate (CAGR) of 15.3%.
In general, pure tea is made up of tea leaves ( Camellia sinensis ) that do not con -
tain any additives. However, flavored/packed teas have many preservatives and 
artificial elements, and tea powders added to them contain possible pesticides, arti -
ficial sweeteners, coloring agents, and other harmful chemicals to keep them fresh 
and increase their longevity. Colored tea can legally contain, but in practice rarely 
contains, artificial colors that are openly advertised as such. However, there has 
been a persistent problem with people adding unreported colorings to tea, often as a 
way to “stretch” low-quality tea further. These colorings can include unsafe ingre -
dients. Some undisclosed ingredients in teas include coal tar dye, indigo, soapstone, 
Prussian blue, and gypsum. Even renowned tea brands are suspected of overusing 
additives. Concentrated supplements often use lower-quality ingredients, and have a 
higher risk of contamination with heavy metals or other toxins, whereas high-quality 
loose-leaf teas are less likely to contain contaminants. Most of the common addi -
tives to tea are safe, but there are occasional concerns with contaminants or other 
unlabeled additives [ 7]. Some additives, such as citric acid, unwanted sweeteners, or 
natural and artificial flavors, can produce undesirable flavor characteristics and/or 
be used to mask low-quality ingredients. Products marketed primarily for “health” 
reasons, especially those containing concentrated extracts or supplements, pose the 
highest risk of causing harm.
Nowadays, vending machines are connected to the Internet that can send and 
receive signals remotely [ 8]. Over the last decade machine-to machine (M2M) 
communication has been happening [ 9]. Vending machines are refilled using 
telemetry for the daily operations, whereas cashless payment is used just to boost 
sales and increase consumer convenience. Due to high competition in the vend -
ing market in every country, small and big markets are fragmented everywhere 
[10, 11].39 Extracts of Natural Tea Leaves and Other Ingredients
3.2 METHODOLOGY
Methodology includes planning, selection, design, and the complete setup of the 
final product ( Figure 3.1 ).
In this methodology, we first plan the project; for this, a proper survey is con -
ducted, and the problem is identified. To solve the problem, we then research a few existing systems. For better results with the tea, ingredients are added so that a pure and fresh composition is available for the grinding and mixing unit when added to the existing machine. The unit will crush all the ingredients and produce fresh tea. After that, the modification system is connected to the cloud, where it will  
FIGURE 3.1  Stages of tea vending unit.40 Machine Learning for Edge Computing
calculate the data added by users and give more intelligent solutions to users. At the 
same time, it will collect all the user data and create a database. Ingredients will help 
to reduce quantity of the tea; it will be natural as well as will cure many diseases. 
Finally, a complete solution in the form of a complete unit is proposed. It will include 
all the ingredients and provide the desired herbal tea according to the consumer’s 
wishes.
3.3 EXISTING SYSTEM AND MECHANISM
The vending machine is made up of brushes, metals and motors [ 12]. The goal is to 
deliver beverages and snacks to users by operating each block on the motor-based 
mechanism. To place an order, a person has to make payment first after which 
they can collect a printable token and also avail other options; for example, an 
espresso. Nowadays, a variety of drinks, including latte, hot water, milk, and 
green coffee, are also available in the smart vending machine with the bolted 
position. To serve things to the client, a person can open the container and take 
out papers. The same mechanism can also facilitate other vending machines 
used for food delivery, snacks, paper vending as well as beverages vending, and a 
continuous health monitoring status and timely dosage of patients. The proposed 
system helps healthcare officials to view the effects of medication and remotely 
monitor their patients.
3.4 BIOGRAPHY OF THE VENDING MACHINE
The first vending machine was invented by Hero of Alexandria, one of the famous 
designers and mathematicians in Roman Egypt. The machine worked using coins, 
also known as metal currency. When a coin was inserted via a slot at the top of the 
machine, it fell upon a container attached to a lever and released holy water. The 
container continued to tilt with the weight of the coin until it fell off, and the valve 
would close. In 1822, Richard Carlile manufactured a paper-administering machine 
for a book shop. Simeon Denham, an Englishman, used this working principle for 
the collection of warm water. It was the first vending machine tested in the hospital 
in 1867 [ 13, 14].
3.5 EXISTING SYSTEM
With the advancements in technology, vending machines can now sell simple to 
complex products to the users opting different services. A large amount of data 
is collected every day that connects such vending machines to the internet. And 
now tea and coffee vending machines are also using the Internet of Things (IoT) 
devices. As per the inputs received from the users, the desired beverage is made. 
All around the world, tea and coffee play a very important role in everyone’s life. 
Working people, such as multinational corporation (MNC) employees and manu -
facturers, want to have their favorite beverages at their workplace. Although an 
MNC can afford beverages from the cafeteria, small organizations cannot. In the 
case of small organizations, beverage requirements are fulfilled through the street 41 Extracts of Natural Tea Leaves and Other Ingredients
vendors every day, but the quality of their products can be questionable, as they 
use tap water to prepare such beverages, which can drastically affect their quality. 
Another issue is cleanliness because the utensils used to prepare these beverages 
cannot always be trusted. Moreover, different people have different preferences for 
the beverages; for example, some people take strong sugar, some take less sugar, 
and others consume beverages without even using sugar. Meeting these require -
ments can be a bit difficult when one relies on street vendors for beverages [ 15]. 
The major issue is the time taken to dispense them in cups because a single minute 
is very precious for the working people and it takes more than 15 minutes just to 
reach consumers. Here, the use of tea and coffee vending machines is the com -
plete solution to the problem [ 16]. Tea and coffee vending machines are already 
available in the market, but their price and size make them unaffordable to small 
organizations. The advancements in vending machines are being made continu -
ously due to which these have been improving logically as well as in their facility 
as compared to earlier vending machines. These advancements have added many 
important features to the vending machines such as sensor availability, web and 
camera availability, lower power consumption rate, lower costs, more computer -
ized contacts, and radio frequency identification (RFID). The advanced vending 
machines provide the advantage to the consumers with [ 17] different experiences 
and decreases working expense through wireless communication. Some latest 
technologies such as artificial intelligence (AI), sensors, and camera features are 
also incorporated in the vending machines to enhance the web-based social net -
working user experience. 
3.6 LITERATURE SURVEY
Nourishment items provide nourishment and refreshment to people. But when 
they get involved with the contribution element, it results in performance and 
technology-based improvement in the existing system; the same is the case 
with vending machines. An outline methodical audit assessment is carried out 
on the basis of smart evaluation technology and present procedure, and survey 
results are collected [ 19]. An evaluation of the implementation factors includes 
the type of apparatus used for the experiment, the items accessed, the openness 
of machine, the area of the study, the size of partitioning, advancements in the 
items, and the logic used to train [ 20]. The result of the audit is that logical 
practices are used only 22%, smart vending machine accessibility is 39%, the 
assessed cost is 48%, advancement is 52%, 70% is the segment assessed, acces -
sibility of item assessed is 91%, and built-up invigorating effect criterion is 
96% assessed from the investigated articles. It is found that from the numerous 
articles, only 87% gave resolutions of the vending machine condition [ 21]. In 
an analysis between these existing automatic systems, it is difficult to achieve 
results; however, it is easy to agree with the dependable vending machine appa -
ratus [ 22, 23]. Coffee beverage is offered by the IoT-based vending machine to 
the clients.42 Machine Learning for Edge Computing
Further, it also includes a few items, such as hot coffee/tea, espresso, latte, 
titbits, and cool drink beverages, as a refreshment to the users [24]. Earlier, all 
the payments are currency-based, but the existing systems use RFID-based tech-nology and smart cards. This framework identifies the user access by reading the card using radio frequency. After identifying the product [25] and cost via RFID, database is updated and the desired output (product) is dispensed. If any problem or issue arises, the RFID number needs to be fixed; the presence of any magne -
tize material gives an error to the RFID card and it stops working. A small chip is placed in the smart system reader of an online payment-based coffee vending system. According to the necessity of the users per day, the quantity of beverages in the cup is customized [26]. As desired by the user, each cup is dispensed by the representative, and if a user desires to have more quantity of beverages than the usual one, then the sum can easily be cut from the compensation account. While using vending machines, human assistance is not required to serve cus-tomers. Simple steps are performed by the consumers just to get the diamond and platinum jewelry from the vending machine after inserting the card [27]. A controlled system is used just to perform payment, communication, and product extraction. Food and nonfood items and other convenience products are sold by the different markets using these vending machines. In the past years, selling hot and cold drinks was the main business, but nowadays, due to the variety of food and other services, the market for the vending machine and its products is increasing constantly. The vending machine is recognized [29] differently by different countries and their vending associations. Vending is defined for the small article or product, operated by the coin, and the small article or product is sold by using a vending machine (Oxford University Press 1994, p. 568) [30]. In Japan, large quantities of products are also sold through vending machines; for example, a customer can buy ten kilogram rice bags from the vending machines. Credit card is used in the slot of card in the vending machine to pay for goods and services.
According to the National Automatic Merchandising Association (NAMA) in 
the United States, vend is the delivery of a single unit of merchandise (NAMA, www.vending.org, 2005). For vending, there is a slogan in the United States, “Coffee, Candy, Cola” (Cola” o. V. 1999 Coffee, Candy, Cola, p. 17ff) [31]. The term “Coffee” indicates hot drinks like hot chocolate, soup, and coffee. “Candy” illustrates the sweets, and “Cola” is used for different types of carbonated soft drinks. The early stages of the vending industry started with the concept of the 4Cs: cigarettes, coffee, candy, and cola. Once the consumer rate increased, this industry grew to 7Cs from the 4Cs concept. These 7Cs are candy, cold drinks, coffee, cigarettes, canned drinks, cold cups, chips (NAMA, www.vending.org/nama_vision/index.php?page =definitions, 2005) [32]. In Europe, extensive prod-
ucts are involved in the vending machine (EVA, European Vending Association) (EVA, www.eva.be, 2005) [33]. In Germany, daily use products, including food items, drinks, and and nonfood products, are sold using vending machines.
According to the Australian Association, vending is used to sell all products, 
including drinks, food, parking tickets, as well as photos. Apart from these prod-ucts, pin balls, slot machines, lockers, telephones, and copying machines are also 43 Extracts of Natural Tea Leaves and Other Ingredients
included (ÖVV, Österreichische Verkaufsautomaten Vereinigung, 2005) [ 34]. 
The vending machine is treated as a store in which all the products are stored for 
the retail trade industry. It follows an automatic procedure of selling the product.  
A customer just needs to select the item, pay for it, and collect the item. In the United States, for the selection of the product, the machine products appear behind the glass so that customers can select the product easily. A spiral mecha -
nism facilitates easy delivery of the product to the customer. And the product is dispensed at the bottom of the machine for delivery (NAMA, www.vending.org , 
2005) [ 35].
Therefore, vending is the term that is used to provide all the services and 
ease to the customers by collecting cash in different formats, such as cash, credit, and by text messaging as well as electronically. There are a few tea and coffee vending machines with simple working, such as the Morphy Richard Tea Maker and Philips HD 7450 [36]. These vending machines produce a very limited amount of beverages. The working principle of these vending machines is based on the requirement of users because they use plastic containers in which the tea and coffee powder is already mixed. The water reservoir is con -
nected to the heater just to mix the tea and coffee powder. According to the user’s desired quantity, the beverage is served to the user once the water gets heated and mixed with the powder. Generally, these vending machines are used in hotels, offices, and cafeterias, and all the operations performed in the vend-ing machine are further controlled by the electronic and mechanical system [37]. The interface is already decided in the specific part of the machine where 
the user has to give the desired input just to get the beverage. After complete registration of the user’s input, the timing circuit provides all the control while mixing the powder and adding the water, and the beverage is delivered to the main cup with the flow.
Xie et al., 2017 [ 38] have proposed a routing implementation of efficient energy 
that introduces the mobility of the objects with Wireless Sensor Networks (WSNs) for collection of information of mobile data. The term used for sensing moveable sensors is known as Mobile Data Collectors (MDCs). MDC starts the movement of data from its initial place-based station, transfers the data periodically to all the stations, and finally returns to the based station again. This whole movement of data from a station to another is decided using Floyd Warshall’s complete graph algorithm.
Chandra et al., 2017 [ 39] discuss techniques based on data-driven results that 
help improve increasing productivity yields in the agriculture field. Moreover, this technology has a high rate of manual information gathering in bare implementa -
tion and gives a limited connectivity result. The authors present an uninterrupted IoT platform known as FarmBeats for agricultural solutions to collect data from the various drones, cameras, and sensors. FarmBeats is a method that helps moni -
tor crop through the internet and has been implemented in 2017 in the two U.S. farms.
Miqdad et al., 2017 [ 40] developed a real-time low-cost WSNs monitoring system 
for sensing building space. The monitoring system collects data such as temperature, illumination, quality of air, and humidity. This data is taken from all corners of the 44 Machine Learning for Edge Computing
building to balance the amount of energy and client comfort. In the experiment, the 
authors used the NodeMCU module with a DHT11 sensor for measuring humidity and temperature. The authors used a 6-volt power supply source to provide supply to the module NodeMCU.
AlSkaif et al., 2017 [41] compared environment monitoring of smart cities 
in four specific areas, that is smart building/house, smart animal, agriculture farming, and municipal resilience. The main purpose of the research is to make a comparative analysis of Multiple Access Control (MAC) protocol and energy utilization in low-rate data Wireless Multimedia Sensor Networks (WMSNs). Comparatively, the analysis includes the multiclass traffic model to analyze the utilization of energy of MAC protocol in low data rates. Modieginyane et al., 2018 [42] reviewed the challenges in the application faced with environment monitoring by WSNs.
Sethi and Sahoo [43] have proposed an application based on Health Monitoring 
System (HMS). The application is purely based on WSNs, which help monitor medical information or records in real time. Apart from industrial uses, the appli -
cation is used to collect data from adult patients and intensive care units. In the experiment, the authors used ZigBee, Wi-Fi to transfer the data to the application.
Kaur et al., 2020 [44] discuss the effective role of IoT in the healthcare devices 
sector. IoT-based devices monitor the patient’s health status and track their activities according to the current time and situation. Further, a viewpoint is given in the field of electronics and technology [ 45] which is related to the Internet of Healthcare Things 
(IoHTs). Finally, the authors discuss the benefits of IoHTs in the medical sector.
3.7 OBJECTIVES
• Proposed a new tea vending machine that will prepare tea from the fresh tea leaves on the spot without adding any artificial preservatives to give tea lovers a healthy tea.
• The cleanly washed fresh leaves are kept in green/dry format in the trans -
parent jar from which tea powder is processed and directly sent to the exist -
ing tea vending machine.
• This type of machine does not exist in the market, and it will give a new experience to the end users.
3.8 ARCHITECTURE
Figure 3.2  gives the overview of the proposed machine; the functional units are 
described here.
1. Transparent Funnel:  This can store 250 grams of fresh/dry tea leaves; 
this container will have an automatic open and close window to release a specific amount of the tea leaves. The operator can set the amount of leaves that are released at a time. It is fast since the quantity of the leaves taken is very limited and we have fast grinders and driers to improve the overall speed of the process. 45 Extracts of Natural Tea Leaves and Other Ingredients
2. Grinding Unit:  This unit will crush and grind the leaves to the standard 
size of tea particles; it is a four-layer unit that will grind, crush, and filter the 
tea leaves. A soundproof system is optional to operate this grinder. 
3. Dryer:  It will dry all the leaves and remove moisture at the required tem -
perature. There will be a cooling unit to cool down the dried particles. This entire unit is optional since there will be two modes of operations: fresh leaves or dry leaves. This unit is only functional when fresh tea leaves are fed.
4. Tea Particles:  After grinding and drying, we will get tea. The granules of 
0.33 mm size were found to show 98% dissolution rate as compared to 68% in case of 1.99 mm granules. 
5. Collector:  It will collect all the tea particles in it. The collector size is vari -
able, and the capacity of what is collected will be based on the size of the tea vending machine. 
6. Flavors: We have three to four flavors with us. The flavors come from natu -
ral ingredients; the small grinding/crushing units act individually to add the individual flavors. 
7. Tea Vending Machine: It will work same as the existing tea vending 
machine. This machine will collect tea powder from the fresh leaves and will function according to the existing setup. Figure 3.3  provides an over -
view of the grinding and sieve units. Grinding unit is portable and custom -
izable and can be operated either as a separate one or can be embedded with the existing tea vending machine.
FIGURE 3.2  Design of the tea vendor.46 Machine Learning for Edge Computing
3.9 COMPLETE SYSTEM
Figure 3.4  depicts the wired diagram of the proposed model. There are two types of 
containers: one for processing normal tea leaves and other for processing the other 
leaves, such as jamun, papaya, mango, lemon, guava, tulsi, mint, lemon grass, and hibiscus for various health benefits. This machine can make tea in various combina-tions, such as normal tea, naturally flavored tea, tea for health with the help of other natural leaves, tea with sugar/without sugar, and tea with milk/without milk.
1. Collection Unit:  This unit collects the required ingredients to make tea 
according to the specifications.
2. Grinding Unit:  This unit grinds the leaves ingredients as per the required sizes. 
3. Distribution Unit:  This unit takes the responsibility of dispersing the pow -
der/sugar/milk ingredients and water. 
4. Processing Unit:  This is the central unit, which has a processor/memory 
and controls for the actuators that function the complete system. This unit is attached with the IoT Section to transmit the numbers to the cloud servers.
3.10 WORKING UNITS
As shown in Figure 3.4 various flavors are added instantly in the vending machine as labeled 1-3 in the figure. The  natural flavors  from ingredients are processed instantly thorough these unit. Label 4-7 are the feedback system used in the machine.  
FIGURE 3.3  Grinding and mixing unit.47 Extracts of Natural Tea Leaves and Other Ingredients
The feedback system consists of a set of feedback buttons, which represent scales 
such as Excellent, Very Good, Good, and Poor. 
1. IoT Transmitter:  This transmitter sends the processed data to the cloud; 
the framework of data packet is i) Name: Ginger Tea; ii) Quantity: 2;  
iii) Date Time: 23/11/2019 11.04 A.M.; iv) Place: Chitkara University, 
Punjab, India; v) Feedback: Excellent. This data is sent to the cloud for 
analyzing the patterns. 
2. Cloud Server:  The cloud server will receive the data from this tea vend -
ing machine, which will be of huge amount since tea vending machines 
are a source of data generation, and will use for predicting future patterns, 
such as people’s interests, the movement pattern of various kinds of teas, 
business predictions, advertisement effects, and other types of predictions. 
Predictions can be made with the available data sets and feedback can be 
analyzed intensively to improve the overall quality. 
3. Printing Unit:  This unit will give the health benefits of having various 
kinds of teas, and a small prediction of future health benefits when people 
continue using the same tea for a week, a month, or a year. The health ben -
efits and good effects of various leaves, such as a reduction in glucose level, 
blood pressure control, weight loss, and others will attract more customers 
when things are printed and given to them on the spot.
FIGURE 3.4  Complete design of tea vending machine.48 Machine Learning for Edge Computing
3.11 MACHINE COMPONENTS
3.11.1 s ervo motor
The vending machine uses a servo motor as an electronic component. The servo 
motor consists of a rotary and linear actuator to control velocity, acceleration, 
and angular position. For a particular position, feedback sensor is connected with the motor and controller, but a specific design module is required to use a servo motor.
3.11.2 Ardu Ino uno mIcrocontroller
AT mega328P is the microcontroller-based board Arduino Uno. It uses 14 input 
and output digital pins. Out of 14 pins, 6 are used as the analog inputs, 2 are used as an ICSP header and a reset button, whereas the other 6 are used as Pulse Width Modulation (PWM) outputs. Apart from that, the reset button, power jack, In-Circuit Serial Programming (ICSP) header, Universal Serial Bus (USB) con -
nection, and 16 MHz quartz crystal are connected with the computer, either via an AC to DC adapter or a battery just to get started [ 46].
3.11.3 WAter heAtIng unIt
It consists of the following units:
1. Water heating element
2. Cut-off thermostat
3. Solenoid valve
The resistive unit is used for the water heating element. The heating coil is heated when the electric current is passed through it, and with the coil, water presents in the bottle gets heated. The thermostat in the heating unit ensures that the temperature does not increase above the threshold value. The main objective of the solenoid valve is to ensure that the required quantity of water reaches the cup, and valve closes and opens on time.
3.11.4 c ontAIner
Food-grade plastic steel and food-grade plastic [ 47] are used to make containers 
[48]. Material in the cup contain a powder mix that arranged and controlled by the 
container. The quantity of the cup is controlled by using the crank mechanism. The container contains the prefix powder, and its quantity is 154 ml.
3.11.5 ApplIAnce body
Mild steel is used to make the appliance body. It also provides the structure and sup -
port to all the components [ 49]. Material selection is described in Table 3.1 .49 Extracts of Natural Tea Leaves and Other Ingredients
3.12 INNOVATION AND SPECIFIC FUNCTIONS PERFORMED
Table 3.2 lists various innovation and functions.
3.13 FUTURE ENHANCEMENT
There are many automatic tea vending machines, but they use tea powder as 
the ingredient. In general, tea powders have a lot of artificial flavors and preser -
vatives and are not good for health. Further, consumers do not have any idea of the freshness of tea powders because they feel fresh after taking the tea. Coffee vending machines are available today that prepare coffee from the beans, but they do not require any drier since coffee beans are available as dried beans. In case of tea, there is powder in the unit which is refilled when gets consumed com -
pletely. In future work, there should be a mechanism that will show the container  
is about to become empty and it should be filled. In the proposed machine, our unit has TABLE 3.1
Description of Material Selection
Sr. No. Name of Components Material Used
1. Container Food-grade steel or plastic
2. Servo motor Standard component
3. Connecting rod Mild steel or plastic
4. Crank Mild steel or plastic
5. Body Mild steel
6. Heating coil Nichrome
Source:  Adapted from [50].
TABLE 3.2
Description of Functions
Sr. No. Innovation and Specific Function Performed
1. Freshness
2. No loss of natural flavor3. No loss of natural flavor4. Replace of old powders5. Fast6. Portable7. Customisable8. Can be fixed with the existing tea vending machines9. Natural flavors without any preservatives
10. Healthy11. Global market12. Corporate offices/institutions/hospitals/markets/shopping  
malls/airports/railway stations50 Machine Learning for Edge Computing
been modified, and in this, we have added a drier/heater unit that will complete the 
process of preparing fresh tea. It will also provide a health monitoring assistant that  
can further work as a recommendation system and can give users extra features  
that currently do not exist in the existing system.
3.14 C ONCLUSION
Our proposed model is completely IoT-enabled, and we take advantage of cloud and AI support to improve the efficiency of the overall process. The system can col -
lect user input such as Age: 38, Sex: Male/Female, Weight: 80 kg, Smoking: Yes/No, Drinking: Yes/No, and using AI, it will suggest the type of tea the user can take for better health benefits. This predictive model and suggestive tea vending sys -
tem is very unique and not found in any existing systems. Additionally, some extra functionality can be added as a future scope so that consumers can gain additional benefits from the proposed machine.
REFERENCES
1.Data Transfer Standard EVA DTS 6.1.1, Dec 2010, European Vending Association
AISBL, 44 Rue Van Eyck – 1000 Brussels Alharbi.
2.Multi-Drop Bus/Internal Communication Protocol Version 4.2, February, 2011, National
Automatic Merchandising Association, 20 N. Wacker Drive, Suite 3500 Chicago, Illinois
60606-3120 USA.
3.T. Yokouchi (2010). Today and Tomorrow of Vending Machine and Its Services in
Japan. Proceedings of IEEE, Service Systems and Service Management (ICSSSM),
7
th International Conference, pp. 1–5, doi: 10.1109/ICSSSM.2010.5530240
4.Y. Park & S. Yoon (2011). A Comparison Study of Stock-Out Policies in Vending
Machine Systems. Proceedings of IEEE, Engineering and Industries (ICEI), International Conference, pp. 1–4.
5.T. C. Poon, K. L. Choy, C. K. Cheng, & S. I. Lao (2010). A Realtime Replenishment
System for Vending Machine Industry. Industrial Informatics (INDIN). 8th IEEEInternational Conference, pp. 209–213, July 10, doi: 10.1109/INDIN.2010.5549432
6.L. Atzori, A. Iera, & G. Morabito (2010). The Internet of Things: A Survey. Computer
Networks, 54, pp. 2787–2805.
7.Z. Wen & Z. X. Long (2010). Design and Implementation of Automatic Vending Machine 
Based on the Short Massage Payment. Proceedings IEEE, Wireless CommunicationsNetworking and Mobile Computing (WiCOM). 6th International Conference, pp. 1–4,doi: 10.1109/WICOM.2010.5600192
8.M. Jovanovic & M. Organero (2011). Analysis of the latest trends in mobile commerce
using the NFC technology. Journal of Selected Areas in Telecommunications (JSAT),1–12.
9.C. Wenshan, H. Yanqun, & L. Minyang (2015). Influential Factors of Vending Machine 
Interface to Enhance the Interaction Performance. 8th International Conference onIntelligent Computation Technology and Automation (ICICTA), IEEE.
10.V. Vaid (2014). Comparison of Different Attributes in Modeling a FSM Based Vending
Machine in 2 Different Styles. International Conference on Embedded Systems (ICES), IEEE.
11.K. Kwangsoo (2014). Smart Coffee Vending Machine Using Sensor and Actuator
Networks. IEEE International Conference on Consumer Electronics (ICCE), IEEE.51 Extracts of Natural Tea Leaves and Other Ingredients
 12. PSG College of Technology (2007), Design Data Book, Coimbatore.
 13. M. Zhou, Q. Zhang, & Z. Chen (2006). What Can Be Done to Automate Conceptual 
Simulation Modelling? Proceedings of the 2006 Winter Simulation Conference, pp. 
809–814. International Journal of VLSI design & Communication Systems (VLSICS), 
3(2), April 2012, p. 27.
 14. B. Roy & B. Mukherjee (2010). Design of Coffee Vending Machine Using Single 
Electron Devices, Proceedings of 2010 International Symposium on Electronic System 
Design, pp. 38–43.
 15. C. J. Clement Singh, K. Senthil Kumar, Jayanto Gope, Suman Basu, & Subir Kumar 
Sarkar (2007). Single Electron Device based Automatic Tea Vending Machine. 
Proceedings of International Conference on Information and Communication 
Technology in Electrical Sciences (ICTES 2007), pp. 891–896.
 16. P. Smith (1997). Automatic Hot-Food Vending Machine. Trends in Food Science & 
Technology, 81, October 1997, p. 349.
 17. M. Zhou, Y. J. Son, & Z. Chen (2004). Knowledge Representation for Conceptual 
Simulation Modeling. Proceedings of the 2004 Winter Simulation Conference,  
pp. 450–458.
 18. J. Komer (2004). “Digital Logic and State Machine Design,” 2nd ed., Oxford.
 19. M. Ali Qureshi, A. Aziz, & H. Faiz Rasool (2011). Design and Implementation of 
Automatic Ticket System Using Verilog HDL. Proceedings of International Conference 
on Information Technology, pp. 707–712.
 20. S. Kilts (2004). “Advanced FPGA Design: Architecture, Implementation, and optimi -
zation,” Wiley-IEEE Press.
 21. S. B. Z. Azami & M. Tanabian (2004). Automatic Mobile Payment on a Non-Connected 
Vending Machine. Proceedings of Canadian Conference on Electrical and Computer 
Engineering, pp. 731–734.
 22. A. Ayman (2020). Vending Machine for Smart Gifting Under-Privileged People. 
International Journal of Scientific Development and Research (IJSDR), 5(1), 8–14.
 23. L. P. Aljadir, W. M. Biggs, & J. A. Misko (1981). Consumption of Foods from 
Vending Machines at the University of Delaware. Journal of American College Health 
Association, 30(3), pp. 149–150.
 24. I. Baranovski, S. Stankovski, G. Ostojic ́, D. Oros, & S. Horvat (2018). Software Support 
for Self-Service Automated Systems”. 17th International Symposium INFOTEH-
JAHORINA (INFOTEH), pp. 1–4. IEEE.
 25. R. Kondo, I. Harashima, & D. Sunouchi (1989). Automatic Coffee Vending Machine 
Being Able to Serve a Straight Coffee and a Blended Coffee Selectively, US Patent 
4,815,633.
 26. M. A. Matthews & T. M. Horacek (2015). Vending Machine Assessment Methodology. 
A Systematic Review. Appetite, 90, pp. 176–186.
 27. S. A. New & M. B. E. Livingstone (2003). An Investigation of the Association 
between Vending Machine Confectionery Purchase Frequency by Schoolchildren 
in the UK and Other Dietary and Lifestyle Factors. Public Health Nutrition, 6(5), 
pp. 497–504.
 28. N. T. Nguyen, X. T. Nguyen, J. Lane, & P. Wang (2011). Relationship between Obesity 
and Diabetes in a US Adult Population: Findings from the National Health and Nutrition 
Examination Survey, 1999–2006. Obesity Surgery, 21(3), pp. 351–355.
 29. C. Ogden, M. Carroll, B. K. Kit, & K. M. Flegal (2014). Prevalence of Childhood and 
Adult Obesity in the United States”, 2011–2012. Jama, 311(8), pp. 806–814.
 30. M. A. Qureshi, A. Aziz, H. Rasool, M. Ibrahim, U. Ghani, & H. Abbas (2011). Design 
and Implementation of Vending Machine Using Verilog HDL. 2nd International 
Conference on Networking and Information Technology, IPCSIT, volume 17.52 Machine Learning for Edge Computing
 31. S . Saydah, K. M. Bullard, Y. Cheng, M. K. Ali, E. W. Gregg, L. Geiss, & G. Imperatore 
(2014). Trends in Cardiovascular Disease Risk Factors by Obesity Level in Adults in the 
United States. NHANES 1999–2010, Obesity, 22(8), pp. 1888–1895.
 32. S . Stankovski, G. Ostojic ́, L. Tarjan, M ˇ. Stanojevic ́, & M. Babic ́ (2019). Challenges of 
IoT Payments in Smart Services. Annals of DAAAM & Proceedings, 30.
 33. S . Tegeltija, B. Tejic ́, I. Sˇenk, L. Tarjan, & G. Ostojic (2020). Universal IoT Vending 
Machine Management Platform. 19th International Symposium INFOTEH-JAHORINA  
(INFOTEH), pp. 1–5. IEEE.
 34. Br anislav Tejic ́, Sran Tegeltija, Sabolc ˇ Horvat, Miroslav Nic ́in, Milos S ˇtanojevic ́, & 
Mladen Babic (2019). Payment Methods in Vending Machines. Journal of Mechatronics, Automation and Identification Technology, 4(3), pp. 20–25.
 35. N AMA Vision/Industry Definitions, in: www.vending.org/nama_vision/index.php ? 
page=definitions, August 8, 2005, CET 16:47.
 36.  E VA, European Vending Association: www.eva.be , August 3, 2005, CET 11:13.
 37. Ö VV, Österreichische Verkaufsautomaten Vereinigung: www.ovv.at , June 16, 2005, 
CET 14:07.
 38.  G . Xie, K. Ota, M. Dong, F. Pan, & A. Liu (2017). Energy-Efficient Routing for Mobile 
Data Collectors in Wireless Sensor Networks with Obstacles. Peer-to-Peer Networking 
and Applications , 10(3), pp. 472–483.
 39.  D . Vasisht, Z. Kapetanovic, J. Won, X. Jin, R. Chandra, S. Sinha, &, S. Stratman 
(2017). FarmBeats: An IoT Platform for Data-Driven Agriculture. 14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17),  
pp. 515–529.
 40.  A . Miqdad, K. Kadir, & S. F. Ahmed (2017). Development of Data Acquisition System 
for Temperature and Humidity Monitoring Scheme. 2017 IEEE 4th International 
Conference on Smart Instrumentation, Measurement and Application (ICSIMA) ,  
pp. 1–4. IEEE.
 41.  T . AlSkaif, B. Bellalta, M. G. Zapata, & J. M. B. Ordinas (2017). Energy Efficiency 
of MAC Protocols in Low Data Rate Wireless Multimedia Sensor Networks:  
A Comparative Study. Ad Hoc Networks , 56, pp. 141–157.
 4 2.  K . M. Modieginyane, B. B. Letswamotse, R. Malekian,  & A. M. Abu-Mahfouz (2018). 
Software Defined Wireless Sensor Networks Application Opportunities for Efficient Network Management: A Survey. Computers & Electrical Engineering , 66, pp. 274–287.
 43.  S . Sethi & R. K. Sahoo (2019). Design of WSN in Real Time Application of Health 
Monitoring System. In “ Virtual and Mobile Healthcare: Breakthroughs in Research 
and Practice ,” pp. 643–658. IGI Global.
 44.  H . Kaur, M. Atif, & R. Chauhan (2020). An Internet of Healthcare Things (IoHT)-
Based Healthcare Monitoring System. In “ Advances in Intelligent Computing and 
Communication ,” pp. 475–482. Springer.
 45. V . Kukreja & P. Dhiman (2020, September). A Deep Neural Network Based Disease 
Detection Scheme for Citrus Fruits. International Conference on Smart Electronics and Communication (ICOSEC), pp. 97–101. IEEE.
 46. B DV, Bundesverband der Deutschen Vending Automaten-Wirtschaft e.V.: www.  
bdvonline.de , June 16, 2005, CET 13:54.
 47. H . Diller, (2001). Vahlens Großes Marketinglexikon, Verlag C. H. Beck, Munich 2001, 
p. 1830.
 48. N AMA Vision/Industry Definitions, in: www.vending.org , August 3, 2005, CET 12:19.
 49. N . Monssen (1999). Vending – Ein Markt mit Zukunft. BDV (Bundesverband Deutscher 
Verpflegungs- und Vending-Unternehmen e. V.) (Hrsg.), Köln.
 50.  H . M. Jungbluth (2002). High-Tech Contra Anonymität. In: gv-praxis Nr. 9, September 
4, 2002, p. 64 (translated by authors).53 DOI: 10.1201/9781003143468-4
Recent Trends in OCR 
Systems: A Review
Aditi Moudgil , Saravjeet Singh , and Vinay Gautam
Chitkara University Institute of Engineering &  
Technology, Chitkara University  Punjab, India4
CONTENTS
4.1 Introduction  .................................................................................................... 53
4.1.1  Application Areas of OCR  ................................................................. 54
4.1.2  Working of OCR System  .................................................................... 54
4.1.3  Various Issues Encountered in OCR  .................................................. 55
4.1.4  Potential Areas for Research  .............................................................. 56
4.1.5  Segmentation  ...................................................................................... 57
4.1.5.1  Page Segmentation  ............................................................... 57
4.1.5.2  Line Segmentation  ............................................................... 57
4.1.5.3  Word Segmentation  .............................................................. 58
4.1.5.4  Character Segmentation  ....................................................... 58
4.2 Literature Review  ........................................................................................... 58
4.2.1  Page Segmentation  .............................................................................. 58
4.2.2  Line Segmentation  .............................................................................. 58
4.2.3  Word Segmentation ............................................................................. 59
4.2.4  Character Segmentation  ...................................................................... 60
4.3 Discussion and Analysis  ................................................................................. 63
4.4 Conclusion  ...................................................................................................... 65
References  ................................................................................................................ 65
4.1 INTRODUCTION
Optical character recognition (OCR) is one of the most emergent applications of pattern recognition. It is a fully grown, advancing, energizing, and helpful field and is going about as an establishment stone for emerging advanced applications, such as those in the medical field, speech recognition, biometric recognition, etc. (Babu and Soumya, 2019). OCR is a technique that is used to convert scanned images like handwritten text or PDF files into an editable format. In a nation like India, a plenitude of data and invaluable information is available as original copies, old writings, books, manuscripts, family data, and so on. They are customarily accessible either in printed or handwritten formats, and such literature becomes inefficient when information is searched among thousands of pages. Today’s era is an advanced era that accepts all the documents and 54 Machine Learning for Edge Computing
available information in a digitized form. Thus, the process of digitizing such lit -
erature came to the rescue. The conversion of digitized information into a textual 
form becomes a challenging task, but it is mandatory to make information read-able by any machine processing a million pages per second. When it comes to analyzing manuscripts that might not even be in readable form and the text is deteriorated, it becomes even more challenging to get that precious text and make it available. The old documents that sometimes look blurry, have been inked over, or having missing characters, making them difficult to read, often use character recognition, which is an art of segregating characters from the complete word. The next section shows the application areas of OCR. A sample of Devanagari script is shown in Figure 4.1 .
4.1.1 ApplIcAtIon AreAs of ocr
• Digitizing libraries
• Legal billing documentation
• Data extraction
• Language translation
• Postal address recognition
4.1.2 WorkIng of ocr s ystem
Figure 4.2  represents a typical model for the OCR system. The main phases 
of OCR include image acquisition (collection of images and their digitiza -
tion), pre-processing (enhancement, noise removal, slant, and skew correction), 
FIGURE 4.1  Devanagari sample manuscript.55 Recent Trends in OCR Systems: A Review
segmentation (can be classified as page segmentation, line segmentation, word 
segmentation, and character segmentation), feature extraction (multiple tech-niques have been implemented, such as zoning, aspect ratio, directional features, projection features, etc.), classification (various classification techniques, such as Multi Layer Perceptron (MLP), Convolutional Neural Network (CNN), Support Vector Machine (SVM), Artificial Neural Network (ANN), Binary tree, K-nearest neighbor (KNN), etc.), and post-processing. Figure 4.3  shows the detailed process 
of document analysis.
4.1.3 v ArIous Issues encountered  In ocr
1. Noisy and degraded documents make pre-processing a complex task.
2. Touching and overlapping characters make segmentation difficult.
3. Thick and uneven characters make segmentation and recognition very difficult.
4. Faded documents make recognition very complex.
FIGURE 4.2  Working model of a typical OCR.56 Machine Learning for Edge Computing
5. Collection of the database, being a manual task, is tedious.
6. Feature extraction and classification techniques must be chosen with care 
and after thorough experimentation.
4.1.4 p otenti Al AreAs for reseArch
Manuscripts are invaluable assets of a community that contain valuable information. The data preserved in these manuscripts is a treasure that can be extracted to be implemented in our daily lives. The data present is degraded due to certain reasons such as torn pages, overwriting, ink stains, missing headlines, etc.
•
 To make the data readable, different mechanisms can be employed that motivate the research idea of deploying a system to recognize ancient characters.
•
 Many of the systems have already been built up to drill down on the ancient characters, thus making it easy to read manuscripts in different languages, such as Devanagari, Sanskrit, Telugu, Malayalam, English, Gurmukhi, Chinese, etc.
•
 Various feature extraction techniques, such as context information, trans-fer learning, dataset augmentation, zonal features, shape transformations, 
FIGURE 4.3  Document analysis and recognition.57 Recent Trends in OCR Systems: A Review
curvelet transformations, wavelet transform coefficients, etc., have been 
carried out to achieve higher accuracies.
• Also, various classifiers, such as ANN, CNN, feed-forward neural network, 
SVM, KNN, Maximum Mean Discrepancy (MMD), MLP, and Ada boost, 
have already been used, but a combination of any of the above-mentioned 
features and classifiers can be used to optimize the results of character 
recognition.
• The variability of datasets can also be considered as one of the factors to 
improve the character recognition rate.
4.1.5 s egment AtIon
Segmentation, in general terms, means to decompose the image into sub-images 
so that the background and foreground pixels can be separated. The division stage 
isolates the original copy picture in different legitimate parts. This is the first and 
foremost step to develop an OCR, and if it is not done correctly, all other levels of 
accuracy are affected. So, this step should be carried out most carefully followed by 
post processing (Kumar et al., 2015). Segmentation is further divided into multiple  
phases:
1. Page segmentation
2. Line segmentation
3. Word segmentation
4. Character segmentation
4.1.5.1  Page Segmentation
Kumar et al. (2015) in 2013 defined content zones and non-content zones in a 
report picture. Page division is used to segment content zones from non-content 
zones. Saini et al. (2019) proposed to have a competition based on the Chinese 
Historical Reading Challenge in which coders were told to finally make a system 
that can read Chinese family records. For this evaluation, a dataset consisting of 
more than 10,000 pages was provided by family search. The extent of this chal -
lenge was to fragment the page into multiple classes by allotting an alternate pixel 
value for each class.
4.1.5.2  Line Segmentation
Line segmentation means splitting the manuscripts into lines in which a top-down 
approach uses the concept of larger manuscripts that are further split into smaller 
pieces, which makes it easy to segment lines, whereas the bottom-up approach 
uses the opposite concept, where smaller pieces are segmented for lines of text and 
are gathered to form a larger manuscript, thus providing better results. The hybrid 
approach is a mixture of top-down and bottom-up approaches, which gives the best 
results (Narang et al., 2019a).58 Machine Learning for Edge Computing
4.1.5.3  W ord Segmentation
Word segmentation is a process of separating words from lines that were seg -
mented via line segmentation. Specifically, a word is a collection of multiple 
characters.
4.1.5.4  C haracter Segmentation
Character segmentation is the last step performed by OCR in which each character is 
segmented followed by the recognition of characters. Section 4.2 shows the literature 
review for manuscript recognition.
4.2 L ITERATURE REVIEW
This section covers the review of different techniques used in different parts of OCR models.
4.2.1 p Age segment AtIon
Kumar et al. in 2013 defined content zones and non-content zones in a report 
picture. Saini et al. (2019) proposed to have a competition based on the Chinese Historical Reading Challenge in which coders were told to finally make a system that can read Chinese family records. For this evaluation, a dataset consisting of more than 10,000 pages was provided by family search. The extent of this chal -
lenge was to fragment the page into multiple classes by allotting an alternate pixel value for each class.
4.2.2 l Ine segment AtIon
Completely isolated lines are really easy to find, but problems can arise for the fol -
lowing reasons: 
• Skewed lines
• Overlapping lines
• Touching components
• Curvilinear lines
• Broken characters
• Lines are degraded due to noise
There are three approaches followed for line segmentation:
1. Top-down approach
2. Bottom-up approach
3. Hybrid approach
Arivazhagan et al. (2007) proposed a new technique with the concept of painting. The document was decomposed into vertical stripes out of which each row was 59 Recent Trends in OCR Systems: A Review
painted with the intensity pixel values. Piece-wise potential separation of lines was 
done. The partial projection was used to detect the line number and skewness of the 
lines. Also, horizontal borders were provided to segment the text. The contouring 
method was carried out in the same direction in which the text was written and also 
in the opposite direction.
Arivazhagan et al. (2007) proposed a method for skewed lines that are also some -
times overlapping and worked on Arabic documents. The author traversed the lines 
around the handwritten documents. Zahour et al. (2001) used a partial contour-based 
method to segment lines from manuscripts. Liwicki et al. (2007) used the concept of 
dynamic programming for text line detection and separation. This novel approach 
focused on studying the text strokes for both online and offline handwritten texts that 
used an evade function and added a forfeiture if the identified path was detected clos -
est to the strokes. Misclassified strokes were reported, and thus, text line detection 
proved to be a difficult task.
4.2.3 Word segment AtIon
As explained earlier, we can separate words from the complete lines using the following 
two methods. 
1. Distance-based method
2. Recognition-based method
The distance-based approach is based on calculating the distance between two words 
or connected components. Measurement of this distance metric can be done using 
various units, such as Euclidian distance (Louloudis et al., 2009), convex hull metric 
(Lelore and Bouchara, 2011), run-length distance, etc. Based on the distance calcula -
tion, the metric units are classified as inter-word or intra-word distances. Whereas, in 
the recognition-based method, boundaries are provided to each word, thus making it 
easy to recognize.
Angadi and Kodabagi proposed an innovative technique to segment Kannada 
historical documents into lines following by word and character segmentation. The 
proposed technique works even for modifiers and overlapping lines. Extraction of 
the word made the use of k-means algorithm that calculated the threshold value to 
identify words. Accuracies up to 99% were achieved in all three fields (Angadi and 
Kodabagi, 2014).
Manmatha and Rothfeder (2005) evidenced that for character segmentation, 
the first and foremost step should be line segmentation followed by word seg -
mentation. The authors worked on historical manuscripts in which they faced a 
great challenge in background noise and other aspects. Cleaning the document was 
done by removing margins. A projection profile algorithm was used to segment 
the documents into lines, thus producing smaller blobs that may result in bigger 
partitions called words, and smaller partitions called characters (Manmatha and 
Rothfeder, 2005). Saha et al. (2010) wrote literature in which the ultimate goal 
was to segment the words into characters. A Hough-based efficient method was 
designed for standardized performance. The new technique published was applied 60 Machine Learning for Edge Computing
to low-quality camera captured images. The accuracy measured for word segmen -
tation was 85.7% (Saha et al., 2010).
4.2.4 c hArActer segment AtIon
Character segmentation is the last step performed by OCR in which each character 
is segmented followed by the recognition of characters.
Sonika Narang in 2019 presented a paper in which the author designed an 
ancient manuscript recognition system able to recognize the Devanagari script, 
which is currently in fragile condition. In this paper, the author proposed a sys -
tem that was based on the following steps: digitization, pre-processing, segmen -
tation, feature extraction, and classification. In the first step, the author converted 
the hard copy of the manuscripts into electronic form. In the next step of pre-
processing, two conditions were applied: using the autocorrect feature of Office 
Image Viewer, incomplete words were attempted to be completed; secondly, 
binarization was done that converted the electronic image into binary format. In 
segmentation, line was segmented to form words, and words were further broken 
down to form characters that could be later on processed to form a complete 
recognition system. Vertical and horizontal projection methods were applied. 
For character recognition, the headline also called Shirorekha was separated and 
then the overlapping characters were figured out. The threshold value calculated 
in the previous step was used to compare the aspect ratio to find the overlapping 
characters (Narang et al., 2019b).
Narang et al. (2019) worked on the dataset consisting of 100 pages written in 
Devanagari and followed multiple steps to make the ancient data readable with the 
highest accuracy and reliability. With the pre-segmented set of images, the steps fol -
lowed for this work include feature extraction followed by the classification method, 
which is responsible for maximizing the accuracy, specificity, and sensitivity of data. 
In the last step, the author tried to improve the accuracy by using AdaBoost and bag -
ging methods to improve the recognition results. Implementing these methods sepa -
rates the high- and low-frequency components, out of which a few low-frequency 
components are meaningful that are selected as features. Classification is a very 
significant method that identifies to which class an image will belong (Narang et al., 
2019a).
Kumar et al. (2018) presented an OCR to recognize Devanagari manuscripts col -
lected from different libraries and museums. The process worked out in different 
phases. In the first phase of digitization, the image was converted into a grayscale 
image followed by a segmentation phase, in which the pages were segmented into 
lines followed by words and finally characters. For line segmentation, the page was 
divided into vertical lines and the average height was calculated, which resulted in 
the identification of under-segmented and over-segmented lines. The next step was 
word segmentation, which was based on the black color pixel intensity. Character 
segmentation proved to be the most difficult step as it was done without removing the 
shirorekha. For this, the author developed a new method called the drop flow method. 
This step took place in multiple iterations for which a hypothetical drop of water was 
forced from downward to upward; if the drop of water finds its way to move up, then 61 Recent Trends in OCR Systems: A Review
there was no shirorekha, but on the other hand, if the water drop finds no way up, 
then there may be a case of touching characters or shirorekha. For the recognition of 
touching characters, an accuracy of about 96% was obtained (Narang et al., 2019b). 
M. Buchler emphasized Coptic texts – ancient Egyptian scripts. Two different fonts 
of Coptic texts were experimented upon (Bohairic and Sahidic). The character sets 
that have been experimented with were taken from several published manuscripts 
and the problems faced were the different frequency of characters, punctuations, 
page breaks, and line indications. The Coptic font pages were divided into testing 
and training data such that each page was considered as a part of training data. 
For pre-processing, the ScanTailor method was used to eliminate the borderlines 
and page cutting. The output of pre-processing was tested by the coptologist, and a 
transcript was generated. The transcript generated was proofread and the necessary 
corrections were made. Thirty thousand training steps were carried out to train the 
system. Testing pages were evaluated based on the training data and an accuracy of 
greater than 99% was achieved after following the above-mentioned steps (Narang 
et al., 2019a). Palakollu et al. (2012) presented a new method for the recognition of 
Devanagari documents. In this method, for line segmentation, an average line-height 
of 30 pixels (px) was considered due to the variation in the handwriting of different 
people.
Here, line segmentation was based on the horizontal projection method. The word 
segmentation process was considered somewhat easier as the minimum 3 px dis -
tance was considered in-between two words. In zone segmentation, the actual header 
line was compared to the expected header line, and subsequently, the header line 
was straightened and the three segments were separated – upper modifier, header 
line, and other characters. As a result, an accuracy of 93.6% was figured out while 
segmenting a line; a subsequent accuracy of 98.6% was calculated while segmenting 
a word (Palakollu et al., 2012).
Pengcheng et al. (2017) worked on Chinese calligraphy, which is written in 
multiple styles and is not recognizable easily except for the new learners. This paper 
proposed a system that automatically examines Chinese calligraphy. Three types of 
feature extraction techniques have been used in this field, namely, the global feature 
Global Image Descriptor (GIST), local feature Scale-Invariant Feature Transform 
(SIFT) descriptor, and scale-invariant feature transform technique. Also, three dif -
ferent deep feature extraction classifiers have been used – Convolutional Neural 
Network (CNN), Support Vector Machine (SVM), and neural network. These three 
classifiers are made to implement on pre-existing datasets, the unconstrainted real-
world calligraphic character dictionary (CCD) consisting of historical calligraphies, 
and the standard calligraphic character library (SCL). CCD consists of more than 
110,000 character images while SCL consists of 18,770 character images. When 
these tests were done on two different datasets, an accuracy of 99.78% was achieved 
on the SCL dataset, and comparatively a lower accuracy of 94.22% was achieved on 
the CCD dataset (Pengcheng et al., 2017).
Yang et al. (2018) proposed a new method named recognition guided detector 
(RGD) to recognize dense and tight characters in Chinese historic documents. 
The architecture of this method divides it into three parts, text line segmenta -
tion, proposal generation, and character-level detection. The input image is first 62 Machine Learning for Edge Computing
segmented into text lines using vertical projection. The recognition guided proposal 
method) is used for character generation from the segmented text lines. Finally, the 
précised boundary box around the characters is obtained by using RGD. Irrespective 
of many approaches, the vertical projection approach has been used for text line seg -
mentation where each input image is divided into vertical text lines to avoid confu -
sion. The experiments were performed on two datasets: the Multiple Tripitaka in Han 
(MTH) and Tripitaka Koreana in Han (TKH) dataset, respectively. For character-  
level detection, the authors developed a new approach to RGPN based on multiple 
layers of CNN.
Mahto et al. (2015) present a new technique for skewed word detection and 
correction. If the aspect ratio of the word comes out to be less than 0.7%, then 
the word is considered skewed, and for its correction, different algorithms are 
followed. The word is rotated clockwise and anticlockwise, and finally, the height 
is calculated. The point where the minimum height was calculated is noted, and 
the same is done for word width as well. The slope of the word is used to deter -
mine the skewness, and as a result, the word is straightened. Clanuwat et al. in 
2018 worked on applying machine learning techniques on three datasets, namely, 
Kuzushiji (cursive Japanese), Kuzushiji-49, and Kuzushiji-Kanji from classical 
Japanese literature. Multiple machine learning techniques were applied such as 
the KNN, CNN, and manifold methods, and the expected outcome was measured, 
which was lying between 95–99% depending on the dataset experimented upon 
(Clanuwat et al., 2018).
Nguyen et al., 2017 worked on the query by string approach to recognize 
keywords in Japanese manuscripts and used the CNN method for feature extrac -
tion and accuracy measurements. First, the Japanese documents were converted 
into images to apply the CNN method further, and the results were compared 
with the papers using the query by example approach. The paper shows that the 
query by string method is more accurate when compared to the state-of-the-art  
methods. The authors tried to read the deformed characters of Japanese man -
uscripts by applying the procedure in three levels. For level one, CNN in a 
combination of 1-dimensional Long Short-Term Memory (LSTM) has been 
implemented in 20 epochs, followed by level two, in which CNN in combina -
tion with 2-dimensional Bidirectional LSTM (BLSTM) was applied recurrently 
to improve the performance. In level three, CNN without pre-segmentation was 
applied with 2-dimensional BLSTM. Rajithkumar et al., (2015) proposed the 
character recognition rate of stone inscriptions in Kannada by following the 
steps given ahead.
Stone inscriptions were captured; a noise removal technique was applied to 
filter out the noisy images followed by edge detection, and a thickening of edges 
was done to read the text properly. Mean, standard deviation, and the sum of 
absolute difference algorithm were applied to 40 datasets consisting of 16 char -
acters to achieve an accuracy of 98.75%. Madhavaraj et al. (2014) developed an 
OCR that was able to effectively segment the characters of merged old Kanadda 
manuscripts. Correlation and discrete wavelet transform feature extraction tech -
niques were used and the appropriate dataset was divided into test and train 
data. The SVM algorithm was implemented, which proved to be 91.2% accurate. 63 Recent Trends in OCR Systems: A Review
Mohana and Rajithkumar (2014) developed a new technique called the Advance 
Recognition Algorithm (ARA) to recognize Kannada stone inscriptions from Hoysala and Ganga periods. The ARA method was used to determine the era in which a stone was inscribed by recognizing the font, shape, and size of the stone inscriptions of medieval periods. An accuracy rate of almost 100% was achieved.
Sandhya and Krishnan (2016) discussed the problems for the degradation of 
old texts and tried to solve the problem of degraded text due to broken characters while rebuilding the broken characters. Old manuscripts in Kannada were collected. Sandhya and Krishnan extracted the zonal features while applying the neural net -
work technique to complete the broken characters. Around 50 features were worked upon. After rebuilding, a recognition accuracy of 98.9% was achieved. The work was carried out on synthetic datasets. Fischer et al. (2010) developed ground truth creation for German manuscripts on some pre-defined German databases. Text areas bounded with polygons were selected, and the difference of Gaussians method was applied for binarization. Based on horizontal inclination, text lines were segmented along with the skewness adjustment. The text was manually corrected using the Java application. The hidden Markov method was used to segment the text lines into words. A sliding window was used for feature extraction. After manual correction, almost 100% accuracy was achieved.
Bertholdo et al. (2009) presented a very easy binarization technique called lin -
earization that worked in four steps: global mean calculation for grayscale images, segmentation of images into horizontal text lines, application of threshold value for each row bearing text, and lastly, the application of Kavallieratou technique. The method followed was unable to produce good results for newspaper clippings (Bertholdo et al., 2009). But it somehow showed good results when it came to the separation of foreground characters and background noise. So, Bertholdo was able to separate the text bearing areas of the page, thus providing the best results. Figure 4.4  shows recent publication trends and the types of dataset. This figure 
provides a brief information about the recent publication, size of dataset, classifi -
cation technique used, and accuracy obtained (Avadesh and Goyal, 2018; Azawi et al., 2013; Chamchong et al., 2010; Demilew and Sekeroglu, 2019; Diem and Sablatnig, 2009; Narang et al., 2018, 2021; Nguyen et al., 2017; Sharma et al., 2018; Van Phan et al., 2011).
4.3 D ISCUSSION AND ANALYSIS
The graph shown in Figure 4.5  represents the multiple classification techniques used 
in different time slots, i.e., 2011–2013, 2014–2016, 2017–2019, and 2020–2021.
One can easily interpret from this graph that since 2011, the use of SVM tech -
nique has increased and this technique is widely used today. CNN is also an emerg -
ing technique these days, and over the past decade, the use of CNN has also been increasing. The studies show that ANN has been used since 2011. MLP and random forest techniques also play a vital role in classifying handwritten text. So, these tech -
niques can even be used today for any kind of classification to be performed on historic manuscripts.64 Machine Learning for Edge Computing
FIGURE 4.5  Year-wise classification technique analysis.
FIGURE 4.4  Recent literature in the field of OCR and dataset used.65 Recent Trends in OCR Systems: A Review
4.4 CONCLUSION
OCR is a process that helps in the identification of characters, either in handwritten 
or printed text. As shown in the literature, there are many OCR systems that work on 
the concept of deep learning and use various kinds of feature extraction, segmenta -
tion, and classification techniques, along with accuracy measurement techniques, 
mainly in ancient documents. As reported by many authors, the research in this field 
is still in its initial stages. The literature presented in this report focuses on various 
classifications and feature extraction techniques along with the language for which 
OCR has been designed. The literature focuses on various application areas of the 
OCR briefly. There is no standard database available for Devanagari manuscripts. 
Work has already been done on basic characters that can further be extended to 
modifiers and conjuncts. Lacking behind is the need to design novel OCR for manu -
script recognition (Azawi et al., 2013).
REFERENCES
Angadi, S. A. and Kodabagi, M. (2014). A robust segmentation technique for line, word 
and character extraction from Kannada text in low resolution display board images. 
International Journal of Image and Graphics , 14(01n02):1450003.
Arivazhagan, M., Srinivasan, H., and Srihari, S. (2007). A statistical approach to line seg -
mentation in handwritten documents. In Document Recognition and Retrieval XIV , 
volume 6500, page 65000T. International Society for Optics and Photonics.
Avadesh, M. and Goyal, N. (2018). Optical character recognition for Sanskrit using con -
volution neural networks. In 2018 13th IAPR International Workshop on Document 
Analysis Systems (DAS) , pages 447–452. IEEE.
Azawi, M. A., Afzal, M. Z., and Breuel, T. M. (2013). Normalizing historical orthography 
for OCR historical documents using LSTM. In Proceedings of the 2nd International 
Workshop on Historical Document Imaging and Processing , pages 80–85.
Babu, N. and Soumya, A. (2019). Character recognition in historical handwritten documents–
a survey. In 2019 International Conference on Communication and Signal Processing 
(ICCSP) , pages 0299–0304. IEEE.
Bertholdo, F., Valle, E., and de A. Arau ´jo, A. (2009). Layout-aware limiarization for read -
ability enhancement of degraded historical documents. In Proceedings of the 9th ACM 
Symposium on Document Engineering , pages 131–134.
Chamchong, R., Fung, C. C., and Wong, K. W. (2010). Comparing binarisation techniques 
for the processing of ancient manuscripts. In Entertainment Computing Symposium , 
pages 55–64. Springer.
Clanuwat, T., Bober-Irizar, M., Kitamoto, A., Lamb, A., Yamamoto, K., and Ha, D. (2018). 
Deep learning for classical Japanese literature. arXiv preprint, arXiv:1812.01718 .
Demilew, F. A. and Sekeroglu, B. (2019). Ancient geez script recognition using deep learning. 
SN Applied Sciences , 1(11):1–7.
Diem, M. and Sablatnig, R. (2009). Recognition of degraded handwritten characters using 
local features. In 2009 10th International Conference on Document Analysis and 
Recognition , pages 221–225. IEEE.
Fischer, A., Indermu ¨hle, E., Bunke, H., Viehhauser, G., and Stolz, M. (2010). Ground truth 
creation for handwriting recognition in historical documents. In Proceedings of the  
9th IAPR International Workshop on Document Analysis Systems , pages 3–10.
Kumar, M., Sharma, R., and Kumar, M. G. (2015). Offline handwritten Gurmukhi script  
recognition . PhD thesis.66 Machine Learning for Edge Computing
Kumar, M., Jindal, M. K., Sharma, R. K., and Jindal, S. R., Performance Comparison of 
Several Feature Selection Techniques for Offline Handwritten Character Recognition. 
In 2018 Proceedings of International Conference on Research in Intelligent and 
Computing in Engineering , pages 1–6. https://ieeexplore.ieee.org/document/8509076 .
Lelore, T. and Bouchara, F. (2011). Super-resolved binarization of text based on the FAIR 
algorithm. In International Conference on Document Analysis and Recognition ,  
pages 839–843.
Liwicki, M., Indermuhle, E., and Bunke, H. (2007). On-line handwritten text line detec -
tion using dynamic programming. In Ninth International Conference on Document 
Analysis and Recognition (ICDAR 2007) , volume 1, pages 447–451. IEEE.
Louloudis, G., Gatos, B., Pratikakis, I., and Halatsis, C. (2009). Text line and word segmenta -
tion of handwritten documents. Pattern Recognition , 42(12):3169–3183.
Madhavaraj, A., Ramakrishnan, A., Kumar, H. S., and Bhat, N. (2014). Improved  
recognition of aged Kannada documents by effective segmentation of merged char -
acters. In 2014 International Conference on Signal Processing and Communications 
(SPCOM) , pages 1–6. IEEE.
Mahto, M. K., Bhatia, K., and Sharma, R. (2015). Combined horizontal and vertical projection 
feature extraction technique for Gurmukhi handwritten character recognition. In 2015 
International Conference on Advances in Computer Engineering and Applications , 
pages 59–65. IEEE.
Manmatha, R. and Rothfeder, J. L. (2005). A scale space approach for automatically seg -
menting words from historical handwritten documents. IEEE Transactions on Pattern 
Analysis and Machine Intelligence , 27(8):1212–1225.
Mohana, H. and Rajithkumar, B. (2014). Era identification and recognition of Ganga and 
Hoysala phase Kannada stone inscriptions characters using advance recognition algo -
rithm. In 2014 International Conference on Control, Instrumentation, Communication 
and Computational Technologies (ICCICCT) , pages 659–665. IEEE.
Narang, S., Jindal, M., and Kumar, M. (2019a). Devanagari ancient documents recognition 
using statistical feature extraction techniques. S¯ adhan¯ a , 44(6):1–8.
Narang, S. R., Jindal, M. K., and Kumar, M. (2019b). Drop flow method: an iterative algo -
rithm for complete segmentation of Devanagari ancient manuscripts. Multimedia Tools 
and Applications , 78(16):23255–23280.
Narang, S. R., Jindal, M. K., and Sharma, P. (2018). Devanagari ancient character recogni -
tion using hog and DCT features. In 2018 Fifth International Conference on Parallel, 
Distributed and Grid Computing (PDGC) , pages 215–220. IEEE.
Narang, S. R., Kumar, M., and Jindal, M. (2021). DeepNetDevanagari: a deep learning model 
for Devanagari ancient character recognition. Multimedia Tools and Applications , 
80:20671–20686. https://doi.org/10.1007/s11042-021-10775-6 .
Nguyen, H. T., Ly, N. T., Nguyen, K. C., Nguyen, C. T., and Nakagawa, M. (2017). Attempts 
to recognize anomalously deformed kana in Japanese historical documents. In 
Proceedings of the 4th International Workshop on Historical Document Imaging and 
Processing , pages 31–36.
Palakollu, S., Dhir, R., and Rani, R. (2012). Handwritten Hindi text segmentation techniques 
for lines and characters. In Proceedings of the World Congress on Engineering and 
Computer Science , volume 1, pages 24–26.
Pengcheng, G., Gang, G., Jiangqin, W., and Baogang, W. (2017). Chinese calligraphic style 
representation for recognition. International Journal on Document Analysis and 
Recognition (IJDAR) , 20(1):59–68.
Rajithkumar, B., Mohana, H., Uday, J., Bhavana, M., and Anusha, L. (2015). Read and 
recognition of old Kannada stone inscriptions characters using novel algorithm. In 
2015 International Conference on Control, Instrumentation, Communication and 
Computational Technologies (ICCICCT) , pages 284–288. IEEE.67 Recent Trends in OCR Systems: A Review
Saha, S., Basu, S., Nasipuri, M., and Basu, D. K. (2010). A Hough transform based technique 
for text segmentation. arXiv preprint arXiv :1002 .4048 .
Saini, R., Dobson, D., Morrey, J., Liwicki, M., and Liwicki, F. S. (2019). ICDAR 2019 histori -
cal document reading challenge on large structured Chinese family records. In 2019 
International Conference on Document Analysis and Recognition (ICDAR) , pages 
1499–1504. IEEE.
Sandhya, N. and Krishnan, R. (2016). Broken Kannada character recognition—a neural net -
work based approach. In 2016 International Conference on Electrical, Electronics, 
and Optimization Techniques (ICEEOT) , pages 2047–2050. IEEE.
Sharma, A. K., Adhyaru, D. M., and Zaveri, T. H. (2018). A novel cross correlation-based 
approach for handwritten Gujarati character recognition. In Proceedings of First 
International Conference on Smart System, Innovations and Computing , pages  
505–513. Springer.
Van Phan, T., Zhu, B., and Nakagawa, M. (2011). Development of nom character segmenta -
tion for collecting patterns from historical document pages. In Proceedings of the 2011 
Workshop on Historical Document Imaging and Processing , pages 133–139.
Yang, H., Jin, L., Huang, W., Yang, Z., Lai, S., and Sun, J. (2018). Dense and tight detection of 
Chinese characters in historical documents: datasets and a recognition guided detector. 
IEEE Access , 6:30174–30183.
Zahour, A., Taconet, B., Mercy, P., and Ramdane, S. (2001). Arabic hand-written text-line 
extraction. In Proceedings of Sixth International Conference on Document Analysis 
and Recognition , pages 281–285. IEEE.69 DOI: 10.1201/9781003143468-5
A Novel Approach for 
Data Security Using DNA Cryptography with Artificial Bee Colony Algorithm in Cloud Computing
Manisha Rani, Madhavi Popli, and Gagandeep
Department of Computer Science, Punjabi University
Punjab, India5
CONTENTS
5.1 Introduction  .................................................................................................... 69
5.2 Related Work  .................................................................................................. 70
5.2.1  Work Related to DNA Computing Techniques  .................................. 71
5.2.2  Work Related to Artificial Bee Colony Techniques  ........................... 71
5.3 Artificial Bee Colony Optimization  ............................................................... 73
5.4 Proposed Work  ............................................................................................... 75
5.4.1  DNA Sequence and Encoding  ............................................................ 76
5.4.2  Transcription  ....................................................................................... 76
5.5 Results  ............................................................................................................. 78
5.6 Conclusion  ...................................................................................................... 79
References  ................................................................................................................ 80
5.1 INTRODUCTION
Cloud computing is acknowledged as a new prototype that grants on-demand services such as servers, storage, and applications, using a real-time communication network at a minimal cost. It permits vendors to provide reliable and customized informa -
tion technology (IT) services on a flexible basis using internet via service providers. Previously, computer demanded additional space for servers, network devices, hard-ware, and other electronic devices for input processing and produced limited output as compared to advanced computers. Nowadays, these valuable electronic devices are replaced by hard drives and reasonable devices. In the ’90s, telecommunications 70 Machine Learning for Edge Computing
companies proposed point-to-point data circuits to the user; after this, they started 
offering virtual private networks at a lower cost but with the same quality. Shortly, Salesforce, Amazon and Google brought cloud as a new concept and became key play -
ers in the internet marketplace. As many users’ applications and data migrate to cloud, the lack of security poses many threats to the cloud platform. Cloud computing follows a layered structure and transfers data through various levels in the cloud environment. Cloud security is considered a significant challenge in the IT industry. Therefore, cloud security mechanisms are implemented at each level. Secure communication is required while transferring data from one point to another. Trust, confidentiality, avail -
ability, and integrity are some of the security aspects associated with data.
The main objective is to implement an optimized technique to secure data using 
DNA cryptography with the Artificial Bee optimization algorithm. The proposed method is classified into two sections: the first section works on the DNA cryptogra -
phy and the second section works on the Artificial Bee algorithm for key generation. DNA cryptography transforms data into four nucleotides bases: A, G, C, T. The alphanumerical values are encoded and transformed into DNA nucleotides bases. English alphabets and the numerical digits are transformed into random DNA nucle -
otides sequence, which indicates a regular computer can understand the information encoded by these four sequences as well as the information in a binary sequence (Kalsi, Kaur, and Chang 2018). The Artificial Bee algorithm generates an optimized random key for the encryption and decryption process. The decryption method is a reverse procedure of the encryption process. DNA computing is able to solve many computational problems and provides better performance. This chapter introduces a unique concept of DNA cryptography, whose results are virtually unbreakable, and additionally manage a part of storage issues. A completely random key is generated using the Artificial Bee algorithm that is considered an optimized technique inspired by the foraging behavior of honey bees. This algorithm behaves as a source for a key generation employed for the encryption and decryption methods on the encoded text. The probability is evaluated for the solution, and a new solution is produced from the current results. This process is repeated until the number of cycles approaches its limit.
The rest of this chapter is organized as follows: Section 5.2 presents the existing 
encryption techniques and approaches applied by multiple authors to protect data from malicious users. Section 5.3 briefly demonstrates the Artificial Bee Colony Optimization algorithm. Section 5.4 presents the proposed algorithm, which starts with DNA computing accompanied by a brief explanation of the encryption and decryption process using the Artificial Bee algorithm for a key generation along with the experimental outcomes in terms of time and file size. Section 5.5 discuses the results of the experiments. Finally, Section 5.6 presents the conclusion.
5.2 RELATED WORK
This section outlines the relevant work based on DNA cryptography and the Artificial Bee Colony (ABC) algorithm. This part is divided into two sections: Section 5.2.1 focuses on techniques based on DNA computing and Section 5.2.2 focuses on the Artificial Bee Colony optimization techniques.71 Data Security Using DNA Cryptography
5.2.1 Work relAted to dnA c omput Ing techn Iques
Modern DNA cryptography has adopted DNA cryptography as a new mechanism 
for an encryption process. The DNA sequences are transformed and given strength 
to the existing encryption techniques by adding more elements to the confusion and 
diffusion process. Different techniques have been applied by multiple authors in 
order to protect data from intruders, which can be external or internal. In order to 
secure data from attacks, several approaches are implemented using DNA.
Muthiah and Rajkumar (2014) compared the Genetic algorithm (GA) with the 
ABC algorithm to minimize the span of the job scheduling process. Overall, the pro -
cessing time was analyzed and the total processing time in ABC was less than the time 
taken in the GA. Barkha et al. (2016) proposed the Bi-directional DNA Encryption 
Algorithm (BDEA) to solve data security problems. DNA bases encode and trans -
form data into American Standard Code for Information Interchange (ASCII) values 
and later into binary values. Kumar, Iqbal, and Kumar (2016) introduced an algo -
rithm for image encryption. The DNA sequence encoded the Red Green Blue (RGB) 
image with asymmetric encryption using Elliptic Curve Diffie–Hellman Encryption 
(ECDHE). Wang and Liu (2017) implemented an image encryption algorithm using 
chaos and DNA encoding rules. The Piecewise Linear Chaotic Map (PWLCM) gen -
erated a key image. Different rows and columns of the plain image were extracted 
with the logistic map and encoded with the key image using DNA encoding rules. 
Dokeroglu, Sevinc, and Cosar (2019) proposed the ABC algorithm for solving qua -
dratic problems. In this chapter, employed, onlooker, and scout bees’ behavior were 
modeled by using a distributed memory parallel computation program to solve large and 
complex problems. Elhadad (2020) proposed a DNA-proxy re-encryption framework to 
protect cloud data sharing from unauthorized access. In this, three keys were generated: 
one for the user who wants access, and two others for the owner and the proxy. A user can 
store encrypted data on the cloud using a key, and if they wish to access data, they can 
access it via proxy after data re-encryption using a proxy key and decryption using the 
third key. Kolate and Joshi (2021) proposed a DNA-based security technique as an infor -
mation carrier. The DNA-based Advanced Encryption Standard (AES) can be used to 
develop a new data-security system. The proposed framework is aimed to protect records 
throughout transmission, and is essential whenever a communication or data transmis -
sion between sender and recipient has to be confidential. Along with DNA-based AES 
encryption, a protected DNA-based cryptographic algorithm provides many levels of 
security. Compression methods can also be used for AES-based DNA cryptography. It 
can be used to protect confidential material for industrial purposes.
5.2.2 Work relAted to ArtIfIcIAl bee colony  techn Iques
Cui et al. (2016) applied the Depth-first search (DFS) framework on the ABC algo -
rithm to speed up the convergence rate problem. The DFS framework allocated more 
computing resources to the food sources and improves quality. The bee colony is 
addressed by Intrusion Detection Systems (IDSS) where individuals perform dif -
ferent tasks based on the fitness values. Shah et al. (2018) presents the 3G-ABC 
algorithm hybrid of Gbest Guided ABC (GGABC) and Global ABC Search (GABCS) 72 Machine Learning for Edge Computing
for a strong discovery and exploitation process. The implementation is based on the 
fitness value instead of number of cycles. Lin et al. (2018) designed a novel artificial 
bee algorithm with local and global search (ABCLGII) to enhance the convergence 
speed. The local interaction mechanism was applied between employed bees to 
make searches corporative and directional, and the global interaction mechanism 
was employed for onlooker bees to exploit good information of some good solutions. 
Gao et al. (2018) proposed a novel-based ABC algorithm that employed two strate -
gies. One was based on direction learning, and the other was based on elite learning. 
The direction learning guided searches toward promising areas, whereas elite learn -
ing increased the convergence rate without any loss of population diversity. Choong, 
Wong, and Lim (2019) regulated the search heuristics by applying the modified choice 
function utilized by employed and onlooker bees. Authors integrated Lin-Kernighan 
local search strategy to improve the performance of the proposed model. Zhou et al. 
(2019) proposed a multi-colony ABC algorithm (IDABC) that divided the colonies 
into three sub-colonies based on the fitness value of the individual as inferior, mid, 
and superior sub-colonies. In this proposed method, the synergy of the multi-colony 
was addressed by IDSS where individuals perform different tasks based on the fit -
ness values. Agarwal and Yadav (2019) present a state-of-the-art review of ABC, 
and it shows recent modifications with an in-depth assessment and analysis of recent 
common ABC variants. ABC underwent numerous changes to overcome drawbacks 
and improve its performance in order to become the best for complex optimization 
problems. Agarwal and Yadav have provided a detailed introduction of ABC and 
modifications by researchers in detail. Ilango et al. (2019) designed a map/reduce 
program configured and implemented in a multi-node setting for the ABC algorithm. 
The main goal of the proposed ABC approach was to reduce execution time and 
optimization for different dataset sizes. The outcome was evaluated for different fit -
ness and probability parameters determined from the employed and onlooker stages 
of the ABC algorithm, from which the classification error percentage was further 
calibrated. The Hadoop environment using map-reduce programming was used to 
implement the ABC algorithm. The proposed ABC scheme decreased the execution 
time and classification error for selecting optimal clusters.
Recently, Garg et al. (2020) proposed a new ensemble-based anomaly detection 
technique for the cloud environment. The authors identified non-linear node behav -
ior among dataset attributes that trigger performance bottlenecks when it comes 
to detecting malicious performance across various nodes. This paper proposed an 
ensemble Artificial Bee Colony (En-ABC) based on the anomaly detection scheme 
for multi-class datasets in a cloud environment based on information. The ABC-
based fuzzy clustering technique was used to achieve an optimal clustering based on 
two objective functions: mean square deviation and Dunn index. Since the amount 
of data is increasing at such a rapid rate, data clustering using traditional algorithms 
is becoming increasingly difficult. Taher and Kadhim (2020) proposed a method to 
improve traditional GA using the ABC algorithm. Earlier, random generation was 
used to generate an initial population in traditional GA. The authors improved the 
traditional GA by generating an initial population using ABC algorithm. They used 
random number generation and traveling salesman problem as a case study for test -
ing the performance of the proposed work. A hybrid of GA and the ABC algorithm 73 Data Security Using DNA Cryptography
provided a good fitness function for the Random Number Generation (RNG) prob -
lem. The generated final keys were unique, random, and cryptographically strong. 
The relative error rate and average tour rate results were better with high convergence 
rate as compared to traditional algorithm. Shi et al. (2020) proposed an improved 
artificial honey bee colony for the efficiency of mutual cooperation. The traditional 
ABC algorithm suffers from slow optimization efficiency and poor performance. 
Aiming at the defects, the authors implemented an ABC algorithm that initialized 
honey sources by homogenizing chaotic mapping. The frequency distribution and 
information entropy were evaluated using a homogenization logistic mapping model. 
During initial honey point, a homogenization logistic map was used to iteratively 
produce a suitable global uniform distribution with a suitable approach. Under the 
condition of limited bee colonies, the algorithm improved convergence performance, 
optimal solution accuracy, computation efficiency, and reliability. Deng, Xu, and Wu 
(2021) used ABC algorithm to optimize a block chain investment portfolio. A tradi -
tional ABC algorithm solved the single-objective optimization problem. However, 
the authors improved this algorithm by constructing an external population. The 
findings showed that the improved ABC algorithm can optimize several features in 
an investment portfolio at the same time, reduce investor decision-making errors, 
and enhance the transparency between asset expenses and profits. The ABC algo -
rithm can overcome the issue of portfolio optimization, boost asset securitization 
security, and improve the balance of investment return and risk to some extent.
5.3 ARTIFICIAL BEE COLONY OPTIMIZATION
Cryptography is an art of achieving security by using mathematical functions to 
encode messages into non-readable form using different encryption techniques. A 
cryptographic algorithm works in association with a key to encrypt and decrypt 
data, and the protocol is known as cryptosystem (Geetha and Akila 2019). Each sys -
tem must supply some security to protect data from unwanted users. Confidentiality, 
integrity, authentication, availability, privacy, etc. are some of the goals that can be 
achieved by cryptography.
Evolutionary and swarm-based optimization techniques have been widely used 
in recent years. Traditional optimization algorithms are stimulated by natural and 
social phenomena that are used to solve many complex problems and developed 
using multiple algorithms, such as the GA, particle swarm optimization, ant colony 
optimization, artificial bee optimization, and flower pollination algorithm (Saad, 
Dong, and Karimi 2017). The main objective of the optimization is to achieve the 
best attainable value using the fitness function and to produce a suitable crypto -
graphic algorithm. The algorithms usually begin with the production of the initial 
population and new solutions are generated using a solution generated by the pre -
vious population. The fitness function evaluates the quality of the solution. This 
algorithm was proposed by Dervis Karaboga in 2005 to solve optimization prob -
lems that were inspired by the foraging behavior of honey bees (Karaboga 2005). 
Employed, onlooker, and scout bees are three components of the ABC algorithm. In 
ABC, artificial forager bees search for the food sources. The number of employed 
bees is equal to the number of onlooker bees around the hive. The employed bees 74 Machine Learning for Edge Computing
dance on the food sources area and broadcast information to the onlooker bees about 
the food sources. The onlooker bees elect the food sources found by the employed 
bees; the food of higher quality will have more chances to be selected by onlooker 
bees than the one of lower quality. The scout bees got abandoned and converted into 
employed bees, search for the new food source.
The ABC generates an initial population of randomly distributed swarm solutions 
denoted by SN, where SN is swarm size. Let Xi = {xi,1, xi,2, xi,3, …. xi,D} be the ith solu -
tion of swarm and D denotes dimensional vector. Each employee bee generates the 
new solution vij using Eqn. (5.1).
 /uni00A0 vx rand xx ij ij ij ij kj () =+ −  (5.1)
where k ∈{1,2,…, SN}, j ∈{1,2,…, D}, i ≠ j ≠ k, and rand ij∈[−1,1]. Each employee 
bee xij produces candidate food using Eqn. (5.1). Once the new candidate food is 
generated, a new fitness value is evaluated. If the fitness value of vij is better than xij, 
then the value of xijis updated with vij; otherwise, the old value remains unchanged. 
After all the employed bees complete the search process, they transmit information 
to onlooker bees using the waggle dance. The onlooker bees choose the food source 
depending on the nectar amount with a probability. The richer the source is, the 
higher the probability of its selection. The probability is a roulette wheel selection 
mechanism that can be described using Eqn. (5.2).
 
1pfit
fitii
jSN
j∑=
= (5.2)
where fiti represents the fitness value of the ith solution. If the number of trials 
reaches its limit and any bee is unable to find a food source, then, in this case, the 
food source is assumed to be abandoned and scout bees discover new food sources 
randomly using Eqn. (5.3).
 /uni00A00 ,1 Pl ower rand uppe rl ower bound bound bound () () =+ ∗−  (5.3)
The ABC algorithm helps to choose the best and optimal solutions from a number 
of solutions. In this, bees search the neighborhood to find the best solutions. The fit -
ness function can be evaluated for minimization problems using Eqn. (5.4).
 /uni00A01
1,/uni00A0 0
1| |,/uni00A00 /uni00A0fitff
ffiii
ii=+≥
+<



 (5.4)75 Data Security Using DNA Cryptography
The main steps of ABC algorithm are as follows:
Step 1:  Generate a random initial population.
Step 2:  Evaluate the fitness function of the entire solution and memorize the 
best solution.
Step 3:  Employed bees generate new solutions from the old ones.
Step 4:  Evaluate the fitness function in all new solutions and apply the greedy 
selection method to keep the best solution.
Step 5:  Onlooker bees calculate the probability for the solution and generate 
new solutions from the current solutions depending on the probability and 
keep the best solution.
Step 6:  Abandoned solutions are determined by the scout bees and replaced by 
randomly generated solutions.
Step 7:  Keep the best solution and increment the cycle, cycle = cycle + 1.
Step 8:  If the number of cycles reaches its maximum limit, then stop; other -
wise go to step 3.
5.4 PROPOSED WORK
The main purpose of the proposed scheme is to provide strong security for cloud 
data by using a hybrid DNA with the ABC algorithm-based password or secret 
key, which is generated through several processes. The same key is used for 
the encryption and decryption process. In the cloud, the proposed scheme will 
tolerate a variety of attacks. The secret key that enables the device to protect 
against many security attacks is produced using DNA bases and complementary  
rules.
In this chapter, the ABC algorithm forms the basis of key generation that will 
later be used along with the encoded text for the encryption and decryption process. 
The innovative actions of honey bees have drawn a lot of attention from research -
ers who aim to invent new approaches. The optimized solution is generated from 
the randomly generated population. The fitness value of each solution is evaluated, 
and the best solution is memorized by the bees. The probability for the solution is 
evaluated and new solutions are generated from the current ones depending on the 
probability, and thus, bees keep the best solution. This process is repeated until the 
number of cycles reaches its limit.
Cryptography is one of the prominent solutions for cloud data security. 
This chapter represents a new hybrid approach for providing security. Due 
to the complex nature, the theory of DNA has received much interest in the 
IT era. Unlike typical methods such as 0 and 1, data in the DNA encryption 
is encrypted and processed using the DNA bases: A (Adenine), C (Cytosine), 
G (Guanine), and T (Thymine). The random DNA generated key is not secure 
enough; so to make it secure, this key is embedded with the key generated by 
the ABC algorithm. The ABC algorithm is the most common evolution opti -
mization algorithm.76 Machine Learning for Edge Computing
5.4.1 dn A sequence  And encod Ing
It is a process of determining the nucleotides bases (As, Ts, Cs, and Gs) in a sequence. 
English alphabets and the numerical digits are transformed into the random DNA 
nucleotides sequence.
For example, “DNA computing” text is encoded in the random DNA nucleotides 
sequence using Table 5.1 .
5.4.2 t rAnscr IptIon
When the data is encoded into a sequence of DNA nucleotides bases, this encoded 
text is transcripted into a complementary DNA form (A → U, C → G, G → C, T → A). 
TABLE 5.1
Random Generated DNA Sequence of Nucleotides Bases
Alphabet Newly Assigned Value Numbers Newly Assigned Value P Pʹ
a ATCG 0 TTCC d ACTG
b CATG 1 CGAT n GAGA
c CTTA 2 TCGA a ATCG
d ACTG 3 CCTT ATGC
e CTAG 4 GTTA c CTTA
f TGCA 5 GGTT o TAGC
g GTCA 6 GATC m AGAG
h ACGC 7 ATTG p AGGA
i ACCG 8 GCGG u TAGT
j GCTA 9 GTAC t CGCG
k AGCC “ ” ATGC i ACCG
l TACA n GAGA
m AGAG g GTCA
n GAGA
o TAGC
p AGGA
q ATAG
r ATGG
s AAGG
t CGCG
u TAGT
v GGAA
w TGGT
x GACT
y TCCT
z TGTG
 PP = DNA computing where P′  = ACTGGAGAATCGATGCCTTATAGCAGAGAGGATAGTCGCGAC
CGGAG AGTCA77 Data Security Using DNA Cryptography
This random sequence is further transformed into a binary sequence. The sequence 
to convert encoded text into a complementary and binary form is shown in Tables 5.2 and 5.3.
P’ = UGACCUCUUAGCUACGGAAUAUCGUCUCUCCUAUCAGCGCUGGCCUCUCAGUP’ = 11100001011101111100100111000110100000110011011011011101110101110011010010011001111010010111011101001011
The binary sequence of the encoded sequence is divided into 8-bit block size.
P’ = 11100001 01110111 11001001 11000110 10000011 00110110 11011101 11010111 00110100 10011001 11101001 01110111 01001011
Each byte is right shifted by 2 bits each.
P’ = 10000111 11011101 00100111 00011011 00001110 11011000 01110111 01011111 11010000 01100110 10100111 11011101 00101101
Convert each byte into the decimal equivalent and divide each decimal number 
with the number generated by the Artificial Bee algorithm. This number will act as the key.
D= 135 221 39 27 14 216 119 95 208 102 167 221 45
The key generated by the Artificial Bee algorithm is 0.7126563561694389. 
Operations are performed on the decimal numbers using the key generated using TABLE 5.2
Complementary Strand
Nucleotides Bases Complementary Strand
A U
C G
G C
U A
TABLE 5.3
Random Binary Sequence
Nucleotides Bases Random Sequence
A 00
C 10
G 01
U 1178 Machine Learning for Edge Computing
the ABC algorithm and the result is stored in R. This result is further converted into 
hexadecimal form. The complete encryption process is shown in Figure 5.1 .
This algorithm is implemented using python. Random sets of DNA nucleotides 
bases (As, Cs, Gs, and Ts) are assigned to the alphabets and numerical digits. This encoded text is transcripted and transformed into a complementary and binary form. The key is generated using the Artificial Bee algorithm and operations are per -
formed on the encoded text using this key. This text is re-encrypted by converting into hexadecimal. The decryption process follows the exact reverse process of the encryption method.
5.5 RESULTS
Many studies are being done to find an optimized solution, in order to meet large amounts of computational storage, and operations that are able to create new meth -
ods. DNA computing is considered the best algorithm that does not provide vast storage capacity but increases the complexity, and it is becoming the future for cryp -
tography. This algorithm is based on the DNA computing with the Artificial Bee algorithm. DNA cryptography increases the complexity, where as ABC provides the optimized solution and helps in generating the key to increase the algorithm speed. In this proposed technique, time is considered an influential factor for analyzing pur -
poses during the encryption and decryption process for file sizes greater than 1 KB. 
FIGURE 5.1  Encryption process using ABC algorithm.79 Data Security Using DNA Cryptography
This algorithm is designed at the digital level and the time it takes for the encryption 
and decryption process is shown in Table 5.4 . The graph plot is shown in Figure 5.2 .
5.6 CONCLUSION
Artificial Bee Colony optimization is as warm-based optimization technique that achieves the best accessible value using the fitness function to generate a suit -
able cryptographic algorithm. In this chapter, DNA cryptography is used for data encryption along with the Artificial Bee Colony algorithm for key generation. DNA computing is the process of encoding alphabets and numbers into the nucleotides bases (As, Ts, Cs, and Gs) and transcripting and transforming them into a com -
plementary and binary form. DNA computing is able to solve many computational problems and provide better performance. This chapter introduces a unique concept of DNA cryptography, with results that are virtually unbreakable and, additionally, manage a part of storage issues. The key is generated using the Artificial Bee algo -
rithm and operations are performed on the encoded text using this key. This text is further re-encrypted by converting into hexadecimal. The decryption process is the exact reverse process of the encryption process.TABLE 5.4
Time Taken for Different File Sizes
File Size (KB) Encryption Time (ms) Decryption Time (ms)
3 0.0204 0.0322
6 0.0415 0.0633
10 0.0653 0.0903
FIGURE 5.2  Encryption and decryption times.80 Machine Learning for Edge Computing
REFERENCES
Agarwal, Shiv Kumar, and Surendra Yadav. 2019. “A Comprehensive Survey on Artificial 
Bee Colony Algorithm as a Frontier in Swarm Intelligence.” Ambient Communications 
and Computer Systems . Springer: 125–34.
Barkha, Prajapati. 2016. “Implementation of DNA Cryptography in Cloud Computing 
and Using Socket Programming.” In 2016 International Conference on Computer 
Communication and Informatics (ICCCI) , 1–6. IEEE.
Choong, Shin Siang, Li-Pei Wong, and Chee Peng Lim. 2019. “An Artificial Bee Colony 
Algorithm with a Modified Choice Function for the Traveling Salesman Problem.” 
Swarm and Evolutionary Computation  44: 622–35.
Cui, Laizhong, Genghui Li, Qiuzhen Lin, Zhihua Du, Weifeng Gao, Jianyong Chen, and 
Nan Lu. 2016. “A Novel Artificial Bee Colony Algorithm with Depth-First Search 
Framework and Elite-Guided Search Equation.” Information Sciences  367: 1012–44.
Deng, Yulin, Hongfeng Xu, and Jie Wu. 2021. “Optimization of Blockchain Investment 
Portfolio under Artificial Bee Colony Algorithm.” Journal of Computational and 
Applied Mathematics  385. Elsevier: 113199.
Dokeroglu, Tansel, Ender Sevinc, and Ahmet Cosar. 2019. “Artificial Bee Colony Optimization 
for the Quadratic Assignment Problem.” Applied Soft Computing 76. Elsevier: 595–606.
Elhadad, Ahmed. 2020. “Data Sharing Using Proxy Re-Encryption Based on DNA 
Computing.” Soft Computing  24 (3). Springer: 2101–8.
Gao, Weifeng, Hailong Sheng, Jue Wang, and Shouyang Wang. 2018. “Artificial Bee 
Colony Algorithm Based on Novel Mechanism for Fuzzy Portfolio Selection.” IEEE 
Transactions on Fuzzy Systems 27 (5). IEEE: 966–78.
Garg, Sahil, Kuljeet Kaur, Shalini Batra, Gagangeet Singh Aujla, Graham Morgan, Neeraj 
Kumar, Albert Y Zomaya, and Rajiv Ranjan. 2020. “En-ABC: An Ensemble Artificial Bee Colony Based Anomaly Detection Scheme for Cloud Environment.” Journal of 
Parallel and Distributed Computing  135. Elsevier: 219–33.
Geetha, M, and K Akila. 2019. “Survey: Cryptography Optimization Algorithms.” International 
Journal of Emerging Technology and Innovative Engineering 5 (1): 123–135.
Ilango, S Sudhakar, S Vimal, M Kaliappan, and P Subbulakshmi. 2019. “Optimization Using 
Artificial Bee Colony Based Clustering Approach for Big Data.” Cluster Computing  22 
(5). Springer: 12169–77.
Kalsi, Shruti, Harleen Kaur, and Victor Chang. 2018. “DNA Cryptography and Deep 
Learning Using Genetic Algorithm with NW Algorithm for Key Generation.” Journal 
of Medical Systems  42 (1). Springer: 1–12.
Karaboga, Dervis. 2005. “An Idea Based on Honey Bee Swarm for Numerical Optimization.” 200.
Kolate, Varsha, and R B Joshi. 2021. “An Information Security Using DNA Cryptography 
along with AES Algorithm.” Turkish Journal of Computer and Mathematics Education 
12 (1S): 183–92.
Kumar, Manish, Akhlad Iqbal, and Pranjal Kumar. 2016. “A New RGB Image Encryption 
Algorithm Based on DNA Encoding and Elliptic Curve Diffie–Hellman Cryptography.” Signal Processing 125. Elsevier: 187–202.
Lin, Qiuzhen, Miaomiao Zhu, Genghui Li, Wenjun Wang, Laizhong Cui, Jianyong Chen, 
and Jian Lu. 2018. “A Novel Artificial Bee Colony Algorithm with Local and Global Information Interaction.” Applied Soft Computing 62. Elsevier: 702–35.
Muthiah, A, and R Rajkumar. 2014. “A Comparison of Artificial Bee Colony Algorithm and 
Genetic Algorithm to Minimize the Makespan for Job Shop Scheduling.” Procedia 
Engineering  97. Elsevier: 1745–54.
Saad, Abdulbaset El Hadi, Zuomin Dong, and Meysam Karimi. 2017. “A Comparative Study on 
Recently-Introduced Nature-Based Global Optimization Methods in Complex Mechanical System Design.” Algorithms  10 (4). Multidisciplinary Digital Publishing Institute: 120.81 Data Security Using DNA Cryptography
Shah, Habib, Nasser Tairan, Harish Garg, and Rozaida Ghazali. 2018. “Global Gbest Guided-
Artificial Bee Colony Algorithm for Numerical Function Optimization.” Computers  7 
(4). Multidisciplinary Digital Publishing Institute: 69.
Shi, Kexiang, Liyong Bao, Hongwei Ding, Lei Zhao, and Zheng Guan. 2020. “Research on 
Artificial Bee Colony Algorithm Based on Homogenization Logistic Mapping.” In 
Journal of Physics: Conference Series  1624: 42030.
Taher, Ali Abdul Kadhim, and Suhad Malallah Kadhim. 2020. “Improvement of Genetic 
Algorithm Using Artificial Bee Colony.” Bulletin of Electrical Engineering and 
Informatics  9 (5): 2125–33.
Wang, Xingyuan, and Chuanming Liu. 2017. “A Novel and Effective Image Encryption 
Algorithm Based on Chaos and DNA Encoding.” Multimedia Tools and Applications  
76 (5). Springer: 6229–45.
Zhou, Jiajun, Xifan Yao, Felix T S Chan, Yingzi Lin, Hong Jin, Liang Gao, and Xuping Wang. 
2019. “An Individual Dependent Multi-Colony Artificial Bee Colony Algorithm.” 
Information Sciences  485. Elsevier: 114–40.83 DOI: 10.1201/9781003143468-6
Various Techniques 
for the Consensus Mechanism in Blockchain
Shivani Wadhwa and Gagandeep
Department of Computer Science, 
Punjabi University  
Punjab, India6
CONTENTS
6.1 Introduction  .................................................................................................... 83
6.2 Types of Blockchain  ....................................................................................... 84
6.3 Characteristics of Blockchain  ......................................................................... 85
6.4 Applications of Blockchain  ............................................................................. 86
6.5 Related Work  .................................................................................................. 87
6.6 Structure of Block  ........................................................................................... 90
6.7 Blockchain Computation  ................................................................................ 91
6.8 Various Platforms for the Computation of Block  ........................................... 91
6.9 Issues and Challenges  ..................................................................................... 92
6.10 Conclusion  ...................................................................................................... 93
References  ................................................................................................................ 93
6.1 INTRODUCTION
Today, we are at the verge of technological advancements that will drastically revolu -
tionize our living standards (Kukreja & Dhiman, 2020; Kukreja, Marwaha, Sareen, & Modgil, 2020). In 2008, Satoshi Nakamoto gave origin to bitcoin as a digital currency that basically eliminates the double-spending problem (Nakamoto & Bitcoin, 2008). A blockchain is fundamentally a distributed database of the records of all transactions or electronic events that are executed and shared with the parties involved. Blockchain is a peer-to-peer (P2P) network that is a digital, decentralized, secure, and incorrupt -
ible distributed ledger containing information about transactions in financial terms (Mingxiao, Xiaofeng, Zhe, Xiangwei, & Qijun, 2017). It is a linked list of blocks that are cryptographically linked to each other. Figure 6.1  gives an overview of the archi -
tecture of blockchain for governance application. Blocks are verified by a consensus mechanism that further needs compute interface for complex computations.
Blockchain technology is becoming more ubiquitous in various applications 
of information technology. Blockchain is indeed far more than a digital platform for payment. Various companies, such as Ernst & Young, are investing a lot in the 84 Machine Learning for Edge Computing
expansion of privacy tools. It is expected that projects like multiparty computa -
tion (MPC), zero-knowledge, etc., will mature enough as they enter the blockchain 
space. Various IoT deployments will also implement blockchain because of its secure 
framework. IndiaChain is the ambitious project of Niti Aayog that the Indian gov -
ernment is building to incorporate blockchain into various government projects. 
Implementing blockchain in different scenarios by working on their use requires 
huge amount of computations. These computations need an enormous amount of 
power to solve the consensus mechanism of blockchain. So, there is a need to develop 
efficient techniques for blockchain computation to increase the acceptance of block -
chain in various fields.
Blockchain can provide support to smart devices that are generally low pow -
ered and have less capabilities with computation. Huge amounts of computation 
are required for performing the mining task of block in blockchain. There exists 
a requirement to perform the computation of blockchain by optimally utilizing 
resources. So, there arises the need for efficient techniques for the consensus mech -
anism for blockchain computation. The integration of technologies will ease the 
deployment of blockchain for any application by providing access to local computing 
power that supports hashing, consensus mechanism like Proof of Work (PoW), and 
encrypting algorithms. Although some work has been done to integrate computing 
technologies with the blockchain, some well-organized mechanisms are required so 
that computation can be done optimally.
6.2 TYPES OF BLOCKCHAIN
Three types of blockchains exist: public blockchain, private blockchain, and consor -
tium blockchain. All types of blockchains function on a P2P network system with a 
copy of the shared ledger, which is updated frequently. The variation in how differ -
ent types of blockchains function is explained as follows:
1. Public Blockchain:  It is an open-source blockchain in which anyone is 
allowed to participate as a developer, user, or miner. It does not have any 
FIGURE 6.1  Structure of the block.85 Techniques for Consensus Mechanism in Blockchain
access restrictions. Public blockchain is decentralized and fully distributed 
as all transactions are linked in the chain form. Transactions are verified 
using consensus algorithms. After verification, data modifications are not 
allowed. Examples of public blockchain are Bitcoin and Ethereum (Lu, 
2018).
2. Private Blockchain:  It is a permissioned blockchain in which consent 
from the network administrator is required to become part of the net -
work. It is more centralized when compared to public blockchain as only 
few transactions are given permission to join and become part of the net -
work. As data is private, only invited users can make transactions over 
it. Examples of private blockchain are Hyperledger Fabric of The Linux  
Foundation.
3. Consortium Blockchain:  It is a semi-decentralized blockchain as only 
preapproved nodes can become part of the network. It offers a higher level 
of regulation over the network and provides security similar to that of pub -
lic blockchain. Equal powers are given to all participants of consortium 
blockchain. Here, multiple enterprises carry out transactions or exchange 
information on common platforms. Examples of consortium blockchain are 
Hyperledger, Corda, and Quorum.
6.3 CHARACTERISTICS OF BLOCKCHAIN
Blockchain is gaining worldwide acceptance nowadays because of its use in  
most of the current technologies and a large variety of applications. It supports  
various applications through its most favorable characteristics, which are listed 
below:
1. Distributed and Decentralized:  Central authority does not possess all the 
power and information; rather it distributes all the power and information to 
the various nodes connected with it. Each member inside the network has a 
duplicate of the precise identical ledger. If a participant’s ledger is manipu -
lated by an attacker, it’ll be rejected through the bulk of the participants 
inside the network.
2. Immutable and Efficient:  Blockchain does not require a third party for 
appending transactions into the blockchain. In fact, all the nodes present in 
the network take part in reaching a consensus and then perform the task of 
verification of block as well. Once block is created, then no change can be 
made over it.
3. Anonymous Identity:  This feature of the blockchain makes the user pseud -
onymous as no one will come to know who is the actual producer of the 
data, but data will be transparent in nature. It is mostly convenient for pro -
viding protection to the producers.
4. Auditable:  As each of the blockchain transactions are authenticated and 
documented with a timestamp, blockchain users can very easily identify 
previous records by accessing any node throughout the distributed network. 
This feature makes the data traceable and transparent in blockchain.86 Machine Learning for Edge Computing
5. Secure:  Every user has keys to store the fragment of data in an encrypted 
form, which provides complete privacy to the data, without involvement of 
third party. Data is stored in a linear and chronological manner; so each 
node present in the network stores the hash of its own block along with the 
hash of the previous block.
6. Openness:  There are two interpretations of openness in the blockchain. 
First, it is an open source, i.e., it is available to everyone in the network. 
Second, any node in the network can participate in appending block into 
the blockchain.
6.4 APPLICATIONS OF BLOCKCHAIN
Applications that are based on blockchain benefit greatly as they have several unique 
characteristics when compared with standard databases. Following are a few appli -
cations of blockchain:
1. Smart Healthcare:  The data generated by e-healthcare is huge. Providing 
security and privacy to such a data is very important, so that patients, doc -
tors, and healthcare practitioners can easily rely on this data. When this 
set of data is stored in blockchain, it improves the quality of data as well 
as makes it cost-efficient (De Aguiar, Faiçal, Krishnamachari, & Ueyama, 
2020).
2. Logistic Companies:  As businesses grow in size, it becomes more dif -
ficult to manage and track the assets. Asset tracking of logistic companies 
nowadays is done by using blockchain. The maintenance of transactions 
and records becomes much easier between the stakeholders (Lao, Li, Hou, 
Xiao, Guo, & Yang, 2020). Blockchain provides feature of scalability and 
reliability to logistic companies.
3. Smart Cities:  Smart cities include heterogeneous networks, a large variety 
of sensors, and information processing units. It is very important to include 
blockchain in its architecture, so that data can become secure and users can 
easily rely on the services provided by smart cities (Sharma & Park, 2018). 
Embracing blockchain and smart cities together will enhance downstream 
network processing.
4. Real State:  The property transaction process can be streamlined easily 
between the buyer and the seller using blockchain technology. Data tamper -
ing is not possible in blockchain-based records. Data recorded is efficient 
and permanent, which makes the system seamless (Garcia-Teruel, 2020).
5. Smart Energy Sector:  Microgrids are the storage house of all sources of 
electric power that manage and increase the overall efficiency (Xue, Teng, 
Zhang, Li, Wang, & Huang, 2017). Microgrids facilitate the buying and 
selling of the excess of energy. Blockchain is used in microgrids to main -
tain the record of transaction between the buyer and the grid as well as 
between the grid and the customer.
6. Identity Management:  Digital identities play a very vital role in online 
transactions for representing the identity of the user. Identity information 87 Techniques for Consensus Mechanism in Blockchain
for online users needs to be stored securely. Blockchain technology can 
provide independent, tamper-proof, and secure identity management solu -
tions (Zhu & Badr, 2018).
7. Insurance:  Nowadays, policies in different insurance companies are auto -
mated by using smart contracts. Blockchain allows insurance companies 
to expand easily by supporting various clients, insurance companies, and 
many policyholders (Raikwar, Mazumdar, Ruj, Gupta, Chattopadhyay, & 
Lam, 2018). Blockchain makes the system cost-effective and reduces the 
complexity process of insurance claims.
6.5 RELATED WORK
A review of related literature provides a survey and discussion of the existing tech -
nologies in a given area of study. It provides a concise overview of what has been 
studied, argued, and established about a topic. Our work focuses on blockchain 
computation. Literature surveys have been done to gain a basic understanding of 
blockchain, applications of blockchain, and how to perform blockchain computation 
on different platforms. The literature review is divided into two subsections:
1. Based on the Deployment of Blockchain in Various Applications:  Smart 
devices or mobile devices are producing huge amounts of data. As these 
devices do not have a great capacity to perform the computations, the data 
is generally transferred to some other platform where computational tasks 
can be performed. Nowadays, blockchain is used to provide security fea -
tures to the data being transferred. Blockchain technology plays a vital role 
in managing the privacy and security of the IoT devices. Problems that 
can be addressed by blockchain are identity and access management, data 
authentication and integrity, authentication, authorization and privacy of 
users, secure communication, etc. (Khan & Salah, 2018). When blockchain 
is integrated with IoT devices, it can solve many security problems of IoT. 
Features of blockchain like auditability, immutability, security, and reli -
ability are favorable for IoT technology and also bring autonomy in IoT 
devices. The architecture of integrated blockchain and IoT is proposed, 
which provides a novel IoT platform (Casado-Vara, Chamoso, De la Prieta, 
Prieto, & Corchado, 2019). An adaptive controller is developed by queuing 
a theory to achieve the optimal block number to increase the efficiency of 
the mining process. A new model for increased search speeds via hashmap 
is also proposed. The data search in the big databases can be improved by 
using a hashmap stored in the blockchain. Ethereum blockchain has been 
used to create secure virtual zones or bubbles where smart devices can 
recognize and trust each other (Hammi, Hammi, Bellot, & Serhrouchni, 
2018). Different smart devices are evaluated on the basis of their time and 
energy consumptions. Optimizing the number of miners in a defined system 
and the mechanism for selecting miners are left for future work. Hammi  
et al. (2018) discusses the issues related to integrating IoT with blockchain 
technology. Advantages of blockchain for large-scale IoT devices are  88 Machine Learning for Edge Computing
also stated. Scalability, processing time, legal issues, variation in computing 
capabilities, etc., are a few challenges that are mentioned in this paper on 
integrating blockchain technology with IoT (Kumar & Mallick, 2018). This 
paper identifies the use cases of IoT environment on the basis of the block -
chain mechanism. All layers of IoT are explained with security mechanisms 
and various applications of blockchain (Minoli & Occhiogrosso, 2018).
Suggested work includes identifying the best-suited IoT applications for 
applying blockchain mechanisms and implementing optimal distributed 
ledgers for supporting IoT. An approach for managing IoT devices using 
Ethereum is proposed. Smart contracts using the solidity language are writ -
ten over the Ethereum platform to manage meter contracts of IoT devices. 
Investigations show that further studies involving fully scaled multiple 
IoT devices are also feasible on this platform (Huh, Cho, & Kim, 2017). 
Challenges that IoT and blockchain together must address are highlighted 
in the paper of Reyna, Martín, Chen, Soler, & Díaz, 2018. A complete over -
view of the interaction between the IoT paradigm and blockchain on the 
basis of existing platforms and applications is also addressed in the paper 
of Reyna, Martín, Chen, Soler, & Díaz, 2018. It has also been identified that 
Ethereum is used as the blockchain platform for IoT devices. The integra -
tion of IoT and blockchain will greatly increase the use of blockchain, which 
will ultimately provide scalability, storage capacity, security, and privacy to 
IoT devices. Mechanisms related to security and offloading in a multi-user 
mobile edge-cloud computation offloading (MECCO) are proposed in the 
paper of Nguyen, Pathirana, Ding, & Seneviratne, 2019 for delay-sensitive 
IoT applications. An access control mechanism for preventing malicious 
offloading access of cloud resources is used. An optimal offloading policy, 
which is a Deep-Reinforcement-Learning (DRL)-based offloading scheme 
for the IoT network, is created. Decisions related to task offloading are 
considered a joint optimization problem, which is solved efficiently by the 
Deep Q-learning Network (DQN) algorithm. The framework for a time-  
sensitive network management service and the related demands of vari -
ous IoT devices can be enhanced (Nguyen, Pathirana, Ding, & Seneviratne, 
2019). A novel architecture is proposed for IoT devices that use edge layer 
for improving data quality and false data detection using blockchain. This 
architecture allows decentralized data management via blockchain and 
computation distribution is done via the edge computing paradigm. This 
provides optimization to the hybrid end-to-end system (Casado-Vara, 
Chamoso, De la Prieta, Prieto, & Corchado, 2019).
2. Based on Different Computing Platforms for Blockchain Computation:  
A blockchain-based edge computing framework is designed for offloading 
computation processes to ensure data integrity. For balancing computa -
tions offloading, a Nondominated Sorting Genetic Algorithm (NSGA-III) 
is used. Simple Additive Weighting (SAW) and Multiple Criteria Decision 
Making (MCDM) are used as an optimal offloading strategy. Blockchain-
based computation offloading (BCO) is proposed for 5G networks (Guo, 
Hu, Guo, Qiu, & Qi, 2019). It is suggested that the proposed BCO can also 89 Techniques for Consensus Mechanism in Blockchain
be adjusted according to the real-world scenario and more users’ prefer -
ences for Quality of Service (QoS) can be considered. The two-stage stack -
elberg game model is proposed to maximize the profits of the edge service 
provider and individual utilities of different miners (Xiong, Feng, Niyato, 
Wang, Han, 2018).
Nash equilibrium point is derived among the miners, which helps maximize 
the profit of miners. Uniform and discriminatory pricing schemes are used by 
implementing backward induction. A platform is proposed for decentralized com -
putation offloading on the basis of blockchain (Seng, Li, Luo, Ji, & Zhang, 2019). 
A blockchain platform is established for announcing the requests of computation 
offloading and finding edge servers that conduct offloaded computations. A Galey 
Shapley (GS)-based user matching algorithm is designed to match the offloading 
requester’s computation task with the edge server. The user matching algorithm is 
based on execution time and energy consumption. A novel framework for mobile  
edge computing-enabled wireless blockchain is proposed. Mining tasks that are 
computation-intensive are offloaded to edge computing nodes, and the cryptographic 
hash of the blocks are cached in the server of Mobile Edge Computing (MEC) (Liu, 
Yu, Teng, Leung, & Song, 2018). Stochastic game theory is applied to derive the per -
formance measures of delay, energy consumption, and orphaning probability. Then 
the algorithm on the basis of alternating direction method of multipliers (ADMM) 
is applied. It is suggested that other QoS constraints can be considered in wireless 
blockchain networks. A technique is designed to optimize the cost of mobile equip -
ment (ME) by using joint computation offloading and coin-loaning (Zhang, Hong, 
Chen, Zheng, & Chen, 2019). An efficient distributive algorithm is designed on the 
basis of non-cooperative game method. The proposed algorithm quickly achieves 
the Nash equilibrium (NE) point. For execution, smart contracts are deployed on the 
Ethereum networks. For further optimization, utilization of stackelberg game has 
been suggested for MEs, banks, and edge servers at the same time. The BeCome 
method is designed to achieve load balancing and data integrity of smart devices by 
decreasing offloading times and energy consumption (Xu, Zhang, Gao, Xue, Qi, & 
Dou, 2019). The non-dominated sorting genetic algorithm III (NSGA-III) is imple -
mented for possible task offloading schemes. Multiobjective optimization is achieved 
by implementing SAW and MCDM. An extension of this work can be done by imple -
menting it on real scenarios of the IoT. The auction mechanism is proposed for the 
edge service provider to assign the edge computing resources that enable mobile 
blockchain proficiently. The main focus of this work is to maximize the social wel -
fare by providing individual rationality, computational efficiency, and truthfulness 
(Jiao, Wang, Niyato, & Xiong, 2018). It is also stated that this work can be consid -
ered for the various demands of mobile users. A statistical method is proposed to 
solve the complex mathematical puzzle in PoW (Altman, Reiffers, Menasche, Datar, 
Dhamal, & Touati, 2019). The mathematical model of expectations and the polyno -
mial matrix factorization method are used. The proposed approach consumes less 
memory, power, and time by simplifying the system. This model is applicable for 
all consensus algorithms. Suggested work is to implement the proposed model for 
hybrid network consisting of irrational multiagents. A neural network architecture 90 Machine Learning for Edge Computing
based on deep learning is proposed for the allocation of edge resources in mobile 
blockchain networks (Luong, Xiong, Wang, & Niyato, 2018). Miner bids are given as input to the neural networks and the winner representation and payment of miners represent the output of the neural network. Parameters of the network are optimized by Stochastic Gradient Descent (SGD). This work can also be extended by consider -
ing multiple edge computing resource units. An EdgeChain framework is proposed that integrates permissioned blockchain and smart contract with the distributed IoT 
applications (Pan, Wang, Hester, Alqerm, Liu, & Zhao, 2018). EdgeChain imple -
ments a coin system, which is enabled by blockchain to link the resources of the edge 
pool with the IoT device accounts and the resource usage manners. The implemen -
tation of smart contract is done to regulate behavior and enforce policies with IoT devices. This project is ongoing and work is being done for the IoT proxy, intelligent resource provisioning for multiple heterogeneous applications, and the regulation of better IoT device behavior.
6.6 S TRUCTURE OF BLOCK
There are various fields that are required for the construction of block. The first block of the blockchain is known as genesis (i.e., parent) block. Its previous hash field always contains all zeroes as it is the first block of the blockchain. Based on the hash value of the genesis block, a chain of blocks is formed. Figure 6.1  shows the 
structure of the block.
Fields that are part of block are explained as follows:
1. Block:  Block depicts the block number of the blockchain.
2. Time Stamp:  Time stamp gives the information of the time when this block 
is being mined. It contains the Unix time, which is universal and is mostly used for programming purposes. This helps in updating the information of the block after every single second. Time stamp also helps in generating the nonce range.
3. Nonce:  Nonce is number used only once. Nonce gives extra flexibility of 
generating hash as everything else is fixed; only nonce can be varied to cre -
ate the desired hash. Nonce is just a number that can range up to 4 billion. The task of varying nonce is done by miners.
4. Data:  Data of the block consists of the list of transactions. Transactions are 
picked from mempool, which stores the unconfirmed transactions.
5. Previous Hash: Previous hash helps maintain the cryptographic chain of 
blockchain as hash of the previous block must match with the previous hash field of the current block. This makes the chain immutable as no one can change the data of the block. Even if someone tries to tamper with the data, its current hash will change and will no longer match with the previous hash field of the next block; hence, it becomes practically impossible to make any changes over the block.
6. Hash:  Hash is similar to the fingerprint of the block. The SHA-256 hashing 
algorithm is used to create hash of the block. SHA-256 possesses the satis-fying requirements of generating the block, i.e., it is one-way, deterministic, 91 Techniques for Consensus Mechanism in Blockchain
computationally faster, and withstands collision. With one small change 
inside the block, its hash value is affected a lot, which is its most important feature.
6.7 BLOCKCHAIN COMPUTATION
Smart devices or mobile devices generate data that require computations for further processing. The computed data is stored in the blocks and computations are also done for verifying the content of the blocks. Figure 6.2  represents the different stages 
of transferring the data into the block.
1. Initialization of Data: Data originates from smart devices or mobile 
devices. Generally, these devices do not have much capability to do  
computations. To provide security features to these devices, blockchain plays a very important role. The type of data produced depends on the application that produces that data.
2. Generation of N ew Block:  Log events that are part of records can perform 
operations such as reading/writing on the existing record or creating a new record. These logs are compiled, hashed, and finally, become part of the new block.
3. Consensus Mechanism Verifies the B lock:  The consensus algorithm pro -
vides safety and effectiveness to the blockchain. All blocks of the blockchain are of equal status. Such blocks reach consensus by implementing the prior consent of the rules. A few consensus algorithms are PoW, Proof of Stake (PoS), Delegated Proof of Stake (DPoS), Practical Byzantine Fault Tolerance (PBFT), Simplified Byzantine Fault Tolerance (SBFT), Proof of Elapsed Time (PoET), Proof of Relevance (PoR), and Proof of eXercise (PoX).
4. Append Block into the Ledger:  As the block gets verified by the consen -
sus algorithm and also by other miners that are part of mining pool, the block becomes valid to be appended into the blockchain.
6.8 VARIOUS PLATFORMS FOR THE COMPUTATION OF BLOCK
Smart devices or mobile devices are not very capable of performing computations as they are low-powered. This constraint becomes crucial for the applications of block -
chain. Various blockchain technologies use PoW to generate relevant blocks. This process involves solving complex mathematical problem for which huge amounts of computational power is required. It is basically a cryptographic puzzle in which hash must be produced with leading zeroes by varying the nonce field of the block. This cryptographic puzzle is hard to solve as various iterations are done to create nonce, 
FIGURE 6.2  Creation of the block in the blockchain.92 Machine Learning for Edge Computing
but it is easy to verify as hash can be easily verified using all fields of the block. Due 
to a lot of computations involved in this consensus algorithm, other consensus algo -
rithms, such as PoS, PoX, PoR, etc., are also used in various platforms. Mining is a 
process that is used to create secure transactions that are part of blockchain. After 
the process of mining, blocks are added to the blockchain. This process consumes a 
lot of computational power. Groups of people who are involved in mining are known 
as miners. In the case of PoW, miners need to come up with nonce, which produces 
leading zeroes in hash. The brute force approach is applied for creating a nonce. 
The miners need to go through various iterations that can produce nonce. A lot of 
electricity is consumed when producing nonce. Hash produced is the proof of work. 
Before adding block to a system, the block is verified by all systems in the network. 
In this way, a lot of computations are required for verifying and validating block. 
These devices need some platform where blockchain computations can be deployed 
so that computing power for hashing, applying encryption algorithms, and consensus 
algorithms like PoW, etc., can be achieved. The data produced by the smart devices 
is generally offloaded to a blockchain-based network. The most promising approach 
of the blockchain is the distributed computing. MPC is a category of cryptography 
that allows a group of mutually distrusting parties to mutually execute computations 
on the input given to them. Edge nodes that lie very close to the network also have 
sufficient resources for performing computations that are transferred by the smart 
devices. A lot of work is done to integrate smart devices with the edge servers for 
supporting the deployment of blockchain. On the basis of Ethereum, smart contract, 
and Xtrem Web-HEP, a blockchain-based entirely distributed computing infra -
structure known as iEx.ec is projected. Berkeley Open Infrastructure for Network 
Computing (BOINC) is an open-source middleware platform for network computing 
that can support blockchain computations.
6.9 ISSUES AND CHALLENGES
The challenges identified during the literature review of blockchain computations 
are as follows:
1. Scalability:  The size of blockchain of bitcoin has consistently increased 
since its creation. Its size was approximately 269.82 GB at the end of March 
2020, which increases its bootstrap time as well. Although the internet is 
massively big, blockchain still suffers from scalability issues because of 
decentralized networks.
2. Throughput:  The throughput of blockchain is generally low because it 
handles fewer transactions compared to other payment processors (Xie, Yu, 
Huang, Xie, Liu, & Liu, 2019). Blockchains like Bitcoin and Ethereum have 
the capacity to handle 7 to 20 transactions per second on average, whereas 
a Visa credit card has the capacity to handle 2,000 transactions per second 
on average.
3. Latency:  As the size of blockchain increases, it becomes very difficult for 
miners to solve the consensus mechanism, and it takes time to generate 
block. The increase in the time to create block also creates latency issues. 93 Techniques for Consensus Mechanism in Blockchain
Latency is basically the product of two factors, i.e., computational latency 
and transmission latency. As blockchain evolves, some acceptance issues 
may arise because of issues like latency, scalability, etc.
4. Resource Utilization:  A huge amount of resources are required by min -
ers to solve the cryptographic puzzle and for computation purposes. If 
resources are utilized optimally, it may reduce the energy requirement of 
these energy-hungry devices.
5. Data Integrity:  It is very important to preserve the data integrity of the 
data. Offloading confidential data to other platforms may violate the integ -
rity of the data. There is a requirement for more reliable data verification 
techniques for data producers as well as data consumers.
6. Adaptability:  The number of smart devices and applications using the con -
cept of blockchain is increasing day by day. There should be flexibility for 
devices to connect or leave the network, and the system must have capabil -
ity to withstand the fluctuating demands of the users.
7. Computation:  Solving the consensus algorithms requires high computa -
tion power, time, and energy which cannot be efficiently executed on the 
smart devices. Given the mining process of blockchain is computation-
intensive, it is very important to perform the computations in an efficient 
manner. Smart device computations also need some computing platform 
as they cannot handle many computations on their end only (Yang, Yu, Si, 
Yang, Zhang, 2019).
8. Energy:  Smart devices have restricted computation capacity and battery 
power. Running the mining process on smart devices may require too much 
energy consumption, which may restrict the use of blockchain in IoT or 
mobile environments (Li, Li, Peng, Cui, & Wu, 2019).
6.10 CONCLUSION
Blockchain provides security and privacy measures for various IoT and mobile 
devices. This chapter elaborates on its various types, features, and applications. 
For appending a new block into the blockchain, the consensus mechanism plays 
a vital role. An extensive literature survey has been done to find the integration of 
blockchain with other technologies for reaching consensus. Various platforms for the 
computation of newly generated block in the blockchain have been discussed, along 
with the several issues and challenges. This chapter provides integration of various 
technologies with blockchain for its efficient computation.
REFERENCES
Altman, E., Reiffers, A., Menasche, D. S., Datar, M., Dhamal, S., Touati, C. (2019). Mining 
competition in a multi-cryptocurrency ecosystem at the network edge: A conges -
tion game approach. ACM SIGMETRICS Performance Evaluation Review, 46(3),  
114–117.94 Machine Learning for Edge Computing
Casado-Vara, R., Chamoso, P., De la Prieta, F., Prieto, J., Corchado, J. M. (2019). Non-linear 
adaptive closed-loop control system for improved efficiency in IoT-blockchain manage -
ment. Information Fusion, 49, 227–239.
De Aguiar, E. J., Faiçal, B. S., Krishnamachari, B., Ueyama, J. (2020). A survey of block -
chainbased strategies for healthcare. ACM Computing Surveys (CSUR), 53(2), 1–27.
Garcia-Teruel, R. M. (2020). Legal challenges and opportunities of blockchain technology 
in the real estate sector. Journal of Property, Planning and Environmental Law, 12(2), 
129–145. doi: 10.1108/jppel-07-2019-0039.
Guo, S., Hu, X., Guo, S., Qiu, X., Qi, F. (2019). Blockchain meets edge computing: A distrib -
uted and trusted authentication system. IEEE Transactions on Industrial Informatics, 
16(3), 1972–1983.
Hammi, M. T., Hammi, B., Bellot, P., Serhrouchni, A. (2018). Bubbles of Trust: A decentral -
ized blockchain-based authentication system for IoT. Computers Security, 78, 126–142.
Huh, S., Cho, S., Kim, S. (2017, February). Managing IoT devices using blockchain plat -
form. In 2017 19th international conference on advanced communication technology 
(ICACT) (pp. 464–467). IEEE.
Jiao, Y., Wang, P., Niyato, D., Xiong, Z. (2018, May). Social welfare maximization auction in 
edge computing resource allocation for mobile blockchain. In 2018 IEEE international 
conference on communications (ICC) (pp. 1–6). IEEE.
Kukreja, V., Dhiman, P. (2020, September). A deep neural network based disease detection 
scheme for citrus fruits. In 2020 international conference on smart electronics and 
communication (ICOSEC) (pp. 97–101). IEEE.
Kukreja, V., Marwaha, A., Sareen, B., Modgil, A. (2020, June). AFTSMS: Automatic fleet 
tracking & scheduling management system. In 2020 8th international conference 
on reliability, infocom technologies and optimization (trends and future directions) 
(ICRITO) (pp. 114–118). IEEE.
Khan, M. A., Salah, K. (2018). IoT security: Review, blockchain solutions, and open chal -
lenges. Future Generation Computer Systems, 82, 395–411.
Kumar, N. M., Mallick, P. K. (2018). Blockchain technology for security issues and chal -
lenges in IoT. Procedia Computer Science, 132, 1815–1823.
Lao, L., Li, Z., Hou, S., Xiao, B., Guo, S., Yang, Y. (2020). A survey of IoT applications in 
blockchain systems: Architecture, consensus, and traffic modeling. ACM Computing 
Surveys (CSUR), 53(1), 1–32.
Li, J., Li, N., Peng, J., Cui, H., Wu, Z. (2019). Energy consumption of cryptocurrency min -
ing: A study of electricity consumption in mining cryptocurrencies. Energy, 168,  
160–168.
Liu, M., Yu, F. R., Teng, Y., Leung, V. C., Song, M. (2018). Computation offloading and 
content caching in wireless blockchain networks with mobile edge computing. IEEE 
Transactions on Vehicular Technology, 67(11), 11008–11021.
Lu, Y. (2018). Blockchain and the related issues: A review of current research topics. Journal 
of Management Analytics, 5(4), 231–255.
Luong, N. C., Xiong, Z., Wang, P., Niyato, D. (2018, May). Optimal auction for edge 
computing resource management in mobile blockchain networks: A deep learn -
ing approach. In 2018 IEEE international conference on communications (ICC)  
(pp. 1–6). IEEE.
Minoli, D., Occhiogrosso, B. (2018). Blockchain mechanisms for IoT security. Internet of 
Things, 1, 1–13.
Mingxiao, D., Xiaofeng, M., Zhe, Z., Xiangwei, W., Qijun, C. (2017, October). A review on 
consensus algorithm of blockchain. In 2017 IEEE international conference on systems, 
man, and cybernetics (SMC) (pp. 2567–2572). IEEE.95 Techniques for Consensus Mechanism in Blockchain
Nguyen, D. C., Pathirana, P. N., Ding, M., Seneviratne, A. (2019). Secure computation 
offloading in blockchain based IoT networks with deep reinforcement learning. arXiv 
preprint arXiv:1908.07466.
Nakamoto, S., Bitcoin, A. (2008). A peer-to-peer electronic cash system. Bitcoin. https://
bitcoin.org/bitcoin.pdf , 4.
Pan, J., Wang, J., Hester, A., Alqerm, I., Liu, Y., Zhao, Y. (2018). EdgeChain: An edge-IoT 
framework and prototype based on blockchain and smart contracts. IEEE Internet of 
Things Journal, 6(3), 4719–4732.
Raikwar, M., Mazumdar, S., Ruj, S., Gupta, S. S., Chattopadhyay, A., Lam, K. Y. (2018, 
February). A blockchain framework for insurance processes. In 2018 9th IFIP 
international conference on new technologies, mobility and security (NTMS)  
(pp. 1–4). IEEE.
Reyna, A., Martín, C., Chen, J., Soler, E., Díaz, M. (2018). On blockchain and its integra -
tion with IoT. Challenges and opportunities. Future Generation Computer Systems, 88, 
173–190.
Seng, S., Li, X., Luo, C., Ji, H., Zhang, H. (2019, May). A D2D-assisted MEC computa -
tion offloading in the blockchain-based framework for UDNs. In ICC 2019-2019 IEEE 
international conference on communications (ICC) (pp. 1–6). IEEE.
Sharma, P. K., Park, J. H. (2018). Blockchain based hybrid network architecture for the smart 
city. Future Generation Computer Systems, 86, 650–655.
Xie, J., Yu, F. R., Huang, T., Xie, R., Liu, J., Liu, Y. (2019). A survey on the scalability of 
blockchain systems. IEEE Network, 33(5), 166–173.
Xu, X., Zhang, X., Gao, H., Xue, Y., Qi, L., Dou, W. (2019). BeCome: Blockchain-enabled 
computation offloading for IoT in mobile edge computing. IEEE Transactions on 
Industrial Informatics, 16(6), 4187–4195.
Xiong, Z., Feng, S., Niyato, D., Wang, P., Han, Z. (2018, May). Optimal pricing-based edge 
computing resource management in mobile blockchain. In 2018 IEEE international 
conference on communications (ICC) (pp. 1–6). IEEE.
Xue, L., Teng, Y., Zhang, Z., Li, J., Wang, K., Huang, Q. (2017, September). Blockchain 
technology for electricity market in microgrid. In 2017 2nd international conference on 
power and renewable energy (ICPRE) (pp. 704–708). IEEE.
Yang, R., Yu, F. R., Si, P., Yang, Z., Zhang, Y. (2019). Integrated blockchain and edge com -
puting systems: A survey, some research issues and challenges. IEEE Communications 
Surveys Tutorials, 21(2), 1508–1532.
Zhang, Z., Hong, Z., Chen, W., Zheng, Z., Chen, X. (2019). Joint computation offloading 
and coin loaning for blockchain-empowered mobile-edge computing. IEEE Internet of 
Things Journal, 6(6), 9934–9950.
Zhu, X., Badr, Y. (2018). Identity management systems for the internet of things: A survey 
towards blockchain solutions. Sensors, 18(12), 4215.97 DOI: 10.1201/9781003143468-7
IoT-Inspired Smart 
Healthcare Service for Diagnosing Remote Patients with Diabetes
Huma Naz, Rishabh Sharma,  Neha Sharma, and Sachin Ahuja
Chitkara University Institute of Engineering &  
Technology, Chitkara University  Punjab, India7
CONTENTS
7.1 Introduction  .................................................................................................... 97
7.2 Background of Cloud Computing, IoT, and FC  .............................................. 99
7.2.1  Cloud Computing  ................................................................................ 99
7.2.2  IoT ..................................................................................................... 100
7.2.3  Fog Computing  ................................................................................. 100
7.3 Related Work  ................................................................................................ 102
7.4 Methodology  ................................................................................................. 105
7.4.1  Healthcare Application  ..................................................................... 105
7.4.2  Dataset Description  .......................................................................... 106
7.4.3  Deep Feed-Forward Neural Network  ............................................... 106
7.4.4  Pros of Using FC ............................................................................... 108
7.5 Experimental Evaluation  .............................................................................. 108
7.5.1  Evaluation Metrics  ............................................................................ 109
7.5.1.1  Experimental Results and Discussion  ................................ 109
7.6 Conclusion  .................................................................................................... 111
References  .............................................................................................................. 111
7.1 INTRODUCTION
In our bodies, the hormone insulin is responsible for transforming starches, sugar, and other aspects of food into needed energy. If our body doesn’t form or use insulin, the excessive amount of sugar is evacuated through urination, and this is indicative of a disease referred to as diabetes. Usually, Diabetes Mellitus (DM) occurs when a person has high or above normal blood sugar levels and glucose doesn’t reach each body cell. As per the American Diabetes Association [1], 20.8 million children and 98 Machine Learning for Edge Computing
adults were diagnosed with this disease in the United States. This impact of diabetes 
is serious and it can be fatal, and it is related to other medical conditions, such as strokes, blindness, kidney failure, miscarriages, and amputations. Hence, diagnosing diabetes in its early stages plays an essential part in the patient’s treatment process, increasing the quality of life. There are three main types of diabetes: type 1, type 2,  
and gestational diabetes [ 2]. In type 1 diabetes, the pancreatic cells that are respon -
sible for producing insulin are destroyed; type 1 normally occurs up until the age of 20 years. Type 2 diabetes occurs when the body increases the demand for sugar or becomes insulin resistant. Gestational diabetes most often occurs in pregnant women when a sufficient amount of insulin is not generated by pancreatic cells. To avoid the complications associated with this disease, early detection and treatment is needed. However, data collected from patients directly stored on the cloud server to perform tasks, e.g., diagnosing, accessing health records, predicting the disease, is inadequate for real-time processing. In emergency situations related to the disease, instant medi -
cal alerts and quick responses are needed to save lives. Hence, cloud is a centralized repository storage system; and for immediate medical decisions, an intermediate paradigm is required. The IoT cloud-based infrastructure has some drawbacks with latency, delays, availability, distribution, and more. To avoid these kinds of issues, Fog Computing (FC) integrates as a fog layer that acts as a transitional layer between end devices and cloud [ 3]. Fog makes communication, processing, and transmission faster 
instead of the cloud-edge based infrastructure. Multiple fog nodes are deployed in the infrastructure closer to the end devices as well as cloud and get responses without any jitter or delay [ 4]. With predictions, this might be irrelevant if no applicable mechanism 
can be adapted. Therefore, DM approaches and Machine Learning (ML) algorithms are used to reduce transmissions or delay time and cost for better prediction. The DM technique provides better predictions in areas such as healthcare, academics, and many more [ 5, 6]. In this chapter, fog-aided DM and ML approaches, along with supporting 
literature, are discussed. The aim is to diagnose diabetes patients in the early stages to monitor their health using FC delays, bandwidth, and cost of resources.
In any nation’s development and progress, healthcare services are considered an 
important area that must be managed effectually and competently. Nowadays, an emerging need arises for the instant analysis of users’ data as well as real-time deci -
sions without delays. So, cloud, IoT, and fog technology can be combined to develop many smart healthcare applications, which can perform better than the existing healthcare systems.
FC is another emerging technology that brings the cloud services closer to the 
“Thing,” such as sensors, mobile phones, and embedded system. Fog technology places the computing and processing power closer to the IoT devices or mobile phones as compared to cloud computing that reduces latency and communication overhead. This is because the physical distance between IoT devices and fog nodes is shorter, and it takes a minimum response time for real-time decision making. FC also performs the computation of highly sensitive data on a local gateway, which improves the security of sensitive data [ 7]. Due to local computation, FC reduces the 
load from more centralized resources, which leads to less congestion on the network.
This chapter is organized in various sections. Section 7.2 presents the back -
ground of cloud computing, IoT, and FC. A review of related literature is presented in  99 IoT-Inspired Smart Healthcare Service
Section 7.3 . Section 7.4 surveys the healthcare mechanism based on diabetes fol -
lowed by cloud, IoT, and fog-based healthcare frameworks applications. The results 
are discussed in Section 7.5 . This chapter concludes with Section 7.6 .
7.2  BACKGROUND OF CLOUD COMPUTING,  
IoT, AND FC
In this section, an introduction to cloud computing, IoT, and FC is provided.
7.2.1 c loud comput Ing
Cloud computing is an emerging technology that provides cost-effective IT resources 
such as infrastructure, storage, servers, applications, and services to end users.  
It offers easy-to-use software and data through the internet that can be shared among different clients, and requires no technical expertise. In the last few years, cloud computing has gained popularity due to its significant features and benefits like a lower investment upfront, low operating costs, fast deployment, dynamic scalability and automated self-provisioning of resources, architecture abstraction, pay-as-you-go model, low-cost disaster recovery, massive data storage solutions, easy access and fewer maintenance expenses, ubiquity (i.e., device and location independent), and operational expense model. One of the most significant features of cloud computing is its ability to store enormous amounts of data from diverse business establishments that can be shared among them. Cloud computing provides three importance ser -
vices: infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS), respectively as shown in Figure 7.1 . IaaS offers support for hardware, 
storage, software, servers, and other infrastructure components in the form of virtu -
alized computing resources to end users over the internet on a pay-per-use basis [8].  
PaaS is a cloud service provider that provides hardware and software services to its users for the development of application and software. A PaaS provider hosts both the software and hardware applications to its users on its own infrastructure. As a result, PaaS provides a service in which users don’t need to install or run hard -
ware and software applications. SaaS is a software distribution model, which hosts  
the cloud services, making them available to customers over the internet. Among these services, we have especially seen a dramatic growth with database as a ser -
vice (DBaasS), in which enterprises are allowed to store their databases in the cloud and to access them through internet. The data stored in cloud is considered data outsourcing since it is managed by an external party. The DBaaS model is a major shift from the traditional model of data management that provides seamless mechanisms for organizations to operate their databases upon, from anywhere in the world using internet. Outsourcing databases into the cloud suggests great suit -
ability to enterprises since they don’t have to maintain the complexities of direct hardware management and database management on their local machines. Today, many cloud-based databases services, such as Microsoft Azure SQL, Amazon SimpleDB, Google App Engine, and ClearDB, operate entirely in cloud for cus -
tomers and enterprises.100 Machine Learning for Edge Computing
7.2.2 Iot
IoT was originally referred to as the “embedded internet.” The acronym IoT was first 
coined by Kevin Ashton in 1999 [ 9]. IoT refers to the network of physical objects 
or things that are embedded and connected with software or sensors by means of 
exchanging and sharing data with one another. Since IoT came into picture, it is uti -
lized in almost all home automation, smart healthcare applications, smart transport, and utilities applications [ 10]. There are different key-enabling technologies avail -
able for IoT comprising Radio-Frequency Identification (RFID), smart detection, wireless sensor network technology, artificial intelligence, and cloud computing [ 11]. 
IoT is an advanced, rapidly developed network of internet-connected sensors that are embedded in a wide-ranging variety of applications [ 12, 13]. The idea of advance -
ment in the IoT came into existence after a decade; furthermore, development of low-power sensor technologies, connectivity, cloud computing platforms, and ML also enhances its capability.
7.2.3 f og comput Ing
A distributed computing paradigm approach was introduced by CISCO to overcome 
the flaws of cloud computing. FC enables users to access services and computation between cloud and end users closer to each other without any traffic congestion, without delay and security issues, and without depending on the internet. As large amounts of data are generated from the IoT, sensors, and smart wearable devices to monitor the gathered data on cloud, decentralized computing is required. A fog-based layer is integrated with the cloud infrastructure, in which applica -
tions, storage, data analytics, and objects are deployed between the fog nodes and cloud storage. Multiple fog nodes are also deployed according to the infrastruc-ture and network requirements. The key idea of introducing FC is to conquer the  
FIGURE 7.1  Cloud computing architecture.101 IoT-Inspired Smart Healthcare Service
restrictions and obstructions that arise through the cloud while processing, execut -
ing, or replying to requests on a network. Hence, the FC paradigm is beneficial in 
aspects of latency, bandwidth, delays, jitters, fault tolerance, security, responsive -
ness, energy, and speed. It can be said that FC boosts up the overall efficiency of the network. A scenario is introduced in Figure 7.2 , in which a fog layer is located 
between the IoT devices and cloud computing to make cities smarter. Likely in smart 
FIGURE 7.2  Fog computing architecture.102 Machine Learning for Edge Computing
cities, a lot of IoT-driven applications, such as a traffic light and management system, 
smart buildings, health management, parking systems, sanitation, among others, are latency-based and the data generated by these devices is a huge volume that requires huge bandwidth, low latency rates, and minimum response times and costs. But the cloud does not fulfill these mentioned requirements. In order to overcome the cloud drawbacks, FC accomplishes these issues.
7.3 RE LATED WORK
Utility DM techniques have outmoded the extant technologies in terms of accuracy and accurate prediction. DM techniques and ML are efficient in extracting hidden and concealed patterns that may result in true information and remain undetected while using other statistical and analytical methods. ML techniques are capable of training the data at a perfect level, so that the outcome will be more likely to be pre -
cise while testing. On the other hand, FC decentralizes the data center’s resources and makes them closer to the user for improving the user experience and quality of service. This method of computing works as an intermediate node; therefore, computational resources are not required from cloud data centers. The subsequent literature review shows the usability of DM and ML techniques around FC that may have better outcomes for the prognosis of chronic diseases like diabetes.
According to Rabindra Kumar Barik et al. [ 14], mist computing is proven to be 
functional for medical health applications. In this end, devices aided the fog nodes to decrease the latency and augment supporting the edge of the client, and improve the client experience. In this paper, the rise of mist computing has been discussed for DM analytics from healthcare applications. This paper recommends and proposes the mist computing-based framework, i.e., mist learn used to detect patients suffer -
ing from the DM in the real world.
In essence, Rabindra Kumar Barik et al. [ 15] proposed a fog-based framework 
that is used for diabetes prognosis, i.e., fog learns along with the application of this methodology. The proposed methodology implements the Ganga River manage -
ment approach and K-means clustering technique for detecting the diabetic patients through real-world data. It also implements ML techniques for extracting the patho -
logical feature obtained from the smart watch worn by the diabetic patient. The result shows the predicted outcome of the disease with an analysis of medical and real-world big data of the diabetic patient through FC.
Rojalina Priyadarshini et al. [ 16] proposed an FC-based neural network deep fog 
that can analyze the detection and prediction of diabetes, types of stress, and hyper -
tension. This given methodology and the deep fog framework process and group data from individuals and realize the fitness of a particular FC node based on neural networks that can easily handle the mixed and complex data. The proposed system and architecture validated the outcomes and effectiveness for the precise monitoring of critical disease and fitness levels.
In 2017, Gunasekaran Manogaran et al. [ 17] proposed the IoT framework and use 
of big data for smart and secure tracking. Smart wearable watches produce a huge amount of data, including structured and unstructured data, which further results in big data, and it is tricky to extract meaningful, constructive, and functional data 103 IoT-Inspired Smart Healthcare Service
from big data. To solve this issue, IoT-based architecture has been proposed that con -
sists of two sub-parts, named as metadata for redirecting, and grouping for selecting 
architectures. These architectures come together to secure the system by integrating FC and cloud computing. Furthermore prediction of the disease is done through the map-based prediction model. A performance evaluation of this proposed framework has been done with consideration to throughput, effectiveness, efficiency, sensitivity, and F-measures.
IOT technology implements the data in an efficient way to reach out to remote 
and mobile healthcare patient monitoring locations. Despite its reimbursement, this produces extraordinary data that can be easily handled through cloud com -
puting platform. Therefore, Prabal Verma and Sandeep K. Sood [ 18] proposed 
the IOT implementation for health monitoring in smart homes and used the FC framework. This framework can overcome the problems by working as an inter -
mediate node. Some advanced features, such as the data warehouse, appropriate data storage, and a reply service at the boundary of the smart home system have also been introduced. 
Ahmed et al. [ 19] introduced a model in 2016 as a fog-based healthcare monitor -
ing model, named Health fog. In their proposed work, the fog layer is introduced as an intermediator in the middle of the cloud and edge devices. For enhancing safety, a cloud access security broker (CSAB) is incorporated into Health fog along with the integration of cryptographic primitives for securing data to be sent through cloud. IoT plays an essential role in the remote health monitoring systems. Therefore, the Ahmed et al. presented an architectural-based IoT-based health as a u-healthcare monitoring system that can provide services to remote patients. This architecture mainly focuses on the IoT-based service delivered to the edge of smart homes and smart hospitals, so that the end user can get all the healthcare services without dif -
ficulty. In 2018, Negash et al. [ 20] took this tour on continuation and focused on 
an FC-based smart e-health gateway implementation for reaching out to remote patients. They thought of the implantation of gateways as the connection between home and hospital that can connect network. Moreover, the network functions and parameters are discussed.
IoT-based healthcare applications require immediate analysis of health-oriented 
data generated by IoT devices for making real-time decisions and generating rec -
ommendations for a healthy lifestyle. Since IoT is capable of providing advance health diagnoses and medication, Latif et al. [ 21] proposed a novel approach with 
wireless sensors networks and cloud-based servers for patient’s continuous health status monitoring. The methodology consists of a wearable health monitoring sys -
tem, an AI-based tool, cloud big data storage, and an analytic prognostic system and  
medicine-dispensing system. Few tests have been done on system to check if the system achieved the intended objective, including providing a continuous health-monitoring status and timely dosages to the patient. The proposed system helps healthcare officials to view the effects of medication on patients and remotely moni -
tor their health status.
Medicinal services are fundamental to humans. It is crucial for the advance -
ment and improvement of a society. In recent years, IoT has played a very major in the revolution of healthcare industry. Bhatia et al. [ 22] proposed an effective 104 Machine Learning for Edge Computing
and centric urine-based diabetes monitoring system. Mainly, the system contains 
four stages for a systematic diagnosis of a urine-based diabetes infection. The four stages include the data acquisition stage, data classification stage, feature extraction/ mining stage, and diabetic prediction stage. The prediction of diabetes is done using recurrent neural network (RNN). The experiments are validated on four differ -
ent individuals. With this knowledge, Akkas et al. [ 23] discussed advancements in 
healthcare and patient monitoring using IoT. They further presented applications 
of IoT technology in diverse medical fields and some future trends with Bio-IoT 
or Nano-IoT systems. According to the authors, the most important component of a remote patient’s health monitoring is Wireless Body Area Network (WBAN), which is placed on the body of a patient and can communicate wirelessly. The methodology presents the implementations of a biomedical application based on WBAN. The collected data is transmitted from a diverse wireless sensor network using IoT devices.
Data mining with IoT emerged as an influential computing technique in the 
healthcare industry. Researchers have used different data mining techniques with IoT for diabetes and other disease detection so far. However, more efficient techniques are needed for the detection of diabetes and heart disease. Therefore, a smart data mining-IoT enabled advanced technique has been proposed for the early detection of diseases [ 24]. This technique comprises bio sensors, IoT, chatbots, semantic analy -
sis, and granular computing. Bio sensors help get the data of concerned patients. As discussed, IoT has a lot of advantages in the field of healthcare; thus, Fradin et al.  
[25] proposed an IoT-based and cloud-based healthcare diagnosis system with the 
ML algorithms. In the proposed system, the data are recorded through wearable sensors; then the processed signals/data are transmitted to the network in cloud environment. The article also presents the novel hybrid approach of decision tree. In this process, a new feature set is created for testing neural fuzzy model. The diagnosis of specific disease or diabetic problems can be simulated with efficient outcomes. A survey on security of IoT framework was performed by Ammar et al. [26]. In this survey, eight main frameworks of IoT are considered and a detailed 
comparative analysis is performed considering their proposed architecture, issues in the development of third-party smart applications, and hardware and software compatibility for ensuring security. With the growing populations and older peo -
ple, the society needs a personalized healthcare system to avoid and manage the chronic condition. Therefore, Wang et al. [ 27] discussed three key points in the 
paper. The first is to review the key factors of the home-based remote healthcare system; the second is to present the latest advances of remote healthcare system; and the third is to review the recommendations for home-based in-home health -
care monitoring.
FC is another emerging technology that brings the cloud services closer to the 
“Thing,” i.e., sensors, mobile phones, and an embedded system. Thus, Devarajan et al. [ 28] proposed a fog-assisted approach to maintain the glucose level. The deci -
sion tree classifier is used to predict the risk level of diabetes for achieving high 
classification accuracy. With FC, an emergency alert is generated for preventive 
measures. Experimental results illustrated an improved accuracy, computational complexity, and latency.105 IoT-Inspired Smart Healthcare Service
The term IoT for the most part alludes to situations where organized networks and 
registrants’ ability reach out to objects, sensors, and empower these gadgets to cre -
ate, trade, and devour information with little human intervention, utilizing different 
systems’ administration and correspondence models. Over the years, IoT is used in diverse areas such as medical and healthcare systems, smart homes, remote services, and many other areas [29]. The authors in [29] present the different applications of IoT; moreover, the possible potential of the technology in the future trends is also presented.
7.4 M ETHODOLOGY
To accomplish the specified aim, a framework consisting of a three-tier architecture –  
namely, the end layer, fog nodes layer, and cloud computing – is proposed in this chapter. With the help of deep learning, accurate results are attained on the end layer. After that, the data collected by end devices is sent to the fog server and then t
o the cloud. The dataset applied in this dataset is a Pima dataset and a fog layer 
integrated with the cloud infrastructure helps to achieve results without any delay or jitters.
7.4.1 h eAlthc Are ApplIcAtIon
In healthcare applications, IoT-based sensors like electrocardiography (ECG) sensor,  
heart rate monitor, Global Positioning System (GPS) sensor, Radio-Frequency Identification (RFID) sensor, accelerometer, and blood pressure monitor sensor are the major sources for acquiring data such as the vital signs of the user and sur -
rounding environment parameters. The data generated from the sensors can be captured using mobile devices. The information collected from sensors is combined with a user’s personal information, such as name, age, gender, weight, and occupa -
tion. Mobile phones play an important role in the healthcare applications and can be integrated with IoT devices and cloud servers for expanding the network. The data generated by applications is stored on the cloud; thus, realizing the importance and need of remote analysis of patients is extremely significant. Also, the direct analysis of a patient’s real-time data and decisions without any delay is in demand nowadays. Health monitoring applications are considered among the fastest growing applications among mobile applications with the integration of cloud computing and IoT. This integration has been proven as a very helpful resource for gathering users’ data and plays an essential role in diagnosing and generating recommendations to remote patients for a healthy lifestyle. To further improve the quality of service (QoS) and immediate notifications of real-time applications, there is another trend to provide computing and processing services at the edge of the network devices, i.e., mobile phone, routers, hubs, and switches. To achieve this objective, cloud com -
puting raises major issues, such as transmission latency, a high power consumption rate, location awareness issues, and a degradation of services due to huge volume of traffic between IoT devices and cloud. So, FC can be the glue factor for most of the applications running on real-time data generated by mobile and sensor devices to provide computing and processing services at the edge of network. Chronic diseases  106 Machine Learning for Edge Computing
such as hypertension, diabetes, and coronary heart disease (CHD) are the leading 
causes of death. Early detection and prevention of CHD is essential, as doctors rec -
ommend that these diseases can be controlled only with the prior knowledge and healthy diet. It can be said that precautionary measures and timely medical treat -
ment can reduce the mortality rate of diabetic patients. It is also the most important challenge for officials, medical officers, and private healthcare agencies to confirm the safety of their citizens from diseases like diabetes and CHD. Presently, doctors are overburdened with patients as diverse, new viruses are on the rise and there is an inadequate number of caregivers in hospitals. Therefore, physically monitoring every patient’s case is very tough for doctors. Such type of diseases cannot be effec -
tively predicted by using the existing healthcare systems. To overcome these issues, a fog-assisted cloud-based healthcare system with localization technology can be utilized to track the current location of high-risk patients with chronic diseases and their health behaviors to provide quick service as well as improve the quality of care in hospitals and the home. So, incorporating fog into IoT is required to provide the identification, prevention, and control measures for chronic diseases like diabetes, from remote sites, in the early stages.
7.4.2 d AtAset descr IptIon
Various experiments have been done using the Pima Indian Dataset (PID). The data -
set was originally from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK). PID was taken from the UC Irvine Machine Learning (UCI ML) repository for this work [30]. The reason for selecting this dataset is that most of the people in modern times are living with an identical type of lifestyle that includes a high reliance on processed food coupled with declining physical activities. Pima is a group of Native Americans who lived in an area now known as central Arizona. Due to their genetic predisposition, they can survive on low carbohydrates for many years. However, during recent past, the Pima group suddenly shifted from their traditional diet toward processed food, followed by a decrease in their physical activities [31]. Consequently, they were detected with high levels of type 2 diabetes; and for this rea -
son, since 1965, their health data have been used in many studies related to diabetes.
PID includes a certain number of medical predictors and one variable target. 
The predictor variables are the number of pregnancies, body mass index (BMI), blood pressure, skin thickness, insulin level, age, glucose, and diabetes pedigree function shown in Table 7.1 . All the participants in the PID study are females up to 
the age of 21. The dataset has 768 instances divided into 268 non-diabetic instances and 500 diabetic instances. The target variable identifies whether a person is non-  
diabetic (represented by 0) or diabetic (represented by 1). The description of different parameters of each attribute in the dataset, including max value, min value, standard deviation, missing value, mean, and the median are given in Table 7.2 .
7.4.3 d eep feed-forWArd neurAl netWork
Deep feed-forward neural networks are commonly called Multilayer Perceptron 
(MLP) or feed-forward neural networks. A neural network originates from a very 107 IoT-Inspired Smart Healthcare Service
famous ML algorithm known as perceptron [ 32]. Perceptron is a linear classifier 
with a mapping function that partitions feature space using a linear function that 
is a boundary line used to classify data into two classes [ 33]. The MLP neural  
network is a model with multiple layers of input that are associated with some weight, which are presented in the processor in a feed-forward manner [ 34], as shown in 
Figure 7.3 .
Figure 7.3  demonstrates the architecture of the feed-forward network where the 
input layer is denoted as X1 and 1 is the bias value, whereas hidden layers are associ -
ated with the activation function represented as F1, F2, and F3. Each input neuron, TABLE 7.1
Description of PID Attributes
Sr. No. Predicators Description of Predicators Unit
1. Pregnancy Number of times a female participant is pregnant —
2. Plasma glucose Glucose concentration in 2 hours in an oral glucose 
tolerance testmg/dl
3. Diastolic blood pressure Diastolic blood pressure (upper blood pressure) mmHg
4. Triceps skinfold thickness Skin thickness of participant in mmConcluded by the collagen contentmm
5. Insulin Participant’s 2-hour serum insulin mmU/Ml
6. Body mass index Weight of a participant in kg/HEIGHT (IN M) ^2) kg/m
2
7. Diabetes pedigree function Appealing attributes used for diabetes diagnosis —
8. Age Age of participants —
9. Outcome Diabetes onset with diabetic and non-diabetic patients —
TABLE 7.2Detailed Description of PID Attributes
Sr. No. Predicators Missing Values Mean Std Dev Range Data Type
1. Pregnancy 0 3.845 3.370 0–17 Integer
2. Glucose 0 120.89 31.973 0–199 Integer
3. Diastolic blood 
pressure0 316.56 1096.927 0–122 Integer
4. Skinfold thickness 0 51.697 88.690 0–99 Integer
5. Insulin 0 819.49 3873.732 0–846 Integer
6. Body mass index 0 60.769 92.015 0–67.1 Real
7. Diabetes pedigree 
function0 0.472 0.472 0.078–2.42 Real
8. Age 0 33.241 11.760 21–81 Integer
9. Outcome (Diabetic instances 
– 268)
(Non-diabetic 
instances – 500)
(Total instances – 768)Polynomial108 Machine Learning for Edge Computing
whether travelling toward the hidden layer or output layer, has some weight con -
nected to it. The processor uses an activation function to produce the output, which is 
represented as F1, F2, and F3 in the hidden layer’s nodes [ 35]. If the predicted output 
in the output layer is the same as the desired output, then the performance is consid -
ered satisfactory. No change will be made in the weights; otherwise, the weights will be updated to reduce errors.
7.4.4 p ros of usIng fc
Cloud computing is in trend from last few years due to its applications in central stor -
age, data processing and analysis, but due to huge amounts of data being forwarded to cloud, traffic congestion increases significantly. Despite this, the computation process takes more energy, which makes the cloud infrastructure a greater energy consumer. On the other hand, with the use of FC data, transmission and computation processes take place locally as fog makes end users closer to the edge of the network [ 36].  
Hence, in emergency cases, FC provides lower energy consumption, bandwidth and low-latency services, and fends off some grievous consequences.
7.5 E XPERIMENTAL EVALUATION
In this purposed framework, DM, along with ML algorithms, e.g., decision trees and neural networks, is adapted to monitor diabetes patients for accurate predictions. The applied data will be sent to fog nodes through the fog layer, which acts as an intermediary between cloud and end users. To evaluate the efficiency of Fog Device Combined Approach (FDC), four interpretation metrics are considered: recall, accuracy, precision, and f-measure. By using these algorithms and mechanisms, we have simulated our results. The outcome of this framework completely proves that introducing the fog layer improves the efficiency and computation of the network infrastructure.
FIGURE 7.3  Multilayer perceptron model.109 IoT-Inspired Smart Healthcare Service
7.5.1 Evaluation  MEtrics
Different evaluation metrics are used to compute the essence of the proposed pre -
diction model. Evaluation metrics tend to play a significant role in measuring the 
performance of the prediction model. There can be a case where a model can perform well with a specific evaluation metric, but it can be considered less accurate when any other evaluation metric comes into use. So, the selection of suitable performance metrics is an essential part of the development. Several evaluation metrics are avail -
able for evaluating the performance of classification models, such as classification accuracy, sensitivity-specificity, F1-score, receiver operating characteristic (ROC) curve, and area under the curve (AUC), which can be used to measure performance.  
But the evaluation parameters considered to evaluate the performance of the applied technique are accuracy, sensitivity, and specificity, which are discussed here in  
detail.
•
 Accuracy: Accuracy of a measurement is the ratio of correctly classified observations to the total number of observations [37], or accuracy can be referred to as the closeness of a measured value to a standard or known value. Accuracy can be measured using Eqn. (7.1). Accuracy is a good per -
formance measure when the target variable class data is nearly balanced.
 Accu racyTP TN
TP FP FN TN=+
++ +  (7.1)
Here, true positives are abbreviated as TP; true negatives are abbreviated as TN; false positive are abbreviated as FP; and false negatives are abbreviated as FN.
•
 Sensitivity:  Sensitivity is a proportion of the true positive correctly classi -
fied [ 38]. Sensitivity is a performance measure that evaluates the true posi -
tives from each class label and can be measured using Eqn. (7.2).
 SentivityTP
TP FP=+  (7.2)
• Specificity: Specificity is a proportion of true negative correctly classified 
[39]. Specificity is an evaluation metrics that measures the true negatives from each class label and can be measured using Eqn. (7.3).
 SpecificityTN
TN TP=+  (7.3)110 Machine Learning for Edge Computing
precision, recall, and F-measure, were calculated for the classification algorithm 
using the PIMA dataset. Table 7.3 shows that our proposed integrated approach outperformed in every performance measure and provided the best result for the detection of diabetes in remote patients with an accuracy rate of 96.13%. Figure 7.4 shows the comparison between the distinct performance metrics for diabetes detection.
As presented in Figure 7.4 and Table 7.2, our proposed approach provided reliable 
accuracy on the PIMA dataset. The achieved accuracy rate of 96.67% shows that the approach can be used as a prognostic tool for early diabetes detection to remote patients. Those features that do not contribute to the study need to be pruned [40]. In this study, we have used the best-selected attributes for achieving an accurate diagnosis of onset diabetes.TABLE 7.3
Combined Performance Measures of DL Algorithm
Performance MeasuresMethods
Proposed Approach (Weka) Proposed Approach (Rapid Miner)
Accuracy (In %) 96.62 96.23
Precision (In %) 95.06 98.24
Recall (In %) 96.35 97.71
F-Measure (In %) 93.21 99.14
Specificity (In %) 97.86 95.52
Sensitivity (In %) 94.03 96.23
FIGURE 7.4  Comparison of performances of classification method.111 IoT-Inspired Smart Healthcare Service
7.6 C ONCLUSION
A three-tiered cloud-fog based framework consisting of an end layer, fog node layer, 
and cloud computing is proposed in this chapter. The DL algorithm is applied on the end layer to remotely access the data of healthcare patients. Where users can -
not communicate and compute the processes directly with the cloud, a fog layer is introduced in the infrastructure on which user can put their request and perform the computation tasks easily and quickly. An accuracy of 96.67% was achieved on Weka and 96.62% on the rapid miner toolkit. In the future, we will execute this framework with real-time datasets and more parameters, e.g., cost, fault tolerance, security, and delays.
REFERENCES
 1. Pham, Huy Nguyen Anh, and Evangelos Triantaphyllou. “Prediction of diabetes by 
employing a new data mining approach which balances fitting and generalization.” 
Computer and Information Science . Springer, Berlin, Heidelberg, 2008. 11–26.
 2. Barik, Rabindra K., et al. “FogLearn: Leveraging fog-based machine learning for 
smart system big data analytics.” International Journal of Fog Computing (IJFOG 
COMPUTING)  1.1 (2018): 15–34.
 3. Luan, Tom H., et al. “Fog computing: Focusing on mobile users at the edge.” arXiv 
preprint arXiv:1502.01815 (2015).
 4. Bonomi, Flavio, et al. “Fog computing and its role in the internet of things.” Proceedings 
of the first edition of the MCC workshop on Mobile cloud computing . ACM, 2012.
 5. Kamal, Preet, and Sachin Ahuja. “Academic performance prediction using data mining 
techniques: Identification of influential factors effecting the academic performance in undergrad professional course.” Harmony Search and Nature Inspired Optimization Algorithms . Springer, Singapore, 2019. 835–843.
 6. Kamal, Preet, and Sachin Ahuja. “An ensemble-based model for prediction of academic 
performance of students in undergrad professional course.” Journal of Engineering, Design and Technology 17, no. 4 (2019): 769–781.
 7. Stojmenovic, I. “Fog computing: A cloud to the ground support for smart things and 
machine-to-machine networks.” In 2014 Australasian Telecommunication Networks 
and Applications Conference (ATNAC) (pp. 117–122). 2014, November. IEEE.
 8. Ashton, K. “That ‘internet of things’ thing.” RFID Journal  22, no. 7 (2009): 97–114.
 9. Atzori, L., A. Iera, and G. Morabito, “The internet of things: A survey.” Computer 
Networks  54, no. 15 (2010): 2787–2805.
 10. Sundmaeker, H., P. Guillemin, P. Friess, and S. Woelfflé. “Vision and challenges for 
realising the Internet of Things.” Cluster of European Research Projects on the Internet 
of Things, European Commission  3, no. 3 (2010): 34–36.
 11. Buckley, J. 2006. “The internet of things: From RFID to the next-generation pervasive 
networked systems.”
 12. Atzori, L., A. Iera, and G. Morabito. “The internet of things: A survey.” Computer 
Networks  54, no. 15 (2010): 2787–2805.
 13. Gubbi, J., R. Buyya, S. Marusic, and M. Palaniswami. “Internet of Things (IoT): A 
vision, architectural elements, and future directions.” Future Generation Computer 
Systems  29, no. 7 (2013): 16451660.
 14. Barik, Rabindra Kumar, et al. “Leveraging machine learning in mist computing 
telemonitoring system for diabetes prediction.” Advances in Data and Information Sciences . Springer, Singapore, 2018. 95104.112 Machine Learning for Edge Computing
 15.  B arik, Rabindra K., et al. “FogLearn: leveraging fog-based machine learning for 
smart system big data analytics.” International Journal of Fog Computing (IJFOG 
COMPUTING)  1.1 (2018): 15–34.
 16.  P riyadarshini, Rojalina, Rabindra Barik, and Harishchandra Dubey. “DeepFog: Fog 
computing-based deep neural architecture for prediction of stress types, diabetes and 
hypertension attacks.” Computation  6.4 (2018): 62.
 17.  M ohammed, M. N., S. F. Desyansah, S. Al-Zubaidi, and E. Yusuf. “An internet of 
things-based smart homes and healthcare monitoring and management system.” Journal of Physics: Conference Series 1450, no. 1 (2020): 012079. IOP Publishing.
 18.  V erma, Prabal, and Sandeep K. Sood. “Fog assisted-IoT enabled patient health moni -
toring in smart homes.” IEEE Internet of Things Journal 5.3 (2018): 1789–1796.
 19.  A hmad, Mahmood, Muhammad Bilal Amin, Shujaat Hussain, Byeong Ho Kang, 
Taechoong Cheong, and Sungyoung Lee. “Health fog: A novel framework for health and wellness applications.” The Journal of Supercomputing 72, no. 10 (2016): 3677–3695.
 20.  N egash, Behailu, Tuan Nguyen Gia, Arman Anzanpour, Iman Azimi, Mingzhe Jiang, 
Tomi Westerlund, Amir M. Rahmani, Pasi Liljeberg, and Hannu Tenhunen. “Leveraging fog computing for healthcare IoT.” Fog Computing in the Internet of Things , Springer, 
Cham, 2018. 145–169.
 21.  Latif, Ghazanfar, Achyut Shankar, Jaafar M. Alghazo, V. Kalyanasundaram, C. S. 
Boopathi, and M. Arfan Jaffar. “I-CARES: Advancing health diagnosis and medication  
through IoT.” Wireless Networks  26, no. 4 (2020): 2375–2389.
 22.  B hatia, Munish, Simranpreet Kaur, Sandeep K. Sood, and Veerawali Behal. “Internet 
of things-inspired healthcare system for urine-based diabetes prediction.” Artificial 
Intelligence in Medicine  107 (2020): 101913.
 23.  Akka ş, M. Alper, Radosveta Sokullu, and H. Ertürk Çetin. “Healthcare and patient 
monitoring using IoT.” Internet of Things 11 (2020): 100173.
 24.  S harma, Manik, Gurvinder Singh, and Rajinder Singh. “An advanced conceptual diag -
nostic healthcare framework for diabetes and cardiovascular disorders.” arXiv preprint 
arXiv:1901.10530  (2019).
 25.  A bdali-Mohammadi, Fardin, Maytham N. Meqdad, and Seifedine Kadry. “Development 
of an IoT based and cloud-based disease prediction and diagnosis system for healthcare using machine learning algorithms.” International Journal of Artificial Intelligence  
2252, no. 8938 (2020): 8938.
 26.  A mmar, Mahmoud, Giovanni Russello, and Bruno Crispo. “Internet of Things: 
A survey on the security of IoT frameworks.” Journal of Information Security and 
Applications  38 (2018): 8–27.
 27.  P hilip, Nada Y., Joel JPC Rodrigues, Honggang Wang, Simon James Fong, and Jia 
Chen. “Internet of Things for in-home health monitoring systems: Current advances, challenges and future directions.” IEEE Journal on Selected Areas in Communications  
39, no. 2 (2021): 300–310.
 2 8.  D evarajan, Malathi, V. Subramaniyaswamy, V. Vijayakumar, and Logesh Ravi. “Fog-
assisted personalized healthcare-support system for remote patients with diabetes.” Journal of Ambient Intelligence and Humanized Computing  10, no. 10 (2019): 3747–3760.
 29.  M anogaran, Gunasekaran, et al. “A new architecture of Internet of Things and big 
data ecosystem for secured smart healthcare monitoring and alerting system.” Future 
Generation Computer Systems  82 (2018): 375–387.
 30.  “ PIMA Indian Dataset Source” [Online]. Available: http://archive.ics.uci.edu/ml/
datasets/Pima +Indians +Diabetes .
 31.  “ Reason of Choosing PIMA Indian Dataset,” [Online]. Available: https://www.andrea -
grandi.it/2018/04/14/machine-learning-pima-indians-diabetes /.
 3 2.  M iikkulainen, R., J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon, and  
B. Hodjat. (2019). “Evolving deep neural networks.” Artificial Intelligence in the 113 IoT-Inspired Smart Healthcare Service
Age of Neural Networks and Brain Computing,  293–312. https://doi.org/10.1016/
b978-0-12-815480-9.00015-3
 33.  P andi, A., M. Koch, P. L. Voyvodic, P. Soudier, J. Bonnet, M. Kushwaha, and J.-L.  
Faulon. “Metabolic perceptrons for neural computing in biological systems.” Nature 
Communications  10, no. 1 (2019): 1–13. https://doi.org/10.1038/s41467-019-11889-0 .
 34.  T ajmiri, S., E. Azimi, M. R. Hosseini, and Y. Azimi. “Evolving multilayer percep -
tron, and factorial design for modelling and optimization of dye decomposition by bio-  
synthetized nano CdSdiatomite composite.” Environmental Research  182 (2020): 
108997. https://doi.org/10.1016/j.envres.2019.108997 .
 35.  K ukreja, V. and P. Dhiman. “A deep neural network based disease detection scheme 
for citrus fruits.” In 2020 International Conference on Smart Electronics and 
Communication (ICOSEC)  (pp. 97–101). 2020, September. IEEE. 
 36.  G oyal, M., R. Goyal, P. Venkatappa Reddy, and B. Lall. “Activation functions.”  
Deep Learning: Algorithms and Applications  (2019): 1–30. https://doi.org/10.1007/  
978-3-030-31760-7_1 .
 37.  H ung, A. J.,  J. Chen, and I. S. Gill. “Automated performance metrics and machine 
learning algorithms to measure surgeon performance and anticipate clinical outcomes 
in robotic surgery.” JAMA Surgery  153, no. 8 (2018): 770. https://doi.org/10.1001/
jamasurg.2018.1512 .
 38.  K ukreja, V., D. Kumar, and A. Kaur. “GAN-based synthetic data augmentation for 
increased CNN performance in vehicle number plate recognition.” In 2020 4th 
International Conference on Electronics, Communication and Aerospace Technology (ICECA)  (pp. 1190–1195). 2020, November. IEEE.
 39.  T schandl, P., N. Codella, B. N. Akay, G. Argenziano, R. P. Braun, H. Cabo, and H. 
Kittler. “Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classification: An open, web-based, interna -
tional, diagnostic study.” The Lancet Oncology 20, no. 7, (2019): 938–947. https://doi.
org/10.1016/s1470-2045(19)30333-x.
 40.  N andyala, Chandra Sukanya, and Haeng-Kon Kim. “From cloud to fog and IoT-based 
real-time Uhealthcare monitoring for smart homes and hospitals.” International 
Journal of Smart Home 10.2 (2016):187–196.115 DOI: 10.1201/9781003143468-8
Segmentation of Deep 
Learning Models
Prabhjot Kaur and Anand Muni Mishra
Chitkara University Institute of Engineering &  
Technology, Chitkara University  Punjab, India8
CONTENTS
8.1 Introduction  .................................................................................................. 115
8.1.1  Image Classification  .......................................................................... 116
8.1.2  Object Detection  ............................................................................... 116
8.1.3  Semantic Segmentation  .................................................................... 116
8.1.4  Instance Segmentation  ...................................................................... 116
8.2 Overview  ....................................................................................................... 119
8.2.1  Region-Based Segmentation  ............................................................. 119
8.2.1.1  Threshold Segmentation  .................................................... 119
8.2.1.2  Regional Growth Segmentation  ......................................... 120
8.2.2  Edge Detection Segmentation ........................................................... 120
8.2.2.1  Sobel Edge Operator  .......................................................... 121
8.2.2.2  Roberts Edge Detector  ....................................................... 121
8.2.2.3  Prewitt Edge Detector  ........................................................ 122
8.2.2.4  Canny Edge Detector  ......................................................... 122
8.2.3  Clustering Segmentation Method  ..................................................... 122
8.3 Conclusion  .................................................................................................... 123
References  .............................................................................................................. 124
8.1 INTRODUCTION
An image is a means of transmitting data, and there is plenty of valuable infor -
mation in the image. Getting important data from an image or recognition of data is done through digital image technology. With the help of image segmentation, a picture can be understood easily [1]. Image segmentation is a basic work of manipu -
lating pictures, interpreting images, understanding images, and identifying patterns. Extracting the field that users are interested in is done using calculations based on parameters in which the same type of data is grouped. This includes cluster -
ing images into different fragments or entities. In a vast variety of technologies, including medical image processing, autonomous vehicles, video monitoring, and virtual reality, segmentation plays a central role [2, 3]. There is no common stan -
dard procedure for image segmentation because different types of images require 116 Machine Learning for Edge Computing
different partitions to extract significant features. On the other hand, when segment -
ing a particular form of an image, various approaches are not equally efficient, and 
the parameters for determining a satisfactory segmentation depend on the desired objective of the segmentation itself. Therefore, there is no specific outcome to the segmentation dilemma [ 4].
Segmentation approaches have been improved and expanded by advancements 
in computer science and mathematical simulations, and have retained close rela -
tionships with other computer vision methods, such as image recognition and edge detection, while differentiating their purposes from them. These approaches solve various types of problems, considering their close relationships, and generate dif -
ferent outcomes. In Sections 8.1.1–8.1.4, we will define four main computer vision issues, which are listed in increasing order of complexity.
8.1.1 ImAge clAssIfIcAtIon
The first issue is to classify the group of key objects inside an image.
8.1.2 o bject detect Ion
For any known object within an image, classify the object category and locate its 
location using a feature vector.
8.1.3 s emAntIc segment AtIon
For all objects within an image, define the object type of every pixel.
8.1.4 InstAnce segment AtIon
In the image segmentation task, the detection of a particular object before classifica -
tion is known as instance segmentation.
This technique can be conceived as the difficulty of pixel labeling with semantic 
identification (semantic segmentation) or division of single objects (instance seg -
mentation). Semantic segmentation performs pixel-level labeling for all the pixels of an image with a set of objects that are in an image; it is usually a more difficult undertaking than image classification, which foresees a single identification for the whole image. Other than this, in the instance segmentation, the region of interest are detected and classified according to the objects in the image (e.g., the division of every single object) [ 5].
To diagnose an infected part from any image, segmentation is the best step at 
the time of pre-processing. Different techniques of segmentation are used that are either based on intensity (histogram intensity-based segmentation) or pixels (index measure, over lab coefficient). These methods give an accuracy of 98.025% for the segment of the affected region [ 6]. Based on the clustering of an image dataset fusion, 
different clustering algorithms are used for the segmentation of diseased part from an image. Then three color features are extracted using a pyramid of histograms of orientation gradients (PHOG) algorithm. Accuracy achieved for the apple disease 117 Segmentation of Deep Learning Models
segmentation is 90.43% and for the cucumber plant is 92.15% [ 7]. The identifica-
tion and recognition of disease in citrus fruits is done using a hybrid approach. The 
principal component analysis (PCA) score, entropy, and skewness-based covariance hybrid approaches are used for extracting the features of citrus disease, such as black spots, cankers, scabs, greening, etc. The classification accuracy achieved is 97% [ 8]. 
The newly improved multichannel selection-based segmentation model Chan-Vese (C-V) is used for segmenting the wheat lesion from an image. From the multichannel (R, G, B, H, S, V), only the Red, Green, and Blue (RGB) channel is selected. Different segmentation methods are used with the C-V model in which the k-means segmenta-tion method gives a better result. The accuracy achieved by k-means method is 84.11%, which also calculates performance in terms of efficiency and robustness [ 9].
Finding and dividing diseased parts from healthy parts using a program is the 
main challenge when analyzing a plant. Objects and boundaries (lines, curves, etc.) in images are usually located using image segmentation. Some of the parameters used for measuring the performance of the segmentation are Jaccard, dice, variation index, and consistency error method. When compared with different segmentation algorithms,  
the discrete wavelet transform method with k-means clustering gives a better result [10]. For comparing signs and symptoms in a plant leaf, different algorithms are 
proposed in computer vision. Some of the steps in this segmentation algorithm are automatic; the difficult one is the selection of the H-component. The novel algorithm-  
based semi-automatic method allows for versatility without sacrificing speed. Variations in leaf color, symptom color, light disparities, and other factors are all taken into account by the algorithm. The majority of errors were caused by color channel constraints, and some sources of error were unavoidable even with a com -
pletely manual approach [ 11]. Some of the papers consider only a single part of the 
eye image. Research on multiple parts of the eye image is not focused. The authors of reference number 12 propose a model for calculating the multiple regions of an eye image using different segmentation algorithms [ 12]. Early detection of any disease 
will help with curing a person. In this case, the skin cancer disease melanoma can have dangerous effects on human skin. For the detection and segmentation of disease, different segmentation techniques are used to help treat a person at an early stage [ 13].
Brain diagnosis relies heavily on the segmentation of tissues from medical 
images. Since manual segmentation is time-consuming, it is essential to construct a program for the segmentation of data using Deep Learning (DL) networks. It’s still a challenging task to segment for medical images. Techniques such as cropping and normalisation are used to process the MRI images. Then, for segmentation, an Fully Convolutional Network (FCN)-based deep learning model is built. Finally, the per -
formance of images is evaluated and they are uploaded to the Penn Imaging Website so that model performance can be evaluated [ 14]. The segmentation of magnetic 
resonance images is carried out using a differential evolution of the linear popu -
lation size reduction (LSHADE) method. The proposed method is carried out in different steps: first, the image is converted into grayscale, then it is grouped into magnetic resonance, and finally it forms a group of unhealthy images. The output of the proposed method is compared with machine learning using a metaheuristic approach [ 15]. Table 8.1  represents a few literature surveys for the segmentation of 
deep learning models.118 Machine Learning for Edge ComputingTABLE 8.1
Literature Surveys
Ref No.Year of 
PublicationAuthor’s/Authors’ Name(s)Type of  Dataset Summary
[16] 2016 Jayme Garcia  
Arnal  BarbedoPlant leaf Compared with other different methods, the method proposed in this paper gives a more 
accurate result. To differentiate the leaf’s healthy part from the unhealthy part, this method gives effective result.
[17] 2018 Shu H.,  
Fan K. et al.Cotton leaf Proposed an automatic segmentation technique with gradient and local information. Compared 
with different segmentation techniques, such as the Generalized Arc Consistency (GAC) algorithm, C-V algorithm, and Local Best Fit (LBF) algorithm, the automatic segmentation gives a better result. This model not only segments images of cotton leaves in temperate  areas but also can provide technical assistance in accurate cotton disease diagnosis and identification.
[18] 2019 Vijai Singh Sunflower  
leafProposed a model for segmentation using Particle Swarm Optimization (PSO), which does not 
require any extra information for segmentation of the disease image. For better accuracy, the hybrid PSO model is used for the segmentation process.
[19] 2019 Praveen Kumar J.  
Dominic S. Plant leaf Based on segmentation, different steps are followed for counting and segmenting the data. Steps 
include the statistical-based technique, the graph-based method, and finally, the Circular Hough Transform, which achieves an accuracy of 95.4%.
[20] 2019 Li J., Zhang L. et al. Tomato leaf For the clustering of tomato leaf image, different adaptive algorithms are used for the 
segmentation of images. The clustering values are calculated by the Davies-Bouldin index with clustering calculations. The F1 measure and entropy value are calculated for measuring the segmentation accuracy.
[21] 2020 Riehle D.,  
Reiser D.,  Griepentrog H.Maize, Sugar  
beet plantProposed a robust model that segments the image under different conditions, such as 
overexposure and underexposure. The model relies on an index-based semantic method with an accuracy of 97.4%. This proposed index-based model is more accurate than the other index-based methods.119 Segmentation of Deep Learning Models
This chapter explains various segmentation algorithms for analyzing images 
of any size or frame. The algorithms are categorized according to usability and 
analysis. The threshold segmentation and regional growth segmentation come under region-based segmentation. Next is the edge-detection segmentation; under this, the Sobel operator and the Laplacian operator work. Another cat -
egory is the segmentation based on clustering; under this, k-means clustering  
works.
8.2 OVERVIEW
8.2.1 r egIon-bAsed segment AtIon
8.2.1.1  Threshold Segmentation
The threshold segmentation method is one of the easiest and most common methods of linear segmentation used for image segmentation. A basic segmentation algo -
rithm is used under this approach, through which processing the information in the images can be divided easily based on the grayscale value of various points. The seg -
mentation of thresholds is further split into the local threshold and global threshold methods. The image is divided into two regions using the global threshold method through the single threshold. One region is used as the target region, and the other region is used as the background. In the local threshold method, several segmenta-tion thresholds are required. So, the image is divided into numerous target regions and backgrounds [1]. Usually, the Ostu’s method is employed in which optimization of variance between groups is done and a global optimal threshold is identified. Also, many other methods that likely come under threshold segmentation are the relaxation method, moment preserving technique, statistical mode, entropy-based technique, co-occurrence method, and fuzzy set method [22]. Different thresholding techniques are shown in Figure 8.1 .
FIGURE 8.1  Thresholding techniques.120 Machine Learning for Edge Computing
8.2.1.2  Regional Growth Segmentation
This approach is the same as the serial region-based method, and its concept is 
to form a region of those pixels that have identical properties and features. In this method, the initial step is to select the seed pixels. In the next step, identical pix -
els are combined around the identical pixel in the seed pixel location region. Some of the advantages and disadvantages of both threshold segmentation and regional growth segmentation are mentioned in Table 8.2 .
8.2.2 e dge detect Ion segment AtIon
Edges are curves in which there are abrupt brightness shifts or spatial brightness 
derivatives. Brightness variations are where the orientation of the surface changes discontinuously, where one object obscures another, where shadow lines appear, or where the properties of the surface reflection are discontinuous. To maximize the visibility of distorted images, an edge-detection filter may also be used. Edge detection for image segmentation is one of the most significant applications. Image segmentation is used to execute the partition of a digital image in several regions or pixel sets [23]. The edge is the border of two homogeneous regions. The detec-tion of edges refers to the method of recognizing and finding the sharp variance in a picture. Edge detection is used to identify artifacts that are useful for a variety of applications, including medical image processing, biometrics, and so on. As it enables higher-level image processing, edge detection is an active field of study [24].  
Edge-detection methods based on discontinuity are the most widely used methods. TABLE 8.2
Advantages and Disadvantages of Threshold Segmentation and Regional Growth Segmentation
Advantage In the threshold method, the estimate analysis is easy and the speed of action is 
better, specifically, in the variation in the target and background results in the segmentation effect.
The benefit of regional growth segmentation is its ability to divide the associated 
regions with identical features and to offer decent threshold data with the outcomes of segmentation. It clearly defines regional development and needs a limited seed point to achieve this. Also, the parameters for growth can be freely defined in the growth period. Finally, at the same time, it will choose several parameters.
Disadvantage The main drawbacks of image segmentation are the lack of substantial grayscale 
variance, overlapping the values of grayscale in image. This causes difficulties in obtaining precise results as it only considers the image’s gray information without taking into account the image’s spatial information; it is vulnerable to noise and grayscale unevenness, and mixed with other approaches.
The disadvantage of the regional growth segmentation is that the cost of computing 
is high. Noise and grayscale unevenness may also add to voids and separation. The last one is that the picture’s shadow effect is always not very good.121 Segmentation of Deep Learning Models
Roberts edge detection, Sobel edge detection, Prewitt edge detection, Kirsh edge 
detection, Robinson edge detection, LoG edge detection, and Canny edge detection are these techniques.
8.2.2.1  Sobel Edge Operator
Using the Sobel approximation to the derivative, the Sobel method of edge detection for image segmentation seeks edges. It precedes the edges at those points where the gradient is strongest. The Sobel method computes a 2-D spatial gradient quantity on an image, highlighting high-spatial frequency regions that correspond to edges. It’s com -
monly used to find the estimated absolute gradient magnitude in each point of an input grayscale image [24]. Using the discrete discrepancies between the horizontal and ver -
tical lines of a 3 × 3 neighbors shown in Figure 8.2 , the Sobel edge detector computes 
the gradient. The Sobel operator is based on a thin, independent, binary-valued filter converging the image. Figure 8.2  represents the process of the Sobel edge operator.
8.2.2.2  Roberts Edge Detector
This is employed for conducting simple, easy-to-compute 2-D spatial gradient mea -
surement on an image. This approach focuses on the regions having large spatial variances that also summarize a given dataset. The operator input is the grayscale picture that is most commonly used for this procedure, the same as the output [24]. The maximum magnitude of the spatial gradient in the input image at that point is represented by the pixel values at each point in the output. Figure 8.3  shows Robert’s 
convolution mask.
FIGURE 8.2  (a) Gx, (b) Gy.
FIGURE 8.3  Roberts’s convolution mask.122 Machine Learning for Edge Computing
8.2.2.3  Prewitt Edge Detector
This edge detector is a valid method for measuring the edge’s magnitude and direc -
tion. However, separate gradient edge detection requires a very slow calculation for 
estimating the path using magnitudes in the x and y directions; compass edge detec-tion, on the other hand, gets the path straight from the kernel with the best result. It has only eight possible directions; however, experience indicates that it is no longer suitable for other direct path measurements. The Prewitt method is the oldest and the best method for edge detection [25].
8.2.2.4  Canny Edge Detector
The Canny edge detection method is one of the basic techniques. The Canny method is a very significant way of identifying edges by separating noise from the image before finding the edges of the image. Without disrupting the characteristics of edges in the picture, the Canny approach is a safer way to apply the inclination to locate the edges and the value of the threshold.
8.2.3 c luster Ing segment AtIon method
A common theory related to image segmentation does not exist. Though numerous 
new concepts and approaches have been developed in other fields, certain methods of image segmentation have been merged with some particular algorithms. The sup -
posed class has found its application for the set of identical elements. Clustering is commensurate with the standards and rules related to grouping of objects in the system [26]. To segment the pixels in the image space with the corresponding space points, the image space clustering technique is used. Some of the clustering methods are explained below:
a. k‑M eans Clustering:  Based on the distance of pixels from each other, the 
k-means algorithm divides the data into groups of multiple clusters. k-means consider the medium level of computation and make a correspondingly high level of clusters [27]. To minimize the distance between the pixels and cluster center, k-means algorithm is used. Clustering the data means grouping simi -
lar objects together. Based on the similarity of pixels, the k-means paired the data into one group. The k-means algorithm is numerical, performs well with good data, and is a non-deterministic and unsupervised method [1]. The steps that are followed in the processing of k-means are shown in Figure 8.4 . There 
are three basic limitations of the k-means algorithm.
1. It is important to define the number of clusters.
2. Various initial conditions yield various outcomes.
3. The data far from the center pushes the center away from the optimal spot.
b. Fuzzy c ‑Means (FCM):  FCM is one of the unofficial clustering algorithms 
that is applied to a wide variety of attribute analysis, clustering, and clas -
sification architecture concerns. Agricultural engineering, physics, chemis -
try, geology, image analysis, medical diagnosis, form analysis, and target recognition are only a few of the applications of FCM.123 Segmentation of Deep Learning Models
The fuzzy c-means clustering algorithm was proposed in the 1980s with the devel -
opment of the fuzzy theory. Grouping of the same type of data into one cluster is 
done by fuzzy c-means clustering [28]. This clustering is accomplished iteratively minimizing a cost function that relies on the distance of the pixels to the cluster centers in the feature domain. The pixels on an image are strongly clustered, i.e., the pixels have approximately the same attribute details in the immediate neigh -
borhood. The steps followed in the processing of fuzzy c-means are shown in  
Figure 8.5 .
8.3 C ONCLUSION
To solve various image processing problems related to many images, such as medical images, leaf disease images, vehicle images, etc., segmentation algo -
rithms are used. They divide the image data into meaningful and segmented part that is understood by the user easily. With these methods, the segmentation of data is done very easily and that segmented data is used for further processing. Segmentation helps in classification phase to classify data according to data from the user. 
FIGURE 8.4  k-means clustering process.124 Machine Learning for Edge Computing
REFERENCES
1. S. Yuheng and Y. Hao, “Image segmentation algorithms overview,” arXiv , vol. 1, (2017), 
doi.org/10.48550/arXiv.1707.02051.
2.S. Minaee, Y. Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos,
“Image segmentation using deep learning: A survey,” IEEE PAMI , (2021), doi: 10.1109/
TPAMI.2021.3059968 .
3.V. Kukreja and P. Dhiman, “A deep neural network based disease detection scheme
for citrus fruits,” Proc. - Int. Conf. Smart Electron. Commun. ICOSEC 2020 , Icosec
(2020), pp. 97–101, doi: 10.1109/ICOSEC49089.2020.9215359 .
4.L. Antonelli, V. D. Simone, and D. Serafino, “A view of regularized approaches
for image segmentation,” arXiv:2102.05533v3 , (2021), doi: 10.48550/arXiv.2102.
05
533.
5.A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P. Martinez-
Gonzalez, and J. Garcia-Rodriguez, “A survey on deep learning techniques for
image and video semantic segmentation,” Appl. Soft Comput. , vol. 70, (2018)
pp
. 41–65, doi: 10.1016/j.asoc.2018.05.018.
FIGURE 8.5  Fuzzy c-means clustering process.125 Segmentation of Deep Learning Models
6.S . Kalaivani, S. P. Shantharajah, and T. Padma, “Agricultural leaf blight disease seg -
mentation using indices based histogram intensity segmentation approach,” Multimed.
Tools Appl ., vol. 79, no. 13–14, (2020) pp. 9145–9159, doi: 10.1007/s11042-018-
7126-7.
7.S . Zhang, H. Wang, W. Huang, and Z. You, “Plant diseased leaf segmentation and
recognition by fusion of superpixel, K-means and PHOG,” Optik ., vol. 157, (2018)
pp. 866–872, doi: 10.1016/j.ijleo.2017.11.190.
8.M . Sharif, M. A. Khan, Z. Iqbal, M. F. Azam, M. I. U. Lali, and M. Y. Javed, “Detection 
and classification of citrus diseases in agriculture based on optimized weighted
segmentation and feature selection,” Comput. Electron. Agric ., vol. 150, (2018)
pp. 220–234, doi: 10.1016/j.compag.2018.04.023.
9.Q -xia Hu, J. Tian, and D. J. He, “Wheat leaf lesion color image segmentation with
improved multichannel selection based on the Chan–Vese model,” Comput. Electron.
Agric ., vol. 135, (2017) pp. 260–268, doi: 10.1016/j.compag.2017.01.016.
10.N . Valliammal and S. N. Geethalakshmi, “Leaf image segmentation based on the com -
bination of wavelet transform and k means clustering,” Int. J. Adv. Res. Artif. Intell .,
vol. 1, no. 3, (2012) pp. 37–43, doi: 10.14569/ijarai.2012.010307.
11.J . G. A. Barbedo, “A novel algorithm for semi-automatic segmentation of plant leaf
disease symptoms using digital image processing,” Trop. Plant Pathol. , vol. 41, no. 4,
(2016) pp. 210–224, doi: 10.1007/s40858-016-0090-8.
12.R . A. Naqvi, D. Hussain, and W. K. Loh, “Artificial intelligence-based semantic seg -
mentation of ocular regions for biometrics and healthcare applications,” Comput.
Mater. Contin ., vol. 66, no. 1, (2021) pp. 715–732, doi: 10.32604/cmc.2020.013249.
13.U . Jamil, A. Sajid, M. Hussain, O. Aldabbas, A. Alam, and M. U. Shafiq, “Melanoma
segmentation using bio-medical image analysis for smarter mobile healthcare,” J.Ambient Intell. Humaniz. Comput ., vol. 10, no. 10, (2019) pp. 4099–4120, doi: 10.1007/
s12652-019-01218-0.
14.J . Sun, Y. Peng, Y. Guo, and D. Li, “Segmentation of the multimodal brain tumor image 
used the multi-pathway architecture method based on 3D FCN,” Neurocomputing ,
vol. 423, (2021) pp. 34–45, doi: 10.1016/j.neucom.2020.10.031.
15.I. A ranguren, A. Valdivia, B. Morales-Castaneda, D. Oliva, M. A. Elaziz, and M. Perez-
Cisneros, “Improving the segmentation of magnetic resonance brain images using theLSHADE optimization algorithm,” Biomed. Signal Process. Control , vol. 64, (2021),
pp. 102259, doi: 10.1016/j.bspc.2020.102259.
16.J . G. A. Barbedo, “A new automatic method for disease symptom segmentation
in digital photographs of plant leaves,” Eur. J. Plant Pathol ., vol. 147, no. 2, (2017)
pp. 349–364, doi: 10.1007/s10658-016-1007-6.
17.J . -H. Zhang, F. -T. Kong, J. Zhai Wu, S. Qing Han, and Z. Fen Zhai, “Automatic image
segmentation method for cotton leaves with disease under natural environment,” J.
Integr. Agric ., vol. 17, no. 8, (2018) pp. 1800–1814, doi: 10.1016/S2095-3119(18)61915-X.
18.V . Singh, “Sunflower leaf diseases detection using image segmentation based on par -
ticle swarm optimization,” Artif. Intell. Agric ., vol. 3, (2019) pp. 62–68, doi: 10.1016/j.
aiia.2019.09.002.
19.J . P. Kumar and S. Domnic, “Image based leaf segmentation and counting in
rosette plants,” Inf. Process. Agric ., vol. 6, no. 2, (2019) pp. 233–246, doi: 10.1016/j.
inpa.2018.09.005.
20.K . Tian, J. Li, J. Zeng, A. Evans, and L. Zhang, “Segmentation of tomato leaf images
based on adaptive clustering number of K-means algorithm,” Comput. Electron. Agric .,
vol. 165, August (2019) p. 104962, doi: 10.1016/j.compag.2019.104962.
21.D . Riehle, D. Reiser, and H. W. Griepentrog, “Robust index-based semantic plant/back -
ground segmentation for RGB- images,” Comput. Electron. Agric ., vol. 169, December
(2020), p. 105201, doi: 10.1016/j.compag.2019.105201.126 Machine Learning for Edge Computing
 22. N. Senthilkumaran and S. Vaithegi, “Image segmentation by using thresholding 
techniques for medical images,” Comput. Sci. Eng. An Int. J ., vol. 6, no. 1, (2016)  
pp. 1–13, doi: 10.5121/cseij.2016.6101.
 23. S. S. Al-amri, N. V. Kalyankar, and S. D. Khamitkar, “Image segmentation by using 
edge detection,” Int. J. Comput. Sci. Eng ., vol. 2, no. 3, (2010) pp. 804–807.
 24. M. M. Radha, “Edge detection techniques for Image Segmentation,” Int. J. Comput. 
Sci. Inf. Technol ., vol. 3, no. 6, (2011) pp. 259–267.
 25. P. P. Acharjya, R. Das, and D. Ghoshal, “Study and comparison of different edge 
detectors for image segmentation,” Glob. J. Comput. Sci. Technol. Graph. Vis .,  
vol. 12, no. 13, (2012) pp. 29–32.
 26. N. Dhanachandra, K. Manglem, and Y. J. Chanu, “Image segmentation using k-means 
clustering algorithm and subtractive clustering algorithm,” Procedia Comput. Sci ., vol. 
54, (2015) pp. 764–771, doi: 10.1016/j.procs.2015.06.090.
 27. P. Kaur and V. Gautam, “Plant biotic disease identification and classification based 
on leaf image: A review,” Proc. 3rd Int. Conf. Comput. Inform. Networks. Lecture 
Notes in Networks and Systems, vol. 167. Springer, (2021) pp. 597–610, doi: 10.1007/  
978-981-15-9712-1_51.
 28. A. M. Mishra and V. Gautam, “Weed species identification in different crops using 
precision weed management: A review,” CEUR Workshop Proc ., vol. 2786, (2021)  
pp. 180–194.127 DOI: 10.1201/9781003143468-9
Alzheimer’s Disease 
Classification
Monika Sethi, Sachin Ahuja, and Vinay Kukreja
Chitkara University Institute of Engineering & Technology, 
Chitkara University  
Punjab, India9
CONTENTS
9.1 Introduction  .................................................................................................. 127
9.2 Datasets for AD Diagnosis  ........................................................................... 128
9.3 AD Diagnosis Using ML Techniques  ........................................................... 130
9.4 AD Diagnosis Using DL ............................................................................... 133
9.5 Observations  ................................................................................................. 139
9.6 Conclusion and Future Directions  ................................................................ 140
References  .............................................................................................................. 140
9.1 INTRODUCTION
Alzheimer’s disease (AD) is a severe chronic neurodegenerative impairment [ 1] 
that really has no known treatment [ 2] or a precise deterministic definition of the 
disease process itself. While in the later stages, indicators or the markers of the disease are amyloid plaques, neurofibrillary tangles, and neurological damage, it is just not known how they are initially formed. This disease was named after Dr. Alois Alzheimer [ 3]. He found significant differences in the brain tissue of a woman 
who died due to an abnormal psychological illness. Her symptoms included the loss of memory, speech impediments, and irrational behavior. When she expired, he analyzed her brain and identified numerous unusual clumps (now known as amyloid plaques) and clusters of fibers (now termed as tau tangles or neurofibrillary tangles). Such plaques and tangles in the brain are still believed to be among the main symptoms of AD. Yet another feature is the deterioration of connections among neurons in the brain.
This causes the hippocampus and cortex to shrink and the cerebral ventricles to 
widen. The severity of these disruptions depends on the stage of the disease. In the later phases or stages of AD, the substantial shrinkage of the hippocampus [ 4] and 
cerebral cortex and the expansion of the ventricles can be seen clearly in the brain scans (medical resonance images or MRI) [ 5, 6]. Sufferers at the early stages of AD 
have Mild Cognitive Impairment (MCI), but not all sufferers with MCI necessarily develop AD. MCI is the initiation phase between the mental changes that are seen in normal aging and early-stage of AD, in which the individual endures minor behav -
ior changes that are apparent to just the individual and their loved ones only [ 7]. 128 Machine Learning for Edge Computing
MCI sufferers then are considered MCI converters (MCIc) or MCI non-converters 
(MCInc), which suggests that they have or have not progressed to AD during a period of one and a half years. Subsequently, there are two subtypes of MCI that are seldom described in the literature: early MCI (eMCI) and late MCI (lMCI) [ 8].
Approximately 5.7 million people are affected by AD. The number of AD suffer -
ers is expected to increase by 13.8 million by the mid-century. More than 16 million involuntary nurses and relatives of the patient delivered nearly 18.4 billion hours of 
treatment to AD patients. The gross expense was more than $232 billion [ 9].
Diagnosing AD is a huge challenge, particularly in the initial stages. Previous 
noninvasive assessment methods imposed a strong focus on individual medical 
history, cognitive tests, and behavioral assessments conducted by healthcare profes -
sionals. Research findings also show that the language ability of most AD patients is affected, so neuropsychological tests are typically an effective way to detect AD at initial stages. These tests use a variety of measures that include a set of questions related to attention, language, orientation, and visual spatial skills. For instance, Mini Mental State Assessment (MMSA) is a standard cognitive psychological test. It requires the patient to respond to a variety of questions and after that, the doctor assigns a score between zero and thirty representing various cognitive skills. The validity of this method completely depends on the level of expertise and knowledge of the healthcare professional. Given the exponential spike in the number of patients, this type of test requires a lot of time and resources.
Researchers have recently established the susceptibility of various biomarkers for 
earlier diagnosis of both AD and MCI [ 10]. To identify abnormal pathological volu -
metric changes associated with AD, biomarkers from structural MRI can be used to measure brain atrophy. Functional imaging like positron emission tomography (PET) scans can also provide complementary information related to the hypo-metabolism quantification, while cerebrospinal fluid gives evidence of protein changes.
Machine Learning (ML) techniques have been shown to be very effective for 
diagnosing AD. The classification system consists of multiple phases, such as extrac -
tion of features from the given data, selection of features, mapping the features into lower dimensions, and then classification based on those selected features. Generally, to build any predictive model to facilitate the automation, decision support in the medical fields that minimally features extraction and classification algorithm is needed. However, Deep Learning (DL) incorporates the feature extraction phase into the learning model itself. For large datasets, DL is found to be especially useful for image data. Shi et al. indicate that a blend of neural networks (NN) and intel -
ligent agents can be useful for analyzing medical images. In this chapter, various ML and DL techniques are explored related to their performance in classification  
of AD.
9.2 D ATASETS FOR AD DIAGNOSIS
Various open access AD dataset repositories are available to assist researchers in accelerating their research in this area. Datasets such as Alzheimer’s Disease Neuroimaging Initiative (ADNI) ( http://adni.loni.usc.edu /), Open Access Series 
of Imaging Studies (OASIS) ( www.oasis-brains.org ), Minimal Interval Resonance 129 Alzheimer’s Disease Classification
Imaging in Alzheimer’s Disease (MIRIAD) ( http://miriad.drc.ion.ucl.ac.uk /), 
and Australian Imaging, Biomarkers & Lifestyle Flagship Study of Ageing 
(AIBL) (aibl.csiro.au) are commonly used in this area of research [ 11]. The stud-
ies aim to test the feasibility of using various brain scans as an outcome measure for clinical trials of AD treatments, and these datasets generally include various types of neuroimaging modalities, such as MRI, functional MRI (fMRI), PET, Single-Photon Emission Computerized Tomography (SPECT), cognitive and 
clinical assessments, and the demographic information of patients.
Among all the datasets stated before, ADNI is being extensively used alone and 
with a blend of non-ANDI by a few researchers. ADNI was released by commercial 
healthcare organizations, including the National Institute of Aging (NIA), National Institute of Basic Biology (NIBB), and U.S. Food & Drug Administration (FDA), 2003. The North American-based study was designed for 800 adult subjects in total (400 individuals with MCI and 200 each for AD and Normal Controls (NC)). These subjects/groups were tracked by concerned organization for a couple of years. Subjects were hired from more than 50 different locations across Canada and the United States. The ultimate goal of the ADNI is to examine whether the neurologi -
cal scans (such as MRI, fMRI, PET, etc.), genetics, and psychiatric methods can be incorporated to evaluate the various stages of AD.
The next dataset, MIRIAD, comprises 69 longitudinal volumetric MRI brain scans 
in total (46 AD subjects and 23 NC subjects). 46 AD subjects and 23 NC subjects  
were followed by the organization from 2 weeks to 2 years, with multiple scans. Lastly, it comprises around 700 scans acquired by the same scanner as well as the same radiologist. Additional information, such as gender (male or female), age and MMSE points, were also considered.
OASIS ( www.oasis-brains.org ) is an initiative that offers free datasets to the aca -
demic world, comprising longitudinal and cross-sectional MRI brain scans for 150 subjects and 416 subjects, respectively. The longitudinal dataset includes the neuro-scans for older people (from 60 to 96 years old) while the cross-sectional dataset comprises the scans of young people to elderly people (from 18 to 96 years old). A series of studies employed different datasets for training and test purposes; for instance, in [3], the researchers used the OASIS dataset for training and the MIRIAD dataset for test purposes to train a DL Convolutional Neural Network (CNN) model for AD classification.
Another dataset, AIBL, which was released in 2006, aims at exploring the bio -
markers involving cognitive, health, and lifestyle characteristics that are useful for identifying the distinct stages of AD. It entails more than 1,000 participants, includ -
ing different groups, such as AD, MCI, and NC.
To preprocess the data, past researchers employed software such as Statistical 
Parametric Mapping (SPM) and FreeSurfer Library (FSL). FSL helps for brain extraction and tissue segmentation, whereas SPM helps align, spatially nor -
malize, and smooth the brain scans. FreeSurfer has a pre-processing stream that involves three things, namely, skull stripping, non-linear registration, and 
segmentation. In [ 12], the researcher groups utilized a Clinica software frame -
work proposed by ARAMIS Lab that facilitates the working of FSL, SPM, and 
FreeSurfer.130 Machine Learning for Edge Computing
9.3 AD DIAGNOSIS USING ML TECHNIQUES
Generally, neuroimaging data for efficient recognition and classification makes use 
of machine learning techniques such as Support Vector Machine (SVM), K-nearest neighbor (KNN), Random Forest, Naive Bayes, etc. These are based on solitary clas -
sifiers, and SVM is the most well-known of these methods [13]. This is a very stable and commonly used technique for regression [14] as well as classification [15] problems. By including the Systemic Risk Minimization Theory (SRM), SVM offers strong generalization efficiency. To define the data points, SVM uses the maximal margin principal. Classification accuracy of SVM is heavily affected by mainly two param -
eters, namely, the selection of the features and the setting of the kernel parameter. To classify AD, various kernel functions have been utilized for SVM [16] to translate the data to higher dimensions. Several researchers used the SVM linear kernel to identify AD results. This is because there is no kernel parameter to tune in the linear kernel. Some researchers have also used multiple kernels with SVM [10]. Table 9.1  
shows the findings of various researchers for AD classification using the SVM linear  
kernel.
To extricate the highly representatives features from the mean neuro-images 
specific to AD patterns, Vicente et al. in their paper [17], proposed a novel CAD framework based on independent component analysis (ICA). The main purpose of ICA was to locate the independent component sources that are present during the AD phase. Furthermore, it was demonstrated to be a successful technique for fea -
ture reduction dimensionality and choosing the most pertinent information. Then, SVM was utilized to deal with the classification-related task. An accuracy of 87% in distinguishing AD from HC, with a sensitivity of 90% and specificity of 84%,  
was achieved.
Voxel-based Morphometry (VBM) from regular T1-weighted MRI neuroim -
ages demonstrated effective results for measuring AD-related brain atrophy and to empower a genuinely precise classification of AD, MCI, and Healthy Controls (HC) patients. In [18], the authors evaluated two different VBM algorithms (in-house, they were named MorphoBox and FreeSurfer) on the ADNI dataset to classify AD automatically. Their results clearly indicated that both algorithms achieved accu -
racy equivalent to the traditional whole-brain VBM pipeline utilizing the SPM tool for AD vs. HC and MCI vs. HC and early vs. late AD converters, subsequently, exhibiting the capability of VBM to aid in the diagnosis of MCI and AD. The high -
est sensitivity and specificity was achieved for AD vs. HC and was 86% and 91%, respectively.
Authors in [19] utilized Sparse Inverse Covariance Estimation (SICE) to acquire 
undirected graphs for connectivity patterns from multimodalities MRI and PET neuro images (ADNI dataset). They included three different subjects for their research, namely HC (68), AD (70), and MCI (111). AD vs. HC achieved 8% more accuracy as compared to AD vs. MCI.
The SVM-Recursive Feature Elimination (RFE) method was implemented in [4, 
20] to classify AD, cMCI, ncMCI, and HC subjects of sMRI neuro images. The dataset was drawn from ADNI. The maximum accuracy (89%) was achieved while 131 Alzheimer’s Disease Classification
TABLE 9.1
AD Classification Using SVM Linear Kernel
S. No. Ref. No. (Year) DatasetNeuroimaging Modality (Single  or Multimodality) Sample Size Feature ExtractionBinary  
ClassificationPerformance Metrics (%age)
SEN SPE ACC
1 (2015) [17] ADNI sMRI (Single) AD (188), HC (229),  
MCI (401)VBM and FastICA HC/AD 90 84 87
HC MCI 80 74 78
AD/MCI 86 85 85
2 (2015) [18] ADNI sMRI (Single) AD (188), HC (229),  
MCI (401)VBM and V oIBM HC/AD 86 91 NM
HC/MCI 78 68
AD/MCI 69 67
3 (2015) [19] ADNI sMRI+PET 
(Multimodality)AD (70), HC (68),  
MCI (111)VCM and SICE HC/AD 96 86 92
HC/ MCI 90 82 86
AD/MCI 87 81 84
4 (2015) [20] ADNI sMRI (Single) AD (144), HC (189), cMCI 
(136), ncMCI (166)VB and SNM-RFE HC/AD
NM NM89
cMCI/ ncMCI 71
5 (2017) [21] ADNI sMRI (Single) AD(160), HC(162),  
sMCI(65), pMCI (71)VBM+GA HC/AD 89 97 93
pMCI/sMCI 77 73 75
6 (2017) [22] ADNI sMRI (Single) AD (65), HC (135),  
cMCI (132), pMCI (95)MDS+PCA HC/AD 93 98 97
HC/pMCI 87 95 92
sMCI/pMCI 86 91 89
7 (2018) [23]
ADNI DTI (Single) AD(48), HC(51), eMCI(75), 
lMCI (39)LDH and SVM-RFE HC/AD NM NM 90
HC/eMCI 88
HC/lMCI 100
eMCI/lMCI 93
eMCI/AD 85
lMCI/AD 98
(Continued)132 Machine Learning for Edge Computing8 (2019) [24] ADNI fMRI (Single) AD(24), HC(24), eMCI(24), 
lMCI (24)RF-score HC/AD NM NM 96
HC/MCI 94
HC/lMCI 96
eMCI/lMCI 88
lMCI/AD 92
9 (2020) [25] ADNI fMRI (Single) AD(50), HC(50), MCI (50) USVM- RFE HC/AD NM NM 100
HC/MCI 90
MCI/AD 74
Abbreviations: ADNI, Alzheimer’s Disease Neuroimaging Initiative; HC, Healthy Control; AD, Alzheimer’s disease; MCI, Mild Cognitive Impairment; eMCI, Early MCI; 
lMCI, Late MCI; sMCI, Stable MCI; pMCI, Progressive MCI; fMRI, Functional Magnetic Resonance Imaging; NM, Not MentionedTABLE 9.1  (Continued )
AD Classification Using SVM Linear Kernel
S. No. Ref. No. (Year) DatasetNeuroimaging 
Modality (Single  or Multimodality) Sample Size Feature ExtractionBinary  
ClassificationPerformance Metrics (%age)
SEN SPE ACC133 Alzheimer’s Disease Classification
classifying AD with HC against the other binary classification, cMCI vs. ncMCI 
(71%).
In the paper [ 21], the research group proposed an automatic feature-selection 
method (FSM) for AD classification as well as for predicting the conversion time from MCI to AD, reliant on a genetic algorithm (GA). The FSM was realized by extri -
cating the voxel-values as a raw feature from the volume of interest (VOI) attained from VBM analysis. The extricated raw features vectors were then lessened to lower-  
dimensional feature vectors. Using sMRI neuro images drawn from the ADNI data -
set, the system achieved an accuracy of 93% and 75% for AD/HC and sMCI/pMCI, respectively.
In [22], the authors designed an ML model to classify AD or MCI subjects from 
HC and to predict the MCI to AD conversion time by evaluating and examining the regional morphological changes of neuro-scans. sMRI neuroimaging modality data samples (AD-65, HC-135, cMCI- 132, and pMCI-95) were acquired from the ADNI dataset. Their technique achieved an accuracy of 97% in classifying mild AD patients from HC subjects considering the whole brain Gray Matter (GM) or temporal lobe (TL) as the region of interest (ROI), 92% in differentiating pMCI from HC, and 89% in classifying pMCI with sMCI using the hippocampus and amygdale  
as ROI.
The authors of [ 23] applied the ML technique to identify the features related 
to AD and MCI using Diffusion Tensor Imaging (DTI) neuro images (AD: 48 subjects, lMCI: 39 subjects, eMCI: 75 subjects, and HC: 51 subjects) drawn from ADNI dataset. SVM-RFE and Logistic Regression (LR) were combined using cross validation leave-one-out in order to identify features optimally. The out -
come of the model showed that the SVM classifier performed better in terms of stability than the LR classifier. Their findings revealed a suggestion for ML-based image analysis on clinical diagnosis. The accuracy level achieved for MCI vs.  
HC subjects was 100%.
9.4 A D DIAGNOSIS USING DL
ML techniques are not adequate for dealing with complex problems such as AD classification. The increased computing power of Graphics Processing Units (GPU) has allowed the development of cutting edge DL algorithms [ 26]. DL is the branch of 
ML in artificial intelligence that mimics human intelligence in data analysis and pat -
tern recognition to solve complex decision-making problems. DL approaches have revolutionized performance in a variety of areas, such as object detection, image segmentation, identification, audio processing, and mapping.
On the basis of neuroimaging scans, DL models can uncover hidden artifacts, find 
linkages between various sections of images, and recognize disease-related features. DL models have been successfully extended to medical images such as structural and functional MRIs, DTI, and PET. In this way, researchers have recently started to use DL models to detect medical images.
As the most extensively used DL design, CNN has obtained a considerable atten -
tion on its popularity [ 27] in computer vision applications such as image analysis, 
speech recognition, natural language processing, and most recently in the field of 134 Machine Learning for Edge Computing
medical image classification and prediction. Its design is influenced by the 
concept of natural perceptual cortical proposed by Hubel and Wiesel in 1959 [28]. It is based on the theory of receptive areas, i.e., the region of the source images. In 1980, following encouragement from the research of Hubel and Weisel, Fukushima [29] developed a recognition that could be considered the basis of CNN architecture. Further, in 1990 LeCun et al. [30] established the structure of CNN by designing a multi-layer artificial model, that was referred to as LeNet-5, and seems to distinguish handwritten digits. Additionally, other Neural Networks (NNs) could be trained with a back propagation algorithm that provided an opportunity to retrieve different patterns directly from raw images, thereby combating the pre-processing steps needed for the extraction of fea -
tures. Table 9.2  shows the findings of various researchers for AD classification  
using CNN.
Authors of [31] designed and tested a predictive classification model that com -
bined both a sparse auto-encoder (SAE) and CNNs (2D CNN and 3D CNN) to distinguish AD, MCI from HC subjects using MRI images of the ADNI dataset [31]. SAE has been utilized to train the filters for the first convolution layer of two different CNN architectures (2D CNN and 3D CNN). Local patches of MRI images were used as an input. The convolution activities were trailed by pooling tasks, like in the neural system. Then the pooling layers were preceded by a fully connected layer and a softmax layer with probabilities of three independent sub -
jects (AD, MCI, and HC) as output. Experimental results show that the accuracy for the binary classification of AD/HC in both 2D and 3D CNNs is same, i.e., 95.39%. There is a minor improvement in accuracy for AD/MCI, MCI/HC, and AD/MCI vs. HC with an increment of 4.6%, 1.98%, and 3.94%, respectively, for 3D CNN over 2D CNN. Although the authors have achieved the best results with their designed model, which uses the combination of SAE and 3D CNN, the accuracy can be improved further by carrying out in-depth searches for the best hyper-param -
eters in both architectures. Furthermore, the overall accuracy in the future can be enhanced using fine-tuning at the cost of increased computational power during  
training time.
In another research paper, Saraf et al. designed the CNN model for the classifica -
tion of healthy subjects and brains affected by AD in adults ( >75 years) using MRI 
and fMRI images of the ADNI dataset [5]. The reported accuracy was 99.9% for fMRI and 98.84% for MRI images.
In [32, 33] studies, the researchers performed a linear registration, and ROIs were 
extracted using an automated anatomical atlas (AAL). These ROIs are defined as 3D bounding boxes covering all the voxels of the hippocampus. A “2D +ε approach” 
was used in these three studies, i.e., three neighboring 2D slices in the hippocampus region of MRI using the ADNI dataset were used to make a patch. Per patient, only one or three patches were used. So, they did not cover the whole brain region. In the first study [32], one patch comprising only sagittal view slices was seen as an input to the CNN classifier. And then, the CNN comprised two CL layers followed by max-pooling and one top-level fully connected layer for AD classification. In the second study [40], the authors considered all three projections (axial, sagittal, and coronal) of the hippocampus region of the brain and then generated three patches per 135 Alzheimer’s Disease Classification
TABLE 9.2
AD Classification Using CNN
S. No.Ref. No.  
(Year) DatasetNeuroimaging Modality  (Single or Multimodality) Sample SizeData Handling TechniqueDLModel Binary ClassificationACC
(%age)
AD/HC 95
1 (2015)
[31]ADNI sMRI (Single) AD (755)MCI (755)HC (755)Slice-based Sparse Encoder 
with 2D CNNAD/MCI 82
MCI/HC 90
V oxel-based SparseEncoder with 3D 
CNNAD/HC 99
AD/MCI 87
MCI/HC 92
2 (2016)
[5]ADNI sMRI AD (211)HC (91)Slice-based 2D CNN AD/HC 98.8
rs-fMRI AD (52)HC (92)99.9
3 (2017)
[32]ADNI sMRI AD (188)MCI (399)  
HC(288)Slice-based 2D CNN AD/HC 82
AD/MCI 62
MCI/HC 60
4 (2017)
[33]ADNI sMRI + PET
(Multimodality)AD (145)
MCI (192)HC (172)Patch-based  
(Patch size=3)SAE+ 3D CNN AD/HC 93
AD/MCI 82
MCI/HC 89
(Patch size=5) AD/HC 94
AD/MCI 83
MCI/HC 93
(Patch size=7) AD/HC 91
AD/MCI 84
MCI/HC 91
(Continued)136 Machine Learning for Edge Computing5 (2017)
[34]ADNI sMRI AD (47)
HC (34)Slice-based 2D CNN AD/HC 93
6 (2018)
[35]ADNI sMRI +DTI AD (48)MCI (108) HC(58)ROI-based 3D CNN AD/ HC 97
AD/MCI 80
MCI/HC 66
7 (2018)
[36]ADNI sMRI AD(150) HC(391) Slice-based 2D CNN AD/HC 95.9
3D CNN AD/HC 96.8
8 (2019)
[27]ADNI sMRI AD (647) sMCI (441) 
pMCI
(326) HC (731)ROI-based 3D CNN AD/HC 81.19
HC/pMCI -
PET AD/HC 89.11
HC/pMCI -
MRI+PET AD/HC 90
sMCI/pMCI 87.46
9 (2019)
[37]ADNI sMRI+PET AD (93)HC(100)ROI-based 3D CNN AD/HC 95
10 (2021) [38] Kaggle 
DatasetMRI Non-demented (510)Mild demented (46)Very mild demented 
(50)
Moderately demented 
(19)- CNN (Alexnet) Precision
Non-demented (94%)
Mild demented (98%)
Very mild demented (90%)
Moderate demented (100%)TABLE 9.2  (Continued )
AD Classification Using CNN
S. No.Ref. No.  
(Year) DatasetNeuroimaging Modality  (Single or Multimodality) Sample SizeData Handling TechniqueDLModel Binary ClassificationACC
(%age)
(Continued)137 Alzheimer’s Disease Classification
TABLE 9.2  (Continued )
AD Classification Using CNN
S. No.Ref. No.  
(Year) DatasetNeuroimaging Modality  
(Single or Multimodality) Sample SizeData Handling TechniqueDLModel Binary ClassificationACC
(%age)
11 2021 [39] OASIS-3 MRI AD(170),NC(70),Slice-based 2D CNN AD/HC 99.3
12(2019) [2]
ADNIsMRI AD (50)MCI (50) HC(50) Slice-based 2D CNNAD/HC 99
AD/MCI 99.3
HC/MCI 99.22
13 (2019) [13] ADNI (A) sMRI AD (294)MCI (763) HC(352)Slice-based 3D CNN AD/HC (using dataset 
A)99
Non-ADNI 
(B)AD(124) MCI(50) 
HC(55)AD/HC (Using dataset 
A & B)98
Abbreviations: ADNI, Alzheimer’s Disease Neuroimaging Initiative; HC, Healthy Control; AD, Alzheimer’s disease; MCI, Mild Cognitive Impairment; ROI, Region of 
Interest; PET, Positron Emission Tomography; rs-fMRI, Resting State Functional Magnetic Resonance Imaging138 Machine Learning for Edge Computing
subject: three separate CNNs comprising two CL, two pooling layers, followed by 
the ReLu activation function, and an FC layer. These networks were fused using two different techniques: intermediate and late fusion techniques. The reached accuracy was 91.4%, 69.5%, and 65.6% for AD vs. NC, AD vs. MCI and MCI vs. NC, respectively.
The authors presented a computer-aided AD recognition model, which was 
based on 3D CNN [34]. The brain’s 3D topology was considered as a whole while 
AD classification. The CNN comprised three groups of processing layers, two 
fully connected layers, and one classifier layer. Each of the processing layers was made up of three layers, namely, convolutional layers, pooling layers, and normalization layer. Their model was trained and tested on the MRI images of the ADNI dataset.
The authors designed 3D CNN to combine the features from the hippocampus 
area of both the modalities T1-weighted MR and FDG-PET to classify AD [27]. In the case of the MRI, only the hippocampus area was chosen to be the ROI, but for PET images, the authors tried different ROIs that contained only the hippocam -
pus area and both the hippocampi and cortices. They obtained the highest level of accuracy as 90.1%, 87.46%, and 76.9% for CN vs. AD, CN vs. pMCI, and sMCI vs. pMCI, respectively. These results demonstrated that using CNN segmentation is not the prerequisite. Although they achieved 90% accuracy for AD vs. CN classification, only 77% accuracy was recorded for classifying pMCI vs. sMCI. The performance of the model in terms of accuracy could be further enhanced by using new imaging modalities, such as T2-MRI, amyloid protein imaging.
To learn MRI features automatically in a target-oriented manner, various DL 
models have recently been propounded. However, feeding the whole MRI neuro-images to the CNN model directly did not generate the vigorous model, as millions of voxels are present in MRI and every part of the brain region may not be affected by AD. So, the main challenge in MRI-based DL models is identifying informa -
tive areas or regions in the neuroimaging modalities. To address this problem, the authors in [ 35] pre-extracted only the hippocampal ROI and neighboring areas 
in neuro scans (sMRI and DTI neuro images) from the ADNI dataset to train a CNN. The authors also demonstrated the size of the ROI matters for classification accuracy. The recorded accuracy was highest while classifying AD vs. HC (97%) and lowest for MCI vs. HC (66%). The authors in [ 36] propounded a 3D fine-tuned 
CNN model for classifying three brain subjects named HC, AD, and MCI, and the model was tested using the MRI ADNI dataset. They achieved accuracy for three binary classifications for AD/HC, MCI/HC, and AD/MCI as 97%, 93%, and 88%,  
respectively.
Due to inadequate medical images, it is rather hard to fully utilize CNN effec-
tively. To overcome this issue, the authors in [ 37] designed a novel DL framework; 
particularly the goodness of 3D CNN and FSBi-LSTM was exploited. At first, they designed 3D CNN to extract the deep features from both the MRI and PET modalities using the ADNI dataset. To improve its performance further, the fea -
tures maps output coming from 3D CNN was applied to FSBi-LSTM instead of the conventional FC layer to get rich semantic and spatial information. At last, they  139 Alzheimer’s Disease Classification
validated their model on the ADNI dataset. Binary classification accuracy reached 
up levels of 94.82%, 65.35%, and 86.36% for AD vs. NC, sMCI vs. NC, and pMCI 
vs. NC, respectively.
Various studies utilized DL models on MRI brain scans and achieved promis -
ing results. But most of the challenging issues with DL architectures such as CNN 
required a huge amount of dataset to train the model. The authors designed a Political, 
Economic, Social, Technological, Legal and Environmental factors (PESECTL) 
mathematical model based on transfer learning [2]. In their model, a 2D CNN-based 
VGG-16 model trained on the ImageNet dataset was utilized for feature extraction 
to classify ADNI sMRI slices among three different classes, namely AD, CN, and 
MCI. For every single subject (AD, CN, and MCI), the authors considered 32 slices 
based on the information entropy to prepare the dataset for their designed model. 
The three-way classification (CN vs. AD vs. MCI) accuracy reached 95.73% on the 
validation dataset. For future examinations, specialists should attempt other neural 
systems; for example, residual network, inception network, and later cutting edge 
systems as a base model for building the classifier.
Furthermore, the authors designed a DL algorithm based on the CNN architecture 
to discriminate AD, sMCI (MCI patients who remain stable and have not converted 
to AD), and cMCI (MCI patients who do not remain stable and have converted to 
AD) on 3D T1-weighted sMRI from combined ADNI and non-ADNI datasets. The 
CNN model was trained, validated, or tested on the ADNI dataset and combined 
ADNI and non-ADNI datasets separately to measure differences in accuracy [13]. 
Their model achieved almost similar accuracy up to 75% with only ADNI and com -
bined ADNI + non-ADNI dataset while discriminating cMCI and sMCI subjects. 
They excluded the presence of cMCI among s-MCI subjects. For this, a proper clini -
cal follow-up is required. Their model should be tested with a combination of other 
biomarkers, such as PET, cognitive, genetic, and cerebrospinal fluid.
9.5 OBSERVATIONS
It has been observed that dataset selection is crucial and can influence the output 
of the classifier. The findings cannot be compared, even though studies used the 
same dataset, had the same number of subjects, and used the same subject num -
ber code. But still a few common observations, such as structural and functional 
neuroimaging modalities like MRI, fMRI, PET, and DTI, are very useful in identi -
fying AD. Many researchers also use other parameters in addition to neuroimages 
such as age, educational level, cognitive-test points, and genetics to improve the 
accuracy of AD classification. It is still ambiguous to declare the most discrimi -
natory neuroimaging modality. However, multimodalities (a combination of one 
or more modalities together) are considered to be most successful since they rep -
resent the complementary information and provide better results over single 
modality. Next, the classification for MCI vs. HC and the transition time from MCI 
to AD are more valuable than the classification for AD vs. HC. ROI and patch-
based data handling strategies are more efficacious than slices and voxel-based 
techniques, as ROI-based methods lead to low dimension features and are readily  
understood.140 Machine Learning for Edge Computing
9.6 C ONCLUSION AND FUTURE DIRECTIONS
This chapter began with a description of AD and its symptoms, accompanied by 
an overview of the existing diagnostic criteria and associated biomarkers such as MRI, PET, and fMRI. In this chapter, papers using ML techniques, particularly SVM and DL model (CNN), were studied for AD diagnosis. As per this chapter, it can be noted that the most widely used dataset for AD is obtained from ADNI after gaining authorization from the concerned authority. More than 95% of research -
ers that we considered in this chapter have utilized the ADNI dataset. In both the ML and DL techniques, combining the modalities to identify AD gives better results. The advantage of using DL techniques over ML is the elimination of the feature extraction phase, which makes the work of researchers very fast and easy. The classification accuracy for AD vs. HC is always highest when compared with other binary classifications, such as AD vs. MCI and MCI vs. HC. So, in future, more work can be done to achieve comparable accuracy for cMCI vs. ncMCI, MCI vs. HC, and AD vs. HC, and to find the conversion time from the MCI stage  
to AD.
REFERENCES
 1.  L iu, M., Li, F., Yan, H., Wang, K., Ma, Y., Shen, L., & Alzheimer’s Disease Neuroimaging 
Initiative. (2020). A multi-model deep convolutional neural network for automatic hip -
pocampus segmentation and classification in Alzheimer’s disease.  Neuroimage , 208, 
116459.
 2.  J ain, R., Jain, N., Aggarwal, A., & Hemanth, D. J. (2019). Convolutional neural 
network based Alzheimer’s disease classification from magnetic resonance brain 
images.  Cognitive Systems Research , 57, 147–159.
 3.  YİĞİT, A., & I ŞIK, Z. (2020). Applying deep learning models to structural MRI for 
stage prediction of Alzheimer’s disease.  Turkish Journal of Electrical Engineering & 
Computer Sciences , 28(1), 196–210.
 4.  J abason, E., Ahmad, M. O., & Swamy, M. N. S. (2019, August). Classification of 
Alzheimer’s disease from MRI data using an ensemble of hybrid deep convolutional neural networks. In  2019 IEEE 62nd International Midwest Symposium on Circuits 
and Systems (MWSCAS) (pp. 481–484). IEEE.
 5.  S arraf, S., DeSouza, D. D., Anderson, J., & Tofighi, G. (2017). DeepAD: Alzheimer’s 
disease classification via deep convolutional neural networks using MRI and fMRI. BioRxiv , 070441.
 6.  K umar, S. S., & Nandhini, M. (2017). A comprehensive survey: Early detection of 
Alzheimer’s disease using different techniques and approaches.  International Journal 
of Computer Engineering & Technology , 8(4), 31–44.
 7.  C hoi, B. K., Madusanka, N., Choi, H. K., So, J. H., Kim, C. H., Park, H. G., ... & Prakash, 
D. (2020). Convolutional neural network-based MRI image analysis for Alzheimer’s  
disease classification. Current Medical Imaging , 16(1), 27–35.
 8.  E brahimighahnavieh, M. A., Luo, S., & Chiong, R. (2020). Deep learning to detect 
Alzheimer’s disease from neuroimaging: A systematic literature review.  Computer 
methods and programs in biomedicine , 187 , 105242.
 9.  L yu, G. (2018, October). A review of Alzheimer’s disease classification using neu -
ropsychological data and machine learning. In  2018  11th International Congress on 
Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)   
(pp. 1–5). IEEE.141 Alzheimer’s Disease Classification
 10. Alam, S., Kwon, G. R., & Alzheimer’s Disease Neuroimaging Initiative. (2017). 
Alzheimer disease classification using KPCA, LDA, and multi-kernel learning 
SVM.  International Journal of Imaging Systems and Technology , 27(2), 133–143.
 11. Tanveer, M., Richhariya, B., Khan, R. U., Rashid, A. H., Khanna, P., Prasad, M., 
& Lin, C. T. (2020). Machine learning techniques for the diagnosis of Alzheimer’s  
disease: A review.  ACM Transactions on Multimedia Computing, Communications, 
and Applications (TOMM ), 16(1s), 1–35.
 12. Liu, S., Yadav, C., Fernandez-Granda, C., & Razavian, N. (2020, April). On the design 
of convolutional neural networks for automatic detection of Alzheimer’s disease. 
In Machine Learning for Health Workshop  (pp. 184–201). PMLR.
 13. Basaia, S., Agosta, F., Wagner, L., Canu, E., Magnani, G., Santangelo, R., ... & 
Alzheimer’s Disease Neuroimaging Initiative. (2019). Automated classification of 
Alzheimer’s disease and mild cognitive impairment using a single MRI and deep neu -
ral networks.  NeuroImage: Clinical , 21, 101645.
 14. García-Floriano, A., López-Martín, C., Yáñez-Márquez, C., & Abran, A. (2018). 
Support vector regression for predicting software enhancement effort.  Information and 
Software Technology , 97, 99–109.
 15. Richhariya, B., & Tanveer, M. (2018). EEG signal classification using universum  
support vector machine.  Expert Systems with Applications , 106, 169–182.
 16. Cristianini, N., & Scholkopf, B. (2002). Support vector machines and kernel methods: 
The new generation of learning machines.  Ai Magazine , 23(3), 31–31.
 17. Vicente, J. M. F., Álvarez-Sánchez, J. R., De la Paz López, F., Toledo-Moreo, F. J., & 
Adeli, H. (Eds.). (2015).  Artificial Computation in Biology and Medicine: International 
Work-Conference on the Interplay between Natural and Artificial Computation, 
IWINAC 2015, Elche, Spain, June 1–5, 2015, Proceedings, Part I  (Vol. 9107). Springer.  
Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes 
Bioinformatics) , vol. 9107, pp. 78–87, 2015.
 18. Schmitter, D., Roche, A., Maréchal, B., Ribes, D., Abdulkadir, A., Bach-Cuadra,  
M., ... & Alzheimer’s Disease Neuroimaging Initiative. (2015). An evaluation of vol -
ume-based morphometry for prediction of mild cognitive impairment and Alzheimer’s 
disease.  NeuroImage: Clinical , 7, 7–17.
 19. Ortiz, A., Munilla, J., Álvarez-Illán, I., Górriz, J. M., Ramírez, J., & Alzheimer’s 
Disease Neuroimaging Initiative. (2015). Exploratory graphical models of functional 
and structural connectivity patterns for Alzheimer’s disease diagnosis.  Frontiers in 
Computational Neuroscience , 9, 132.
 20. Retico, A., Bosco, P., Cerello, P., Fiorina, E., Chincarini, A., & Fantacci, M. E. (2015). 
Predictive models based on support vector machines: Whole-brain versus regional 
analysis of structural MRI in the Alzheimer’s disease.  Journal of Neuroimaging , 25(4), 
552–563.
 21. Beheshti, I., Demirel, H., Matsuda, H., & Alzheimer’s Disease Neuroimaging 
Initiative. (2017). Classification of Alzheimer’s disease and prediction of mild cogni -
tive impairment-to-Alzheimer’s conversion from structural magnetic resource imaging 
using feature ranking and a genetic algorithm.  Computers in Biology and Medicine , 83,  
109–119.
 22. Long, X., Chen, L., Jiang, C., Zhang, L., & Alzheimer’s Disease Neuroimaging 
Initiative. (2017). Prediction and classification of Alzheimer disease based on quantifi -
cation of MRI deformation.  PloS One , 12(3), e0173372.
 23. Zhang, Y. T., & Liu, S. Q. (2018). Individual identification using multi-metric of DTI in 
Alzheimer’s disease and mild cognitive impairment.  Chinese Physics B , 27(8), 088702.
 24. Sheng, J., Wang, B., Zhang, Q., Liu, Q., Ma, Y., Liu, W., ... & Chen, B. (2019). A novel 
joint HCPMMP method for automatically classifying Alzheimer’s and different stage 
MCI patients.  Behavioural Brain Research , 365, 210–221.142 Machine Learning for Edge Computing
 25. Gosztolya, G., Vincze, V., Tóth, L., Pákáski, M., Kálmán, J., & Hoffmann, I. (2019). 
Identifying mild cognitive impairment and mild Alzheimer’s disease based on spon -
taneous speech using ASR and linguistic features.  Computer Speech & Language , 53, 
181–197.
 26. Bhandare, A., Bhide, M., Gokhale, P., & Chandavarkar, R. (2016). Applications of 
convolutional neural networks.  International Journal of Computer Science and 
Information Technologies , 7(5), 2206–2215.
 27. Huang, Y., Xu, J., Zhou, Y., Tong, T., Zhuang, X., & Alzheimer’s Disease Neuroimaging 
Initiative (2019). Diagnosis of Alzheimer’s disease via multi-modality 3D convolutional 
neural network.  Frontiers in neuroscience , 13, 509.
 28. Hubel, D. H., & Wiesel, T. N. (1968). Receptive fields and functional architecture of 
monkey striate cortex.  The Journal of physiology , 195(1), 215–243.
 29. Fukushima, K. (1980). A self-organizing neural network model for a mechanism of pat -
tern recognition unaffected by shift in position.  Biological Cybernetics , 36, 193–202.
 30. Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., & Chen, T. (2018). Recent 
advances in convolutional neural networks.  Pattern Recognition , 77, 354–377.
 31. Payan, A., & Montana, G. (2015). Predicting Alzheimer’s disease: A neuroimaging 
study with 3D convolutional neural networks.  arXiv preprint arXiv :1502 .02506.
 32. Aderghal, K., Boissenin, M., Benois-Pineau, J., Catheline, G., & Afdel, K. (2017, 
January). Classification of sMRI for AD diagnosis with convolutional neuronal net -
works: A pilot 2-D + ε study on ADNI. In  International Conference on Multimedia 
Modeling  (pp. 690–701). Springer, Cham.
 33. Vu, T. D., & Yang, H. (2017). Detecting Alzheimer’s disease by sparse autoencoder and 
convolutional network on multimodal data pp. 278–281.
 34. Luo, S., Li, X., & Li, J. (2017). Automatic Alzheimer’s disease recognition from MRI 
data using deep learning method.  Journal of Applied Mathematics and Physics , 5(9), 
1892–1898.
 35. Khvostikov, A., Aderghal, K., Benois-Pineau, J., Krylov, A., & Catheline, G. (2018). 
3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease 
studies.  arXiv preprint arXiv:1801.05968 .
 36. Tang, H., Yao, E., Tan, G., & Guo, X. (2018, August). A fast and accurate 3D fine-tun -
ing convolutional neural network for Alzheimer’s disease diagnosis. In  International 
CCF Conference on Artificial Intelligence  (pp. 115–126). Springer, Singapore.
 37. Feng, C., Elazab, A., Yang, P., Wang, T., Zhou, F., Hu, H., ... & Lei, B. (2019). Deep learn -
ing framework for Alzheimer’s disease diagnosis via 3D-CNN and FSBi-LSTM.  IEEE 
Access , 7, 63605–63618.
 38. Fu’adah, Y. N., Wijayanto, I., Pratiwi, N. K. C., Taliningsih, F. F., Rizal, S., & Pramudito, 
M. A. (2021, March). Automated classification of Alzheimer’s disease based on MRI 
image processing using convolutional neural network (CNN) with AlexNet archi -
tecture. In  Journal of Physics: Conference Series  (Vol. 1844, No. 1, p. 012020). IOP 
Publishing.
 39. Al-Khuzaie, F. E., Bayat, O., & Duru, A. D. (2021). Diagnosis of Alzheimer dis -
ease using 2D MRI slices by convolutional neural network.  Applied Bionics and 
Biomechanics , 2021 , 6690539.
 40. Aderghal, K., Benois-Pineau, J., Afdel, K., & Gwenaëlle, C. (2017, June). FuseMe: 
Classification of sMRI images by fusion of deep CNNs in 2D + ε projections. 
In Proceedings of the 15th International Workshop on Content-Based Multimedia 
Indexing  (pp. 1–7).143 DOI: 10.1201/9781003143468-10
Deep Learning 
Applications on Edge Computing
Naresh Kumar Trivedi, Abhineet Anand, Umesh Kumar Lilhore, and Kalpna Guleria
Chitkara University Institute of Engineering &  
Technology, Chitkara University  Punjab, India10
CONTENTS
10.1 Introduction  .................................................................................................. 144
10.1.1  Why Edge Is Preferred  ..................................................................... 147
10.1.2  Latency and Bandwidth  .................................................................... 147
10.1.3  Decentralization and Security  .......................................................... 148
10.1.4  Job-Specific Usage  ............................................................................ 148
10.1.5  Swarm Intelligence  ........................................................................... 149
10.1.6  Redundancy  ...................................................................................... 149
10.1.7  Cost-Effective in the Long Run  ........................................................ 149
10.1.8  Limitations for Deep Learning on the Edge  ..................................... 149
10.1.8.1  Parameter-Efficient Neural Networks  ................................ 149
10.1.8.2  Pruning and Truncation  ..................................................... 150
10.1.8.3  Distillation  ......................................................................... 150
10.1.8.4  Optimized Microprocessor Designs  .................................. 150
10.1.9  Application of Deep Learning Using Edge  ...................................... 150
10.1.9.1  Fraud Detection  .................................................................. 151
10.1.10  Deliver a Quality Experience at Scale  ............................................ 151
10.1.11  Cost Management  ........................................................................... 152
10.2 Limelight Edge Compute Offerings  ............................................................. 152
10.2.1  Bare Metal  ........................................................................................ 152
10.2.2  Virtual Machine  ................................................................................ 152
10.2.2.1  Serverless Compute  ............................................................ 152
10.2.2.2  Healthcare  .......................................................................... 152
10.2.3  Autonomy and Cloud Don’t Go Well Together  ................................ 155
10.2.3.1  Language Translations  ....................................................... 155
10.2.4  AutoML Translation  ......................................................................... 156
10.2.5  Translation API  ................................................................................. 156
10.2.6  Media Translation API  ..................................................................... 156144 Machine Learning for Edge Computing
10.2.7  News Aggregation and Fraud News Detection  ................................. 156
10.2.7.1  A Short History of Content Aggregators  ........................... 156
10.2.7.2  The Impact of Content Aggregators on News  
Consumption  ...................................................................... 157
10.2.8  Fake News Detection Using Edge Computing  ................................. 157
10.2.9  Natural Language Processing  ........................................................... 157
10.2.10  History of NLP  ................................................................................. 158
10.2.10.1  First Phase–The Late 1940s Until Late 1960s  ................. 158
10.2.10.2  Second Phase–The Late 1960s to Late 1970s  .................. 158
10.2.10.3  Third Phase–The Late 1970s to Late 1980s..................... 159
10.2.10.4  Fourth Phase (Lexical & Corpus Phase)–The 1990s  ....... 159
10.2.11  Virtual Assistants  .............................................................................. 159
10.2.11.1  How a Virtual Assistant Works  ........................................ 159
10.2.12  Visual Recognition  ............................................................................ 160
10.2.13  TensorFlow Lite  ................................................................................ 160
10.2.14  Detecting Developmental Delay in Children  .................................... 160
10.2.14.1  Colorization of Black and White Images  ......................... 162
10.2.15  What Is Auto-Encoders?  ................................................................... 162
10.2.16  Adding Sounds to Silent Movies  ...................................................... 163
10.2.17  Automatic Handwriting Recognition  ................................................ 164
10.2.18  Challenges in Handwriting Recognition  ........................................... 164
10.2.18.1  Demographic and Election Predictions  ........................... 165
10.2.18.2  Deep Dreaming  ................................................................ 165
10.3 Conclusion  .................................................................................................... 166
References  .............................................................................................................. 166
10.1 INTRODUCTION
Innovations and the widespread adoption of high-speed mobile networking in personal computing devices mean that today’s crowd-sourced applications are distributed glob -
ally, and crowd-sourced data is rather heterogeneous. Crowd-sourced applications obtain resources by collecting multiple contributors’ raw data. They must also be deployed on cloud platforms for self-service on-demand, unlimited pooling of resources, and elastic scalability. In various crowd-sourced applications, including spoken recognition, recommendation systems, and video classification, modern deep learning techniques have rapidly gained popularity. Nevertheless, there is significant pressure on the infra-structure in the most modern cloud computing paradigms given the high amount of crowd-sourced data and high requirements in the conventional deep learning processes, such as facial recognition or monitoring human resources in camera networks. Edge computing recently suggested that cloud computing be supplemented by other data pro -
cessing operations at the network’s edge. This last paradigm generation demonstrates a significant reduction in machine time, memory costs, and energy use compared to traditional cloud computing over a wide array of Big Data applications [1].
Deep learning is prevalent in various areas of use, such as computer vision, natural 
language processing, and Big Data analysis. For example, in many computer vision competitions over the past few years, deep learning approaches have consistently 145 Deep Learning Applications on Edge Computing
outperformed conventional object recognition and detection methods. However, the 
high degree of precision in deep learning comes at the cost of increased computa -
tion requirements and storage for both training and learning phases. Training a pro -
found learning model is costly in space and computer resources because it iteratively refines millions of parameters over many periods. The results are computationally expensive because the input data (e.g. a high-resolution picture) is theoretically sig -
nificant, and the input data requires millions of calculations. The characteristics of 
deep learning are that it is highly precise and requires a high use of resources [ 2].
1. Latency: For several applications, real-time deduction is essential. 
Autonomous camera frames, for example, need to be rapidly processed to identify and prevent obstacles. A voice-based assistance application must search, understand, and respond quickly to user requests. However, send -
ing cloud data or training data may result in more network queuing and propagation delays and it does not meet strict bottom-to-bottom low-speed requirements for immersive real-time applications. Actual studies have shown, for example, that more than 200 camera frames need to be down-loaded to Amazon Web Services and view work carried out [ 3].
2. Scalability:  The transition of data from the sources to the cloud causes 
scalability problems when the number of connected devices grows when a network connection to the cloud becomes a bottleneck. Network resources are also ineffective for downloading all data to the cloud, particularly when deep learning does not need all data from all sources. Bandwidth-intensive data sources, such as video streams [ 4], are critical.
3. Privacy:  Sending data to the cloud poses privacy problems for users who 
own or store data. Users can carefully upload their cloud information (e.g., face or language) and how the cloud or application uses them. The recent use of cameras and other sensors in a smart city, such as in New Mumbai City, for example, posed a significant concern [ 5–7].
Edge learning, a complementary service for existing computer platforms, is dedi -
cated to addressing the challenge of crowd-sourced, deep learning applications to address the huge network traffic and high computational requirements as well as increase device response times. Edge learning performs data preliminary network learning; raw information in local regions is processed on edge servers to minimize network traffic so that computation in data centers is accelerated.
By 2024, it is estimated that 5G mobile edge computing (MEC) will be a multi-
million dollar market with $73 million in company deployments. The sophistication of the data keeps growing every year. The increase in network complexity systems results from the increase in on-demand and personalized services. Providers of internet services have to handle traffic in connected cars, online gaming, voice over IP (internet protocol), and IoT computer transmissions on the internet for web brows -
ing. A fundamental change in mobile and fixed access networks is necessary for new 
restrictions that are being imposed by on-demand providers, as listed previously. 
Mobile networks of the fifth generation (5G) are being built to meet the growing demand and diversity [ 8].146 Machine Learning for Edge Computing
Network providers use cloud computing techniques to cope with the dynamic traf -
fic requested by modern users. In order to minimize the operating costs of increasing 
mobile networks to provide on-demand service, 5G networks are using Software-Defined Networking (SDN) and virtualizing network functions. In the long term, end users should anticipate improvements to be made to work because 5G is optimized to offer low latency, delays, high capacity, and high bandwidth communications for various applications, such as autonomous vehicles and robotics of Industry 4.0.
Since 5G networks have tight technical specifications, designers have rethought 
the backbone and access network architectures to better accommodate core func -
tions and complex networks. The advent of mobile edge computing disrupts the conventional distinction between end user network connectivity and core network reliability (information computing and storage).
The combination of mobile edge computing and cloud computing gives rise to 
virtualized network extensions and management planes that extend it to end-to-end services. Mobile edge computing will then serve as an essential component of the network transformation, such as quality of service and power usage. Mobile edge computing, in conjunction with SDN, was described by the European 5GPP (a Private-Public Partnership) as a major enabler of meeting throughput, scalability, latency, and automation requirements in 5G [9, 10].
Mobile computing puts computational power that is accessible to the consumer 
closer to the transaction. This proximity reduces the amount of traffic across the core network, increases response speed with latencies of ten milliseconds, and helps make certain computational tasks, such as online machine learning (ML), easier to handle in the cloud. The provision of cloud-based computing and network operators will allow a real-time investigation through location-based collaboration [11].
To infrastructure management, device, service, and network administration, the 
demands of heterogeneous connections pose a dynamic challenge in 5G networks. One of the main advantages of mobile edge computing is delivering, orchestrating, and managing services over time and space, which shift in place and location. A promising approach uses ML to satisfy this new set of demands, which conventional optimization methods can’t cope up. The core network and ML techniques for pro -
cessing in mobile networks need to evolve. In real-time networks, current network optimization techniques are too slow to keep up with the required research. Over the past two decades, ML has gained fame for pattern recognition [3, 12].
Formal artificial intelligence (AI) methods have been studied and used in com -
puter vision and natural language processing shown in Figure 10.1 . Cognitive 
automated 5G automatically configures, optimizes, secures, and recovers while aug -
menting real-time response complexities; however, it comes with extra overhead, capital expenditure, and resource usage. Cloud-based technology and mobile edge computing working together would improve resource use and profits and make ser -
vice providers more energy efficient [12, 13].
Deep learning is a type of ML and is an artificial intelligence subset on the other 
side. AI is a general term referring to techniques that enable computers to imitate human conduct. ML is a series of data-trained algorithms that allow all this.
But deep learning is nothing more than ML based on the human brain’s structure. 
In the context of a given data structure, a deep learning algorithm tries to draw the 147 Deep Learning Applications on Edge Computing
same conclusions. Deep learning uses many algorithms, called neural networks, to 
achieve this aim. Based on the brain anatomy configuration, neural networks were developed. Neural networks can be trained to recognize patterns and to differentiate between a different kind of knowledge. They can see a neural network as a coarse to the fine layer, which increases the chance of accurate result detecting and outputting. The architecture of the human brain works similarly in our minds. Whenever we have new information, our brains attempt to match it to known objects to check it. In other words, deep neural networks enable us to perform multiple operations, such as grouping, trial, and regression [14, 15].
Neural networks may carry out the same tasks as classic ML algorithms. The 
opposite is not true, though.
Scalable deep learning services are the basis for many limitations. Depending on 
your target application, you can expect low latency, improved security, or long-term economic performance. Under such conditions, maybe the best way to host a deep learning model is to use deep-edge learning models that provides additional benefits. Edge applies here to the local consumer goods estimate. Edge applies here to the local consumer goods estimate.
10.1.1 Why edge Is preferred
The reasons for favoring edge computing over cloud computing are abundant. A few 
of its properties are shown in Figure 10.2 .
10.1.2 l Atency  And bAndWIdth
Indeed, a remote server that runs a time trip is called by the API (RT). Applications 
requiring an almost immediate deduction cannot function properly when they are sluggish. For example, if the latency is high enough in auto-driving vehicles, the risk 
FIGURE 10.1  Deep learning.148 Machine Learning for Edge Computing
of injuries can increase substantially. Also, specific animation frames will feature 
rare incidents such as animal crossings and jaywalkers. Reaction times are essential to satisfy the needs of consumers like these. That’s why customized Nvidia hardware is used for edge inferences [12].
On the contrary, adequate bandwidth is significantly reduced if you have multiple 
devices on the same network. Communication channel competition exists when it is done on the outside; the computation can be substantially decreased. Many optical videomaker recorders are in 4K, which saves a lot of bandwidth, like smartphones, laptops, camcorders, and TVs. That’s because we don’t have to give the cloud data. We can efficiently expand this network by not sharing the cloud.
10.1.3 d ecentr AlIzAtIon And secur Ity
Business servers can be hacked and attacked. Of course, if you use a trustworthy 
seller, the danger is negligible. However, you must trust a third party to ensure that your data and IP are protected. By getting computers on the side, you get complete control of your IP.
10.1.4 j ob-specIfIc usAge
You’re going to have a toy factory. It has hundreds of jobs. An image classifica -
tion service is required at each workstation. In each workstation, various artifacts may not work out a single classifier. Moreover, hosting a variety of cloud classifiers 
FIGURE 10.2  Properties of edge computing.149 Deep Learning Applications on Edge Computing
will be very expensive. The system’s economy is that individual cloud classifiers are 
trained to be sent to the edges of the trained models. These computers have now been adapted for your desktop. They will be better than a classification that forecasts every workstation.
10.1.5 s WArm Intell Igence
Further to the above concept, edge devices can also support computer models in 
training. This is particularly useful for strengthening education, for which a large number of “episodes” can be simulated in parallel. Besides, online learning data can also be collected using edge computers (or continuous learning). For example, multiple drones can be used to survey a classification area. A single model with opti -
mizing techniques such as the asynchronous Stochastic Gradient Descent (SGD) can be trained simultaneously between all edge equipment. It can also be used only for aggregating and processing data from various sources.
10.1.6 r edund Ancy
For network architectures, robust memory and redundancies are highly vital. If one 
node in a network fails, the other nodes may have significant impact. Edge systems will provide a decent degree of redundancy in our case. If one of our edge devices fails (a node here), your neighbor takes over temporarily. This guarantees reliability and significantly decreases downtime.
10.1.7 c ost-effect Ive In the long run
Eventually, cloud services’ cost is higher than a certain number of inferior comput -
ers. This applies especially to your devices’ large duty cycles (i.e., most of the time they work). Also, edge equipment, when produced in bulk, is considerably cheaper, minimizing costs.
10.1.8 l ImItAtIons for deep leArnIng on the edge
Profound models of learning are famous for their size and expense. This is a chal -
lenge that is usually frugal to fit into edge devices. These problems can be dealt with in various ways as shown in Figure 10.3 .
10.1.8.1  Parameter-Efficient Neural Networks
Their massive scale is a striking feature of neural networks. Usually, edge devices cannot accommodate large neural networks. This led researchers to reduce the size of the neural networks while preserving accuracy. MobileNet and SqueezeNet are two common neural efficiency parameters [16].
The SqueezeNet contains many techniques, including late sampling and filter 
reduction, to achieve high efficiency at a low level of parameters. They add “Fire mod -
ules,” which have network parameter efficiency “squirting” and “expanding” layers.150 Machine Learning for Edge Computing
10.1.8.2  Pru ning and Truncation
Many neurons are benevolent and do not contribute to the absolute exactness of 
the trained network. In this case, we can save space by plucking these neurons. Learn2Compress by Google has shown that we can reduce the factor by 2 while preserving the accuracy of 97%.
Also, 32-bit float values are the most neural network parameters. On the contrary, 
edge devices may be configured to operate at or below 8-bit values. The model size can be considerably reduced by reducing accuracy. The reduction of the 32-bit model to the 8-bit model preferably decreases the model’s size by one fourth.
10.1.8.3  Distillation
Distillation teaches smaller networks into a giant network of teachers. This is used in the sizing of Google’s Learn2Compress. When combined with transfer learning, this is an extremely effective method for shrinking the model’s size without sacrific -
ing accuracy.
10.1.8.4  O ptimized Microprocessor Designs
We also addressed how neural networks can be stretched to accommodate our edge devices. The performance of the microprocessor will be measured as an alternative (or complementary) method.
A microprocessor, like the popular Nvidia Jetson, would be the easiest solution. 
However, when implemented on a large scale, these devices cannot be economically successful.
10.1.9 ApplIcAtIon of deep leArnIng usIng edge
A few years ago, we never expected profound apps to carry self-driving cars and 
virtual helpers like Alexa, Siri, and Google Assistant. Now, though, these inventions 
FIGURE 10.3  Limitation for deep learning on the edge.151 Deep Learning Applications on Edge Computing
are part of our daily lives. Deep learning keeps us fascinated by its infinite options 
for detecting fraud and pixel restoration. Let us understand deep learning applica -
tions in all industries.
Top applications of deep learning across industries are given in the next 
section.
10.1.9.1  Fraud Detection
The holy grail of businesses has long been identifying and stopping fraud in real 
time. For some time, technology has provided extremely robust tools to prevent 
fraud. Advanced analytics and artificial intelligence move things to a new stage—
well before fraud occurs.
According to an interview of 1,055 managers by the Association of Certified 
Fraud Examiners and SAS, advanced analytics and biometrics are becoming the 
main focus of anti-fraud programs. The study found that 13% of companies use AI  
and machinery to detect and prevent fraud. Another 25% expect to implement  
AI and machinery for the next year or two—an increase of 300% [5].
Biometrics—fingerprint, facial, or keystroke recognition—are the most popu -
lar methods for preventing fraud, with more than a quarter using such tech -
niques. At the same time, another 16% expect biometrics to be used in the next 
two years.
Blockchain/distributed ledger technology and robotics are less common than bio -
metrics, including robotics process automation (9% of organizations for both cat -
egories). However, in the next two years, large numbers of companies expect these 
innovations to be implemented.
The least used technology is virtual or augmented reality in anti-fraud programs. 
Today, just 6% of organizations use this technology, and almost two-thirds do not 
plan to.
• Nearly 35% (72%) of organizations intend to use automatic tracking, excep -
tion reporting, and the identification of anomalies by 2021. Likewise, nearly 
a half of all companies plan to use predictive analytics/modeling (52% up 
from 30 percent), and data shows (47%; currently 35%).
10.1.10 d elIver A quAlIty exper Ience At scAle
Limelight has the option to install and execute network edge video applica -
tions. You may pick locations worldwide to fit your geographical access and 
scale requirements. Various cutting calculation options allow you to simplify 
components that add delays, expenses, and inefficiencies to your service 
architecture.
Moreover, you can boost your delivery by incorporating edge computing in your 
content delivery network (CDN) workflow using CDN services from limelight. 
Using cached locally is much more effective than using a centralized role by ingest -
ing multi-edge Point of Presence (POP) and processing locally. Using one vendor 
also simplifies sourcing, workflow, and operations.152 Machine Learning for Edge Computing
The latency-sensitive workflow, including videos, is suitable for limelight edge 
calculations. Implementing edge will reduce the average latency dramatically over 
centralized computation approaches. The management of workloads at the edge will 
increase latency, efficiency, and reliability in use cases involving a CDN.
10.1.11 c ost mAnAgement
In addition to improving latency and size, edge calculations implementation will 
allow you to reduce costs associated with centralized cloud services, storage, and 
network transport. In the CDN case, you can achieve additional cost optimization 
with just one provider. Limelight offers zero-rate traffic from edge computes to our 
CDNs.
10.2 LIMELIGHT EDGE COMPUTE OFFERINGS
Limelight provides the best possible solution for your particular environment with 
its collection of cutting-edge calculation solutions, which are discussed in the next 
sections.
10.2.1 b Are metAl
Limelight Bare Metal Service (BMaaS) gives you calculating power, where it is 
needed without resource sharing. Bare metal is ideal if complete control and opti -
mum efficiency of computational resources are needed.
10.2.2 v IrtuAl mAchIne
As a solution, Limelight Virtual Machine (VM) provides a virtual computer capabil -
ity that is easy to deploy worldwide and easy to measure and develop as computing 
requirement change.
10.2.2.1  Serverless Compute
Limelight edge functions, integrated tightly with the CDN, automates the code for 
many leading sites worldwide and executes next to your users at your network edge 
to ensure the lowest latency and demand size.
Limelight’s advanced calculation provides benefits from direct peer connectivity 
for more than 1,000 ISPs and cloud providers, thus eliminating much latency and 
confusion in relying on conventional public internet routing.
10.2.2.2  Healthcare
10.2.2.2.1  Rural Medicine
Historically, it has been a struggle to ensure good healthcare in remote rural areas. 
Even today, medical suppliers struggle to provide quick, quality healthcare to people 
living far from hospitals and with limited access to the internet with innovation in 
telemedicine and more easily accessible health data. Traditional healthcare databases 
face significant challenges because of connectivity problems, but the combination 153 Deep Learning Applications on Edge Computing
of IoT medical devices and cutting-edge computer applications will simplify these 
problems.
Portable IoT healthcare equipment built by cutting-edge computer companies is 
in the position, without continuous interaction with network infrastructure, to cap -
ture, store, and analyze sensitive patient information. Patients with wearable IoT devices can be quickly and efficiently diagnosed on-site; and when connections are restored, the information collected can be returned to central servers. IoT healthcare 
systems can expand the scope of existing networks by connecting with an edge data 
center to allow physicians to access critical data, even in weak connectivity areas. This is only one of the edge cases in which the scope of healthcare facilities can be  
extended [ 17].
10.2.2.2.2  P atient-Generated Health Data
Patient-Generated Health Data (PGHD) is a rapidly growing field where the avail -
ability and development of the technologies have, in many instances, outpaced the publication of trials designed to evaluate health outcomes, usability, interoperability, and benefits and harms of these technologies. Therefore, it is essential to determine which available technologies have been assessed to determine efficacy related to health outcomes for consumers with (or at risk for) chronic diseases.
P. Subramaniam and M. J. Kaur report focuses on consumer technologies that 
provide PGHD. These are commercially available devices to consumers and do not require a prescription from a physician. Therefore, this report does not include medi -
cal devices that perform remote patient monitoring, which falls more broadly within telehealth [ 18].
10.2.2.2.3  I mproved Patient Experience
Going to the doctor should not be difficult or irritating. IOT medical devices are among the most critical computing devices that improve the patient experience fully in the healthcare industry from smart devices that allow people to check appoint -
ments whenever they wish to receiving alerts that direct them through a new facility to find the correct office.
Advanced computers can take on a much more integral role in IT infrastructures in 
the healthcare industry, with many IoT medical devices supporting patients and improv -
ing the experience. Many hospitals offer patients streaming entertainment facilities from movies and sports to immersive educational programs. Edge data centers can help to decentralize content with reduced latency and provide it more broadly [ 19].
10.2.2.2.4  S upply Chain
The supply chains in manufacturing are one of the most convincing examples of edge computing in action. Today’s hospitals and clinics are technological marvels, outfitted with cutting-edge medical technology and computer hardware in order to provide the best possible care. They also have less advanced medical equipment, that is no less essential, used to save lives in daily procedures. It is a huge logistical chal -
lenge to keep these facilities operating. Any interference with the supply chain that keeps them working creates significant health risks from costly mechanical elements for robotically assisted surgical tools to the smallest bandage.154 Machine Learning for Edge Computing
IoT edge devices with sensors will revolutionize the management of medical 
facilities’ inventories. Devices that collect data on the patterns of use can use predic -
tive analysis to assess if hardware fails. At the same time, smart Radio Frequency 
Identification (RFID) tags can remove long-term paperwork and manual ordering from inventory administration. The location of vital shipments in real time can be monitored by floating vessels fitted with GPS and other sensors. IoT healthcare sup -
ply chain technologies provide an opportunity to achieve operational efficiency on 
the margins for organizations that struggle to manage rising costs and constitute one 
of the most significant cases for computer technology use [ 20].
10.2.2.2.5  C ost Savings
Regarding cost savings, analysts expect that the widespread use of IoT cutting devices will enable healthcare organizations to reduce their business costs by up to 25%. Some of these savings would come from daily applications, such as safety and surveillance or intelligent building inspections, but patients monitoring and interac -
tion could provide real innovation. Wearable IoT medical devices and implantable sensors are among the most advanced computer applications that could significantly reduce patient costs through the entire treatment cycle.
Another possible source of cost savings is interconnectivity. Medical service 
providers have been plagued for a long time by inconsistent structures and diffi -
cult recording, all but eliminated by IoT medical device networks and cutting-edge computing applications that can interact rapidly and efficiently between organiza -
tions. Any technology that can increase productivity and provide a better product is undoubtedly rapidly implemented, with growing costs posing a continuing challenge to people’s access to health services.
While IoT edge devices are already trendy, their full potential has not yet scratched 
the surface. As the quantity of devices is still growing and network database infra-structures are additionally burdened, advanced computation examples will soon be found in all medical IT strategies. The health sector will enormously benefit from both advances, and the one-two punch of IoT medical devices and edge computing will certainly bring key benefits in the future [ 21].
10.2.2.2.6  S elf-Driving Cars
• Milliseconds matter when you drive a car. Autonomous vehicles are no dif -
ferent, even though your AI powers them. AI = data + calculator, and you 
want your estimate as similar as possible to your data.
10.2.2.2.7  C omputing in Edge
• People are both mindful of the cloud and love it. How about not having to worry about what your gadgets will do and about having almost unlimited elastic storage and calculation power?
• Okay, a couple of things. In the end, the cloud is just another’s machine, as the aphorism goes. Okay, there are maybe millions of computers in super-powerful data centers, organized with respect—but these are all computers belonging to someone else.155 Deep Learning Applications on Edge Computing
• However, does it matter if someone can provide all that you need, poten -
tially more effective than your own company, along with security guar -
antees? It doesn’t in certain situations. But when it comes to autonomous 
vehicles, it is essential.
10.2.3 Autonomy  And cloud don’t go Well together
Let’s look at the notion of autonomy to understand why. Autonomy is characterized 
as “independence or freedom by the will or the action.” If you rely on someone else’s machine, can you be autonomous? Not so. Not so.
Yes, redundancy exists, and yes, even service-level agreement (SLA) can exist. 
But when all is said and done with the cloud, you usually connect to another device over the internet. What happens if you run into connection problems while you are in a moving vehicle and this vehicle depends on a cloud-based computer for its main functions?
This is not the same as loading your favorite cat photos and they are lagging. A 
lag is a matter of life and death in a moving vehicle situation. And in cases like this, what can be done? Computing in edge.
Edge computing is a phenomenon where data is produced outside the data center, 
and the data is measured as similar to the data as possible. This means fragile, pre -
fabricated data centers in real life.
Naturally, small is a subjective concept. Is a container small? Perhaps if you 
equate it with a data center, such as a cloud provider. However, in our houses, it’s not something that any of us could or will have.
However, we still have homes where some of the significant edge computing capa -
bilities are used. For smart homes or smart city situations, connected devices that communicate through IoT sensors are an excellent match for cutting-edge comput -
ing. These scenarios, wholly blown, involve many devices gathering and sharing a large amount of data.
10.2.3.1  L anguage Translations
In November 2016, Microsoft benefited developers and end users with its AI-based machine translation known as Neural Machine Translation (NMT). By leverag -
ing neural processing unit (NPU), an AI-dedicated processor built into Mate 10, Huawei’s new flagship handset, Microsoft took NMT functionality to the edge of the cloud. Even without internet access, this new chip provides AI-powered translations for the device, which enable the machine to produce translations that are in line with the online system’s quality. Microsoft and Huawei researchers and engineers have worked together to adapt the neural translation to this new computer world to achieve this breakthrough [ 22].
The most sophisticated NMT systems in development at present (i.e., those used 
by companies and applications to scale in the cloud) use a neural network architec -
ture that combines many layers of Long Short-Term Memory (LSTM) networks, a care algorithm, and a decoder layer of translation.156 Machine Learning for Edge Computing
10.2.4 Automl t rAnslAtIon
Developers, translators, and localization professionals can easily create production-
ready, high-quality models with minimal ML expertise. Translated language pairs 
are uploaded, and AutoML Translation builds a custom model adapted to particular domain requirements.
10.2.5 t rAnslAtIon ApI
Translation API Basic translates a text for your website or mobile app between more 
than 100 dialects in real time. Advanced Translation API has the same quick and interactive results as Basic Translation API and additional customization options. For domain and context-specific terms or phrases, customization is important.
10.2.6 m edIA trAnslAtIon ApI
With increased precision and easy integration, the Medium Translation API offers 
real-time audio translation directly to your content and applications. You can also boost your user experience through low latency streaming translation and quickly internationalize your business.
Amazon Translate is a neural network machine translation service that delivers a 
high quality, accurate translation at an affordable cost. Neural machine translation is a type of automation in language translation that uses profound learning models to translate more precisely and sound more natural than statistical or rule-based algorithms in translation.
Amazon Translate enables you to quickly find content for your diverse customers, 
including websites and applications, translating vast amounts of text for analysis and allowing cross-language communication among users [ 13].
Recently, Amazon declared Translate the top machine translation provider in 
2020 across 14 language pairs, 16 industry sectors, and 8 content types.
10.2.7 n eWs Aggreg AtIon And frAud neWs detect Ion
10.2.7.1  A S hort History of Content Aggregators
The year 2015 saw the emergence of content aggregators in the digital industry. Since multiple publishers saw syndication as a sin, they opposed aggregators and chose Google News, Yahoo News, and AOL. Neither of them wanted to dig into their budget. Therefore, using Huffington Post, BuzzFeed, Vine et al., we’re searching for free news instead.
The size of the audience at these aggregator sites was small, but it was believed that 
they could expand quickly. Many editors tried to achieve the common aim of “being your readers all over,” and sadly, instead of seeking a better way for the public to read comprehensive news, began to offer their contents free of charge to those aggregators.
In return, they have plenty of content views, but not a great deal of return.When you are concerned about sales, which is a considerable obstacle for 
digital publishers, you can only bet on subscription models and nothing else for 157 Deep Learning Applications on Edge Computing
content aggregators. This does not propose a win-win scenario for digital pub -
lishers [21].
10.2.7.2  T he Impact of Content Aggregators on News Consumption
In the last decade, content discovery has changed considerably, but news consump -
tion has changed dramatically too. Consumers demand high-quality sharable, 
searchable, and readily accessible news content that can be delivered frictionless in exchange for their interest.
Many users now opt to pay once a month for an aggregator platform to access 
news from several publishers. Paying once and consuming complete material during the month is more convenient and cost-efficient.
10.2.8 f Ake neWs detect Ion usIng edge comput Ing
Over the years, fake news has spread rampantly on social media. Fake news has 
become a famous demon that affects the population overall. Its daily users are not only concerned, but also the advertisers are concerned about the effects of fake news on business. Online news outlets are a shield with two rims. Fake news is becoming a threat to our culture more and more. It is commonly used to draw audiences and also to increase advertisement profits for business interests. However, it has been known that media giants with potentially sinister motives generate false news to manipulate global affairs and policies.
Fake news from sarcastic posts to fabricated information and government misin -
formation in certain news outlets today creates various problems. There are wide -
spread issues with the significant social implications of fake news and a lack of confidence in the media. Of course, “fake news” is intentionally deceptive, but the social media debate has lately changed its meaning. Some now use the word to reject evidence that contradicts their favorite points of view.
In particular, after the U.S. presidential election, the role of misinformation was 
a significant concern in American political discourse. The word “fake news” was used particularly to describe factually inaccurate and misleading stories papers often published to make money from page views. The law aims to generate a model that precisely forecasts the probability of false news in a given post.
After the media coverage, Facebook was at the center of much criticism. Model 
introduced a function to report false news on the website when a user sees it and has publicly said that they work to automate the distinction between these posts. This isn’t a simple job, of course. A specific algorithm must be respectfully neutral, as false news on both ends of the spectrum exists and must compare credible news outlets on both ends of the spectrum. Furthermore, it is a complex question of valid -
ity. However, it is required to get an understanding of what fake news is to address this issue. Later, it is necessary to examine how ML and natural language processing help us identify fake news [ 23].
10.2.9  NAturAl LAngu Age Process Ing
To grasp and control the human language, natural language processing (NLP) 
uses algorithms. This technology is one of ML’s most commonly used fields. As  158 Machine Learning for Edge Computing
AI expands, so does the need for building model professionals who analyze lan -
guage and voice, unlock contextual trends, and generate insights into text and  
audio.
The encoding of NLP is one of the most critical information age technologies. A 
key element of artificial intelligence is understanding complex language statements. 
NLP applications are everywhere, and most people communicate in language: web 
search, ads, e-mail, customer care, translation of languages, radiology reports, etc. 
There is a wide range of essential tasks and learning machines for NLP applications. 
Recently, deep learning methods in several different NLP tasks have achieved very 
high efficiency. These models can also be trained using a single end-to-end model 
and require no standard functionality. Students will learn to implement, practice, 
debug, imagine, and invent their model neural network this spring.
We may therefore infer that NLP is the computer science subfield that focuses 
on computer comprehension and processing human language. The critical task of 
NLP is technically to program computers that analyze and process large quantities 
of natural language data.
10.2.10 h Istory  of nlp
It is possible to split the NLP history into four phases. The phases have different 
types of problems.
10.2.10.1 First Phase—The Late 1940s Until Late 1960s
The main subject of the work at this point was machine translation (MT). It was a 
time of excitement and hope.
The NLP research started in the early 1950s after a Warren Weaver’s essay 
on the 1949 Machinery Translation Memorandum (MTM) study by Booth & 
Richens. In 1954 the Georgetown-IBM experiment demonstrated a minimum 
automatic translation experiment between Russian and English. The MT Journal  
was published in the same year. The journal for machine translation in 1952, the 
first Machine Translation (MT) International Conference was held, and in 1956, a 
second conference was held. In 1961, the work on Machine Language Translation 
and Analysis at the International Teddington Conference took center stage, and 
this method was then presented.
10.2.10.2 Second Phase—The Late 1960s to Late 1970s
The work done in this process was mainly linked to world awareness and its role 
in the creation and manipulation of representations of significance. This process is 
often referred to as the AI-flavored phase. The tasks in managing and developing 
data or information databases started at the beginning of 1961—AI inspired this 
work. Also created a question-response system for baseball in the same year. The 
system entry was limited, and the language processing was quick.
Marvin Minsky identified a highly sophisticated device in 1968. This method 
is understood and designed to determine the knowledge base in the interpretation 
and response of the linguistic information compared to the BASEBALL question 
answering system.159 Deep Learning Applications on Edge Computing
10.2.10.3  T hird Phase—The Late 1970s to Late 1980s
The grammatical logical process can be defined as this phase. In the last stages, the 
researchers could not implement logic to reflect information and reasoning in AI because of the failure of the practically constructed method.
By the end of the decade, the grammar-logical approach helped with its strong 
general-purpose phrase processors, such as the SRI core language engine theory and representation of discourses, which offered a means of approaching broader speech. 
In this period, we gained some valuable tools, such as parsers; e.g., Alvey Natural 
Language Tools; and more business processing queries; e.g., for database queries. The 1980s also directed lexicon work toward grammatical logic.
10.2.10.4  F ourth Phase (Lexical & Corpus Phase)—The 1990s
It’s a lexical process; we can understand it. In the late 1980s, the stage had a lexi -
calized approach to grammar. In this decade, the emergence of machine language learning algorithms led to a revolution in managing natural languages.
10.2.11 v IrtuAl AssIstAnts
A separate contractor is a virtual helper that provides administrative support to cus -
tomers while operating outside the client’s office. A virtual assistant usually works from a home office but can access the necessary planning documents from afar, such as shared calendars.
Individuals who work as virtual assistants also have several years of manage -
ment experience. New opportunities will be provided to digital assistants with social media experience, content management, blog post writing, web design, and internet marketing. The demand for skilled virtual workers is expected to increase because employees and employers are more involved in homework.
A virtual assistant is an individual employee who specializes in delivering cus -
tomer administrative services, typically at home from a remote location.
A virtual assistant can perform traditional tasks, including scheduling appoint -
ments, making telephone calls, handling travel agreements and managing e-mail accounts.
Some automated helpers provide graphics, blogs, bookkeeping, social networks, 
and marketing services.
The ability for an employer to contract only for the services they require is a sig -
nificant benefit of employing a virtual assistant [ 24].
10.2.11.1  H ow a Virtual Assistant Works
Digital helpers have become increasingly popular, with small companies and start -
ups relying on virtual offices for cost management. As an independent contractor, a virtual assistant will not receive the same pay and benefits as an employee working in an organization full time.
As the virtual assistant is working off-site, the organization’s office does not need 
any desk or any other workspace. A virtual assistant must pay for and provide their own computers, standard applications, and high-speed internet services.160 Machine Learning for Edge Computing
10.2.12 v IsuAl recogn ItIon
Edge computing contains computing (and some data storage) to devices that generate 
or retrieve data on a cloud-based central infrastructure (particularly in real times). 
There are no latency issues with this data, and this approach reduces transmission and treatment costs. It’s as if it is all done on a laptop instead of in the cloud locally.
Since IoT devices with an internet connection have increased exponentially, 
advanced computing is generated for either cloud information. And many IoT devices produce a lot of information during operation.
Edge computing poses new possibilities in object detection, IoT (and face), lan -
guage processing, and obstacle prevention, especially for those relying on ML. Image data is an excellent addition to IoT and a significant user of resources (power, memory, and processing). “Edge” picture processing, running AI/ML classics mod -
els, is a giant leap!
Express is one of the programs used by most researchers/researchers nowadays in 
the area of visual recognition [ 25, 26].
10.2.13 t ensor floW lIte
TensorFlow Lite is an open-source deep learning framework for small, small-latency 
computer ML. It is built to support you “at the edge” of your network instead of send -
ing back and forth data from a server.
There are two main components in TensorFlow Lite (TF Lite):The TF Lite transformer is transforming TensorFlow models into an efficient 
interpreter type and optimizing binary size and performance.
The TF Lite interpreter works on various hardware, including smartphone, embed-
ded Linux computers, and microcontrollers, with specially optimized versions.
In short, the qualified and stored TensorFlow (like a model h5), which will be 
used by the TF Lite Interpreter inside an edge (like a Raspberry Pi), will translate into a TFLite FlatBuffer (like model Tf Lite).
For instance, in a Mac (the “Server”), we trained from scratch the simple CNN 
image grading model. The last model was trained with 225,610 parameters using CIFAR10 as input: 60,000 pictures (shape: 32, 32, 3). The model trained was 2.7 MB in size (iffar10 model.h5). With the TFLite converter, 905 KB (approximately 1/3 of the original size) was applied to the Raspberry Pi model (model cifar10.tflite). Deduce the results from both (.h5 on Mac and tflite on RPi), the same results are obtained [19].
10.2.14 d etect Ing development Al delAy In chIldren
You child is experiencing a developmental delay if they fall under one or more areas 
of social, behavioral, or physical development behind their peers. Early care is the best way to help your child improve or even catch up if your kid is delayed. In babies and young children, there are several different kinds of developmental delays. They include issues with:
• Language or speech
• Vision161 Deep Learning Applications on Edge Computing
• Movement—motor skills
• Social and emotional skills
• Thinking—cognitive skills
There are periods when two or more of these areas are greatly delayed. It is known 
as “economic development delay” when this occurs. It applies to children and pre-
school children up to five years of age who have a minimum delay of six months. A 
developmental delay is distinct from disorders such as brain paralysis, hearing loss, 
and autism, which generally last for life.
Kids learn to crawl, chat, or use the toilet at various speeds. However, some 
children hit the milestones much later than other children. These delays have many 
causes, including:
• Premature birth
• Genetic disorders such as Down syndrome or muscular dystrophy
• Impaired vision or hearing
• Malnutrition
• Prenatal alcohol or substance use
• Physical violence or neglect
• Inadequate oxygen throughout childbirth
A system uses a validated screening tool to identify developmental delays with appro -
priate precision. However, only 48% of pediatricians used a standardized develop -
mental screening method in 2011. Fifty-two percent of parents reported informally 
about the growth of their children, and 21% reported by filling a questionnaire, 
according to a study from the Centers for Disease Control and Prevention.
Numerous barriers exist when it comes to checking for developmental delays 
in routine clinical practice. According to a study, 82% of primary care physi -
cians cited time constraints as the primary impediment. Other screening hurdles 
include:
• Overlapping health needs
• Lack of accessible referral suppliers
• Personnel requirements
• Lack of agreement about the appropriate screening methods and lack of 
trust in medical training
• The ability to treat behavioral and emotional problems in children 
effectively
There is also a high turnover of employees with subsequent training in tool manage -
ment and a lack of compensation.
Developmental screening tests cannot be used for a developmental disorder diag -
nosis; hence, a method must be used that is most precise to avoid detection and 
overreferences. The literature did not identify an ideal initial screening method. A 
perfect test will cover all growth areas, apply equally to all ages, and have structural 
validity and fewer false negatives and false positives. The American Academy of 162 Machine Learning for Edge Computing
Pediatrics (AAP) recommends systematic assessment methods for fine and gross 
motor abilities, language and communication, problem-solving, adaptive behav -
ior, and personal-social skills. Culturally sensitive screening strategies should be assessed in the mother tongue of the patient.
10.2.14.1
 Colorization of Black and White Images
Most pictures in the nineteenth century were monochromatic because people painted 
them by hand, mostly simply because of aesthetics. Color images looked better on the mantle. For these purposes, contemporary artists continue to add color. The artist and teacher Tina Tryforos started hand-coloring images before going digital. Early in her career, she worked by hand with a minimal palette of colors, because she prefers simplicity. “I love the more nostalgic and painterly look instead of trying to make it look precise,” she says. [36]
Photographer Kenton Waltz enjoys black-and-white images so he can decide 
which colors are used in a picture and which are left out. “The simplicity of color is the elegance of a frame,” he says. “You value light and dark consistency.” [36] 
None of the colorization channels you know is the colorization information of 
the RGB color channel inside the three channels if any one channel did not exist, which destroys the colors in the picture. You know the colorization information of the image.
You want to emit three channels of the RGB components of this black-and-white 
image, which is the critical issue when using this black-and-white image as an input. Now imagine that you have a black-and-white image and have placed it in the black box from the right side, resulting in three RGB elements, one of which is the black box auto-encoder in response.
10.2.15 What Is auto-Encod Ers?
Ian Goodfellow, the inventor of GANs, described Auto-encoders in his 2016 book Deep Learning  as follows:
An auto-encoder is a neural network trained to copy the input to the output. Internally, it has a secret layer h defining the input code. The web consists of two parts: the h =f(x) 
encoder function and the r =g reconstruction decoder (h). Figure 10.4 illustrates this 
architecture. It doesn’t really work if an auto-encoder will learn simply to set g(f(x)) =x 
anywhere. An auto-encoder is instead designed to keep you from perfectly copying. Usually, they are limited so that they can only copy information roughly similar to the training data and only copy input. Since the model is forced to give priority to copying whatever type of the input, useful properties of data are often taught. The definition of an encoder and decoder has been extended to stochastic mappings, p encoder (h|x) and p decoder (x|h), beyond deterministic functions.
Historically, auto-encoders have been used to reduce the number of dimensions 
or to learn new functionalities. Additionally, recent advancements in generative modelling have elevated auto-encoders to the forefront of theoretical relationships between the auto-encoder and latent variable models. Auto-encoders can be thought of as a subset of feedforward networks since they are trained using the same technol -
ogy, usually using minibatch gradient descent following back-propagation measured gradients.163 Deep Learning Applications on Edge Computing
The auto-general encoder’s structure, as mapped by an internal representation or 
code h from an input x to an output (reconstruction). Both the encoder f(mapping x to 
h) and decoder g are composed of two components (mapping h to r). One solution is to create two copies, one of which will be a grey image. The input will be an encoder that extracts the functions of the photo “Latent Space Representations” image that can be used to reconstruct an image for the decoder. In contrast, the other copy will be the same image, but colored to represent the decoder’s target.
Ian Goodfellow, Yoshua Bengio, Aaron Courville [27, 35] 
10.2.16 a ddIng sounds  to sILEnt movIEs
Movies have often added sound effects not captured for more realistic purposes dur -
ing the movie. This is a “Foley” operation. This method has been automated by researchers at the University of Texas. With 12 famous movie events, Jack Foley added Foley effects; he has developed a neural network [37]. Their neural network classifies the sound class to be created and also has a sequential sonic network. Thus, neural networks have used a whole different modality from temporally aligned images to sound generation!
The first thing the researchers did was construct a dataset containing short film 
clips with 12 film events (Automatic Foley Dataset). They created sounds inside a studio for some movie events (such as cutting, footsteps, and a clock sound). They downloaded video clips using YouTube sounds for other events (such as guns, horses running, and fire). The total length of 1,000 videos was five seconds.
The next move was to forecast the correct sound class. They compared two 
approaches for this purpose:
1.
 Frame sequence network (FSLSTM)
2. Frame relation network (TRN)
Every video frame was taken in the frame sequence network approach. They then interpolated frames for greater granularity between the current frames in the video. 
FIGURE 10.4  Auto encoder.164 Machine Learning for Edge Computing
The image features were derived from a ResNet-50 convolutionary neural network 
(CNN). Then the sound class was predicted with Fast-Slow LSTM-fed image fea-tures through a recurrent neural network. They attempted to capture detailed trans-formations and behavior of the objects in less calculative time in the frame relations network. The network structure relationship (the multi-scale temporal relationship network, more precisely) compared characteristics in frames at N-distances, in which N assumes many values. Finally, all these characteristics were combined with a multilayer perceptron again [28].
They took each video frame in the frame sequence network technique. They then 
interpolated frames between the video’s current frames to add granularity. Image attri -
butes were extracted using a ResNet-50 CNN. The sound class is then predicted using an image-based recurrent neural network called Fast-Slow LSTM. They attempted to capture the detailed transformations and behavior of the objects in the Frame Relation Network with the least amount of computational time possible. The frame relation network (or, more specifically, the multi-scale temporal relation network) compares features between frames separated by N, where N can take on any value. Finally, all of these characteristics were combined using a multilayer perceptron.
The results of the algorithm, including a human qualitative test, were ana-
lyzed with four different approaches. Local university students were asked to choose the sound that was the most natural, the most comfortable, the one with the least noise, and the most synchronized. In 73.71% of cases for one model and  
65.96% of cases for another model, the synthesized sound was favored over the origi -
nal sound. The choice depended on the video for each model: one model performed better on the scenes with several random changes in action [29].
10.2.17 a utomat Ic hand WrItIng rEcogn ItIon
By 2025, the Optical Character Recognition (OCR) market is expected to hit  
$13.38 billion; this rise of 13.7% is driven by the rapid digitalization of business processes using OCR to minimize labor costs and free up valuable work hours [30]. Whereas OCR has been viewed as a solution, a crucial factor, handwriting recogni -
tion or manuscript text recognition, continues to be viewed as a challenge. In com -
parison to typed text, the wide variety in handwriting styles between individuals and the poor quality of the writing pose major barriers to translating readable text to the machine. However, it is important to address for several industries, such as health-care, insurance, and banking.
Deep learning techniques, such as transformer architectural growth, have made 
significant strides in recent years toward cracking handwritten text recognition. Smart character recognition is used to refer to handwritten text recognition because the algorithms required for ICR resolution require significantly more intelligence than generic OCR solutions [31].
10.2.18 c haLLEngEs In hand WrItIng rEcogn ItIon
• High inconsistency and ambiguity between the strokes of a person to a person.165 Deep Learning Applications on Edge Computing
• The handwriting style of the individual often differs from time to time and 
is inconsistent.
• The text of the written documents is placed in a straight line, while the indi -
vidual does not need to write a text line straight on a white paper.
• The cursive handwriting process makes separating and identifying charac-ters challenging.
•
 Gathering a good labeling data collection is not inexpensive compared with synthetic data.
ML methods, including hidden Markov models (HMM), Support Vector Machine (SVM), etc., were the initial approaches to solving handwriting recognition. After pre-processing the initial text, the function extraction defines key details of an indi -
vidual character, such as loops and inflection points, aspect ratios, etc. These features are then supplied to an HMM classifier to obtain the data. Due to the manual extrac -
tion process and limited learning ability, ML model’s output is very restricted. The extraction step for each language varies and is therefore not scalable. With this deep learning, the accuracy of handwriting recognition has improved tremendously [32].
RNN(RN)/LSTM (long short-term memory networks) can handle sequential data 
in which time patterns are identified and produced. However, they deal with 1D data and thus do not refer to image data directly.
The LSTM (MDLSTM) is a multi-dimensional block that only substitutes an 
LSTM block to RNN block from the MDRNN discussion above. The entries are divided into 3 × 4 size blocks and are fed into layers of MDSTM, followed by layers of feed-forward ANN. The final output is transformed into a 1D vector and gener -
ated using a Connectionist temporal classification (CTC) function [33].
CTC is an algorithm used to perform tasks such as speech recognition and hand-
writing recognition using only the input and output transcript information. However, no precise alignment information is given; i.e., how a particular audio region for speech or region in handwriting images corresponds to a specific nature. Simple heu -
ristics, such as naming each character, would fail because each character’s amount of space varies from person to person and from time to time [34].
10.2.18.1
 Demographic and Election Predictions
These words tend to be met by election forecasting. First, elections have direct results 
that can be measured for the accuracy of the prediction. Such feedback can help fore -
casters know about wrong prejudices in judgement. Second, theory and empirical evidence about electoral behavior, particularly in the U.S. presidential election, can be used by policy experts to help them read and interpret elections [29].
10.2.18.2
 Deep Dreaming
Deep Dream is an experiment that visualizes neural network patterns [38]. Similarly, 
Deep Dream over interprets and improves patterns in a picture when a child watches clouds and attempts to understand random types.
This is done by transmitting a picture through the network and then measur -
ing the picture’s gradient about the layer activations. The image will be changed to increase those activations, improve network patterns, and create a dreamlike image 166 Machine Learning for Edge Computing
called “Inceptionism.” This phase is a reference to the Inception Net, and the movie 
Inception  (2010).
10.3 C ONCLUSION
This chapter examined the latest state of the art for profound knowledge on the edge of the network. Computer vision, natural language processes, network features, Virtual Reality (VR), Augmented reality (AR), and the need to process end device data in real time were discussed in this chapter. Methodologies were defined for speeding deep learning inferences across terminal devices, edge servers, and the cloud that use the unique DNN models structure and the geospatial location of edge computing user requests.
Significant factors in many works have been found in the trade-off precision, 
latency, and other performance metrics. Deep learning models were trained, in which multiple end-components worked together on a model of DNN, including techniques to enhance privacy (perhaps with the aid of an edge server or a cloud).
There are still several ongoing challenges in further progress in efficiency, pri -
vacy, resource management, benchmarking, and integration with other networking technologies. Technical advances in algorithms, device architecture, and hardware acceleration will solve these challenges.
With the rate of profound learning advancement remaining strong, in addition to 
current prospects for innovation, it may also present new technological challenges in the complex calculation.
REFERENCES
1.Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature , v ol. 521, no. 7553,
p
p. 436–444, 2015, doi: 10.1038/nature14539 .
2.S. Mazumder, “Big Data Tools and Platforms BT - Big Data Concepts, Theories, and
Applications,” S. Yu and S. Guo, Eds. Cham: Springer International Publishing, 2016,
pp. 29–128.
3.G. Muhammad, M. F. Alhamid, M. Alsulaiman, and B. Gupta, “Edge Computing with
Cloud for Voice Disorder Assessment and Treatment,” IEEE Commun. Mag. , vol. 56,
no. 4, pp. 60–65, Apr. 2018, doi: 10.1109/MCOM.2018.1700790 .
4.L. Huang, S. Bi, and Y.-J. A. Zhang, “Deep Reinforcement Learning for Online
Computation Offloading in Wireless Powered Mobile-Edge Computing Networks,”IEEE Trans. Mob. Comput ., vol. 19, no. 11, pp. 2581–2593, Nov. 2020, doi: 10.1109/
TMC.2019.2928811 .
5.Z. Lv, D. Chen, R. Lou, and Q. Wang, “Intelligent Edge Computing Based on Machine
Learning for Smart City,” Futur. Gener. Comput. Syst ., vol. 115, pp. 90–99, 2021, doi:
https://doi.org/10.1016/j.future.2020.08.037 .
6.F. Schuster, B. Engelmann, U. Sponholz, and J. Schmitt, “Human Acceptance
Evaluation of AR-Assisted Assembly Scenarios,” J. Manuf. Syst ., 2021, doi: https://doi.
org/10.1016/j.jmsy.2020.12.012 .
7.R. Chhabra, S. Verma, and C. R. Krishna, “A survey on driver behavior detection tech -
niques for intelligent transportation systems,” in 2017 7th International Conference on
Cloud Computing, Data Science Engineering - Confluence , Jan. 2017, pp. 36–41, doi:
10.1109/CONFLUENCE.2017.7943120 .167 Deep Learning Applications on Edge Computing
 8. M. P. Véstias, R. P. Duarte, J. T. de Sousa, and H. C. Neto, “Moving Deep Learning to 
the Edge,” Algorithms , vol. 13, no. 5, 2020, doi: 10.3390/a13050125 .
 9. R. Dong, C. She, W. Hardjawana, Y. Li, and B. Vucetic, “Deep Learning for Hybrid 
5G Services in Mobile Edge Computing Systems: Learn From a Digital Twin,” IEEE 
Trans. Wirel. Commun ., vol. 18, no. 10, pp. 4692–4707, Oct. 2019, doi: 10.1109/TWC.  
2019.2927312 .
 10. M. McClellan, C. Cervelló-Pastor, and S. Sallent, “Deep Learning at the Mobile 
Edge: Opportunities for 5G Networks,” Appl. Sci ., vol. 10, no. 14, 2020, doi: 10.3390/
app10144735 .
 11. L. Huang, X. Feng, A. Feng, Y. Huang, and L. P. Qian, “Distributed Deep Learning-
based Offloading for Mobile Edge Computing Networks,” Mob. Networks Appl ., 2018, 
doi: 10.1007/s11036-018-1177-x .
 12. E. Li, L. Zeng, Z. Zhou, and X. Chen, “Edge AI: On-Demand Accelerating Deep 
Neural Network Inference via Edge Computing,” IEEE Trans. Wirel. Commun .,  
vol. 19, no. 1, pp. 447–457, Jan. 2020, doi: 10.1109/TWC.2019.2946140 .
 13. Y. He, F. R. Yu, N. Zhao, V. C. Leung, and H. Yin, “Software-Defined Networks with 
Mobile Edge Computing and Caching for Smart Cities: A Big Data Deep Reinforcement 
Learning Approach,”  IEEE Commun. Magazine , vol. 55 (12), pp. 31–37, 2017, doi: 
10.1109/MCOM.2017.1700246.
 14. M. C. Trivedi and N. K. Trivedi, “Audio Masking for Watermark Embedding Under 
Time Domain Audio Signals,” in 2014 International Conference on Computational 
Intelligence and Communication Networks , 2014, doi: 10.1109/CICN.2014.166.
 15. “A Privacy-Preserving Deep Learning Approach for Face Recognition with Edge 
Computing,” Jul. 2018, [Online]. Available: https://www.usenix.org/conference/hotedge18/  
presentation/mao .
 16. E. Kristiani, C. Yang, and C. Huang, “iSEC: An Optimized Deep Learning Model 
for Image Classification on Edge Computing,” IEEE Access , vol. 8, pp. 27267–27276, 
2020, doi: 10.1109/ACCESS.2020.2971566.
 17. B. Yang, X. Cao, C. Yuen, & L. Qian, “Offloading Optimization in Edge Computing for 
Deep-Learning-Enabled Target Tracking by Internet of UAVs,” IEEE Internet Things 
J., vol. 8 (12), pp. 9878–9893, 2020, doi: 10.1109/JIOT.2020.3016694.
 18. P. Subramaniam and M. J. Kaur, “Review of Security in Mobile Edge Computing with 
Deep Learning,” in 2019 Advances in Science and Engineering Technology Inter-
national Conferences (ASET) , Mar. 2019, pp. 1–5, doi: 10.1109/ICASET.2019.8714349.
 19. M. Chen, W. Li, Y. Hao, Y. Qian, and I. Humar, “Edge Cognitive Computing Based 
Smart Healthcare System,” Futur. Gener. Comput. Syst ., vol. 86, pp. 403–411, 2018, 
doi: 10.1016/j.future.2018.03.054.
 20. M. Bensalem, J. Dizdarev ć, and A. Jukan, “Modeling of Deep Neural Network (DNN) 
Placement and Inference in Edge Computing,” in 2020 IEEE International Conference 
on Communications Workshops (ICC Workshops) , Jun. 2020, pp. 1–6, doi: 10.1109/
ICCWorkshops49005.2020.9145449.
 21. A. Ndikumana, N. H. Tran, D. H. Kim, K. T. Kim, and C. S. Hong, “Deep Learning 
Based Caching for Self-Driving Cars in Multi-Access Edge Computing,” IEEE Trans. 
Intell. Transp. Syst ., pp. 1–16, 2020, doi: 10.1109/TITS.2020.2976572.
 22. A. Marchisio et al ., “Deep Learning for Edge Computing: Current Trends, Cross-
Layer Optimizations, and Open Research Challenges,” in 2019 IEEE Computer 
Society Annual Symposium on VLSI (ISVLSI) , Jul. 2019, pp. 553–559, doi: 10.1109/
ISVLSI.2019.00105.
 23. S. R. Sahoo and B. B. Gupta, “Multiple Features Based Approach for Automatic Fake 
News Detection on Social Networks Using Deep Learning,” Appl. Soft Comput ., vol. 100, 
p. 106983, 2021, doi: https://doi.org/10.1016/j.asoc.2020.106983 .168 Machine Learning for Edge Computing
 24. H. Khelifi et al ., “Bringing Deep Learning at the Edge of Information-Centric Internet 
of Things,” IEEE Commun. Lett. , vol. 23, no. 1, pp. 52–55, Jan. 2019, doi: 10.1109/
LCOMM.2018.2875978.
 25. I. Sharma, R. Tiwari, and A. Anand, “Open Source Big Data Analytics Technique,” in 
Proceedings of the International Conference on Data Engineering and Communication Technology , 2017, pp. 593–602.
 26. R. Pandey, A. Singh, A. Kashyap, and A. Anand, “Comparative Study on Realtime Data Processing System,” in 2019 4th International Conference on Internet of Things: Smart Innovation and Usages (IoT-SIU) , Apr. 2019, pp. 1–7, doi: 10.1109/
IoT-SIU.2019.8777499.
 27. A. Luckow et al ., “Artificial Intelligence and Deep Learning Applications for 
Automotive Manufacturing,” in 2018 IEEE International Conference on Big Data (Big Data) , Dec. 2018, pp. 3144–3152, doi: 10.1109/BigData.2018.8622357.
 28. S. Ghose and J. J. Prevost, “Enabling an IoT System of Systems through Auto Sound Synthesis in Silent Video with DNN,” in 2020 IEEE 15th International Conference 
of System of Systems Engineering (SoSE) , Jun. 2020, pp. 563–568, doi: 10.1109/
SoSE50414.2020.9130483.
 29. Y. Liu, H. Qu, W. Chen, and S. H. Mahmud, “An Efficient Deep Learning Model to Infer User Demographic Information from Ratings,”  IEEE Access , vol. 7, pp. 53125–
53135, 2019, doi: 10.1109/ACCESS.2019.2911720.
 30. T. M. Ghanim, M. I. Khalil, and H. M. Abba, “Comparative Study on Deep Convolution Neural Networks DCNN-based Offline Arabic Handwriting Recognition,”  IEEE 
Access , vol. 8, pp. 95465–95482, 2020, doi: 10.1109/ACCESS.2020.2994290.
 31. T. Falas and H. Kashani, “Two-Dimensional Bar-Code Decoding with Camera-Equipped Mobile Phones,” in Fifth Annual IEEE International Conference on Pervasive Computing and Communications Workshops (PerComW’07) , Mar. 2007, 
pp. 597–600, doi: 10.1109/PERCOMW.2007.119.
 32. W. Cho, S.-W. Lee, and J. H. Kim, “Modeling and Recognition of Cursive Words with Hidden Markov Models,” Pattern Recognit ., vol. 28, no. 12, pp. 1941–1953, 1995, doi: 
https://doi.org/10.1016/0031-3203(95)00041-0.
 33. A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese, “Social LSTM: Human Trajectory Prediction in Crowded Spaces,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2016.
 34. C. Gao, A. Rios-Navarro, X. Chen, T. Delbruck, and S. Liu, “EdgeDRNN: Enabling Low-latency Recurrent Neural Network Edge Inference,” in 2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) , Aug. 2020,  
pp. 41–45, doi: 10.1109/AICAS48895.2020.9074001.
 35. I. Goodfellow, Y. Bengio, and A. Courville, “Deep Learning,” MIT Press, 2016.
 36. https://www.adobe.com/in/creativecloud/design/discover/colorize-black-and-white-photos.html
 37. https://www.infoq.com/news/2020/09/ai-created-foley/
 38. https://www.tensorflow.org/tutorials/generative/deepdream169 DOI: 10.1201/9781003143468-11
Designing an Efficient 
Network-Based Intrusion Detection System Using an Artificial Bee Colony and ADASYN Oversampling Approach
Manisha Rani 
Research Scholar, Department of Computer Science, 
Punjabi University   Punjab, India
Gunreet Kaur 
Department of Computer Science and Engineering,  Thapar Institute of Engineering and Technology Punjab, India
Gagandeep
Department of Computer Science, Punjabi University  Punjab, India11
CONTENTS
11.1 Introduction  .................................................................................................. 170
11.2 Related Work  ................................................................................................ 171
11.3 Dataset Used  ................................................................................................. 173
11.3.1  NSL KDD Dataset  ............................................................................ 173
11.4 Intrusion Detection Process  .......................................................................... 174
11.4.1  Data Pre-Processing  ......................................................................... 174
11.4.1.1  Data Conversion  ................................................................. 174
11.4.1.2  Data Normalization  ............................................................ 175
11.4.2  Data Sampling  .................................................................................. 175
11.4.3  Feature Selection  .............................................................................. 177
11.4.3.1  Artificial Bee Colony  ......................................................... 177170 Machine Learning for Edge Computing
11.4.4  Classification  ..................................................................................... 179
11.5 Results and Discussion  ................................................................................. 180
11.5.1  Performance Parameters  ................................................................... 180
11.5.2  Impact of Class Imbalance  ............................................................... 181
11.5.3  Performance of Proposed Work  ........................................................ 181
11.6 Conclusion  .................................................................................................... 183
References  .............................................................................................................. 184
11.1 INTRODUCTION
In the digital world, security has become a critical issue in modern networks over the 
past two decades. It has led to a rapid increase of security threats along with the growth of computer networks. Despite strong security systems to protect against threats, numer -
ous intruders that can violate the security policies such as Confidentiality, Integrity, and Availability (CIA) of the network still exist. An intruder may attempt to obtain unauthorized access to information, manipulate the information, and make the system unreliable by deploying unnecessary services over the network. Therefore, to secure the network from intruder activities, strong security mechanisms need to be designed (Madhavi 2012). A number of security mechanisms are available, such as crypto -
graphic techniques, firewalls, packet filters, an Intrusion Detection System (IDS), and so on. Subsequently, malicious users use different techniques, such as password guess-ing, unnecessarily loading the network with irrelevant data, and unloading network traffic, to exploit system vulnerabilities. Therefore, it is highly unrealistic to protect the network completely from breaches. However, it is possible to detect the intrusions so that the damage can be repaired by taking an appropriate action (Kumar and Kukreja 2021). Thus, the IDS plays a significant role in the network security field by provid -
ing a solid line of defense against malicious users. It detects any suspicious activity performed by intruders by monitoring network traffic and issues alerts whenever any abnormal behavior is sensed (Rani and Gagandeep 2019). IDS is broadly classified into two types: NIDS and HIDS. NIDS stands for network-based IDS, which analyzes the network traffic by reading individual packets through the network layer and trans -
port layer, whereas HIDS stands for host-based IDS, which monitors every activity of individual hosts or devices on the network. IDS can detect known attacks through signature-based detection and unknown attacks through anomaly-based detection. Both approaches have their own limitations, such as the former detection approach is only good for finding known attacks but not good for unknown attacks because it can match the incoming patterns with the stored patterns only. The system must update the database with a new attack signature whenever a novel attack is identified. Whereas anomaly-based detection can detect both known and unknown attacks, it suffers from high false-alarm rates because of its non-linear nature, high dimensional features, and mixed type of features in the datasets (Kukreja and Dhiman 2020). To overcome these challenges, various machine learning algorithms and evolutionary algorithms exist that are mainly used for classification and feature reduction processes, respectively. Recently, various algorithms, such as random forest, naïve Bayes, k-nearest neighbor, etc., have been used as IDS classifiers. However, the presence of irrelevant features in the dataset weakens the performance of a classifier (Montazeri et al. 2013). Thus, to 171 Efficient Network Based Intrusion Detection System
improve the performance of a classifier, it is very important to reduce the dimensional -
ity of the feature space by identifying and selecting relevant features from the original 
feature set that are needed for classification. But selecting the relevant features from the full set is itself a challenging task.
Currently, bio-inspired algorithms, such as a genetic algorithm (Sampson 1976), 
particle swarm optimization (Poli, Kennedy, and Blackwell 2007), ant colony opti -
mization (Dorigo, Birattari, and Stutzle 2006), etc., are emerging techniques used for 
feature selection because of their high convergence power and searching behavior. 
They are inspired by the biological nature of animals, insects, and birds and work based upon the principle of their intelligent evolutionary behavior. These algorithms help solve complex problems with improved accuracy and are very useful in find -
ing an optimal solution to a given problem. Although the feature selection approach helps to reduce the false alarm rate of IDS due to the imbalanced nature of IDS data -
sets, it is unable to improve the performance of classifiers significantly. Generally, IDS datasets contain large amounts of normal data (i.e., majority class instances) when compared to attack data (i.e., minority class instances), which results in a class imbalance problem. It can be addressed either at the data level or classifier level by changing the class distribution of training set itself or by altering the training algo -
rithm rather than training data, respectively. At the data level, various under-sampling techniques, such as one-sided selection and oversampling techniques like Synthetic Minority Oversampling Technique (SMOTE), the clustering approach, etc., are avail -
able to balance the instances by inserting some random instances to minority samples or by removing some instances from majority instances, respectively. At the classifier or algorithmic level, two approaches are employed, i.e., either the threshold approach that can be used by adjusting the decision threshold of a classifier or the cost sensitive approach that can be used by modifying the learning rate parameter of algorithm. In this chapter, data is initially preprocessed using the min-max normalization technique. Then, data is balanced using the ADASYN technique (He et al. 2008). Then, a subset of features is chosen from the original set using the ABC algorithm. The optimality of the feature subset is evaluated using the random forest classifier. The performance of IDS is evaluated using the NSL KDD dataset. After an empirical analysis of the clas -
sification accuracy of various classifiers, random forest was chosen.
The rest of this chapter is organized as follows: Section 11.2 deals with surveys related 
to different feature selection and class imbalance approaches. Section 11.3 describes the 
dataset used in the experimental analysis. Section 11.4 deals with the methodology fol -
lowed for proposed work. Then experimental results and their comparative analysis are reported in Section 11.5 . This chapter concludes with a summary in Section 11.6 .
11.2 RE LATED WORK
Numerous ML-based classifiers and evolutionary algorithms have been proposed for the classification and feature selection process for IDS. To identify attacks from normal traffic, researchers employ various steps, such as data pre-processing, feature selection and reduction techniques, and classification steps. First of all, existing work related to IDS needs to be reviewed to identify research gaps in previous work and then some new work is proposed for further research.172 Machine Learning for Edge Computing
Tavallaee et al. (2009) analyzed the publicly available KDDCup’99 dataset in 
order to solve the inherent problems of the dataset. A major problem occurs due to 
a large number of redundant records; and the level of difficulty in both the training 
and testing sets leads the ML algorithm to be biased toward majority class instances 
and there is a large gap between training and testing accuracy. To solve these prob -
lems, a new version of KDDCup’99—NSL KDD—is proposed. Although NSL KDD 
can be used as a benchmark dataset, it still suffers from a few problems, such as data 
imbalancing, etc. Pervez and Farid (2014) proposed an SVM-based feature selection 
algorithm and classifier to select a subset of features from the original set. It achieved 
82.38% of testing accuracy using 36 features and compared its results with existing 
work. Ingre and Yadav (2015) evaluated the performance of NSL KDD using a back 
propagation algorithm of the Artificial Neural Network (ANN) architecture. Before 
training, data was pre-processed by converting each feature into numerals and nor -
malizing the data into a range [0, 1] using the Z-score approach. Then, 41 features of 
KDDTrain + and KDDTest + were reduced to 23 using the information gain technique 
and then the reduced features were learned through the neural network. Aghdam, 
Kabiri, and others (2016) proposed the IDS model by selecting feature subset using 
the Ant Colony Optimization (ACO) approach followed by the nearest neighbor clas -
sifier to identify attacks from normal traffic. Although it achieved better results than 
existing work, the ACO algorithm suffers from computational memory requirements 
and low speed due to the separate memory required by each ant. Kaur, Pal, and Singh 
(2018) compared the performance of the hybridization of K-Means with the Firefly 
algorithm and Bat algorithm with other clustering techniques and it showed that the 
proposed work outperformed other techniques with huge margins. However, data pre-
processing involving data normalization might also improve the performance of the 
proposed work. Hajisalem and Babaie (2018) proposed another hybrid model using 
ABC and Artificial Fish Swarm (AFS) evolutionary algorithms over NSL KDD and 
UNSW-NB15 datasets. Mazini, Shirazi, and Mahdavi (2019) designed a hybrid model 
for anomaly-based NIDS using the ABC algorithm and Adaboost algorithm to select 
features and to classify attacks and normal data using the NSL KDD and ISCXIDS2012 
datasets, respectively. Although evolutionary algorithms outperformed various exist -
ing works, they could not address the imbalance problem in research work.
Recently, Jiang et al. (2020) designed a hierarchical model based on the deep 
learning approach by addressing the class imbalance problem using the feature 
selection process to classify attack data. It combined the one-sided selection (OSS) 
and SMOTE techniques for handling undersampling and oversampling, respectively. 
Then, it used Convolutional Neural Network (CNN) to choose spatial features and 
used Bi-directional long short-term memory (Bi-LSTM) to extract temporal fea -
tures from the NSL KDD and UNSW-NB15 dataset. It identified attack data from 
normal by achieving 83.58% and 77.16% testing accuracy using the NSL KDD 
and UNSW-NB15 datasets, respectively. Alkafagi and Almuttairi (2021) designed 
Proactive Model for Swarm Optimization (PMSO) for selecting individual optimal 
feature subsets using Particle Swarm Optimization (PSO) and the Bat algorithm fol -
lowed by a decision tree classifier. Both studies were successful in improving the 
performance of the proposed work as compared to existing work but did not address 
the common issue, i.e., the class imbalance problem. Tao et al. (2021) achieved 173 Efficient Network Based Intrusion Detection System
classification accuracy up to 78.47% by combining a K-means clustering approach 
with the SMOTE algorithm to equalize the minority data instances with major -
ity instances in NSL KDD dataset, and then using an enhanced RF algorithm for the classification process. Although it addressed the class imbalance, it could not achieve effective results for minority attack data such as R2L and U2R attack types. Priyadarsini (2021) solved the class imbalance problem at the data level using the borderline SMOTE algorithm through the RF technique, and then the selected fea -
ture subset using the ABC algorithm. Various classifiers, such as SVM, DT, and K Nearest Neighbor (KNN), were applied for the classification of attack data from nor -
mal data using the KDDCup’99 dataset only. Liu and Shi (2022) designed a hybrid model for anomaly-based NIDS using the GA algorithm for feature selection and the RF algorithm for classification. It achieved a high classification accuracy rate using the NSL KDD and UNSW-NB15 training sets but did not test or validate the data results using the testing set of both datasets.
From existing work, it is found that evolutionary algorithms are being extensively 
used for the feature selection process, followed by ML classifiers. It is also predicted that lesser research has been done to address the class imbalance problem. In this chapter, a data sampling technique, such as ADASYN, has been used for balancing the NSL KDD dataset. Then, the ABC algorithm has been applied for finding the optimal subset of features, followed by the RF classifier to build an effective model for IDS.
11.3 DATASET USED
To empirically analyze the results of the IDS model, widely used dataset, i.e., NSL KDD, has been considered. The experiments are performed using 100% of the data -
set. The data is trained over 70% of training set and remaining 30% of the training set is used for validating the results. Then, the results of dataset are tested using the testing set.
11.3.1 nsl kdd d AtAset
Due to inherent problems in the KDDCup’99 dataset, the performance of various 
researchers work was affected. To overcome the issues, an improved version was made and renamed the NSL KDD dataset (Tavallaee et al. 2009). It is the bench -
mark dataset used by every researcher to compare the performance of IDS models. It contains 42 attributes, of which 41 attributes belong to network flow type, whereas the last 42nd attribute indicates the label assigned to each attribute. All attributes except the last one are categorized into three types: content related, time related, and host-based content. On the other hand, the last attribute is categorized into five classes, where four classes belong to the attack class and one class belongs to the normal data. The four attack classes are Distributed Denial of Service (DDoS), Probe, User to Root (U2R), and Remote-2-Local (R2L). It is publicly available over the internet and consists of the training set and testing set KDDTrain + and 
KDDTest +, respectively. A number of instances present in each set are described in  
Table 11.1 :174 Machine Learning for Edge Computing
11.4 INTRUSION DETECTION PROCESS
The IDS follows some basic steps to detect intrusions from network traffic 
data. To monitor network traffic, there must be some genuine data containing 
information about network packets. The first step involves the collection of 
data from available sources over which the entire detection process has to be 
done. In this chapter, a benchmark set (i.e., NSL KDD) has been used for the 
empirical analysis of the data. These datasets contain the metadata of network-
related data and different attacks that can be generated by the intruders. The 
second step involves data pre-processing, which consists of two sub-steps: data 
conversion and data normalization. Additionally, to address the class imbal -
ance issue, the ADASYN oversampling method is employed to balance the 
dataset. Then, in order to reduce the dimensions of data, redundant and noisy 
data is removed from entire dataset and only the relevant features are selected 
through the feature reduction and feature selection process. Then, the reduced 
feature set of data is fed into the classifier to discriminate between normal and 
attack data.
11.4.1 d AtA pre-process Ing
Because of a vast amount of network packets, the unequal distribution of data in 
datasets, and the instability of data toward the changing behavior of the network, it 
is very difficult to classify attack data from normal data with high accuracy rates. 
Therefore, there is a need to pre-process data before putting it over the detection 
model. Before classification, the data is pre-processed through data conversion and 
data normalization.
11.4.1.1  Data Conversion
The NSL KDD contains heterogeneous data, such as numerical and non-numer -
ical types of data. Most of the IDS classifiers accept numerical data types only. 
So it is necessary to convert all the data into a homogenous form; i.e., a numeric 
type. First, the data conversion is involved in the pre-processing step, which con -
verts all non-numerical features into integers. For instance, the protocol feature 
in NSL KDD contains a string type of values, such as TCP, ICMP, and UDP, 
which are converted into numbers by assigning values, 1, 2, and 3, to them, 
respectively.TABLE 11.1
Record Distribution of NSL KDD Dataset
Dataset Total InstancesRecord Distribution
Normal Attack
KDDTrain + 125973 67343 58630
KDDTest +   22544   9711 12833175 Efficient Network Based Intrusion Detection System
11.4.1.2  Data Normalization
After converting all features into integer types, they can be either discrete or con -
tinuous in nature, which are incomparable and may hinder the performance of 
the model. To improve the detection performance significantly, the normalization 
technique is applied in order to rescale the dataset values into a range of interval 
[0, 1]. There are various normalization techniques, such as z-score, Pareto scaling, 
sigmoidal, min-max, etc., available. Based on the empirical analysis, the min-max 
normalization approach has been used in this chapter. Every attribute value is res -
caled into [0, 1] by transforming the maximum value of that attribute into 1, and 
the minimum value of that attribute into 0, while the remaining attribute values are 
transformed into decimal values lying between 0 and 1. For every feature xi, it is 
calculated mathematically as follows:
 ()
() ()=−
−Xxm inx
maxx minxiii
ii/uni00A0/uni00A0 (11.1)
where xi denotes ith feature of dataset x,minx i() and maxx i()denote the minimum 
and maximum values of the ith feature of dataset x, respectively. Xi represents the 
corresponding normalized or rescaled value of the ith feature value.
11.4.2 d AtA sAmplIng
Generally, large datasets suffer from an unequal distribution of records known as 
a class imbalance problem. The training set of the IDS datasets contains more nor -
mal data, known as majority instances, than attack data, known as minority class 
instances. For example, the large gap between normal records and attack records is 
8713 in KDDTrain + of NSL KDD, making the results biased toward normal data 
instances. It is difficult to detect U2R and R2L attacks in NSL KDD because it 
contains only 52 U2R and 995 R2L instances, which are fewer than DoS and Probe 
class instances. Therefore, it fails to detect minority attack types in the presence 
of vast amounts of normal data effectively. In order to give importance to minor -
ity class instances, we have used the ADASYN oversampling technique in which 
the minority instances are oversampled by generating minority data instances based 
on the density distribution of samples so that it can be equally represented as the 
majority instances in the dataset (Liu, Gao, and Hu 2021). It uses the KNN approach 
to add pseudo instances by finding the Euclidean distance between neighbors of 
minority samples. The pseudo instances are added by joining it with the KNN of 
each minority sample of the feature vector. The number of neighbors used to intro -
duce pseudo instances in minority instances is five in this chapter. The normalized 
KDDTrain + dataset represented by TRd is given as input to the ADASYN algorithm. 
It consists of total N number of samples, out of which mi denotes the number of 
minority samples and ma denotes the number of majority samples in the dataset,  
where mi < ma.176 Machine Learning for Edge Computing
This algorithm is mainly based on the density distribution criterion of a nor -
malized ratio such that 1 NRiiΣ= . This ratio automatically decides the number of 
synthetic samples that are needed to be added for each minority sample. The bal -
anced dataset is generated based upon the β coefficient, which defines the desired 
balance level of the whole dataset after adding synthetic samples in the dataset. The 
major advantage of using this technique is its capability to generate more synthetic 
samples for those examples that are difficult to learn. That’s why, ADASYN is con -
sidered a more efficient approach than SMOTE (Chawla et al. 2002), SMOTEBoost 
(Chawla et al. 2003), etc. Therefore, a balanced KDDTrain + set is generated using 
ADASYN to equally represent the minority and majority class instances.ALGORITHM 1: ADASYN FOR CLASS IMBALANCE PROBLEM
Input: TRd: Number of features in set denotes population size
N: Total number of samples
Mi: Number of minority sample
Ma: Number of majority sample
DegTh: Predefined threshold for maximum tolerated degree of class 
imbalance
Output: Balanced dataset
1 Begin
2 degM
Mi
a=  where deg ε (0,1]  (11.2)
3 Check if deg < DegTh
4 SM M ai () ββ =− ∗ where    0,1 ββ[]ε  (11.3)
5 RK i M iii =∆∀∈  (11.4)
6 for (each i)
7 NRR
Rii
iM
ia
1Σ=
= (11.5)
8 i++
9 end for
10 for (each i)
11 SN RS ii=∗  (11.6)
12 i++
13 end for
14 while (Si)
15 sx xx  rand ii ki i 0,1() () =+ −∗
16 end while
17 End177 Efficient Network Based Intrusion Detection System
11.4.3 f eAture select Ion
Apart from data pre-processing, it is very challenging to monitor large amounts of 
network traffic that has ambiguous and redundant data. It is very crucial to reduce 
the dimensions of the entire data in order to secure the network from intrusions 
in real time. The dimensionality reduction of large datasets further improves the 
performance of the IDS classifiers of system. It can be achieved by selecting infor -
mative or relevant features while rolling out redundant and irrelevant features from 
the original set through the feature selection process. On the basis of the evalua -
tion criteria, the feature selection is of two kinds: filter-based and wrapper-based 
feature selection (FS). Filter-based FS assigns weights to features and filters out the 
irrelvant features based on the order of the weight. Although it saves time finding 
the method, there is no role for classification algorithm. On the contrary, wrapper- 
based FS takes into account the effect the classification algorithm has in finding 
the feature subset, which results in high classification accuracy when compared to 
filter-based FS (Li et al. 2021). That’s why, we prefer the wrapper-based FS approach 
in this chapter. The basic procedure of wrapper-based feature selection involves 
selecting the subset of features from the original set and then the generated set is 
evaluated for its optimality. If the generated set is evaluated to be optimal, it is 
chosen as the best subset; otherwise, another subset is evaluated. In recent years, 
swarm intelligence emerged out as an effective approach for feature selection as 
compared to other approaches. It is inspired by the collective behavior of swarms, 
such as honey bees, birds, flocks, insects, etc., which interact with each other through 
specific behavioral patterns. The swarm intelligence-based algorithms are capable 
of solving non-linear complex problems within less computational time, with low 
computational complexity (Blum and Li 2008). After studying the pros and cons of 
various evolutionary techniques, we elected to use the ABC for feature selection in 
this chapter. ABC has been used to find the optimal subset of features from the NSL  
KDD dataset.
11.4.3.1 Artificial Bee Colony
ABC is meta-heuristic evolutionary algorithm based on swarm intelligence intro -
duced by Karaboga (2005). It is biologically inspired from the collective behavior 
of honeybees. It mainly consists of three kinds of bees, employed bees, onlooker 
bees, and scout bees, that accomplish the task through global convergence. The 
employed bees search for the food source in the hive on the basis of few proper -
ties of food, like the concentration of energy (i.e., nectar amount), closeness 
to the hive, etc. They share their information about the searched food source 
with the onlooker bees in the dancing area of the hive. Then the onlooker bees  
further search for the most profitable food source on the basis of the information 
acquired from the employed bees in the hive. The location of the most profitable 
food source is memorized by the bees and they start exploiting that location. On 
the other hand, the employed bees turn into scout bees when the food source is 
abandoned. Then, the scout bees start searching for a new food source in a random  
manner.178 Machine Learning for Edge Computing
In this algorithm, the solution is initiated by a number of employed bees present 
in the hive. The solution is represented by the number of features contained in the 
dataset. Initially, the solution in the population denoted by S is defined as Si = {1, 2, 
3,…, N}, where N denotes the number of attributes and Si denotes the ith food source. 
For NSL KDD, the value of N is 41. The values or records contained in the attribute are taken from a balanced set of the dataset. In this chapter, the ABC algorithm has been divided into three phases, as follows:
•
 Initialization Phase:  In this phase, the food source S of the population is 
initialized using following eqn. (11.7):
  0 ,1 ,Sl ul rand ij jj j () () =+ −∗  (11.7)
where lj and uj indicate the lower and upper bound of solution ,Sij, respec -
tively, and 0,1 rand()  is the random function used to generate the number 
in the range (0, 1).
• Employed Phase:  After initializing the food source, the employed bees 
start searching for another food source in their neighborhood and update the position of food source using eqn. (11.8):
  Ø ,, ,, , NS SS S ij ij ij kj ij () =+ ∗−  (11.8)
where ,NS ij denotes the possible candidate food source, ,Skj represents 
the new food source searched by employed bees in the neighborhood of the prior food source 
,Sij, and Ø,ij is the random number that lies in the 
interval range [-1, + 1].
• Onlooker Phase:  After exploring the neighborhood, the employed bees 
share their information with onlooker bees in the dancing area. Then onlooker bees exploit that food source based on the probability function using  eqn. (11.9):
 Probfitness
fitnessfunc ii
i=∑()  (11.9)
where fitness i denotes the fitness value of ith food source. This fitness 
value is evaluated by employed bees using eqn. (11.10). Then, the neighbor -
hood is again explored by employed bees using eqn. (11.5). If the new food source is better than the previous food source, it is replaced by new one; otherwise employed bees find another food source for the hive.
  1
1,  0
1| |, 0  fitnessOO
OOiii
ii=+≥
+<



 (11.10)179 Efficient Network Based Intrusion Detection System
where Oi represents the objective function of ith food source. When 
the food source gets abandoned, the employed bee becomes a scout bee 
and randomly looks for a new food source using eqn. (11.7). In this way, this algorithm is iterated again and again until each and every feature gets explored and the optimal feature subset is generated by the ABC algorithm.
11.4.4 c lAssIfIcAtIon
To classify normal data from attack data and to evaluate the chosen optimal feature 
subset using the ABC algorithm in the previous steps, the random forest classifier is used in this chapter. It mainly solves the problems based on classification and regression by building multiple decision trees to reduce the noise effect so that more accurate results can be achieved. The multiple decision trees are created randomly using the feature subset generated by the ABC algorithm in the previous steps. Then, it predicts the data as either normal or attack data based upon majority of aggregated votes made from each sub-tree. For binary classification, it usually gives output in 0 and 1 form in such a way that if an attack gets detected, it gives value 1; other -
wise, it gives output value 0. The proposed flowchart for the IDS model is shown in Figure 11.1 .
FIGURE 11.1  Flowchart of proposed IDS model.180 Machine Learning for Edge Computing
11.5 RESULTS AND DISCUSSION
The proposed work has been implemented using python language using tensorflow 
and sklearn, with an Intel Core i5Processor and 8 GB of RAM. For experimental analysis, NSL KDD a widely used dataset has been considered in this chapter. The entire training sets of both datasets were used to train the data, out of which 70% was used for training the algorithm, whereas remaining 30% of the set was used for validating the proposed results. The classification was done using a 10-fold cross validation approach. The features were selected using the ABC algorithm, whose initial values of parameters are defined in Table 11.2 .
11.5.1 p erform Ance pArAmeters
The performance of the proposed work was evaluated based on five parameters, such 
as classification accuracy, precision, recall, F-score, and receiver operating charac -
teristic (ROC). All these parameters were calculated using the confusion matrix of the model. The confusion matrix was built using the following four indicators:
• True Positive (TP) defines the anomaly records that are correctly recorded as anomaly
• True Negative (TN) defines the normal records that are correctly recorded as  
normal
• False Positive (FP) defines the normal records that are incorrectly recorded as anomaly
• False Negative (FN) defines the anomaly records that are incorrectly recorded as normal
The performance parameters were calculated using the confusion matrix using the above four indicators as follows:
• Classification accuracy is calculated as the ratio of the number of cor -
rectly recorded instances to the total number of instances.
 AccuracyTP TN
TP TN FP FN=+
++ + (11.11)TABLE 11.2
Parameter Initialization of ABC Algorithm
Parameters Values
Maximum no. of iterations 100
Population size 10
Population dimension size N(Feature count)For NSL KDD, N = 42181 Efficient Network Based Intrusion Detection System
• Precision  is defined as the ratio of the correctly recorded attacks to the total 
number of instances recorded as attacks.
 PrecisionTP
TP FP=+ (11.12)
• Recall  is the ratio of the total number of instances that are correctly 
recorded to the total number of anomaly records in the dataset.
 RecallTP
TP FN=+ (11.13)
• F‑score  is the harmonic mean of recall and precision.
 2Fs coreprecision recall
precision recall−=∗∗
+ (11.14)
• ROC  is the receiver operating characteristic curve that records the perfor -
mance of the binary classification model using the true positive rate (i.e., 
recall) and false positive rate.
11.5.2 ImpAct of clAss ImbAlAnce
Because of the unequal distribution of class instances in NSL KDD, they suffer 
from a class imbalance problem. This problem has adverse effects on the perfor -
mance of the IDS model. Therefore, it is very important to solve this problem using 
some suitable approaches. It can be solved at either the data level or the classifier 
level. In this chapter, it is solved at data level using the ADASYN technique. It 
balances the dataset by oversampling the minority class instances in such a way 
that both the majority and minority class instances are equally represented in the 
dataset. The normal and attack data is classified using the random forest classifier 
without any feature selection process in this section. The classification accuracy 
without using any imbalance technique is found to be 77.71%, whereas when using 
the ADASYN technique, it is evaluated to be 78.76%. From the results, it is evident 
that the accuracy has been improved by 1.05% for NSL KDD, which is of great  
importance.
11.5.3 p erform Ance of proposed  Work
In order to find the optimal feature subset, we have used the ABC algorithm for 
feature selection because of its ability for exploration and exploitation. Further, to 
address the data imbalance problem, oversampling is done using the ADASYN 
technique. The impact of class imbalance problem at data level is discussed in the 
previous section. The experimental results are drawn after training the data over 
70% of the training set, and then cross-validating it using 10-fold of remaining 30% 182 Machine Learning for Edge Computing
validation set after going through 100 epochs. The confusion matrix using the orig -
inal set without any modification and after applying ADASYN and ABC feature 
selection for NSL KDD dataset are given in Table 11.3 .
It selected 35 attributes out of 42 attributes using the NSL KDD dataset, which 
shows the dimensionality reduction of the full dataset. With this, it is able to mini -
mize the storage requirements of the program effectively. The list of features selected using the ABC algorithm over NSL KDD is shown in Table 11.4 .
The proposed work is evaluated in terms of various parameters, such as classifi -
cation accuracy, precision, recall, F-score, and ROC curve. The accuracy is tested over the KDDTest + set of NSL KDD and it is evaluated to be 81.98%, which shows 
a huge improvement against the original accuracy. Similarly, other parameters have shown tremendous results compared to unbalanced full datasets. Therefore, the data imbalance problem plays a crucial role, along with the feature selection process, in TABLE 11.3
The Confusion Matrix Using NSL KDD 
(a) Using the Original Set
Normal Anomaly
Normal 9424 287
Anomaly 3775 9058
(b) Using the ADASYN and ABC Algorithm
Normal Anomaly
Normal 9434 277
Anomaly 4749 8084
TABLE 11.4List of Selected Features Using NSL KDD
Dataset Number of Features Selected List of Selected Features
NSL KDD 35 {dur, service, flag, src_bytes, dst_bytes, land, 
wrong_fragment, urgent, hot, logged_in, num_compromised, root_shell, su_attempted, num_root, num_file_creations, num_shells, num_access_files, num_outbound_cmds, is_hot_login, is_guest_login, count,srv_serror_rate, rerror_rate, srv_rerror_rate, same_srv_rate, diff_srv_rate, srv_diff_host_rate, dst_host_count, dst_host_same_srv_rate, dst_host_diff_srv_rate, dst_host_srv_diff_host_rate, dst_host_serror_rate, dst_host_srv_serror_rate, dst_host_rerror_rate, dst_host_srv_rerror_rate}183 Efficient Network Based Intrusion Detection System
classifying normal and attack data in the IDS model. The ROC curve values are also 
above 95% over the dataset, which shows the proposed work is superior. The empiri -
cal results for the NSL KDD dataset are shown in Table 11.5.
The performance of the proposed approach is also compared with existing litera-
ture in Table 11.6. From the comparison table, it is evident that the proposed work has shown great improvement compared to previous work.
11.6 CONCLUSION
In this chapter, an effective NIDS is designed using the swarm intelligence-based ABC optimization algorithm followed by a random forest classifier for binary clas-sification. Due to the large size of IDS datasets, they usually suffer from a class imbalance problem, which becomes a crucial barrier to the system performance. To address this issue, data is balanced using the ADASYN oversampling technique in order to give importance to minority instances in the dataset. The impact of class imbalance is effectively evaluated based upon an empirical analysis of classification accuracy in this chapter. In order to improve the performance of the system to a large extent, data is pre-processed using the min-max normalization technique followed by data balancing using the oversampling approach. Later on, the dimensions of large size datasets are reduced by extracting relevant feature set using the ABC optimization TABLE 11.5
Proposed Results for Binary Classification
Parameters (in %)NSL KDD
Original Dataset ADASYN+ABC
Accuracy 77.70 81.98
Precision 96.68 96.92
Recall 62.99 70.58
F-score 76.28 81.68
ROC 96.48 97.00
TABLE 11.6Comparison of Proposed Work with Existing Literature
References Classification Accuracy
Ibrahim, Basheer, and Mahmod (2013) 75.49%
Li et al. (2017) (using GoogLeNet) 77.04%
Li et al. (2017) (using ResNet 50) 79.14%
Al-Yaseen (2019) 78.89%
Tao et al. (2021) 78.47%
Rani and Gagandeep (2021) 80.83%
Proposed work 81.98%184 Machine Learning for Edge Computing
algorithm followed by the random forest classifier. The classification accuracy is cal -
culated to be 81.98% through empirical analysis using the NSL KDD dataset. The 
proposed work has successfully outperformed the existing literature. Future work  
will focus on reducing the computational time of the ABC algorithm as it takes long time to train the large datasets, which is very time-consuming.
REFERENCES
Aghdam, Mehdi Hosseinzadeh, Peyman Kabiri, and others. 2016. “Feature Selection for 
Intrusion Detection System Using Ant Colony Optimization.” International Journal of 
Network Security , 18 (3): 420–32.
Al-Yaseen, Wathiq Laftah. 2019. “Improving Intrusion Detection System by Developing 
Feature Selection Model Based on Firefly Algorithm and Support Vector Machine.” 
IAENG International Journal of Computer Science , 46 (4).
Alkafagi, Salam Saad, and Rafah MAlmuttairi. 2021. “A Proactive Model for Optimizing 
Swarm Search Algorithms for Intrusion Detection System.” Journal of Physics: Conference Series , 1818:12053.
Blum, Christian, and Xiaodong Li. 2008. “Swarm Intelligence in Optimization.” In Swarm 
Intelligence , 43–85. Natural Computing Series (NCS) Springer.
Chawla, Nitesh V, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegel Meyer. 2002. 
“SMOTE: Synthetic Minority Over-Sampling Technique.” Journal of Artificial Intelligence Research , 16: 321–57.
Chawla, Nitesh V, Aleksandar Lazarevic, Lawrence O Hall, and Kevin W Bowyer. 2003. 
“SMOTEBoost: Improving Prediction of the Minority Class in Boosting.” In European 
Conference on Principles of Data Mining and Knowledge Discovery , 107–19.
Dorigo, Marco, Mauro Birattari, and Thomas Stutzle. 2006. “Ant Colony Optimization.” 
IEEE Computational Intelligence Magazine , 1 (4): 28–39.
Hajisalem, Vajiheh, and Shahram Babaie. 2018. “A Hybrid Intrusion Detection System Based 
on ABC-AFS Algorithm for Misuse and Anomaly Detection.” Computer Networks , 
136: 37–50.
He, Haibo, Yang Bai, Edwardo A Garcia, and Shutao Li. 2008. “ADASYN: Adaptive 
Synthetic Sampling Approach for Imbalanced Learning.” In 2008 IEEE International 
Joint Conference on Neural Networks (IEEE World Congress on Computational 
Intelligence) , 1322–28.
Ibrahim, Laheeb M, Dujan T Basheer, and Mahmod S Mahmod. 2013. “A Comparison Study for 
Intrusion Database (Kdd99, Nsl-Kdd) Based on Self Organization Map (SOM) Artificial 
Neural Network.” Journal of Engineering Science and Technology , 8 (1): 107–19.
Ingre, Bhupendra, and Anamika Yadav. 2015. “Performance Analysis of NSL-KDD 
Dataset Using ANN.” In 2015 International Conference on Signal Processing and 
Communication Engineering Systems , 92–96.
Jiang, Kaiyuan, Wenya Wang, Aili Wang, and Haibin Wu. 2020. “Network Intrusion 
Detection Combined Hybrid Sampling with Deep Hierarchical Network.” IEEE 
Access , 8: 32464–76.
Karaboga, Dervis. 2005. “An Idea Based on Honey Bee Swarm for Numerical Optimization.”Kaur, Arvinder, Saibal K Pal, and AmritPal Singh. 2018. “Hybridization of K-Means and 
Firefly Algorithm for Intrusion Detection System.” International Journal of System 
Assurance Engineering and Management , 9 (4): 901–10.
Kukreja, Vinay, and Poonam Dhiman. 2020. “A Deep Neural Network Based Disease 
Detection Scheme for Citrus Fruits.” In 2020 International Conference on Smart 
Electronics and Communication (ICOSEC) , 97–101.185 Efficient Network Based Intrusion Detection System
Kumar, Deepak, and Vinay Kukreja. 2021. “N-CNN Based Transfer Learning Method for 
Classification of Powdery Mildew Wheat Disease.” In 2021 International Conference 
on Emerging Smart Computing and Informatics (ESCI) , 707–10.
Li, Xin, Peng Yi, Wei, Yiming Jiang, and Le Tian. 2021. “LNNLS-KH: A Feature Selection 
Method for Network Intrusion Detection.” Security and Communication Networks , 
2021. Hindawi.
Li, Zhipeng, Zheng Qin, Kai Huang, Xiao Yang, and Shuxiong Ye. 2017. “Intrusion Detection 
Using Convolutional Neural Networks for Representation Learning.” In International 
Conference on Neural Information Processing , 858–66.
Liu, Jingmei, Yuanbo Gao, and Fengjie Hu. 2021. “A Fast Network Intrusion Detection System 
Using Adaptive Synthetic Oversampling and LightGBM.” Computers & Security.   
102289.
Liu, Zhiqiang, and Yucheng Shi. 2022. “A Hybrid IDS Using GA-Based Feature Selection 
Method and Random Forest.” International Journal of Machine Learning and 
Computing , 12 (2).
Madhavi, M. 2012. “An Approach for Intrusion Detection System in Cloud Computing.” 
International Journal of Computer Science and Information Technologies , 3 (5): 
5219–22.
Mazini, Mehrnaz, Babak Shirazi, and Iraj Mahdavi. 2019. “Anomaly Network-Based 
Intrusion Detection System Using a Reliable Hybrid Artificial Bee Colony and 
AdaBoost Algorithms.” Journal of King Saud University-Computer and Information 
Sciences , 31 (4): 541–53.
Montazeri, Mohadeseh, Mitra Montazeri, Hamid Reza Naji, and Ahmad Faraahi. 2013. “A 
Novel Memetic Feature Selection Algorithm.” In the 5th Conference on Information 
and Knowledge Technology , 295–300.
Pervez, Muhammad Shakil, and Dewan Md Farid. 2014. “Feature Selection and Intrusion 
Classification in NSL-KDD Cup 99 Dataset Employing SVMs.” In the 8th International 
Conference on Software, Knowledge, Information Management and Applications 
(SKIMA 2014) , 1–6.
Poli, Riccardo, James Kennedy, and Tim Blackwell. 2007. “Particle Swarm Optimization.” 
Swarm Intelligence  (1): 33–57.
Priyadarsini, Pullagura Indira. 2021. “ABC-BSRF: Artificial Bee Colony and Borderline-
SMOTE RF Algorithm for Intrusion Detection System on Data Imbalanced Problem.” 
In Proceedings of International Conference on Computational Intelligence and Data 
Engineering: ICCIDE 2020 , 15–29.
Rani, Manisha, and Gagandeep. 2019. “A Review of Intrusion Detection System in Cloud 
Computing.” In Proceedings of International Conference on Sustainable Computing 
in Science, Technology and Management (SUSCOM) , Amity University Rajasthan , 
Jaipur-India .
Sampson, Jeffrey R. 1976. “Adaptation in Natural and Artificial Systems (John H. Holland).” Society 
for Industrial and Applied Mathematics  SIAM Review , Bradford Books, 18 (3): 529–30.
Tao, Wu, Fan Honghui, Zhu HongJin, You CongZhe, Zhou HongYan, and Huang XianZhen. 
2021. “Intrusion Detection System Combined Enhanced Random Forest with SMOTE 
Algorithm.” In 2013 International Conference on Cloud & Ubiquitous Computing & 
Emerging Technologies , Pune, India.
Tavallaee, Mahbod, Ebrahim Bagheri, Wei Lu, and Ali A Ghorbani. 2009. “A Detailed 
Analysis of the KDD CUP 99 Data Set.” In 2009 IEEE Symposium on Computational 
Intelligence for Security and Defense Applications , 1–6.187
Index
A
Access control, 9 , 16–18, 20
Ada boost, 57
Adaptive synthetic oversampling, 175 –176
Algorithms, 119 , 122 , 123
Alzheimer, 127 –129, 132 , 137
Alzheimer’s disease neuroimaging  
initiative (ADNI), 128 –140
Artificial bee colony, 177 –179
Artificial intelligence, 146
prediction, 47process improvements, 49
Auditable, 85Australian Imaging Biomarkers and Lifestyle 
Study of Ageing (AIBL), 129
Authentication, 6 , 8–11, 20
Auto-encoders, 162
B
Bagging, 57Behavior profiling, 17Beverages vending machine, 40Big data, 144Binarization, 60 , 63
Binary classification, 179Biomarker, 128 –129, 139 –140
Block chain, 151Blocksim, 77BLSTM, 62
C
Canny edge detector, 122CART, 29Cashless payment, 38Character
recognition, 55segmentation, 55 , 58
Class imbalance problem, 173 , 175
Classification, 60 , 72, 116 , 117, 128 –140
accuracy, 180
Cloud, 45, 155
big data storage, 103computing, 100computing architecture, 100servers, 46
Cloudlet, 2Clustering, 122 , 123 , 124CNN, 57 , 61
Computer vision, 116 , 117
Computing, 154Confidentiality, 10 , 15, 17, 21
Confusion matrix, 182Conjuncts, 65
Consensus, 83Consortium, 85Convolutional neural network (CNN), 57 , 61, 129 , 
133–140, 164
Coptic, 61Corda, 85Cotton leaf, 118Cryptographic, 89
D
Data
normalization, 174 –175
oversampling, 175 –176
pre-processing, 174 –175
set, 58 , 61, 116 , 118 , 128 , 132
Decentralized, 83Decision tree, 104Deep feed forward network, 106Deep learning, 65 , 117, 128 –129, 133 , 135 –140
Denial of service, 11 , 17, 18, 173
Detection, 119 –122
Devanagri, 56Diabetes, 30 , 97
mellitus, 97prevention, 106Type 1 , 97–98
Type 2 , 97–98
Diagnosis, 128 , 130 , 133 , 140
Differential privacy, 11 , 12
Diffusion tensor imaging (DTI), 131 , 133 , 136 , 
138–139
E
Edge
chain, 90computing, 144detector, 120 , 121
operator, 121
Encryption, 9, 10, 11, 12, 15, 16, 17, 18, 20, 21, 92
Ethereum, 85Extra functionality, 49188 Index
F
Feature
extraction, 61 , 128 , 131 –132, 139 –140
selection, 176 –179
Feedback system, 46
Flavor units, 46
Fog
aided ML, 98 –99
based healthcare model, 103computing, 1 –22, 100
computing architecture, 100nodes, 98nodes layer, 105
G
Global threshold, 119
H
Hashmap, 87Healthcare application, 105Healthy controls (HC), 130 –140
Hidden Markov models, 165Hippocampus, 127 , 133 –134
Homomorphic encryption, 12 , 16
Hyperledger, 85
I
Image processing, 120, 123
Immutable, 85Ingredients, 41–43
flavoured, 47natural, 45required, 46
Instance segmentation, 116Internet of things (IoT), 40 –41, 99, 153
devices, 42driven applications, 102enabled vending machine, 42healthcare, 44platforms, 43transmitter, 46 –47
Intrusion detection, 17
host based (HIDS), 170 –171
network based (NIDS), 170 –171
K
K-means, 122K-nearest neighbour (KNN), 28, 175  
L
Latency, 92 , 145Learning 
reinforcement, 27semi-supervised, 26supervised, 26unsupervised, 26
Ledger, 85Logistic, 86LSTM, 62
M
Machine learning (ML), 27 , 128 , 130 , 133 ,  
140, 146
Malware, 17 , 18, 20, 21
Man-in-the-middle, 13Masking algorithm, 16MECCO, 88Medical resonance image (MRI), 127 –140
Mild cognitive impairment (MCI), 127 –140
Miners, 87Mini mental state assessment (MMSE), 128 –129
Mining, 84
Min-max normalization, 175MLP, 55Modifiers, 65Monitoring system, 44Multicast, 8 , 9
Multilayer perceptron model, 108Multiparty computation, 84Multi-tenancy, 18, 21
N
Natural language, 157Neural networks, 146Neuroimaging modality, 128 , 129 , 131 –133, 
135–139
O
Object detection, 116 , 124
OCR, 53 , 54, 55 
Offloading, 88Open Access Series of Imaging Studies (OASIS), 
128–129, 137
Openness, 86
Optical character recognition, 164
P
PIMA Indian Dataset, 106Pixels, 119 , 122 , 123
Plant, 116 –118
Plant leaf, 117
, 118
Positron emission tomography (PET), 128 –140
Practical byzantine fault tolerance, 91Privacy preservation, 12 , 13189 Index
Proof of stake, 91
Proof of work (POW), 84
PSO, 118
Public key, 8, 11
Q
Quorum, 85
R
Random forest, 179
Receiver operating characteristics, 181
Recursive feature elimination, 130
Redundancy, 149
Regional growth segmentation, 119, 120
RFID enabled vending machine, 41
RGB, 117
Root to local, 173–174
S
Scalability, 145
Security, 3–6, 13–22, 148
Segmentation, 115–117
line, 55, 57
page, 56, 57, 58
semantic, 116
threshold, 119, 120
word, 55, 58
Sensitivity, 109
Service level agreement, 7
Service provider, 7
Short-term memory, 155Sieve units, 45 
Smart contract, 87
Sobel edge detector, 121
Social 
media, 157
networking, 41
Software defined networking, 4, 13
Stackelberg, 89
Stochastic, 89
Supply chain, 153
Support vector machine (SVM), 28, 130–133, 
140, 165
Swarm intelligence, 149
T
TensorFlow, 160
Three-tier fog based architecture, 111
Threshold, 119, 120
Timestamp, 85
V
Vending machine, 38
Verifiable computing, 15, 16
Virtual assistants, 159
Virtual machine, 152
Virtual private network, 21
Virtualization, 20
W
Wireless security, 18, 205G IoT and Edge
Computing for Smart
HealthcareThis page intentionally left blank5G IoT and Edge
Computing for Smart
Healthcare
Edited by
Akash Kumar Bhoi
KIET Group of Institutions, Delhi-NCR, Ghaziabad, India
Victor Hugo Costa de Albuquerque
Federal University of Cear ´a, Fortaleza, Graduate Program on Teleinformatics
Engineering, Fortaleza/CE, Brazil
Samarendra Nath Sur
Department of Electronics and Communication Engineering, Sikkim Manipal
Institute of Technology, Majitar, Sikkim, India
Paolo Barsocchi
Information Science and Technologies Institute (ISTI),
National Research Council (CNR), Pisa, Italy
Series Editor
Fatos Xhafa
UPC-BarcelonaTech, Barcelona, SpainIntelligent Data-centric SystemsAcademic Press is an imprint of Elsevier
125 London Wall, London EC2Y 5AS, United Kingdom
525 B Street, Suite 1650, San Diego, CA 92101, United States50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, United Kingdom
Copyright © 2022 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or
mechanical, including photocopying, recording, or any information storage and retrieval system, without permission
in writing from the publisher. Details on how to seek permission, further information about the Publisher ’s
permissions policies and our arrangements with organizations such as the Copyright Clearance Center and theCopyright Licensing Agency, can be found at our website:
www.elsevier.com/permissions .
This book and the individual contributions contained in it are protected under copyright by the Publisher (other than
as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience broaden our
understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any
information, methods, compounds, or experiments described herein. In using such information or methods theyshould be mindful of their own safety and the safety of others, including parties for whom they have a professionalresponsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability
for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or
from any use or operation of any methods, products, instructions, or ideas contained in the material herein.
ISBN: 978-0-323-90548-0
For Information on all Academic Press publications
visit our website at
https://www.elsevier.com/books-and-journals
Publisher: Mara Conner
Editorial Project Manager: Emily Thomson
Production Project Manager: Prasanna Kalyanaraman
Cover Designer: Victoria Pearson
Typeset by MPS Limited, Chennai, IndiaContents
List of contributors ............................................................................................................. ................ xiii
CHAPTER 1 Edge-IoMT-based enabled architecture for smart healthcare
system ................................................................................................... 1
Joseph Bamidele Awotunde, Muhammed Fazal Ijaz, Akash Kumar Bhoi,
Muyideen AbdulRaheem, Idowu Dauda Oladipo and Paolo Barsocchi
1.1 Introduction ................................................................................................................ 1
1.2 Applications of an IoMT-based system in the healthcare industry .......................... 3
1.3 Application of edge computing in smart healthcare systems ................................... 7
1.4 Challenges of using edge computing with IoMT-based system in
smart healthcare system ........................................................................................... 11
1.5 The framework for edge-IoMT-based smart healthcare system ............................. 13
1.6 Case study for the application of edge-IoMT-based systems enabled for the
diagnosis of diabetes mellitus .................................................................................. 15
1.6.1 Experimental results....................................................................................... 16
1.7 Future prospects of edge computing for internet of medical things....................... 18
1.8 Conclusions and future research directions ............................................................. 21
References................................................................................................................. 22
CHAPTER 2 Physical layer architecture of 5G enabled IoT/IoMT system ............ 29
Anh-Tu Le, Munyaradzi Munochiveyi and Samarendra Nath Sur
2.1 Architecture of IoT/IoMT system............................................................................ 29
2.1.1 Sensor layer.................................................................................................... 31
2.1.2 Gateway layer ................................................................................................ 312.1.3 Network layer................................................................................................. 32
2.1.4 Visualization layer ......................................................................................... 33
2.2 Consideration of uplink healthcare IoT system relying on NOMA ....................... 33
2.2.1 Introduction .................................................................................................... 33
2.2.2 System model................................................................................................. 34
2.2.3 Outage probability for UL NOMA................................................................ 35
2.2.4 Ergodic capacity of UL NOMA .................................................................... 38
2.2.5 Numerical results and discussions................................................................. 38
2.3 Conclusions .............................................................................................................. 41
References................................................................................................................. 41
CHAPTER 3 HetNet/M2M/D2D communication in 5G technologies ...................... 45
Ayaskanta Mishra, Anita Swain, Arun Kumar Ray and Raed M. Shubair
3.1 Introduction .............................................................................................................. 45
v3.2 Heterogenous networks in the era of 5G................................................................. 48
3.2.1 5G mobile communication standards and enhanced features....................... 48
3.2.2 5G heterogeneous network architecture ........................................................ 50
3.2.3 Intelligent software defined network framework of 5G HetNets ................. 54
3.2.4 Next-Gen 5G wireless network ..................................................................... 54
3.2.5 Internet of Things toward 5G and heterogenous wireless networks ............ 55
3.2.6 5G-HetNet H-CRAN fronthaul and TWDM-PON backhaul:
QoS-aware virtualization for resource management..................................... 57
3.2.7 Spectrum allocation and user association in 5G HetNet mmWave
communication: a coordinated framework.................................................... 58
3.2.8 Diverse service provisioning in 5G and beyond: an intelligent
self-sustained radio access network slicing framework................................ 58
3.3 Device-to-Device communication in 5G HetNets................................................... 58
3.4 Machine-to-Machine communication in 5G HetNets ............................................. 61
3.4.1 Machine-to-Machine communication in 5G: state of the art architecture,
recent advances and challenges ..................................................................... 61
3.4.2 Recent advancement in the Internet of Things related standard:
oneM2M perspective...................................................................................... 62
3.4.3 M2M traffic in 5G HetNets ........................................................................... 66
3.4.4 Distributed gateway selection for M2M communication cognitive
5G5G networks .............................................................................................. 68
3.4.5 Algorithm for clusterization, aggregation, and prioritization of
M2M devices in 5G5G HetNets.................................................................... 69
3.5 Heterogeneity and interoperability .......................................................................... 70
3.5.1 User interoperability ...................................................................................... 70
3.5.2 Device interoperability................................................................................... 72
3.6 Research issues and challenges................................................................................ 72
3.6.1 Resource allocation........................................................................................ 73
3.6.2 Interference management............................................................................... 743.6.3 Power allocation............................................................................................. 74
3.6.4 User association ............................................................................................. 75
3.6.5 Computational complexity and multiaccess edge computing....................... 75
3.6.6 Current research in HetNet based on various technologies .......................... 76
3.7 Smart healthcare using 5G5G Inter of Things: a case-study .................................. 77
3.7.1 Mobile cellular network architecture: 5th generation................................... 77
3.7.2 ZigBee IP ....................................................................................................... 783.7.3 Healthcare system architecture using wireless sensor network and
mobile cellular network ................................................................................. 78
3.8 Conclusions .............................................................................................................. 82
References................................................................................................................. 82vi ContentsCHAPTER 4 An overview of low power hardware architecture for edge
computing devices .............................................................................. 89
Kushika Sivaprakasam, P. Sriramalakshmi, Pushpa Singh and M.S. Bhaskar
4.1 Introduction .............................................................................................................. 89
4.2 Basic concepts of cloud, fog and edge computing infrastructure........................... 91
4.2.1 Role of edge computing in Internet of Things.............................................. 93
4.2.2 Edge intelligence and 5G in Internet of Things based smart healthcare
system............................................................................................................. 94
4.3 Low power hardware architecture for edge computing devices ............................. 95
4.3.1 Objectives of hardware development in edge computing............................. 95
4.3.2 System architecture........................................................................................ 964.3.3 Central processing unit architecture .............................................................. 96
4.3.4 Input /C0output architecture.............................................................................. 98
4.3.5 Power consumption........................................................................................ 99
4.3.6 Data processing and algorithmic optimization.............................................. 99
4.4 Examples of edge computing devices ................................................................... 100
4.5 Edge computing for intelligent healthcare applications........................................ 101
4.5.1 Edge computing for healthcare applications ............................................... 101
4.5.2 Advantages of edge computing for healthcare applications ....................... 102
4.5.3 Implementation challenges of edge computing in healthcare systems....... 1044.5.4 Applications of edge computing based healthcare system ......................... 104
4.5.5 Patient data security in edge computing ..................................................... 105
4.6 Impact of edge computing, Internet of Things and 5G on smart healthcare
systems ................................................................................................................... 106
4.7 Conclusion and future scope of research............................................................... 107
References............................................................................................................... 107
CHAPTER 5 Convergent network architecture of 5G and MEC ........................... 111
Ayaskanta Mishra, Anita Swain, Arun Kumar Ray and Raed M. Shubair
5.1 Introduction ............................................................................................................ 111
5.2 Technical overview on 5G network with MEC .................................................... 114
5.2.1 5G with multi-access edge computing (MEC): a technology enabler........ 115
5.2.2 Application splitting in MEC ...................................................................... 117
5.2.3 Layered service oriented architecture for 5G MEC.................................... 119
5.3 Convergent network architecture for 5G with MEC............................................. 122
5.4 Current research in 5G with MEC......................................................................... 125
5.5 Challenges and issues in implementation of MEC ............................................... 129
5.5.1 Communication and computation perspective ............................................ 131
5.5.2 Application perspective ............................................................................... 133
5.6 Conclusions ............................................................................................................ 134
References............................................................................................................... 135vii ContentsCHAPTER 6 An efficient lightweight speck technique for edge-IoT-based
smart healthcare systems ................................................................. 139
Muyideen AbdulRaheem, Idowu Dauda Oladipo, Alfonso Gonz ´alez-Briones,
Joseph Bamidele Awotunde, Adekola Rasheed Tomori andRasheed Gbenga Jimoh
6.1 Introduction ............................................................................................................ 139
6.2 The Internet of Things in smart healthcare system............................................... 141
6.2.1 Support for diagnosis treatment................................................................... 1426.2.2 Management of diseases .............................................................................. 143
6.2.3 Risk monitoring and prevention of disease................................................. 144
6.2.4 Virtual support ............................................................................................. 1446.2.5 Smart healthcare hospitals support.............................................................. 145
6.3 Application of edge computing in smart healthcare system ................................. 146
6.4 Application of encryptions algorithm in smart healthcare system ....................... 148
6.4.1 Speck encryption.......................................................................................... 150
6.5 Results and discussion............................................................................................ 152
6.6 Conclusions and future research directions........................................................... 157
References............................................................................................................... 158
CHAPTER 7 Deep learning approaches for the cardiovascular disease
diagnosis using smartphone ............................................................. 163
Abdulhamit Subasi, Elina Kontio and Mojtaba Jafaritadi
7.1 Introduction ............................................................................................................ 163
7.2 Disease diagnosis and treatment ............................................................................ 167
7.3 Deep learning approaches for the disease diagnosis and treatment...................... 170
7.3.1 Artificial neural networks ............................................................................ 171
7.3.2 Deep learning ............................................................................................... 171
7.3.3 Convolutional Neural Networks .................................................................. 172
7.4 Case study of a smartphone-based Atrial Fibrillation Detection.......................... 173
7.4.1 Smartphone data acquisition........................................................................ 175
7.4.2 Biomedical signal processing ...................................................................... 176
7.4.3 Prediction and classification ........................................................................ 177
7.4.4 Experimental data ........................................................................................ 181
7.4.5 Performance evaluation measures ............................................................... 182
7.4.6 Experimental results..................................................................................... 183
7.5 Discussion............................................................................................................... 184
7.6 Conclusion.............................................................................................................. 186
References............................................................................................................... 186viii ContentsCHAPTER 8 Advanced pattern recognition tools for disease diagnosis ............ 195
Abdulhamit Subasi, Siba Smarak Panigrahi, Bhalchandra Sunil Patil,
M. Abdullah Canbaz and Riku Kl ´en
8.1 Introduction ............................................................................................................ 195
8.2 Disease diagnosis ................................................................................................... 199
8.3 Pattern recognition tools for the disease diagnosis ............................................... 203
8.3.1 Artificial neural networks .......................................................................... 204
8.3.2 K-nearest neighbor..................................................................................... 204
8.3.3 Support vector machines............................................................................ 205
8.3.4 Random forests .......................................................................................... 2058.3.5 Bagging ...................................................................................................... 205
8.3.6 AdaBoost.................................................................................................... 206
8.3.7 XGBoost..................................................................................................... 206
8.3.8 Deep learning ............................................................................................. 206
8.3.9 Convolutional neural network ................................................................... 207
8.3.10 Transfer learning ........................................................................................ 207
8.4 Case study of COVID-19 detection....................................................................... 210
8.4.1 Experimental data ........................................................................................ 213
8.4.2 Performance evaluation measures ............................................................... 213
8.4.3 Feature extraction using transfer learning................................................... 213
8.4.4 Experimental results..................................................................................... 214
8.5 Discussion............................................................................................................... 220
8.6 Conclusions ............................................................................................................ 221
References............................................................................................................... 222
CHAPTER 9 Brain-computer interface in Internet of Things environment ......... 231
Vijay Jeyakumar, Palani Thanaraj Krishnan, Prema Sundaram and
Alex Noel Joseph Raj
9.1 Introduction ............................................................................................................ 231
9.1.1 Components of BCI ..................................................................................... 232
9.1.2 Types of BCI................................................................................................ 233
9.1.3 How does BCI work?................................................................................... 2339.1.4 Key features of BCI..................................................................................... 234
9.1.5 Applications ................................................................................................. 234
9.2 Brain-computer interface classification ................................................................. 235
9.2.1 Noninvasive BCI.......................................................................................... 235
9.2.2 Semiinvasive or partially invasive BCI....................................................... 237
9.2.3 Invasive BCI ................................................................................................ 237ix Contents9.3 Key elements of BCI.............................................................................................. 237
9.3.1 Signal acquisition......................................................................................... 238
9.3.2 Preprocessing or signal enhancement.......................................................... 2389.3.3 Feature extraction......................................................................................... 238
9.3.4 Classification stage ...................................................................................... 238
9.3.5 Feature translation or control interface stage.............................................. 239
9.3.6 Device output or feedback stage ................................................................. 239
9.4 Modalities of BCI................................................................................................... 240
9.4.1 Electrical and magnetic signals ................................................................... 240
9.4.2 Metabolic signals ......................................................................................... 241
9.5 Computational intelligence methods in BCI/BMI................................................. 242
9.5.1 State of the prior art..................................................................................... 242
9.6 Online and offline BCI applications...................................................................... 245
9.7 BCI for the Internet of Things............................................................................... 245
9.8 Secure brain-brain communication ........................................................................ 249
9.8.1 Edge computing for brain /C0to/C0things......................................................... 250
9.9 Summary and conclusion ....................................................................................... 251
9.10 Future research directions and challenges............................................................. 251
Abbreviations.......................................................................................................... 252
References............................................................................................................... 253
CHAPTER 10 Early detection of COVID-19 pneumonia based on ground-glass
opacity (GGO) features of computerized tomography (CT)angiography .......................................................................................
257
H.M.K.K.M.B. Herath, G.M.K.B. Karunasena and B.G.D.A. Madhusanka
10.1 Introduction ............................................................................................................ 257
10.2 Background............................................................................................................. 259
10.2.1 Ground-glass opacity (GGO)..................................................................... 25910.2.2 Support vector machine (SVM)................................................................. 260
10.2.3 Histogram of oriented gradients (HOG) algorithm................................... 260
10.2.4 Convolutional neural network (CNN) ....................................................... 261
10.2.5 Literature .................................................................................................... 262
10.3 Materials and methods ........................................................................................... 262
10.3.1 Dataset description..................................................................................... 262
10.3.2 Methodology .............................................................................................. 263
10.4 Results and analysis ............................................................................................... 266
10.4.1 Test results of the COVID-19 pneumonia detection system .................... 26710.4.2 Analysis of the test results......................................................................... 271
10.5 Conclusion.............................................................................................................. 274
References............................................................................................................... 275x ContentsCHAPTER 11 Applications of wearable technologies in healthcare:
an analytical study ........................................................................... 279
Hiren Kumar Thakkar, Shamit Roy Chowdhury, Akash Kumar Bhoi and
Paolo Barsocchi
11.1 Introduction ............................................................................................................ 279
11.2 Application of wearable devices............................................................................ 281
11.3 The importance of wearable technology in healthcare ......................................... 283
11.3.1 Personalization ........................................................................................... 283
11.3.2 Remote patient monitoring ........................................................................ 283
11.3.3 Early diagnosis........................................................................................... 283
11.3.4 Medication adherence ................................................................................ 28411.3.5 Complete information ................................................................................ 284
11.3.6 Cost savings ............................................................................................... 284
11.4 Current scenario of wearable computing............................................................... 284
11.5 The wearable working procedure .......................................................................... 286
11.6 Wearables in healthcare ......................................................................................... 286
11.6.1 Weight loss................................................................................................. 286
11.6.2 Medication tracking ................................................................................... 287
11.6.3 Virtual doctor consultations....................................................................... 287
11.6.4 Geiger counter for illnesses ....................................................................... 28811.6.5 Hydration tool ............................................................................................ 288
11.6.6 Pregnancy and fertility tracking ................................................................ 288
11.7 State-of-the-art implementation of wearables ....................................................... 289
11.7.1 Detection of soft fall in disabled or elderly people .................................. 289
11.7.2 The third case study is based on the detection of stress
using a smart wearable band ..................................................................... 292
11.7.3 Use of wearables to reduce cardiovascular risk........................................ 293
11.8 Future scope and conclusion.................................................................................. 296
References............................................................................................................... 296
Index .......................................................................................................................... ........................ 301xi ContentsThis page intentionally left blankList of contributors
Muyideen AbdulRaheem
Department of Computer Science, University of Ilorin, Ilorin, Kwara State, Nigeria
Joseph Bamidele Awotunde
Department of Computer Science, University of Ilorin, Ilorin, Kwara State, Nigeria
Paolo Barsocchi
Institute of Information Science and Technologies, National Research Council, Pisa, Italy
M.S. Bhaskar
Renewable Energy Lab, Department of communication and Networks Engineering, College of
Engineering, Prince Sultan University, Riyadh, Saudi Arabia
Akash Kumar Bhoi
KIET Group of Institutions, Delhi-NCR, Ghaziabad, India
M. Abdullah Canbaz
Computer Science Department, Indiana University Kokomo, Kokomo, IN, United States
Shamit Roy Chowdhury
School of Computer Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha, India
Alfonso Gonza ´lez-Briones
Research Group on Agent-Based, Social and Interdisciplinary Applications, (GRASIA),Complutense University of Madrid, Madrid, Spain; BISITE Research Group, University of
Salamanca, Salamanca, Spain; Air Institute, IoT Digital Innovation Hub, Salamanca, Spain
H.M.K.K.M.B. Herath
Faculty of Engineering Technology, The Open University of Sri Lanka, Nugegoda, Sri Lanka
Muhammed Fazal Ijaz
Department of Intelligent Mechatronics Engineering, Sejong University, Seoul, South Korea
Mojtaba Jafaritadi
Faculty of Engineering and Business, School of Information and Communications Technology,Turku University of Applied Sciences, Turku, Finland; Faculty of Technology, Department ofComputing, University of Turku, Turku, Finland
Vijay Jeyakumar
Department of Biomedical Engineering, Sri Sivasubramaniya Nadar College of Engineering,
Chennai, Tamil Nadu, India
Rasheed Gbenga Jimoh
Department of Computer Science, University of Ilorin, Ilorin, Kwara State, Nigeria
G.M.K.B. Karunasena
Faculty of Engineering Technology, The Open University of Sri Lanka, Nugegoda, Sri Lanka
Riku Kle ´n
Turku PET Centre, University of Turku, Turku, Finland
xiiiElina Kontio
Faculty of Engineering and Business, School of Information and Communications Technology,
Turku University of Applied Sciences, Turku, Finland
Palani Thanaraj Krishnan
Department of Electronics and Instrumentation Engineering, St. Joseph’s College of
Engineering, Chennai, Tamil Nadu, India
Anh-Tu Le
Faculty of Electronics Technology, Industrial University of Ho Chi Minh City, Ho Chi Minh City,Vietnam
B.G.D.A. Madhusanka
Faculty of Engineering Technology, The Open University of Sri Lanka, Nugegoda, Sri Lanka
Ayaskanta Mishra
School of Electronics Engineering, Kalinga Institute of Industrial Technology, Deemed to be
University, Bhubaneswar, Odisha, India
Munyaradzi Munochiveyi
Department of Electrical and Electronics Engineering, University of Zimbabwe, Harare,
Zimbabwe
Idowu Dauda Oladipo
Department of Computer Science, University of Ilorin, Ilorin, Kwara State, Nigeria
Siba Smarak Panigrahi
Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur,Kharagpur, West Bengal, India
Bhalchandra Sunil Patil
Department of Mechanical Engineering, Indian Institute of Technology Kharagpur, Kharagpur,
West Bengal, India
Alex Noel Joseph Raj
Key Laboratory of Digital Signal and Image Processing of Guangdong Province, Department ofElectronic Engineering, College of Engineering, Shantou University, Shantou, P.R. China
Arun Kumar Ray
School of Electronics Engineering, Kalinga Institute of Industrial Technology, Deemed to be
University, Bhubaneswar, Odisha, India
Raed M. Shubair
Department of Electrical and Computer Engineering, New York University (NYU) Abu Dhabi,
Abu Dhabi, United Arab Emirates
Pushpa Singh
Department of Computer Science & Information Technology, KIET Group of Institutions, Delhi-NCR, Ghaziabad, Uttar Pradesh, India
Kushika Sivaprakasam
School of Electrical Engineering, Vellore Institute of Technology, Chennai, Tamil Nadu, Indiaxiv List of contributorsP. Sriramalakshmi
School of Electrical Engineering, Vellore Institute of Technology, Chennai, Tamil Nadu, India
Abdulhamit Subasi
Faculty of Medicine, Institute of Biomedicine, University of Turku, Turku, Finland
Prema Sundaram
Department of Biomedical Engineering, RVS Educational Trust’s Group of Institutions, Dindigul,
Tamil Nadu, India
Samarendra Nath Sur
Department of Electronics and Communication Engineering, Sikkim Manipal Institute of
Technology, Sikkim Manipal University, Majitar, Rangpo, Sikkim, India
Anita Swain
School of Electronics Engineering, Kalinga Institute of Industrial Technology, Deemed to beUniversity, Bhubaneswar, Odisha, India
Hiren Kumar Thakkar
Department of Computer Engineering, Marwadi University, Rajkot, Gujarat, India
Adekola Rasheed Tomori
Computer Services and Information Technology, University of Ilorin, Ilorin, Kwara State, Nigeriaxv List of contributorsThis page intentionally left blankCHAPTER
1Edge-IoMT-based enabled
architecture for smart healthcaresystem
Joseph Bamidele Awotunde1, Muhammed Fazal Ijaz2, Akash Kumar Bhoi3,
Muyideen AbdulRaheem1, Idowu Dauda Oladipo1and Paolo Barsocchi4
1Department of Computer Science, University of Ilorin, Ilorin, Kwara State, Nigeria2Department of Intelligent
Mechatronics Engineering, Sejong University, Seoul, South Korea3KIET Group of Institutions, Delhi-NCR,
Ghaziabad, India4Institute of Information Science and Technologies, National Research Council, Pisa, Italy
1.1 Introduction
The emergence of the smart healthcare system h as created new opportun ities in medical indus-
tries such as medical diagnosis, prediction, treatment, a nd clinical appointments with the doctor
by the patients, thus bringing ab out a reconsideration of trad itional methods in the healthcare
system ( Adeniyi, Ogundokun, & Awotunde, 2021 ). The implementation of telemedicine and
new technological digital health will drastically decrease unn ecessary clinical doctor-patient
appointments and help in early disease diagnos is. Moreover, healthcar es y s t e m sf o c u s e do n
telemedicine and smart healthcare system wo uld allow medical services to be real-time and
cost-effective creating an extraordinarily co nvenient time for both patients and physicians
(A d e n i y ie ta l . ,2 0 2 1 ). The healthcare system that depends on the Internet of Medical Things
(IoMT) assists individuals and aids their vita l everyday life activities. The affordability and
user-friendliness of the usage of IoMT has begu n to revolutionize healthcare services. The
IoMT and its related technologies have emerged as the most preferred use cases in the health-care industry. IoMT-based wearable technologies have encouraged widespread use of the trans-formation of smart healthcare systems in recent years (
A d e n i y ie ta l . ,2 0 2 1 ;D o n ge ta l . ,2 0 2 0 ).
Besides, teleoperation and remo te-operated equipment are b ecoming a viable method for
remote healthcare surgery technology managem ent. Smart healthcare platforms can make medi-
cal procedures more time-efficient, cost-effec tive, and portable, making them easier to access
even in the most remote areas ( Ning et al., 2020 ).
A decentralized database that can continuously maintain update patient information provides the
healthcare industry with many benefits. When various parties require access to the same informa-tion, these benefits become particularly interesting. Edge computing technology can be used to cre-ate additional value in a smart healthcare system in elderly care, chronic diseases, and medical
treatment processes. The involvement of many parties in the medical system has caused serious
challenges, and this huge dataset in the healthcare system has disrupted the patients’ treatment.
15G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00006-1
©2022 Elsevier Inc. All rights reserved.During the treatment of a patient in a situation where many parties are involved can cause huge
media distractions. This can be time-consuming during information processes especially when itinvolves various stakeholders, and the resource-intensive authentication becomes problematic.
All the IoMT-based system requirements can never be met with the traditional cloud computing
database architectures alone, because of latency transfers from the network edge to the data center
for processing. A better and powerful computing model is required that can reduce the higher
latency data transfers that create a dominant strategy. The cloud computing bandwidth is quicklyoutpaced by traffic from thousands of users. Also, the cloud servers neglect other protocols theIoMT devices use and interact only with IP. However, edge technology helps the IoMT-baseddevices capture data to be analyzed close to the machines that generate and function on that data.Hence, edge computing can be used to close the gap and be the bridge linking IoMT-based devicesin the processing of a huge amount of data produced. The processing becomes easy with the edgecomputing model and handling and outlining the data from IoMT devices becomes easy and greatly
improved.
Both cloud and edge computing are similar in terms of versatility and scalability of computing,
storage, and networking resources on-demand supplies, and mutually built with virtual systems.Although with the emerging trend in networking in terms of demand, the two technologies have awide barrier. The businesses and end-users are free to use cloud computing from defining certainspecifics, such as storage capacity, limits on computing, and cost of network connectivity. There isstill the rising problem of real-time latency-sensitive applications within nodes to meet their delayrequirements in IoMT-based systems (
Bonomi, Milito, Zhu, & Addepalli, 2012; Saad, 2018 ). The
issue of security of this huge volume of data should also be the main concern for any business-
minded experts because the problem hurts their reputation and they are constrained by the law tokeep all data safe.
But comparing edge to cloud computing, edge computing brings computation, storage, and net-
working closer to the data source, reducing travel time and latency dramatically. Instead of sendingdata back and forth all the time, the processes take place close to the device or at the network’sedge, allowing for quicker response times. Edge applications decrease the amount of data that mustbe transferred, as well as the traffic generated by those transfers and the distance that data must
travel.
The IoMT-based cloud provides the liberty of accessing data from the service providers anytime
in any part of the world, hence, exposure the IoMT-based data to security and privacy threats. Togain more accurate diagnosis results, edge computing has been widely used to decrease the burdenon the medical experts, and help in decreasing the decision time of traditional methods of the diag-nosis process. There are significant improvements in the treatment, prediction, screening, drug/vac-cine development processes, and application of medication in healthcare sectors with continuingexpansion in IoMT-based edge computing. The applications of IoMT-based edge computing have
reduced human intervention in medical processes and the cost of medical applications has reduced.
5G technology with edge computing offers great advantages in supporting the IoMT for medical
diagnosis, monitoring, prediction, treatment, among others efficiently (
Magsi et al., 2018 ). Also,
supports ultra-reliable low latency communication (uRLLC) with a higher data rate, and helps inthe areas of connection of various IoMT-based devices. The use of 5G technology has increasedthe communication and transfer of data within wireless networks. The introduction of the 5G net-work in the IoMT-based system has benefited many in various ways. Also, various fields like2 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemeducation, business organizations, and medical and governmental agencies have benefited from 5G
technology. The use of 5G in medical applications with reduced energy consumption has beenproved by many types of research (
Sodhro & Shah, 2017 ). Furthermore, integrating 5G and edge
computing in an IoMT-based system will enhance patient examination quality, and will be usefulin the area of Wireless Body Area Networks by providing a protected system in the healthcare
industry (
Aldaej & Tariq, 2018; Jones & Katzis, 2018 ). While every sector will receive enjoyment
and benefit from Interne of Things (IoT)-based edge computing systems, why should IoMT-basedmedical systems stay behind from the benefits of edge computing? IoMT-based platforms can beenhanced and equipped with edge computing to ensure accurate diagnosis, and treatment of patientsremotely. A smart healthcare system with proper motivation and proper care will contributeimmensely to the medical system and overcome its obstacles.
Therefore, this chapter discusses the areas of applicability of the architecture of Edge enabled
IoMT system in the healthcare system. It will also present extraordinary opportunities brought by
edge-enabled IoMT system in healthcare, and their research challenges in the healthcare system are
discussed. The chapter finally proposes a framework of an edge-enabled IoMT-based system forthe healthcare system.
1.2 Applications of an IoMT-based system in the healthcare industry
Massive medical costs and the maintenance of big data during any disease outbreak require techni-
cal advances so that at any time and anywhere, everybody has access to healthcare services. Thedevelopment of technology has allowed telehealth to provide online healthcare facilities. Forpatients that are permitted to travel, for villages in rural zones, and for individuals that do not haveaccess to medical care, remote facilities are useful. The uses for telemedicine include the transmis-sion and storage of medical images, video conference patient counseling, continuing education, andfacilities in the electronic healthcare field. Sadly, the use of telemedicine technology is hindered bytechnical and financial costs (
Jin & Chen, 2015 ). To this effect, studies have given cloud computing
that offers, among other things, remote support capability, accessible transparent resources, efficient
large internet connectivity, scalable and resources pooling, robust medical data sharing and proces-sing, and the sharing of big data patient records.
Digital wellness innovations provide huge incentives to reshape current healthcare programs.
Digital health innovations have offered improved quality of care at a more affordable cost, fromthe introduction of automated therapeutic annals to portable medical devices to other innovativetechnology. With healthcare programs, politicians are continually researching, embracing, andimplementing information and communication technology (ICT) (
Sust et al., 2020 ). This forms the
way people and patients view the structures and communicate with them. The road to digital medi-
cal care (eHealth) is a systemic evolution of the conventional medical care system that incorporatesnumerous devices together with universal entry to automated medical annals, online tracking sys-tems, inmate services, wearable devices, portable medical applications, data analytics, and furthertransformative innovations (
Mesko ´, Drobni, B ´enyei, Gergely, & Gy ˝orffy, 2017; Sust et al., 2020 ).
Owing to various pandemics, there is an immediate need to make good use of current tech-
nology. IoMT is known to be one of the greatest innovative innovations with tremendous3 1.2 Applications of an IoMT-based system in the healthcare industrypromise in fighting diseases and pandemic outbreaks ( Oladipo, Babatunde, Awotunde, &
Abdulraheem, 2021 ). The IoMT consists of a sparse network where the IoMT systems feel the
world and transmit valuable data across the Net work. IoMT-based is one of the promising tech-
nologies that will change our lives with seamle ss connections and vigorous integration with
other technologies ( Hussain, Hussain, Hassan, & Hossain, 2020; Sundwall, Munger, Tak,
Walsh, & Feehan, 2020 ). The IoMT-based can be useful in reducing disease spread within an
environment and provided various functions like tracking, and monitoring of the patient inreducing the risk and spread of diseases (
Albahri et al., 2020; Saeed, Bader, Al-Naffouri, &
Alouini, 2020 ).Fig. 1.1 displays possible applications of IoMT-based devices that can be used
effectively to reduce any disease outbreak.
IoMT-based systems in healthcare are used to monitor and control the human body’s vital signs
and connect to healthcare facilities using communication infrastructure ( Rodrigues et al., 2018 ).
The accessibility to a quality physician is now unlimited with the introduction of telemedicine with
various factors attached to them and is getting popular in remote areas ( Chui, Liu, Lytras, & Zhao,
2019 ). For example, patients can be tracked remotely without being physically present at the hospi-
tal using devices and sensors like blood pressure, heart rate, electrocardiography, diabetes, and
IoMT enable Ambulance
Hospital Automatio n
Blockchain-based IoMT Wearable DevicesDigital Telehealth
Smart HealthcareAI-based ForecastingSmart Gadgets
IoMT-based 
Applications in Smart 
Healthcare System
FIGURE 1.1
Potential applications of IoMT for smart healthcare system.4 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemsigns of the vital body. Examples are sensors and actuators that can be used to capture and collect
data to be sent to the cloud from the patient using a local gateway. The results from processed datacan be used by a medical doctor to provided and notify the patient about their status or report(
Adeniyi et al., 2021 ).
Many studies have found that inadequate access to patient information is the explanation for
most medical errors especially during infectious diseases ( Sundwall, Munger, Tak, Walsh, &
Feehan, 2020 ). The IoMT-based medical system has been regarded as a possible system to increase
openness and reduce the extent of medical errors during disease diagnosis to correct health data(
Chui et al., 2019; Firouzi et al., 2018 ). Many medical organizations have also chosen IoMT-based
cloud storage to obtain and store broad patient data and maintain their electronic health records sys-tems. Electronic health records have evolved rapidly over the last decade, providing a basis fordata mining to recognize designs and styles in the big data industry in healthcare. Another commonpoint for exchanging medical data is the interchange of electronic health records. By communicat-
ing at a common hub, these businesses facilitate healthcare sectors to transmit information rather
than maintaining ties with many peer businesses (
Regola & Chawla, 2013 ).
IoMT-based cloud systems also offer secure storage and share resources that can reduce the
amount of local traffic to make organizations agile ( Rubı´& Gondim, 2019; Syed, Jabeen,
Manimala, & Alsaeedi, 2019 ). By reducing the cost needed for starting up automated medical
records, which is lacking in many healthcare segment facilities, this will improve the efficiency ofthe healthcare sector (
Schweitzer, 2012 ). During a disease outbreak, prescriptions and diagnoses,
for instance, can be shared through the cloud over different systems. Therefore, for service
enhancement and higher standards, hospitals and doctors exchange patient records. The primary
advantages of electronic health record cloud storage are the capacity to exchange patient recordswith other specialists at home and overseas, the facility to pool data in one location, and the capac-ity to access files anytime, anywhere. Electronic health record cloud computing enables patients toview, replicate, and transfer their secure health records (
Chen, Chiang, et al., 2016 ). Regardless of
the influences of the IoMT-based system to capture and store large health data, the prime problemis the failure of the network, protection, and privacy of patient information that users, hackers, mal-ware, and so on are exploiting (
Kumari, Tanwar, Tyagi, & Kumar, 2018b; Muhammed, Mehmood,
Albeshri, & Katib, 2018 ).
This new emergence of these technologies is a result of their high availability, simplicity to per-
sonalize, and easy accessibility; thus enabling the providers to deliver personalized content cost-effectively on large scale easily. Also, big data analytics and IoMT are progressively gaining moreattraction for the next generation of smart healthcare systems. Though the new fields evolving rap-idly, they also have their shortcomings, particularly when the goal is healthcare systems with acomplicated problem, difficult in energy-efficient, safe, flexible, suitable, and consistent solutions,especially when it comes to the issue of security and privacy of IoT generally. It has been projected
that IoT will rise to a market scope of $300B by 2022 in healthcare covering the medical devices,
systems, applications, and services sectors (
Firouzi et al., 2018 ). IoT allows a broad range of intelli-
gent applications and resources to solve the problems facing individuals or the healthcare sector(
Medaglia & Serbanati, 2010 ). For instance, P to D (Patient-to-Doctor), P to M (Patient to
Machine), S to M (Sensor to Mobile), M to H (Mobile to Human), D to M (Device to Machine), Oto O (Object to Object), D to M (Doctor to Machine), T to R (Tag to Reader) have dynamic IoMTlink capabilities. This brings people, computers, smart devices, and complex systems together5 1.2 Applications of an IoMT-based system in the healthcare industryintelligently to ensure a productive healthcare system ( Tuli et al., 2020; Zafar, Khan, Iftekhar, &
Biswas, 2020 ).
The IoMT has greatly contributed to the innovations in smart healthcare systems interconnected
devices and medical sensors to promote knowledge-gathering, storage, communication, and sharing.The dramatic changes in traditional healthcare systems into a smarter healthcare system use various
wireless technologies as a catalyst like wearable sensors, wireless sensor networks, radio frequency
identification (RFID), Bluetooth, Li-Fi, and Wi-fi among others has greatly helped and change thehealthcare industry (
Baker, Xiang, & Atkinson, 2017; Chen, Hu, & McAdam, 2020; Fernandez &
Pallis, 2014 ). The use of IoT has penetrated all fields in recent years in various fields like agricul-
ture, education, transportation, and most especially in the healthcare sectors, thus paving the waytowards technological transformations (
Guy, 2019; Tripathi, Ahad, & Paiva, 2020 ). There has been
tremendous growth in the healthcare system using IoMT-based devices to achieve a great level ofautomation. There is countersigning of the beginning of smart healthcare systems to achieve ubiqui-
tous and holistic healthcare facilities with possible improvement where all stakeholders are inter-
connected using IoMT-based devices.
There is an increasing influx of people to urban areas today. Healthcare facilities are one of the
most critical characteristics that have a major effect on people arriving in city centers during infec-tious disease outbreaks globally. Metropolises are therefore financing a digital transition to offerresidents healthy environments (
Marston & van Hoof, 2019 ). On the other hand, because of its
huge number, high speed, and high variety, conventional models and methods for full conserva-tional performance assessment are threatened by the advent of big data (
Song, Fisher, Wang, &
Cui, 2018 ). Also, because of their carbon emissions, conventional ICT systems damage the atmo-
sphere ( Petri, Kubicki, Rezgui, Guerriero, & Li, 2017 ). On the other hand, cloud services are a
cost-effective medium for accommodating large-scale infrastructure systems have gained consider-able acceptance. The use of cloud computing is, therefore, a significant phase in the green proces-sing process that saves resources and protects the atmosphere. The use of sufficient equipment andcloud space saves the organization’s resources and eliminates the costs related to cooling systems,computers, and central servers. Nevertheless, cloud computing supports renewable computing withenergy savings, rendering dangerous articles less harmful (
Pazowski, 2015 ).
By using intelligent mobile comput ers, IoMT-based cloud systems ha ve inspired hea lthcare specia-
lists to observe the wellbeing of patients at home remotely ( Bhatia, 2020 ). Besides, IoT will build a net-
work by leveraging integrated sensors to track the p atient’s real-time health status and control the
treatment process. The IoT plays a significant role in the healthcare sector and this will continue for the
next generation. Although health mon itoring systems for IoT-based pa tients are popular, observing out-
door hospital requirements increas es the IoMT’s cloud computing capab ilities for the handling and stor-
ing of health data ( Ghanavati, Abawajy, Izadi, & Alelaiwi, 2017 ). Nevertheless, the sum of IoMT-based
gadgets is anticipated to rise signif icantly in the app roaching years ( Al-Turjman, Nawaz, & Ulusar,
2020 ) and the complexity that exists in various IoMT m echanisms (system crossing point, communica-
tion protocols, data structure, sy stem semantics) would bring intero perability and confidentiality-
correlated difficulties ( Edemacu, Park, Jang, & Kim, 2019 ). A universal healthcare framework must be
robust enough to address all of these principles in this way. The incorporation of IoMT technologies in
an interoperable setting and the crea tion of software for the collection, analysis, and extensive distribu-
tion of IoMT-based data are now becoming important.6 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare system1.3 Application of edge computing in smart healthcare systems
Greater efforts have been made in the area of the smart healthcare system to build and design a
reliable and convenient framework for IoMT-based device systems ( Pham, Mengistu, Do, & Sheng,
2018 ). In the biomedical industry and the rise in wearable devices, smart healthcare systems have
gradually reshaped the conventional medical system ( Lu et al., 2020; Wen et al., 2020 ). These
devices and sensors are majorly used for collecting blood pressure, respiratory rate, motion func-
tion, blood glucose data, electrocardiogram (ECG), Electroencephalogram (EEG), and body temper-ature among others for primary medical examinations. To provide early diagnosis using this datahelps in the preservation of the healthcare system and the removal of other complications duringpatients’ treatment (
Athavale & Krishnan, 2020 ).
IoMT-based is used in the medical system to manage doctor’s advice to patients, medical tools,
patients’ records, disease diagnosis, and patient treatment. The application of Machine Learning(ML) algorithms with the IoMT-based system makes the smart healthcare system highly effective
in the area of disease diagnosis, prediction, health monitoring system, and before human utilization
(
Pustokhina et al., 2020 ). IoMT-based systems allow telemedicine like telesurgery, telerehabilita-
tion, and telehealth that remotely monitoring, treating, and diagnosing patients’ in real-time. Theyuse the IoMT-based model to transfer medical data to the database using IoMT-based cloud mod-els. The models are comprised of three major components, namely the Body Sensor Network(BSN), the gateways, and the cloud server center. In recent years, the IoMT-based system supportshealthcare services in real-time to distant stakeholders. The capture data using IoMT-based devicesare provided to physicians and relevant stakeholders to validate and provide useful information to
patients’ whenever it is needed.
The IoMT-based system with edge computing lowers latency services is energy-effective and
cost-effective, and provides maximum satisfaction for healthcare contributors. Most IoMT-basedenvironments depend on a cloud platform for massive smart health systems (
Janet & Raj, 2019 ).
The model can be used to forward captured data produced from IoMT devices through the Internetto the cloud, and thereby used for diagnosis to provide useful reports using learning algorithms likeML or deep learning (DL). But the IoMT-Cloud system is inappropriate, especially where lowerlatency is necessary. Hence, IoMT-based systems require a faster and low latency protection tech-
nique with delay-sensitive, smart, secure, stable smart healthcare management. Edge computing is
the answer to this with a prolonged type of cloud computing where IoMT-based data can be com-puted closer to the edge of the network where data are produced (
Abdulraheem, Awotunde, Jimoh,
& Oladipo, 2021 ).
Edge computation reduces latency, data traffic, and data distance to the network since it is
running at a local processing level closed to the cloud database. Edge computing has becomerelevant and important since devices can recogn ize data instinctively, thus become useful in
IoMT-based systems to reduce the latency to a lower level.
Fig. 1.2 depicts the edge comput-
ing architecture, where the first part of a network uses the IoMT-based devices and sensors to
collect data to be processed through a gateway using a Radio Access Network that uses edgedevices to compute data aggregated by the network locally. Once the data processing hasbeen done, the full computing operations and memory storage have been processed to thecloud.7 1.3 Application of edge computing in smart healthcare systemsThe edge layer is like a junction point where enough networking, computing, and storage
resources are available to manage local data collection, which can be readily obtainable and deliver
fast results. Low-power system-on-chip (SoC) systems are used in most situations because they aremeant to preserve the trade-off between processing performance and power consumption. Cloudservers, on the other hand, have the power to conduct advanced analytics and ML jobs to combinetime series generated by a variety of heterogeneous or mixed kinds of items (
Rehman, Khan, &
Habib, 2020 ).
The IoT-based system is used to generate large medical data with the introduction of wireless
technology, and customized enhance services. Such big medical data are generated from countless
sources, and the cloud server is used to store, analyze, and process such data like text, multimedia,
and image among others ( Devarajan, Subramaniyaswamy, Vijayakumar, & Ravi, 2019 ). The high
latency, security problems, and network traffic arise as a result of the handling of big cloud medicaldata. Fog computing was introduced to minimize the burden of the cloud been a new computingplatform. The fog also helps in bringing the cloud service closer to the network edge and thusallows refined and secured healthcare services.
The edge of the network is an ideal place for analyzing real-time health information where data
is created. The feature of edge computing placed it ahead of cloud computing like data
Cloud Storage 
Repository
Edge
Real-time Data 
Processing 
Basic AnalyticsData caching 
Buffering 
OptimizationMachine 
to 
Machine
Internet of Medical Things
FIGURE 1.2
The architecture structure of edge computing in IoMT-based system.8 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systempreprocessing, local data analytics, data security and privacy, temporary storage, data trimming,
distributed, decentralized storage. Both distributed edge and centralized cloud servers are needed inan IoMT-based application like health monitoring systems to efficiently perform big data analytics.The use of edge computing as an intermediary has created a better way of handling cloud databaseson IoMT-based devices for real-time healthcare systems (
Devarajan et al., 2019 ). Edge computing
takes resources to the edge of the network as an extension to cloud computing. This effectively
brings the benefits and power of the cloud closer to the place where the data is produced, therebyassisting and speeding up “on-the-fly solutions” for applications in a smart healthcare system. Thisdecentralized model’s main objective is to bring devices and software to the edge of the networkwhere the data is generated. Edge computing’s main aim is to reduce the amount of data that istransmitted to cloud data centers for processing and analysis. It also improves security, a key issuein the IoMT industry (
Pan & McElhannon, 2017; Xu et al., 2019 ).
Table 1.1 compares the cloud IoMT-based and Edge IoMT-based computing using IoT-based
requests. Edge computing cannot completely replace cloud computing because it is essentially an
extension of the perception of cloud computing. The computing paradigm is complementary andcollaborative. To process a huge amount of big data in real-time quickly, edge computing ends isvery paramount, but most of the captured data is not used once. The cloud is still used for the stor-age of capture data, and useful in the linkage of various edge nodes, and the management of edgeand virtualization resources rely solely on the cloud. The combination of both cloud and edge willbring about various IoT-based devices together to accomplish various demand situations, therebyoptimizing the application benefit of both technologies.
The Cloud IoMT-based alone can no longer handle the huge amount of data generated by
IoMT devices, new powerful computing models are required. The security concerns, low
Table 1.1 Comparison of IoMT-based cloud and IoMT-based fog computing.
Requirements IoMT-based cloud computing IoMT-based edge computing
Mark user Internet users Mobile users
Location of servers Within Internet Edge nodesService type Global information Localized information servicesGeographical Distribution Centralized DistributedDistance between client and server Multiple hops Single hopDelay jitter High LowLatency High Low
Data Processing Slow Fast
Reliability High LowType of connectivity Leased line WirelessLocation awareness No YesServer nodes Few LargeComputing cost High LowN/W bandwidth More LowReply time Minutes Milliseconds, sub-seconds
Security Less secure Very secure9 1.3 Application of edge computing in smart healthcare systemslatency, speedy processing requirements need new powerful computing techniques to best
place processing, conserve network bandwidth, and making IoMT-based systems operate in areliable environment (
Nandyala & Kim, 2016 ). All of these IoMT-based system requirements
can never be met with traditional c loud computing architectures alone; therefore, a better and
powerful computing model is required. Latency transfers data from the network edge to the
data center for processing thus creates the domin ant strategy. Bandwidth is quickly outpaced
by traffic from thousands of users. Also, the cloud servers neglect other protocols the IoTdevices use and interact only with IP. The best location for most IoMT data to be analyzed isclose to the machines that generate and functio n on that data and this is called computing with
an edge.
It is important to recognize that cloud and edge computing are two distinct, non-interchangeable
technologies that cannot be used interchangeably. Time-sensitive data is processed using edge com-puting, while data that is not time-sensitive is processed using cloud computing. In remote areas
where there is little or no access to a centralized location, edge computing is favored over cloud
computing. Edge computing is the ideal option for local storage in these areas, which is equivalentto micro-network infrastructure.
Specialized and intelligent systems benefit from edge computing as well. Although these
devices are similar to personal computers (PCs), they are not multifunctional computing devices.These specialized computing devices are intelligent and respond in a specific way to specificmachines. Edge computing, on the other hand, suffers from this specialization especially in smarthealthcare that needs fast responses. Edge computing differs from cloud computing in that it takes
time to relay information to a centralized data center, which can take up to 2 s, slowing decision-
making. Since signal latency can result in business losses, organizations prefer edge computing tocloud computing.
The smart healthcare system is different from most existing offloading frameworks, thus excep-
tionally delay-sensitive. Hence, the delay constraint in cloud servers makes it difficult to alwaysprovide satisfactory services (
Dong et al., 2020 ). Edge computing is used to reduce transmission
latency to solve this obstacle. In edge computing-enabled health monitoring systems, which can bemaintained by using hybrid cloud computing, the privacy problem is established in (
Pace et al.,
2018 ).Gu, Zeng, Guo, Barnawi, and Xiang (2015) suggest a cost-efficient healthcare system with
the convergence of edge computing and health monitoring. The system under review takes intoaccount the combination of servers, the allocation of tasks for medical research, and the implemen-tation of virtual machines.
The wearable sensor is treated with minimum power at the edge devices platform in the health
monitoring system. Without decreasing the working role of the IoMT-based system, the edgedevices limited energy power and, thus, reduce the application computation and energy consump-tion to grow edge-dependent healthcare sectors. The combination of cloud-edge computing into a
healthcare monitoring system is one of the skillful strategies for integrating agile computing. The
advantages of edge and computing under the application of hierarchical structure helps to extendthe computation between cloud and edge devices in the analysis of the data collected using IoMT-based devices. The delay-sensitive healthcare applications have been increased using edgecomputing while the integration of higher storage capacity and maximum resources to computewas provided using cloud computing. The combination of cloud and edge computing enhanced theperformance of IoMT-based devices in medical fields.10 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemIn various computation models, DL has been widely utilized for intelligence in numerous fields
like natural language processing, object recognition, speech segregations, image classification(
Ayo, Ogundokun, Awotunde, Adebiyi, & Adeniyi, 2020; Oladele, Ogundokun, Awotunde,
Adebiyi, & Adeniyi, 2020 ). DL, due to its characteristic of self-teaching, can be used to learn input
data feature hierarchically and automatically, and compression ability to highpoint the concealed
patterns ( Awotunde, Matiluko, & Fatai, 2014; Ayo, Awotunde, Ogundokun, Folorunso, &
Adekunle, 2020 ). Therefore, for IoMT-based applications, the DL method has become an effective
approach for learning features and classification. The DL technologies become highly thoroughbecause of their productiveness inbuilt layers’ structure. The minimum-powered edge devices aretherefore not applicable in the DL system since the DL method is not capable of meeting fullcomputational cost requirements. The key challenge involved in implementing an efficient healthmonitoring system for latency-consciousness is the incorporation of DL inference into edge devicesthat have minimal computational capabilities (
Parsa, Panda, Sen, & Roy, 2017 ).
1.4 Challenges of using edge computing with IoMT-based system in
smart healthcare system
Several technologies have been used and are comprised in smart healthcare systems, which include
5G technology, IoMT-based system, edge computing, cloud computing, medical devices and sen-
sors, artificial intelligence, and DL. These technologies have been put together to better the perfor-
mance of a smart healthcare system. In recent years, edge computing has been identified as the bestcomputing to be applied with a system that requires lower latency and cost-efficient like IoMT-based systems. The big data 5Vs data importance had results of the huge amount of patients’ datareceives from the medical device such as volume, veracity, variety, and velocity. As a result, edgecomputing is needed to connect to receive, store, process, and communicates with IoMT-baseddevices. The system administrative configuration must be controlled to forestall the data fluctuationbetween the edge and IoMT-based cloud database. To handle various types of data like text, videos,
audios, and image files, edge protocols, and data format are needed from various ways like smart-
phone, and a smartwatch. For regular data transfer and urgent data requests, the Smart eHealthgateway must be aware of sufficient routing.
The data collection takes place either from medical sensors or portable devices. There is a need
for adequate protection for medical facilities so that patient can use their smartphone for health sta-tus updates. The use of a smart healthcare system creates possible ways of expanding the healthcaresystem to the whole population. The appointment time used by patients to see physicians, or waitsfor diagnosis outcomes can be reduced with the use of the intelligent healthcare system. This also
provides direct access to real-time medical care and services. To maintain trust between patients
and medical experts, the scalability of a smart healthcare system must be taking with all serious-ness, and this will in turn save quality time. It is the main concern and it is not appropriate toobtain information from end-users through unauthorized entities, this, also, poses threats to the per-sonal safety of medical data. The main problem in the introduction and deployment of the digita-lized medical system is security and privacy. However, with the integration of these layers, securityis needed in any layer, such as the system layer, fog layer, and cloud layer (
Puthal et al., 2018 ).11 1.4 Challenges of using edge computing with IoMT-basedAnother problem facing fog computing is heterogeneity that refers to the various communication-
capable devices. Devices like smartphones, autonomous vehicles with other IoT smart devices areat the bottom of the layers within the IoT-based system.
Cloud computing infrastructure has series of difficulties due to limited networking in a small
number of datacentres services, centralized computation, security, and privacy challenges. The
issues may be due to the relatively long gap between remote cloud services and edge devices. Edge
cloud and edge computing tend to be a promising possibility to overcome this problem, whichoffers services closer to resource-poor edge IoMT-based devices and can potentially foster a newecosystem of IoMT innovation. A variety of technological innovations, including cloud services ofnetwork functions and software-defined networking, allow for this prospect.
Sensor interoperability, system connectivity, protection, knowledge management, privacy, and
device management barriers are some of the challenges of IoT and Artificial Intelligence (AI) inthe smart healthcare system. Devices and sensors are used to capture data that experts used to diag-
nose and monitor in real-time healthcare settings, and the information obtained from heterogeneous
data sources involves several problems (
Lin et al., 2016 ). There are natural and uncontrolled pro-
blems in IoT-based devices and sensors, where such challenges are often unexplained errors indevices like smartwatches and cell phones like battery capacity, inconsistencies between uniquephysical attributes, frequent complexities, and environmental variations. These suggest that the usesof multimodal signal and several IoMT devices can cause different problems. To promote the gen-eral acceptance of such smart healthcare, a streamlined and simpler fusion approach should beaddressed.
A major concern is the abuse of access privileges by authorized insiders either because of irre-
sponsible, or for individual criminal reasons, and the information disclosure normally happens withhealthcare facilities. Also, in exchange for illicit benefits, they can reveal confidential patient infor-mation to unauthorized persons. Health updates from celebrities and politicians can easily be leakedfrom a centralized cloud healthcare database to the media. This is always caused by insiders of theregulation and the records to which they can access, such as health staff who do not take care ofactual patients, for instance, and retired workers who are not yet restricted from data queries. Byaccessing each other’s protected data, a dissatisfied group can create problems for others. To infil-
trate, intruders attempt to pretend to be healers. Unhealthy medical procedures have high costs,
such as adverse effects on their image, fines, civil liability, and much more.
For physicians, conventional AI-based systems used in smart healthcare systems may not be
suitable. Therefore, it is possible to deploy explainable AI-based systems where doctors can imag-ine the identification or detection of diseases. Edge-intelligent algorithms can be effectively opti-mized to maximize edge capital (
Ghoneim, Muhammad, Amin, & Gupta, 2018; Hossain &
Muhammad, 2017; Rahman, Hossain, Islam, Alrajeh, & Muhammad, 2020 ). Literature seldom
addresses the functional utility of IoMT-activated healthcare systems. The key issue is that busi-
nesses own the most important data, and it is not available to the public. In practice, the successful
implementation and use of data fusion would allow more accurate assessment and evaluation ofday-to-day physical activity using low-cost monitors that can lead to simpler and better chronic dis-ease preventive care. The next generation of wireless networks has created a tremendous opportu-nity for intelligent healthcare (
Abdulsalam & Hossain, 2020; Alhamid, Rawashdeh, Al Osman,
Hossain, & El Saddik, 2015; Vizitiu, Ni¸ t˘a, Puiu, Suciu, & Itu, 2020 ). The 5G and beyond has
allowed for smart healthcare systems to be accessed more easily and quicker than ever before, thus12 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemmaking the edge-based and federated DL faster and simpler ( Hossain, Rahman, & Muhammad,
2017; Muhammad, Alhamid, & Long, 2019 ).
The heterogeneity within IoT-based systems arises during data processing, data formatting, and
data clearing thus creates difficulty in processing medical information. The enabling of a networkto connect with various sensors is an example challenge in a smart healthcare system for the moni-
toring of patients. For this to take place, heterogeneity must be present when the data is transferred
to another system for processing or analysis. The edge layer while going up to the next layerinvolves various nodes, clusters, switches, and other devices that are needed during data processingand communication facilities (
Yi, Li, & Li, 2015 ). In the design of architecture that enables multi-
ple monitoring of devices, heterogeneity is, therefore, an important factor to communicate with thepatient using devices like heart rate, body temperature, and blood pressure sensors (
Mouradian
et al., 2017 ).
There is another research issue in fog computing due to the location of the network, their pro-
tection can create a concern. The threats that are not present in an organized cloud architecture
pose a threat and arises sometimes at the edge of the network. The major threat from this problemis when an attacker transmits and changes contact between two parties, patients are in the middleof attack (
Mouradian et al., 2017 ). There may be a compromised gateway between patient monitor-
ing sensors and fog nodes that processes the information from the patient in the smart healthcaresystem. A serious problem may arise if the intruder changed the data being handled by an IoT-based system, this may create serious implications for the welfare of these patients (
Awotunde,
Bhoi, & Barsocchi, 2021 ).
There are not standard rules and regulations in the smart healthcare system for the products and
services in edge computing, especially in network protocols and interfaces. There should be stan-dardization in place to standardize the healthcare system like a dedicated agency is needed to solvethis problem. This will help in data dissimilarity and helps to accomplish the real-time response.For good standardization issues like communication protocol, system interfaces should seriously beconsidered (
Kumari, Tanwar, Tyagi, & Kumar, 2018a ).
There is a broad range of message linkers through various procedures, such as Wi-Fi, similar to
the application diversity issue. To be interoperable, edge computing should therefore perform the
required protocol translation on various internal layers, such as network layers, message layers, and
edge layer data annotation layers. Also, there should be complex regulatory systems before health-care tools and equipment are available on the market for consumers to use. The stakeholders andend-users of e-health products should be part of the design team to provide input on their likes, dis-likes, and comforts. This is going a long way to help in building user-friendly interfaces andpatient-centric intelligent medical devices (
Kumari et al., 2018a ).
1.5 The framework for edge-IoMT-based smart healthcare system
The cloud computing technology after calculation returns the results to the terminal equipment using
the capture data from the IoMT-based system by uploads medial data terminal tools to the remotecloud. But, the pressure on the cloud becomes higher if the data generated are huge, thus causingdelay and energy consumption due to a huge load of data in the cloud. In this kind of satiations, cloud13 1.5 The framework for edge-IoMT-based smart healthcare systemcomputing alone cannot help to provide real-time response and lower latency. The main aim of the
medical IoMT-based cloud is to effectively expand the ability of cloud computing providing at theedge of the network using distributed computing resources (
Lin, Song, & Jamalipour, 2019 ); thus,
edge computing is a perfect application to meet this computing demand ( Adeniyi et al., 2021; Ning,
Wang, & Huang, 2018 ).Fig. 1.3 shown the overall framework of the proposed Edge-IoMT-based
system. The framework contains four layers and key technologies. The four layers are IoMT-based
devices, edge computing, cloud computing, and end-users/alert generation layers. The IoMT-baseddevices layer is used to collect and capture data from patients and transfer it to the edge computingthrough the gateway, the collected data will be processed from the edge and send the result to thecloud computing database for further use. The last layer is for the end-user that makes use of the gen-erated results and alert generation that can be sent to the user (patients).
Internet of 
Medical Things
Cloud Storage 
RepositoryEdge
Real-time Data 
Processing 
Basic AnalyticsData caching 
Buffering 
OptimizationMachine 
to 
Machine
End-Users and Alert Generations
FIGURE 1.3
The proposed architecture for edge-IoMT-based smart healthcare system.14 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemThe IoMT-based layer consists of various medical-related technologies like wearable devices
and medical sensors. Originally, during the transmission of health data, IoMT devices are used tocollect patient health information and the interconnected devices interact with other equipment. Itcollects medical information such as an ECG, body temperature, heart rate, pulse oximetry, choles-terol level, and heartbeat while the IoMT-based devices are kept in the body. To evaluate the health
problems of the patient, this information is sent to edge computing.
The various edge network nodes can be used as smart terminal devices in devices like laptops,
smartphones among others, or have network devices gateways like routers and Wi-Fi. The nodesare distributed within the cloud database and terminal equipment in IoMT-based devices. Edgecomputing provided a more agile and sensitive network due to some limited hops within variousnodes and the terminal equipment used in the IoMT-based system platform with network resourcesand cloud storage. This will prevent and reduce the delay of requests and other security problemsthat are important in smart healthcare systems.
1.6 Case study for the application of edge-IoMT-based systems enabled
for the diagnosis of diabetes mellitus
Here, the UCI dataset for diabetes mellitus (DM) with relevant medical data is generated, and
IoMT-based sensors for diagnosing patients affected with diabetes rigorously. The edge IoMT-
based system was proposed to run the data close to network nodes. A ML model based on Long-
Short-Term Memory (LSTM) was used for the diagnosis of DM, and the experiment was conductedusing a standard UCI dataset repository. Even when dealing with extremely large amounts of data,ML models play a significant role in the decision-making process (
Adeniyi et al., 2021;
Ogundokun et al., 2021 ). Defining data types such as velocity, variety, and volume is part of the
process of applying data analysis techniques to particular areas ( Awotunde, Jimoh, Oladipo, &
Abdulraheem, 2021 ). Standard data analysis models include neural network models, classification
models, and clustering methods, as well as the use of efficient algorithms ( Kumar, Lokesh,
Varatharajan, Babu, & Parthasarathy, 2018 ). Data can be produced from a variety of sources with
different data types, so designing methods that can handle these characteristics is essential.
The UCI database consists of a DM dataset with relevant features for the diagnosis of the dis-
ease. The dataset has the history of the patient data collected from the hospital. The capture ofpatient information is stored in the cloud database. The data capture layer is used for the collectionof important data from the cloud storage module. The edge was introduced to secure the clouddatabase, create a platform that allows the capture data to be run close to the cloud node network,takes time to relay information to a centralized data center and low latency signal. The Java pro-
gramming language and Amazon cloud have been used to implement the proposed system. The pre-
processing, such as common and disease affected with frequency, is a major task in this study. Theexperiments were carried out using UCI datasets, and the proposed study was evaluated using crite-ria such as precision, sensitivity, and specificity.
The staging procedure of LSTM is as follows:The LSTM’s sigmoid feature recognizes data that is not needed for any process and uses it as
X
tðÞ at time t21 and produces output Vt21 ðÞ at time t21. The sigmoidal function determines15 1.6 Case study for the application of edge-IoMT-basedwhich part of the output will be isolated from the previous output. The Forgetting Gate ftðÞis the
name of this level. Forget about the ftgate; the value is a vector that matches each number in the
cell Ct21 ðÞ and ranges from 0 to 1.
ft5σwfVt21;Xt ½/C138 1bf/C0/C1
(1.1)
where σis the sigmoidal function, wf5weights and bf5forget gate.
Both equations have two states: the current input’s ignoring and storing conditions, as well as
the cell state’s Xt. The sigmoidal layer and the tanh layer are the two layers. To the sigmoidal layer
given, use 0 or 1 to determine if the new information needs to be changed. The tanh updates thesecond layer’s weights and transfers the values between them ( 21 to 1). The principles are chosen
based on their degree of significance. Both values are changed as shown in
Eq. (1.4) and the new
cell state is formed.
mt5σwfVt21;Xt ½/C138 1bm/C0/C1
(1.2)
Nt5tanh w fVt21;Xt ½/C138 1bn/C0/C1
(1.3)
Ct5Ct21ft1Ntmt (1.4)
The output Vtis multiplied by the new tanh C tðÞ layer created in the final stage, which is based
on the output of sigmoidal gates Qt.
Qt5σwfVt21;Xt ½/C138 1bq/C0/C1
(1.5)
Vt5Qttanh C tðÞ (1.6)
The weights and biases for the output gates are wqandbqrespectively.
1.6.1 Experimental results
The proposed system was evaluated using several instances during the experiments. Four various
algorithms like K /C0Nearest Neighbor (KNN), Support Vector Machine (SVM), Random Forest
(RF), and Naı ¨ve Bayes (NB) that have been used for DM diagnosis to compare the performance of
the proposed classification model. Table 1.2 andFig. 1.4 show the diagnosis accuracy of the pro-
posed model and the existing methods. Various records like 2000, 4000, 6000, 8000, and 10,000
were considered for conducting five various experiments.
Table 1.2 Comparison of the accuracy of the proposed method with other machine learning.
Models 2000 4000 6000 8000 10,000
KNN 87% 90% 86% 89% 90%
SVM 73% 76% 75% 77% 80%Random Forest 91% 92% 90% 94% 97%Naı¨ve Bayes 77% 80% 76% 84% 89%
Proposed model 95% 97% 98% 94% 98%16 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemFrom Table 1.2 and Fig. 1.4 , the results of the accuracy show that the proposed LSTM per-
formed better when compared with the existing methods with an accuracy of 98%, followed by RFwith an accuracy of 97%, and the lowest of the five models is the SVM with an accuracy of 80%.This is because of the use of LSTM and the classification time constraints.
From
Table 1.3 andFig. 1.5 , the results of the specificity show that the proposed LSTM per-
formed better when compared with the existing methods with an accuracy of 99%, followed by RF0%20%40%60%80%100%120%
12345ACCURACY PERFORMANCE
KNN SVM Random Forest Naïve Bayes Proposed Model
FIGURE 1.4
The accuracy performance of the proposed model.
Table 1.3 Comparison of the specificity of the proposed method with other machine learning.
Models 2000 4000 6000 8000 10,000
KNN 85% 90% 87% 91% 90%
SVM 72% 75% 73% 78% 83%Random Forest 93% 93% 92% 93% 95%Naı¨ve Bayes 76% 80% 77% 85% 90%
Proposed model 94% 97% 99% 96% 99%17 1.6 Case study for the application of edge-IoMT-basedwith an accuracy of 95%, and the lowest of the five models is the SVM with an accuracy of 83%.
This is due to the introduction of edge computing to boost the cloud paradigm by removing timeconstraints during classification and reduce latency.
From
Table 1.4 and Fig. 1.6 , the results of the sensitivity show that the proposed LSTM per-
formed better when compared with the existing methods with an accuracy of 96%, and the lowestof the five models is the SVM with an accuracy of 75%. This is due to the introduction of edgecomputing to boost the cloud paradigm by removing time constraints during classification and
reduce latency.
1.7 Future prospects of edge computing for internet of medical things
The edge technology “Mobile Edge Computing (MEC)” created by European Telecommunications
Standards Institute (ETSI) used 4G, 5G, and Radio Access Networks (RANs) as the main target0%20%40%60%80%100%120%
2000 4000 6000 8000 10000SPECIFICITY PERFORMANCE
KNN SVM Random Forest Naïve Bayes Proposed Model
FIGURE 1.5
The specificity performance of the proposed model.18 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare system(Beck, Werner, Feld, & Schimper, 2014; Klas, 2015 ). MEC offers edge computing in the smart
healthcare system at the processing resources base stations, and by recommending a collocation ofstorage. Initiated by Microsoft, the Micro Data Centers (MDCs) by expanding cloud data centerswith small-sized versions of data centers (
Avelar, 2015 ), hence, expand the cloud services close to
the end-users. The cloudlet definition is very close to MDC as defined by The Carnegie MellonTable 1.4 Comparison of the sensitivity of the proposed method with other machine learning.
Models 2000 4000 6000 8000 10,000
KNN 83% 87% 84% 87% 87%
SVM 68% 72% 69% 74% 75%Random Forest 87% 87% 85% 86% 91%Naı¨ve Bayes 72% 78% 73% 81% 88%
Proposed model 89% 87% 95% 93% 96%
0%20%40%60%80%100%120%
2000 4000 6000 8000 10000SENSITIVITY PERFORMANCE
KNN SVM Random Forest Naïve Bayes Proposed Model
FIGURE 1.6
The sensitivity performance of the proposed model.19 1.7 Future prospects of edge computing for internet of medical thingsUniversity (CMU), where the virtualized data center was very small, and in distributed fashion,
serves users near the edge ( Bilal, Khalid, Erbad, & Khan, 2018; Ren, Zhang, He, Zhang, & Li,
2019; Satyanarayanan, Bahl, Caceres, & Davies, 2009 ). These terms are also related to other simi-
lar objects and concepts like Nano-data centers ( Valancius, Laoutaris, Massouli ´e, Diot, &
Rodriguez, 2009 ).
Edge technologies can work together and cooperate, even though various edge technologies are
defined independently ( Borcoci, 2016 ). Taking into account the futuristic aspects of the IoMT-
based structure and recent developments in technology collaboration, such as HeterogeneousNetworks (HetNets) and interconnected Content Delivery networks (CDNs). The IoEs can be effi-cient and more effective using various edge technologies working together. The IDC estimated that45% of the IoT-produced data would be processed, stored, and analyzed on the edge by the year2019 (
Klonoff, 2017; Mutlag, Abd Ghani, Arunkumar, Mohammed, & Mohd, 2019 ).
Edge computing is still in its infancy stages with no standardized concepts, designs, and proprie-
ties. Edge technologies are characterized by different researchers from their viewpoints that are
required for non-unvarying skills. This pattern was also established in cervical cancer (CC) beforethe National Institute of Science and Technology (NIST) standardization of the official concept ofcloud computing in 2011 (
Langmead & Nellore, 2018 ). In the relationship between edge technolo-
gies, IoT, and the cloud, the lack of a common description contributes to misconceptions. In the lit-erature, where authors say that edge computing technologies can “move” or “replace” the cloudwith fog or decentralize the cloud paradigm to edges, examples of such misconceptions are listed.Cloud computing is not enough for the processing and storage of IoT-based big data generated,
thus these have to be a move to the edge of the network for the data processing (
Naranjo et al.,
2016 ). The innovations brought in by edge computing should be seen as a replacement for the
cloud model, because this was just created as supplementary agents for cloud services, and to helpin expanding cloud services for real-time applications are to meet (
Sarkar & Misra, 2016 ). In big
data analytics, the cloud is necessary for resource-intensive batch tasks and long-time processing ofcapture data using IoT-based devices. Similarly, various authors regard edge computing as microda-ta centers, and the understanding and perceiving of edge technologies are often confounding(
Aazam & Huh, 2015 ), while others concentrate specifically on the concept of improving and
equipping additional processing and storage capacities for networking components ( Sarkar &
Misra, 2016 ).
The use of edge computing in the processing of huge amounts of data that IoT devices produce
would provide strategies for big data analysis at the edge of the network to provide real-time dataprocessing. Edge computing has brought elastic on-demand for the processing of big data locallywithout necessarily send the capture data to the cloud, thus removed the problems of higher latencyand bandwidth consumption. Big data collection, aggregation, and preprocessing can be handled bya mix of edge and cloud computing, minimizing data transport and cloud storage. For example, at
the edge layer, local data can be collected and processed for an environmental surveillance system
to provide timely input in emergency circumstances.
As multimedia demands more bandwidth, processing, and memory, it is a difficult task to man-
age such huge amounts in terms of decision making, processing, and storage (
Aazam & Huh,
2016 ). Edge computing is intended to help reduce the total end-to-end use, delivery, efficient func-
tioning, and processing of multimedia resources in such situations ( Bilal & Erbad, 2017; Chen,
Chen, et al., 2016; Chen, Zhang, et al., 2015 ). High costs are also incurred through multimedia20 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemdistribution. CDNs, such as CloudFront, charge high prices when considering the provision of Tbps
data. YouTube Live and Twitch have been reported to have reached 1 Tbps during peak hours in2014 (
Pires & Simon, 2015 ). The study conducted using Twitch trace analysis has been stated to
have exceeded the distribution of 1.5 Tbps of video content to viewers worldwide ( Chen, Jiang,
et al., 2015 ). It must also be taken into account that any word improves Internet availability and
data speeds, which means higher consumer access to data.
Also, the combination of cloud and edge computing in building a robust IoMT-based system
will help in advancing a strong and fast processing system in a smart healthcare system. Also, theultra-responsiveness and ultra-reliability need algorithms running in the device to guarantee touchapplications. The edge system provided an enabling platform for running various tasks related toTactile Internet applications in edges or cloud layers. There is a need for novel algorithms for taskplanning due to network traffic that may occur on the computing nodes with other variables wheretasks are performed and delay sometimes will far surpass the 1 sm threshold. This is necessary to
ensure that activities are carried out within the cumulative threshold and are not surpassed 1 Ms.
To have optimal task scheduling, novel ML and artificial intelligence algorithms are required. Topredict behavior and reactions, they may run exclusively in the edge stratum or even in a distrib-uted manner through a cloud and/or edge strata. Edge computing helps to reduce the total networkload during network connection in the IoMT-based platform, and this will help meet the latencyrequirement of 1 Ms. A various sophisticated algorithm, like neural network-based techniques andsimple regression models, can be considered in modeling the system (
Firouzi, Farahani, Barzegari,
& Daneshmand, 2020; Sakr, Georganas, Zhao, & Shen, 2007 ).
1.8 Conclusions and future research directions
The emergence of cloud computing has touched almost all human life domains, especially smart
healthcare system, which have greatly benefited from the paradigm. Cloud computing has broughta technological revolution needed by IoT-based services like high processing, storage capabilities,
heterogeneity, and computation resources among others. Nevertheless, the various increase in
IoMT-based devices enabled and application in the smart healthcare system in real-time responserequire technologies that will replace the traditional cloud paradigm with several challenges likecomputation, scalability, and latency. Edge computing can be applying to overcome the about pro-blems. Edge computing technology with lower latency, high computational paradigm, and scalabil-ity will help IoMT-based system devices in real-time data collection and processing. Thedeployment of cloud data is far away from the network; this causes the response time delay in real-time. Moreover, the cloud may cause significant overhead on the backbone network to the user
application due to the huge amount of big data sent to the cloud. Hence, the application of edge
computing will bring the storage resources and computational closer to the end-user devices, thusreduce the burden on the cloud. The edge computing architecture is heterogeneous devices, andgeographically distributed ubiquitously connected at the end of a network to provide collaborativelyvariable and flexible communication, storage services, and computation. Edge computing has vari-ous characteristics that place it in a better position when compared with cloud computing like lowlatency, real-time process, high level of the security model, and high response time. Therefore, this21 1.8 Conclusions and future research directionschapter presents a general review of edge computing technology in the smart healthcare system.
The applicability of edge on smart healthcare systems is discussed, the challenges and the prospectsof edge computing are elaborately discussed. The edge IoMT-based system computing presents bet-ter infrastructure by providing low latency, distributed processing, better security, fault tolerance,and good privacy when compared with the cloud IoMT-based infrastructure. The edge IoMT-based
sufficiently produces various nodes, virtualized data centers, and edge device networks to connect
the IoT-based devices to implement large storage and rich cloud computing. Edge computing offersmillisecond to sub-second latency faster than real-time interaction, performs better in terms of low-latency applications, and supports multitenancy where cloud computing cannot have provided. Infuture work, the prospects of edge models will be extended in the smart healthcare system to pro-vide a quick and real-time health diagnosis and monitoring for patients suffering from any diseases.Finally, edge computing is very necessary because the cloud’s recorded healthcare data may besubject to different types of security risks.
References
Aazam, M., & Huh, E. N. (2015, March). Dynamic resource provisioning through fog micro datacenter. In
2015 IEEE international conference on pervasive computing and communication workshops (PerCom
workshops) (pp. 105 /C0110). IEEE.
Aazam, M., & Huh, E. N. (2016). Fog computing: The cloud-IoT/IoE middleware paradigm. IEEE Potentials ,
35(3), 40 /C044.
Abdulraheem, M., Awotunde, J. B., Jimoh, R. G., & Oladipo, I. D. (2021). An efficient lightweight crypto-
graphic algorithm for IoT security. Communications in Computer and Information Science ,1350 ,
444/C0456.
Abdulsalam, Y., & Hossain, M. S. (2020). Covid-19 networking demand: An auction-based mechanism for
automated selection of edge computing services. IEEE Transactions on Network Science and Engineering .
Adeniyi, E. A., Ogundokun, R. O., & Awotunde, J. B. (2021). IoMT-based wearable body sensors network
healthcare monitoring system .IoT in healthcare and ambient assisted living (pp. 103 /C0121). Singapore:
Springer.
Albahri, A. S., Alwan, J. K., Taha, Z. K., Ismail, S. F., Hamid, R. A., Zaidan, A. A., ...Alsalem, M. A.
(2020). IoT-based telemedicine for disease prevention and health promotion: State-of-the-art. Journal of
Network and Computer Applications ,173, 102873.
Aldaej, A., & Tariq, U. (2018, April). IoT in 5G aeon: An inevitable fortuity of next-generation healthcare. In
2018 1st International conference on computer applications & information security (ICCAIS) (pp. 1 /C04).
IEEE.
Alhamid, M. F., Rawashdeh, M., Al Osman, H., Hossain, M. S., & El Saddik, A. (2015). Towards context-
sensitive collaborative media recommender system. Multimedia Tools and Applications ,74(24),
11399 /C011428.
Al-Turjman, F., Nawaz, M. H., & Ulusar, U. D. (2020). Intelligence in the Internet of medical things era: A
systematic review of current and future trends. Computer Communications ,150, 644 /C0660.
Athavale, Y., & Krishnan, S. (2020). A telehealth system framework for assessing knee-joint conditions using
vibroarthrographic signals. Biomedical Signal Processing and Control ,55, 101580.
Avelar, V. (2015). Practical options for deploying small server rooms and micro data centers. Schneider
Electric , 174, White paper.22 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemAwotunde, J. B., Bhoi, A. K., & Barsocchi, P. (2021). Hybrid cloud/fog environment for healthcare: An
exploratory study, opportunities, challenges, and future prospects. Hybrid Artificial Intelligence and IoT in
Healthcare (pp. 1 /C020). Singapore: Springer.
Awotunde, J. B., Jimoh, R. G., Oladipo, I. D., & Abdulrah eem, M. (2021). Prediction of malaria fever using long-
short-term memory and big data. Communications in Computer and Information Science ,1350 ,4 1/C053.
Awotunde, J. B., Matiluko, O. E., & Fatai, O. W. (2014). Medical diagnosis system using fuzzy logic. African
Journal of Computing & ICT ,7(2), 99 /C0106.
Ayo, F. E., Awotunde, J. B., Ogundokun, R. O., Folorunso, S. O., & Adekunle, A. O. (2020). A decision sup-
port system for multi-target disease diagnosis: A bioinformatics approach. Heliyon ,6(3), e03657.
Ayo, F. E., Ogundokun, R. O., Awotunde, J. B., Adebiyi, M. O., & Adeniyi, A. E. (2020, July). Severe acne
skin disease: A fuzzy-based method for diagnosis . Lecture Notes in Computer Science (including subseries
Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12254 LNCS, pp. 320 /C0334.
Baker, S. B., Xiang, W., & Atkinson, I. (2017). Internet of things for smart healthcare: Technologies, chal-
lenges, and opportunities. IEEE Access ,5, 26521 /C026544.
Beck, M. T., Werner, M., Feld, S., & Schimper, S. (2014, November). Mobile edge computing: A taxonomy.
InProceedings of the sixth international conference on advances in future internet (pp. 48 /C055). Citeseer.
Bhatia, M. (2020). Fog computing-inspired smart home framework for predictive veterinary healthcare.
Microprocessors and Microsystems ,78, 103227.
Bilal, K., & Erbad, A. (2017, April). Impact of multiple video representations in live streaming: A cost, band-
width, and QoE analysis. In 2017 IEEE international conference on cloud engineering (IC2E)
(pp. 88 /C094). IEEE.
Bilal, K., Khalid, O., Erbad, A., & Khan, S. U. (2018). Potentials, trends, and prospects in edge technologies:
Fog, cloudlet, mobile edge, and micro data centers. Computer Networks ,130,9 4/C0120.
Bonomi, F., Milito, R., Zhu, J., & Addepalli, S. (2012, August). Fog computing and its role in the internet of
things. In Proceedings of the first edition of the MCC workshop on mobile cloud computing (pp. 13 /C016).
Borcoci, E. (2016, August). Fog computing, mobile edge computing, cloudlets-which one. In SoftNet confer-
ence (pp. 1 /C0122).
Chen, F., Zhang, C., Wang, F., Liu, J., Wang, X., & Liu, Y. (2015). Cloud-assisted live streaming for crowd-
sourced multimedia content. IEEE Transactions on Multimedia ,17(9), 1471 /C01483.
Chen, N., Chen, Y., You, Y., Ling, H., Liang, P., & Zimmermann, R. (2016, April). Dynamic urban surveil-
lance video stream processing using fog computing. In 2016 IEEE second international conference on mul-
timedia big data (BigMM) (pp. 105 /C0112). IEEE.
Chen, S. C. I., Hu, R., & McAdam, R. (2020). Smart, remote, and targeted health care facilitation through con-
nected health: Qualitative study. Journal of Medical Internet Research ,22(4), e14201.
Chen, S. W., Chiang, D. L., Liu, C. H., Chen, T. S., Lai, F., Wang, H., & Wei, W. (2016). Confidentiality pro-
tection of digital health records in cloud computing. Journal of Medical Systems ,40(5), 124.
Chen, Z., Jiang, L., Hu, W., Ha, K., Amos, B., Pillai, P., ...Satyanarayanan, M. (2015, May). Early imple-
mentation experience with wearable cognitive assistance applications. In Proceedings of the 2015 work-
shop on wearable systems and applications (pp. 33 /C038).
Chui, K. T., Liu, R. W., Lytras, M. D., & Zhao, M. (2019). Big data and IoT solution for patient behaviour
monitoring. Behaviour & Information Technology ,38(9), 940 /C0949.
Devarajan, M., Subramaniyaswamy, V., Vijayakumar, V., & Ravi, L. (2019). Fog-assisted personalized
healthcare-support system for remote patients with diabetes. Journal of Ambient Intelligence and
Humanized Computing ,10(10), 3747 /C03760.
Dong, P., Ning, Z., Obaidat, M. S., Jiang, X., Guo, Y., Hu, X., ...Sadoun, B. (2020). Edge computing-based
healthcare systems: Enabling decentralized health monitoring in Internet of Medical Things. IEEE
Network .23 ReferencesEdemacu, K., Park, H. K., Jang, B., & Kim, J. W. (2019). Privacy provision in collaborative ehealth with
attribute-based encryption: Survey, challenges, and future directions. IEEE Access ,7, 89614 /C089636.
Fernandez, F., & Pallis, G. C. (2014, November). Opportunities and challenges of the Internet of Things for
healthcare: Systems engineering perspective. In 2014 4th International conference on wireless mobile com-
munication and healthcare-transforming healthcare through innovations in mobile and wireless technolo-
gies (MOBIHEALTH) (pp. 263 /C0266). IEEE.
Firouzi, F., Farahani, B., Barzegari, M., & Daneshmand, M. (2020). AI-driven data monetization: The other
face of data in IoT-based smart and connected health. IEEE Internet of Things Journal .
Firouzi, F., Rahmani, A. M., Mankodiya, K., Badaroglu, M., Merrett, G. V., Wong, P., & Farahani, B. (2018).
Internet-of-Things and big data for smarter healthcare: From device to architecture, applications, andanalytics.
Ghanavati, S., Abawajy, J. H., Izadi, D., & Alelaiwi, A. A. (2017). Cloud-assisted IoT-based health status
monitoring framework. Cluster Computing ,20(2), 1843 /C01853.
Ghoneim, A., Muhammad, G., Amin, S. U., & Gupta, B. (2018). Medical image forgery detection for smart
healthcare. IEEE Communications Magazine ,56(4), 33 /C037.
Gu, L., Zeng, D., Guo, S., Barnawi, A., & Xiang, Y. (2015). Cost-efficient resource management in fog com-
puting supported medical cyber-physical systems. IEEE Transactions on Emerging Topics in Computing ,5
(1), 108 /C0119.
Guy, J. S. (2019). Digital technology, digital culture, and the metric/nonmetric distinction. Technological
Forecasting and Social Change ,145,5 5/C061.
Hossain, M. S., & Muhammad, G. (2017). Emotion-aware connected healthcare big data towards 5G. IEEE
Internet of Things Journal ,5(4), 2399 /C02406.
Hossain, M. S., Rahman, M. A., & Muhammad, G. (2017). Cyber-physical cloud-oriented multi-sensory smart
home framework for elderly people: An energy efficiency perspective. Journal of Parallel and Distributed
Computing ,103,1 1/C021.
Hussain, F., Hussain, R., Hassan, S. A., & Hossain, E. (2020). Machine learning in IoT security: Current solu-
tions and future challenges. IEEE Communications Surveys & Tutorials .
Janet, B., & Raj, P. (2019). Smart city applications: The smart leverage of the internet of things (IoT) para-
digm .Novel practices and trends in grid and cloud computing (pp. 274 /C0305). IGI Global.
Jin, Z., & Chen, Y. (2015). Telemedicine in the cloud era: Prospects and challenges. IEEE Pervasive
Computing ,14(1), 54 /C061.
Jones, R. W., & Katzis, K. (2018, April). 5G and wireless body area networks. In 2018 IEEE wireless commu-
nications and networking conference workshops (WCNCW) (pp. 373 /C0378). IEEE.
Klas, G. I. (2015). Fog computing and mobile edge cloud gain momentum open fog consortium, ETSI mec,
and cloudlets . Google Scholar.
Klonoff, D. C. (2017). Fog computing and edge computing architectures for processing data from diabetes
devices connected to the medical internet of things. Journal of Diabetes Science and Technology ,11(4),
647/C0652.
Kumar, P. M., Lokesh, S., Varatharajan, R., Babu, G. C., & Parthasarathy, P. (2018). Cloud and IoT-based dis-
ease prediction and diagnosis system for healthcare using Fuzzy neural classifier. Future Generation
Computer Systems ,86, 527 /C0534.
Kumari, A., Tanwar, S., Tyagi, S., & Kumar, N. (2018a). Fog computing for Healthcare 4.0 environment:
Opportunities and challenges. Computers & Electrical Engineering ,72,1/C013.
Kumari, A., Tanwar, S., Tyagi, S., & Kumar, N. (2018b). Verification and validation techniques for streaming
big data analytics in internet of things environment. IET Networks ,8(2), 92 /C0100.
Langmead, B., & Nellore, A. (2018). Cloud computing for genomic data analysis and collaboration. Nature
Reviews Genetics ,19(4), 208.24 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemLin, K., Song, J., Luo, J., Ji, W., Hossain, M. S., & Ghoneim, A. (2016). Green video transmission in the
mobile cloud networks. IEEE Transactions on Circuits and Systems for Video Technology ,27(1),
159/C0169.
Lin, P., Song, Q., & Jamalipour, A. (2019). Multidimensional cooperative caching in CoMP-integrated ultra-
dense cellular networks. IEEE Transactions on Wireless Communications ,19(3), 1977 /C01989.
Lu, L., Zhang, J., Xie, Y., Gao, F., Xu, S., Wu, X., & Ye, Z. (2020). Wearable health devices in health care:
Narrative systematic review. JMIR mHealth and uHealth ,8(11), e18907.
Magsi, H., Sodhro, A. H., Chachar, F. A., Abro, S. A. K., Sodhro, G. H., & Pirbhulal, S. (2018, March).
Evolution of 5G in Internet of medical things. In 2018 International conference on computing, mathemat-
ics, and engineering technologies (iCoMET) (pp. 1 /C07). IEEE.
Marston, H. R., & van Hoof, J. (2019). Who doesn’t think about technology when designing urban environ-
ments for older people?” A case study approach to a proposed extension of the WHO’s age-friendly cities
model. International Journal of Environmental Research and Public Health ,16(19), 3525.
Medaglia, C. M., & Serbanati, A. (2010). An overview of privacy and security issues on the internet of things .
The internet of things (pp. 389 /C0395). New York: Springer.
Mesko ´, B., Drobni, Z., B ´enyei, E ´., Gergely, B., & Gy ˝orffy, Z. (2017). Digital health is a cultural transforma-
tion of traditional healthcare. Mhealth ,3.
Mouradian, C., Naboulsi, D., Yangui, S., Glitho, R. H., Morrow, M. J., & Polakos, P. A. (2017). A comprehen-
sive survey on fog computing: State-of-the-art and research challenges. IEEE Communications Surveys &
Tutorials ,20(1), 416 /C0464.
Muhammad, G., Alhamid, M. F., & Long, X. (2019). Computing and processing on the edge: Smart pathology
detection for connected healthcare. IEEE Network ,33(6), 44 /C049.
Muhammed, T., Mehmood, R., Albeshri, A., & Katib, I. (2018). UbeHealth: A personalized ubiquitous cloud
and edge-enabled networked healthcare system for smart cities. IEEE Access ,6, 32258 /C032285.
Mutlag, A. A., Abd Ghani, M. K., Arunkumar, N. A., Mohammed, M. A., & Mohd, O. (2019). Enabling tech-
nologies for fog computing in healthcare IoT systems. Future Generation Computer Systems ,90,6 2/C078.
Nandyala, C. S., & Kim, H. K. (2016). From cloud to fog and IoT-based real-time U-healthcare monitoring for
smart homes and hospitals. International Journal of Smart Home ,10(2), 187 /C0196.
Naranjo, P. G. V., Shojafar, M., Vaca-Cardenas, L., Canali, C., Lancellotti, R., & Baccarelli, E. (2016,
September). Big data over SmartGrid-a fog computing perspective. In Proceedings of the 24th interna-
tional conference on software, telecommunications and computer networks (SoftCOM 2016) (pp. 22 /C024).
Split, Croatia.
Ning, Z., Dong, P., Wang, X., Hu, X., Guo, L., Hu, B., ...Kwok, R. Y. (2020). Mobile edge computing
enabled 5G health monitoring for Internet of medical things: A decentralized game-theoretic approach.
IEEE Journal on Selected Areas in Communications ,1/C016.
Ning, Z., Wang, X., & Huang, J. (2018). Mobile edge computing-enabled 5G vehicular networks: Toward the
integration of communication and computing. IEEE Vehicular Technology Magazine ,14(1), 54 /C061.
Ogundokun, R. O., Sadiku, P. O., Misra, S., Ogundokun, O. E., Awotunde, J. B., & Jaglan, V. (2021).
Diagnosis of long sightedness using neural network and decision tree algorithms. Journal of Physics:
Conference Series ,1767 (1), 012021.
Oladele, T. O., Ogundokun, R. O., Awotunde, J. B., Adebiyi, M. O., & Adeniyi, J. K. (2020, July).
Diagmal: A malaria coactive neuro-fuzzy expert system . Lecture Notes in Comput er Science (including
subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12254 LNCS,
pp. 428 /C0441.
Oladipo, I. D., Babatunde, A. O., Awotunde, J. B., & Abdulraheem, M. (2021). An improved hybridization in
the diagnosis of diabetes mellitus using selected computational intelligence. Communications in Computer
and Information Science ,1350 , 272 /C0285.25 ReferencesPace, P., Aloi, G., Gravina, R., Caliciuri, G., Fortino, G., & Liotta, A. (2018). An edge-based architecture to
support efficient applications for the healthcare industry 4.0. IEEE Transactions on Industrial Informatics ,
15(1), 481 /C0489.
Pan, J., & McElhannon, J. (2017). Future edge cloud and edge computing for the internet of things applica-
tions. IEEE Internet of Things Journal ,5(1), 439 /C0449.
Parsa, M., Panda, P., Sen, S., & Roy, K. (2017, July). Staged inference using conditional deep learning for
energy-efficient real-time smart diagnosis. In 2017 39th Annual international conference of the IEEE engi-
neering in medicine and biology society (EMBC) (pp. 78 /C081). IEEE.
Pazowski, P. (2015). Green computing: Latest practices and technologies for ICT sustainability. In Managing
intellectual capital and innovation for sustainable and inclusive society: Managing intellectual capital andinnovation; Proceedings of the MakeLearn and TIIM joint international conference 2015
(pp. 1853 /C01860). ToKnowPress.
Petri, I., Kubicki, S., Rezgui, Y., Guerriero, A., & Li, H. (2017). Optimizing energy efficiency in operating
built environment assets through building information modeling: A case study. Energies ,10(8), 1167.
Pham, M., Mengistu, Y., Do, H., & Sheng, W. (2018). Delivering home healthcare through a cloud-based
smart home environment (CoSHE). Future Generation Computer Systems ,81, 129 /C0140.
Pires, K., & Simon, G. (2015, March). YouTube live and twitch: A tour of user-generated live streaming sys-
tems. In Proceedings of the 6th ACM multimedia systems conference (pp. 225 /C0230).
Pustokhina, I. V., Pustokhin, D. A., Gupta, D., Khanna, A., Shankar, K., & Nguyen, G. N. (2020). An effective
training scheme for a deep neural network in edge computing enabled Internet of medical things (IoMT)
systems. IEEE Access ,8, 107112 /C0107123.
Puthal, D., Obaidat, M. S., Nanda, P., Prasad, M., Mohanty, S. P., & Zomaya, A. Y. (2018). Secure and sus-
tainable load balancing of edge data centers in fog computing. IEEE Communications Magazine ,56(5),
60/C065.
Rahman, M. A., Hossain, M. S., Islam, M. S., Alrajeh, N. A., & Muhammad, G. (2020). Secure and prove-
nance enhanced internet of health things framework: A blockchain managed federated learning approach.IEEE Access ,8, 205071 /C0205087.
Regola, N., & Chawla, N. V. (2013). Storing and using health data in a virtual private cloud. Journal of
Medical Internet Research ,15(3), e63.
Rehman, H. U., Khan, A., & Habib, U. (2020). Fog computing for bioinformatics applications. Fog
Computing: Theory and Practice , 529 /C0546.
Ren, J., Zhang, D., He, S., Zhang, Y., & Li, T. (2019). A survey on end-edge-cloud orchestrated network com-
puting paradigms: Transparent computing, mobile edge computing, fog computing, and cloudlet. ACM
Computing Surveys (CSUR) ,52(6), 1 /C036.
Rodrigues, J. J., Segundo, D. B. D. R., Junqueira, H. A., Sabino, M. H., Prince, R. M., Al-Muhtadi, J., & De
Albuquerque, V. H. C. (2018). Enabling technologies for the internet of health things. IEEE Access ,6,
13129 /C013141.
S Rubı ´, J. N., & L Gondim, P. R. (2019). IoMT platform for pervasive healthcare data aggregation, processing,
and sharing based on OneM2M and OpenEHR. Sensors ,19(19), 4283.
Saad, M. (2018). Fog computing and its role in the internet of things: Concept, security and privacy issues.
International Journal of Computer Applications ,975, 8887.
Saeed, N., Bader, A., Al-Naffouri, T. Y., & Alouini, M. S. (2020). When wireless communication faces
COVID-19: Combating the pandemic and saving the economy. arXiv preprint arXiv:2005.06637.
Sakr, N., Georganas, N. D., Zhao, J., & Shen, X. (2007, July). Motion and force prediction in haptic media. In
2007 IEEE international conference on multimedia and expo (pp. 2242 /C02245). IEEE.
Sarkar, S., & Misra, S. (2016). Theoretical modeling of fog computing: A green computing paradigm to sup-
port IoT applications. IET Networks ,5(2), 23 /C029.26 Chapter 1 Edge-IoMT-based enabled architecture for smart healthcare systemSatyanarayanan, M., Bahl, P., Caceres, R., & Davies, N. (2009). The case for VM-based cloudlets in mobile
computing. IEEE Pervasive Computing ,8(4), 14 /C023.
Schweitzer, E. J. (2012). Reconciliation of the cloud computing model with US federal electronic health record
regulations. Journal of the American Medical Informatics Association ,19(2), 161 /C0165.
Sodhro, A. H., & Shah, M. A. (2017, April). Role of 5G in medical health. In 2017 International conference
on innovations in electrical engineering and computational technologies (ICIEECT) (pp. 1 /C05). IEEE.
Song, M. L., Fisher, R., Wang, J. L., & Cui, L. B. (2018). Environmental performance evaluation with big
data: Theories and methods. Annals of Operations Research ,270(1/C02), 459 /C0472.
Sundwall, D. N., Munger, M. A., Tak, C. R., Walsh, M., & Feehan, M. (2020). Lifetime prevalence and corre-
lates of patient-perceived medical errors experienced in the US ambulatory setting: A population-basedstudy. Health Equity ,4(1), 430 /C0437.
Sust, P. P., Solans, O., Fajardo, J. C., Peralta, M. M., Rodenas, P., Gabalda `, J., ...Monfa, R. R. (2020).
Turning the crisis into an opportunity: Digital health strategies deployed during the COVID-19 outbreak.JMIR Public Health and Surveillance ,6(2), e19106.
Syed, L., Jabeen, S., Manimala, S., & Alsaeedi, A. (2019). Smart healthcare framework for ambient assisted
living using IoMT and big data analytics techniques. Future Generation Computer Systems ,101, 136 /C0151.
Tripathi, G., Ahad, M. A., & Paiva, S. (2020). S2HS-A blockchain-based approach for the smart healthcare
system ,.Healthcare (8, No. 1, p. 100391). Elsevier.
Tuli, S., Basumatary, N., Gill, S. S., Kahani, M., Arya, R. C., Wander, G. S., & Buyya, R. (2020). Healthfog:
An ensemble deep learning-based smart healthcare system for automatic diagnosis of heart diseases in inte-
grated IoT and fog computing environments. Future Generation Computer Systems ,104, 187 /C0200.
Valancius, V., Laoutaris, N., Massouli ´e, L., Diot, C., & Rodriguez, P. (2009, December). Greening the internet
with nano data centers. In Proceedings of the 5th international conference on emerging networking experi-
ments and technologies (pp. 37 /C048).
Vizitiu, A., Ni¸ t˘a, C. I., Puiu, A., Suciu, C., & Itu, L. M. (2020). Applying deep neural networks over homo-
morphic encrypted medical data. Computational and Mathematical Methods in Medicine ,2020 .
Wen, F., He, T., Liu, H., Chen, H. Y., Zhang, T., & Lee, C. (2020). Advances in chemical sensing technology
for enabling the next-generation self-sustainable integrated wearable system in the IoT era. Nano Energy ,
105155.
Xu, X., Liu, Q., Luo, Y., Peng, K., Zhang, X., Meng, S., & Qi, L. (2019). A computation offloading method
over big data for IoT-enabled cloud-edge computing. Future Generation Computer Systems ,95, 522 /C0533.
Yi, S., Li, C., & Li, Q. (2015, June). A survey of fog computing: Concepts, applications, and issues. In
Proceedings of the 2015 workshop on mobile big data (pp. 37 /C042).
Zafar, S., Khan, S., Iftekhar, N., & Biswas, S. (2020). Consociate healthcare system through biometric based
internet of medical things (BBIOMT) approach. EAI Endorsed Transactions on Smart Cities ,4(10).27 ReferencesThis page intentionally left blankCHAPTER
2Physical layer architecture of 5G
enabled IoT/IoMT system
Anh-Tu Le1, Munyaradzi Munochiveyi2and Samarendra Nath Sur3
1Faculty of Electronics Technology, Industrial University of Ho Chi Minh City, Ho Chi Minh City, Vietnam
2Department of Electrical and Electronics Engineering, University of Zimbabwe, Harare, Zimbabwe3Department of
Electronics and Communication Engineering, Sikkim Manipal Institute of Technology, Sikkim Manipal University,
Majitar, Rangpo, Sikkim, India
2.1 Architecture of IoT/IoMT system
The healthcare industry has changed rapidly as life expectancy has increased in step with rapid
development and wealth. This has not come without its own unique challenges as more cases ofchronic disease have also escalated, putting more pressure on already resource strained healthcaresystems. This has seen the rise of telemedicine, as hospitals try to decongest and find innovativeways to deliver care without increasing hospitals and hospital beds. To reduce patient overload,telemedicine has become an essential innovation in the healthcare field. However, telemedicine is
heterogeneous in design as it is designed to monitor a single disease at a time, for example, remote
stroke monitoring and management, etc. This design feature ends up being a design flaw as it doesnot reduce the number of patients as diseases increase. Therefore there is need for a solution that isgeneralizable and scalable. Internet-of-Medical Things (IoMT) is deemed as a promising solutionto better tackle this unique healthcare challenge (
Gatouillat, Badr, Massot, & Sejdi ´c, 2018 ).
The rapid advancements in microelectromechanical systems (MEMS) and machine-to-machine
(M2M) communications has resulted in new Internet-of-Things (IoT) concepts applicable to manynetworking applications. IoMT for healthcare systems are among the applications that have been
introduced with the emergence of IoT. IoMT is a new branch of the family of healthcare IoT
devices that allow remote monitoring of chronic diseased patients. Therefore quick diagnosticresults can be provided which can save patient’s lives in emergencies (
Ghubaish et al., 2021 ).
According to ( Ghubaish et al., 2021 ), IoMTs can be classified into two categories:
1.Implanted medical things (IMTs): A device that is implanted into the human body to support or
replace a biological organ is defined as an IMT. Pacemakers, cochlear implants, deep brainstimulators, etc., are among some of the commonly used IMTs as shown in
Fig. 2.1 . Due to
their location in the human body, IMTs are mostly designed to be very small and have verylong battery life. Therefore energy consumption, is an important consideration, if the IMT isintended to stay inside the human body for a very long time (
Ghubaish et al., 2021 ).
2.Wearable Internet-of-Things (WIoT): These are devices that monitor individuals biometrics as
they go about their daily routine, for example, smartwatches, electrocardiogram (ECG)
295G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00009-7
©2022 Elsevier Inc. All rights reserved.monitors, and blood pressure monitors as shown in Fig. 2.1 . Smartwatches are one of the most
popular noncritical form of monitoring for heart rate and movement. However, these devicesare limited by sensor inaccuracy and poor battery life; hence, are not likely to replace IMTsfrom monitoring critical conditions (
Ghubaish et al., 2021 ).
Many researchers have studied and proposed different IoT architectures such as machine-to-
machine (M2M), web-of-things, autonomous and sensor based architecture. M2M architecture isthe most widely used IoT architecture, and IoMT is the prevalent representation in the field of med-ical IoT devices (
Sun, Jiang, Ren, & Guo, 2020 ). The current IoMT systems can be categorized
into four layers, as shown in Fig. 2.2 . The sensor layer starts with the collection of biometric data
by IMTs sensors, then the data is transmitted over the physical link layer to a gateway host whichhandles some of sensor data preprocessing and simple analytics. The processed sensor data is thentransmitted to the network layer via the Internet. The network layer is responsible for storage, anal-
ysis and secure access. Finally, the visualization layer is where all the data is analyzed by a physi-
cian. The patient can also visualize their health status via this layer. This is better than having toconstantly carry different sets of radiology scans and blood tests from one physician to another.
FIGURE 2.1
IoMTs and their placement in and on the human body.30 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemMost IoMTs share the same architecture as seen in Fig. 2.2 (Ghubaish et al., 2021 ). In the follow-
ing subsections, we zoom into the individual layers in greater detail.
2.1.1 Sensor layer
This layer is composed of wearables and small implantable sensors that collect the patient’s health
data. The biometric data is transmitted to the next layer via short-distance wireless technology,such as Wi-Fi, ZigBee, Bluetooth, etc. (
Ghubaish et al., 2021; Sun et al., 2020 ).
2.1.2 Gateway layer
Due to the small form factor of IoMTs, processing and storage is not possible; therefore the data
are transmitted as unprocessed to the gateway layer. This layer consists of devices that are morepowerful in general than the IoMT sensors such as the patient’s smartphone or smart watch, dedi-cated access points (APs), and microcontrollers. These devices can also serve as edge nodes in
FIGURE 2.2
Overview of IoMT architecture.31 2.1 Architecture of IoT/IoMT systemedge computing. These devices can perform some computationally light preprocessing operations,
such as data storage, and simple processing using machine learning (ML) or deep learning (DL) togain insight into hidden data patterns as well as diagnose and alert critical health conditions as seenin in
Fig. 2.3 (Amin & Hossain, 2021 ). Thereafter, the data is transferred to the cloud via the
Internet for intensive processing and classification ( Ghubaish et al., 2021; Sun et al., 2020 ).
2.1.3 Network layer
Sophisticated data storage, big data analysis, and secure access is carried out in the network/cloud
layer. Advanced ML and DL are employed to analyze the streaming data from the IoMT sensors.
The analysis reveals any changes in the patient’s health and presents them to the health practitioneror patient for immediate action. However, cloud computing alone is not adequate in copying withthe large volumes of uploaded medical data to the cloud, as this results in huge delays due to band-width bottlenecks. Such a situation is undesirable in healthcare monitoring, where visualization andlatency must meet real-time constraints to prevent delays in addressing medical emergencies.Moreover, the data transmission leads to high energy consumption. Several works have studiedsolutions to expand the ability of cloud IoMT. Authors in (
Limaye & Adegbija, 2018 ) investigate
the impact of moving the computation closer to the sensor layer by introducing edge computing in
the gateway layer as seen in Fig. 2.3 . Furthermore, ( Gatouillat et al., 2018 ) identifies the security
of patient health data as very essential, the authors discuss possible solutions from the cyber-physical community to address this challenge. Also, (
Egala, Pradhan, Badarla, & Mohanty, 2021;
Garg et al., 2020 ) propose Blockchain technology as seen in Fig. 2.4 as a potential solution to
securing patient data in the cloud. In ( Ding et al., 2021 ), the authors propose securing medical
images by using DL-based encryption and decryption techniques. In addition, ( Awan et al., 2020 ),
FIGURE 2.3
Edge computing integrated with AI.32 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemthe authors propose utilizing neural networks to predict and eliminate malicious and compromised
devices in the two-way communication between patient and physician.
2.1.4 Visualization layer
This layer also known as the application layer, is where the data is accessed by the physician and
patients to track their health. The physician can make recommendations and upload actions tobe taken by the patient based on the patient’s health conditions. These actions can include drugprescriptions or referrals to other specialized physicians if the disease needs specialized care(
Ghubaish et al., 2021 ).
2.2 Consideration of uplink healthcare IoT system relying on NOMA
2.2.1 Introduction
One of the most promising technology to enhance massive spectral efficiency is nonorthogonal
multiple access (NOMA) since it can serve multiple users using similar resources at a single pointof time. Compared to the traditional orthogonal multiple Access technique (OMA), which utilizesfrequency, time, and code division multiple access techniques to communicate. NOMA transmitsthe superimposed signals utilizing similar resources as frequency, time and code by differing the
FIGURE 2.4
Blockchain secured cloud layer.33 2.2 Consideration of uplink healthcare IoT system relying on NOMArespective power coefficients of each user ( Baig, Ali, Asif, Khan, & Mumtaz, 2019; Li et al., 2020;
Qi, Feng, Chen, & Wang, 2017; Wan, Wen, Ji, Yu, & Chen, 2018; Zheng, Wang, Wen, & Chen,2017
). NOMA utilizes the Successive Interference Cancellation (SIC) procedure to detect the origi-
nal signal at the receiver since the transmitted signals are superimposed. Channels with a highergain are less likely to be restricted by noise whereas, channels with lower gain are more likely to
be restricted by noise. Studies on NOMA have reportedly shown that the reduction in transmission
delay will pointedly attain massive user connectivity, and user waiting times can be reduced (
Chen,
Cai, Cheng, Yang, & Jin, 2017; Ding & Poor, 2016; Ding et al., 2017; Do, Le, & Lee, 2020; Do,Nguyen, Jameel, Ja ¨ntti, & Ansari, 2020; Do, Le, & Afghah, 2020; Islam, Avazov, Dobre, & Kwak,
2017; Islam, Zeng, Dobre, & Kwak, 2018; Wang, Gao, Jin, Lin, & Li, 2018; Yang, Wang, Ng, &Lee, 2017
). Authors in ( Islam et al., 2017 ) have provided a survey study on NOMA in 5G commu-
nications with the view of investigating capability, power allocation tactics, user fairness and user-pairing systems. The authors in (
Islam et al., 2018 ) have discussed resource allocation methods for
downlink NOMA concerning optimizing power allocation, low complexity, and security resource
allocation. Authors in ( Ding & Poor, 2016; Wang et al., 2018 ) have utilized massive multiple-input
multiple-output (MIMO) and communication methods to replace traditional NOMA antenna whichare massive in number. Whereas, the authors in (
Yang et al., 2017 ) have considered checking the
secrecy performance of NOMA when utilizing single antenna and multiple antennas, this is shownto perform better against the traditional OMA scheme. The authors in (
Chen et al., 2017 ) discuss
downlink MIMO NOMA system with the intention to study resource performance allocation, andthe authors also design a NOMA scheme that can attain approximate optimal sum-rate performance
in both perfect and imperfect channel state information (CSI) scenario.
The different properties of 5G communication systems require studying NOMA systems in
the presence of device-to-device (D2D) communications, in which legitimate users are allowed tocommunicate directly without the requirement of a base station (BS) (
Boccardi, Heath, Lozano,
Marzetta, & Popovski, 2014; Lei, Zhong, Lin, & Shen, 2012 ). Authors in ( Asadi, Wang, &
Mancuso, 2014 ) have explained that D2D users can reuse the spectrum band by facing the mutual
interference between the cellular and D2D links. Combining D2D with cellular systems will intro-duce interference to the broadcasting channels. Authors in (
Do, Nguyen et al., 2020; Liuand &
Erkip, 2016 ) have mentioned that the performance of SIC in this scenario should be studied more
as the capacity-achieving schemes are not perfectly known. The authors in ( Zhao, Liu, Chai, Chen,
& Elkashlan, 2017 ) have studied the utilization of D2D in NOMA communications, where the D2D
links are employed with NOMA protocol for data transmissions.
Motivated by the recent study ( Zhao et al., 2017 ), this book chapter presents the IoT based
healthcare system which contains several healthcare devices, relays and AP. We consider the ana-lytical expression of outage probability and ergodic capacity for uplink (UL) IoT system.
2.2.2 System model
In this paper, we consider UL NOMA, which consists of an AP (A) and two IoT devices Di(iA{1,
2}) to provide health care, as in Fig. 2.5 . In addition, we assume the channel between DiandAis
hiand follow Nakagami-m fading channel, and diis the distance between DiandA(Liu, Ding,
Elkashlan, & Poor, 2016 ). Moreover, the perfect channel state information (CSI) is available for
signal detection.34 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemHere, the IoT devices Disend the UL signal to A. The received UL signal at Ais given by
rA5ﬃﬃﬃﬃﬃP1ph1x1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ11dα
1p 1ﬃﬃﬃﬃﬃP2ph2x2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ11dα
2p 1nA (2.1)
where xiandPiare the signal and transmit power of Di, respectively, αis the path-loss exponent
andnACNð0;N0Þis the Additive White Gaussian Noise (AWGN). Follow NOMA scheme ( Zaidi,
Hasan, & Gui, 2018a ), the signal to interference and noise ratio (SINR) is detected as the signal x1
atAis given by
ΓUL ;x15P1h1jj2ð11dα
2Þ
ð11dα
1ÞP2h2jj21ð11dα
1Þð11dα
2ÞN0
5ηh1jj211dα
2/C0/C1
ð11dα
1Þηh2jj2111dα
1/C0/C1
11dα
2/C0/C1(2.2)
where η5P1
N05P2
N0. Next apply SIC, the SINR at Awhen detected the own signal x2is given by
ΓUL ;x25P2h2jj2
11dα
2/C0/C1
N05ηh2jj2
11dα
2(2.3)
2.2.3 Outage probability for UL NOMA
In this section, we derive the expression close-form outage probability UL of the signal xiatA.F o r
more insight, the asymptote of outage probability is expressed. The probability density function of
Healthcare
HealthcareIOT device 2IOT device 1
Access point
IOT network in healthcare
FIGURE 2.5
System model.35 2.2 Consideration of uplink healthcare IoT system relying on NOMAhiis given as ( Do, Le et al., 2020 )
fhijj25mi
λi/C18/C19mixmi21
ΓmiðÞe2mi
λix(2.4)
where mis the fading severity and λiis the mean power.
2.2.3.1 Outage probability of x 1
The outage probability of x1is given by ( Zaidi et al., 2018a )
Px1512PrΓUL ;x1.ε1/C0/C1
(2.5)
where ε152Ri21 is the threshold SINR and Riis the target rate.
Lemma 1 : The close-form of x1can be expressed by
Px1512Xm121
k50Xk
n50k
n/C18/C19ðm21n21Þ!e2m1ε1ð11dα
1Þ
λ1η
k!Γðm2Þ
3m1ε1ð11dα
1Þ
ηλ1/C16/C17k
ηλ2
m2ð11dα
2Þ/C16/C17n
11m1λ2ε1ð11dα
1Þ
λ1m2ð11dα
2Þ/C16/C172m22n(2.6)
Proof: With help from (2.2) ,(2.6) is calculated by
Px1512Prηh1jj2ð11dα
2Þ
ð11dα
1Þηh2jj21ð11dα
1Þð11dα
2Þ.ε1/C18/C19
512Prh1jj2.ε1ð11dα
1Þh2jj2
ð11dα
2Þ1ε1ð11dα
1Þ
η/C18/C19
512ÐN
0fh2jj2ðxÞÐN
ε1ð11dα
1Þ
ð11dα
2Þx1ε1ð11dα
1Þ
ηfh1jj2ðyÞdydx(2.7)
Substituting (2.4) into (2.7) , we have
Px1512m1
λ1/C16/C17m1m2
λ2/C16/C17m2 1
Γðm1ÞΓðm2Þ
3ÐN
0xm221e2m1
λ1xÐN
ε1ð11dα
1Þ
ð11dα
2Þx1ε1ð11dα
1Þ
ηym121e2m1
λ1ydydx(2.8)
Based on Eq. 3.351.2 in Gradshteyn & Ryzhik, 2014
Px1512Xm121
k50e2m1ε1ð11dα
1Þ
λ1η
k!Γðm2Þm1ε1ð11dα
1Þ
λ1ð11dα
2Þ/C16/C17k
m2
λ2/C16/C17m2
3ÐN
0xm221x1ð11dα
2Þ
η/C16/C17k
e2m2
λ2x2m1ε1ð11dα
1Þ
λ1ð11dα
2Þxdx(2.9)36 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemThen, using Eq. 1.111 and Eq. 3.351.3 in Gradshteyn & Ryzhik, 2014 it can be obtained as
Px1512Xm121
k50Xk
n50k
n/C18/C19e2m1ε1ð11dα
1Þ
λ1η
k!Γðm2Þm1ε1ð11dα
1Þ
λ1ð11dα
2Þ/C16/C17k
3m2
λ2/C16/C17m2ð11dα
2Þ
η/C16/C17k2nÐN
0xm21n21e2m2
λ21m1ε1ð11dα
1Þ
λ1ð11dα
2Þ/C16/C17
xdx
512Xm121
k50Xk
n50k
n/C18/C19ðm21n21Þ!e2m1ε1ð11dα
1Þ
λ1η
k!Γðm2Þm1ε1ð11dα
1Þ
ηλ1/C16/C17k
3ηλ2
m2ð11dα
2Þ/C16/C17n
11m1λ2ε1ð11dα
1Þ
λ1m2ð11dα
2Þ/C16/C172m22n(2.10)
This completes the proof.
2.2.3.2 Outage probability of X 2
The outage probability UL at Aofx2is given by ( Zaidi et al., 2018a )
Px2512PrðΓUL ;x2.ε2Þ (2.11)
Submitting (2.3) into (2.11) , the outage probability of x2is expressed by
Px25Xm221
k501
k!m2ε2ð11dα
2Þ
λ2η/C18/C19k
e2m2ε2ð11dα
2Þ
λ2η (2.12)
Similarly, the close-form outage probability for x2is given as
Px2512Prh2jj2.ε2ð11dα
2Þ
η/C18/C19
5ÐN
ε2ð11dα
2Þ
ηfh2jj2ðxÞdx(2.13)
2.2.3.3 Asymptotic
In this section, we derive the asymptote of outage probability for xi. We apply the first-order
Maclaurin series expansions e2x/C2512x(Gradshteyn & Ryzhik, 2014 ). The asymptotic outage
probability of x1is given as
PN
x1/C2512Xm121
k50Xk
n50k
n/C18/C19ðm21n21Þ!
k!Γðm2Þ12m1ε1ð11dα
1Þ
λ1η/C18/C19
m1ε1ð11dα
1Þ
ηλ1/C16/C17k
ηλ2
m2ð11dα
2Þ/C16/C17n
11m1λ2ε1ð11dα
1Þ
λ1m2ð11dα
2Þ/C16/C172m22n(2.14)
Next, the asymptotic outage probability of x2can be shown as
PN
x2/C2512Xm221
k501
k!m2ε21ð11dα
2Þ
λ2η/C18/C19k
12m2ε2ð11dα
2Þ
λ2η/C18/C19
(2.15)37 2.2 Consideration of uplink healthcare IoT system relying on NOMA2.2.4 Ergodic capacity of UL NOMA
The Ergodic capacity of x1is given by ( Zaidi, Hasan, & Gui, 2018b )
Cx151
ln 2ðN
012FΓUL ;x1ðxÞ
11xdx (2.16)
The term FΓUL ;x1ðxÞis given by
FΓUL ;x1ðxÞ512Xm121
k50Xk
n50k
n/C18/C19ðm21n21Þ!
k!Γðm2Þηλ2
m2ð11dα
2Þ/C16/C17n
3m1xð11dα
1Þ
ηλ1/C16/C17k
11m1λ2xð11dα
1Þ
λ1m2ð11dα
2Þ/C16/C172m22n
e2m1xð11dα
1Þ
λ1η(2.17)
The term (2.16) is given by
Cx151
ln 2Xm121
k50Xk
n50k
n/C18/C19
ðm21n21Þ!
k!Γðm2Þηλ2
m2ð11dα
2Þ/C16/C17nm1ð11dα
1Þ
ηλ1/C16/C17k
3ÐN
0xk
11x11m1λ2ð11dα
1Þx
λ1m2ð11dα
2Þ/C16/C172m22n
e2m1ð11dα
1Þ
λ1ηxdx(2.18)
Using Gaussian Chebyshev quadrature ( Gradshteyn & Ryzhik, 2014 ) in which
ϕn5cos2a21
2Iπ/C0/C1
, we have
Cx1/C251
ln 2Xm121
k50Xk
n50k
n/C18/C19
m21n21 ðÞ !
k!Γðm2Þηλ2
m2ð11dα
2Þ/C16/C17nm1ð11dα
1Þ
ηλ1/C16/C17k
3XI
a51π
Iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
12ϕ2
nq
sec2ð11ϕnÞπ
4/C18/C19tanð11ϕnÞπ
4/C16/C17k
11tanð11ϕnÞπ
4/C18/C19
311m1λ2ð11dα
1Þ
λ1m2ð11dα
2Þtanð11ϕnÞπ
4/C16/C17 /C16/C172m22n
e2m1ð11dα
1Þ
λ1ηtanð11ϕnÞπ
4/C16/C17(2.19)
The ergodic capacity of x2is given by ( Zaidi et al., 2018b )
Cx251
ln 2ðN
012FΓUL ;x2ðyÞ
11ydy (2.20)
FΓUL ;x2ðyÞis given as
FΓUL ;x2ðyÞ512Xm221
k501
k!m2ð11dα
2Þ
λ2η/C18/C19k
yke2m2ð11dα
2Þ
λ2ηy(2.21)
Based on Eq. 3.353.5 in Gradshteyn & Ryzhik, 2014
Cx251
ln 2Xm221
k501
k!m2ð11dα
2Þ
λ2η/C16/C17k
ð21Þk21em2ð11dα
2Þ
λ2ηEi2m2ð11dα
2Þ
λ2η/C18/C19
1Xk
c50ðc21Þ!ð21Þk2c m2ð11dα
2Þ
λ2η/C16/C172c"#
(2.22)
2.2.5 Numerical results and discussions
In this section, we set a52,d155,d252,m5m15m252,λ15λ251,R150:5 and R151. In
Fig. 2.6 , we can see the outage probability can be reduced significantly at high SNR region of the38 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemdestinations. This is the outage probability for the detection of the first destination’ signal at the
AP. Further, the quality of channel and required target rate led to improved outage probability. Theexact curves match with Monte Carlo curves at all range of transmit SNR as expected.0 1 02 03 04 05 010-1100
m = 3m = 2
FIGURE 2.6
Outage probability of x1versus ηwith different R1andm.
0 1 02 03 04 05 010-410-310-210-1100
m = 1m = 2
FIGURE 2.7
Outage probability of x2versus ηwith different R2andm.39 2.2 Consideration of uplink healthcare IoT system relying on NOMA0 5 10 15 20 25 3000.511.522.533.544.55Ergodic capaccity of x2
FIGURE 2.9
Ergodic capacity of x2versus ηwith different m.0 1 02 03 04 05 000.10.20.30.40.50.6Ergodic capacity of x1
FIGURE 2.8
Ergodic capacity of x1versus ηwith different m.40 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemInFig. 2.7 , we can see the significant reduction of outage probability for the detection of the
second destination’ signal at the AP. Similar performance can be observed as Fig. 2.6 . The exact
curves match with Monte Carlo curves at all range of transmit SNR as expected.
InFig. 2.8 , we provide system performance for ergodic capacity of the first destination’s signal
which is detected at the AP. The ergodic capacity can be improved at high transmit SNR. The rea-
son is that higher transmit SNR leads to higher SINR, then corresponding ergodic capacity could
be higher. The quality of channel contributes the improvement of such ergodic capacity.
We provide system performance for ergodic capacity of the second destination’s signal which is
detected at the AP, shown in Fig. 2.9 . Similar observations can be seen for this figure, and it looks
likeFig. 2.8 .
2.3 Conclusions
In this work, we have studied the impact of channels on two performance metrics (outage probabil-ity and ergodic capacity) of UL health care IoT system. The AP needs to examine how it can detectthe signals in UL from two destinations. We provided the closed-form expressions of two perfor-mance metrics. Further, we found that higher quality channels lead to significant improvements ofthese performance metrics. This book chapter provides guidelines of UL IoT with advances of theNOMA scheme.
References
Amin, S. U., & Hossain, M. S. (2021). Edge intelligence and Internet of Things in healthcare: A survey. IEEE
Access ,9,4 5/C059. Available from https://doi.org/10.1109/ACCESS.2020.3045115 .
Asadi., Wang, Q., & Mancuso, V. (2014). A survey on device-to-device communication in cellular networks.
IEEE Communications Surveys and Tutorials ,16(4), 1801 /C01819, 4th Quart.
Awan, K. A., Din, I. U., Almogren, A., Almajed, H., Mohiuddin, I., & Guizani, M. (2020). Neurotrust -artifi-
cial neural network-based intelligent trust management mechanism for large-scale internet of medicalthings. IEEE Internet of Things Journal . Available from
https://doi.org/10.1109/JIOT.2020.3029221 .
Baig, S., Ali, U., Asif, H. M., Khan, A. A., & Mumtaz, S. (2019). Closed-form BER expression for Fourier
and wavelet transform-based pulse shaped data in downlink NOMA. IEEE Communications Letters ,23(4),
592/C0595.
Boccardi, F., Heath, R. W., Jr., Lozano, A., Marzetta, T. L., & Popovski, P. (2014). Five disruptive technology
directions for 5G. IEEE Communications Magazine ,52(2), 74 /C080.
Chen, C., Cai, W., Cheng, X., Yang, L., & Jin, Y. (2017). Low complexity beamforming and user selection
schemes for 5G MIMO-NOMA systems. IEEE Journal on Selected Areas in Communications ,35(12),
2708 /C02722.
Ding, Y., et al. (2021). DeepEDN: A deep-learning-based image encryption and decryption network for
Internet of Medical Things. IEEE Internet of Things Journal ,8(3), 1504 /C01518. Available from https://doi.
org/10.1109/JIOT.2020.3012452 .
Ding, Z., Lei, X., Karagiannidis, G. K., Schober, R., Yuan, J., & Bhargava, V. K. (2017). A survey on non-
orthogonal multiple access for 5G networks: Research challenges and future trends. IEEE Journal on
Selected Areas in Communications ,35(10), 2181 /C02195.41 ReferencesDing, Z., & Poor, H. V. (2016). Design of massive-MIMO-NOMA with limited feedback. IEEE Signal
Processing Letters ,23(5), 629 /C0633.
Do, D.-T., Le, A., & Lee, B. M. (2020). NOMA in cooperative underlay cognitive radio networks under imper-
fect SIC. IEEE Access ,8, 86180 /C086195.
Do, D.-T., Le, C.-B., & Afghah, F. (2020). Enabling full-duplex and energy harvesting in uplink and downlink
of small-cell network relying on power domain based multiple access. IEEE Access ,8, 142772 /C0142784.
Do, D.-T., Nguyen, M.-S. V., Jameel, F., Ja ¨ntti, R., & Ansari, I. S. (2020). Performance evaluation of relay-
aided CR-NOMA for beyond 5G communications. IEEE Access ,8, 134838 /C0134855.
Egala, B. S., Pradhan, A. K., Badarla, V. R., & Mohanty, S. P. (2021). Fortified-Chain: A Blockchain Based
Framework for Security and Privacy Assured Internet of Medical Things with Effective Access Control.IEEE Internet of Things Journal . Available from
https://doi.org/10.1109/JIOT.2021.3058946 .
Garg, N., Wazid, M., Das, A. K., Singh, D. P., Rodrigues, J. J. P. C., & Park, Y. (2020). BAKMP-IoMT:
Design of blockchain enabled authenticated key management protocol for Internet of Medical Thingsdeployment. IEEE Access ,8, 95956 /C095977. Available from
https://doi.org/10.1109/ACCESS.2020.
2995917 .
Gatouillat, A., Badr, Y., Massot, B., & Sejdi ´c, E. (2018). Internet of Medical Things: A review of recent con-
tributions dealing with cyber-physical systems in medicine. IEEE Internet of Things Journal ,5(5),
3810 /C03822. Available from https://doi.org/10.1109/JIOT.2018.2849014 .
Ghubaish, A., Salman, T., Zolanvari, M., Unal, D., Al-Ali, A., & Jain, R. (2021). Recent Advances in the
Internet-of-Medical-Things (IoMT) systems security. IEEE Internet of Things Journal ,8(11), 8707 /C08718.
Available from https://doi.org/10.1109/JIOT.2020.3045653 .
Gradshteyn, I. S., & Ryzhik, I. M. (2014). Table of integrals, series, and products . Academic press.
Islam, S. M. R., Avazov, N., Dobre, O. A., & Kwak, K.-S. (2017). Power-domain non-orthogonal multiple
access (NOMA) in 5G systems: Potentials and challenges. IEEE Signal Processing Letters ,19(2),
721/C0742, 2nd Quart.
Islam, S. M. R., Zeng, M., Dobre, O. A., & Kwak, K.-S. (2018). Resource allocation for downlink NOMA sys-
tems: Key techniques and open issues. IEEE Wireless Communication ,25(2), 40 /C047.
Lei, L., Zhong, Z., Lin, C., & Shen, X. (2012). Operator controlled device-to-device communications in LTE-
advanced networks. IEEE Wireless Communication ,19(3), 96 /C0104.
Li, J., Peng, Y., Yan, Y., Jiang, X.-Q., Hai, H., & Zukerman, M. (2020). Cognitive radio network assisted by
OFDM with index modulation. IEEE Transactions on Vehicular Technology ,69(1), 1106 /C01110.
Limaye, A., & Adegbija, T. (2018). HERMIT: A benchmark suite for the Internet of Medical Things. IEEE
Internet of Things Journal ,5(5), 4212 /C04222. Available from https://doi.org/10.1109/JIOT.2018.2849859 .
Liu, Y., Ding, Z., Elkashlan, M., & Poor, H. V. (2016). Cooperative non-orthogonal multiple access
with simultaneous wireless information and power transfer. IEEE Journal on Selected Areas in
Communications ,34(4), 938 /C0953.
Liuand, Y., & Erkip, E. (2016). Capacity and rate regions of a class of broadcast interference channels. IEEE
Transactions on Information Theory ,62(10), 5556 /C05572.
Qi, T., Feng, W., Chen, Y., & Wang, Y. (2017). When NOMA meets sparse signal processing: Asymptotic
performance analysis and optimal sequence design. IEEE Access ,5, 18516 /C018525.
Sun, L., Jiang, X., Ren, H., & Guo, Y. (2020). Edge-cloud computing and Artificial Intelligence in Internet of
Medical Things: Architecture, technology and application. IEEE Access ,8, 101079 /C0101092. Available
from https://doi.org/10.1109/ACCESS.2020.2997831 .
Wan, D., Wen, M., Ji, F., Yu, H., & Chen, F. (2018). Non-orthogonal multiple access for cooperative commu-
nications: Challenges, opportunities, and trends. IEEE Wireless Communications ,25(2), 109 /C0117.
Wang, B., Gao, F., Jin, S., Lin, H., & Li, G. Y. (2018). Spatial- and frequency wideband effects in millimeter-
wave massive MIMO systems. IEEE Transactions on Signal Processing ,66(13), 3393 /C03406.42 Chapter 2 Physical layer architecture of 5G enabled IoT/IoMT systemYang, Q., Wang, H.-M., Ng, D. W. K., & Lee, M. H. (2017). NOMA in downlink SDMA with limited feed-
back: Performance analysis and optimization. IEEE Journal on Selected Areas in Communications ,35(10),
2281 /C02294.
Zaidi S.K., Hasan S.F. and Gui X., (2018a). SWIPT-aided uplink in hybrid non-orthogonal multiple access. In
Proceedings of the IEEE wireless communications and networking conference , pp. 1 /C06.
Zaidi, S. K., Hasan, S. F., & Gui, X. (2018b). Evaluating the Ergodic Rate in SWIPT-Aided Hybrid NOMA.
IEEE Communications Letters ,22(9), 1870 /C01873.
Zhao, J., Liu, Y., Chai, K. K., Chen, Y., & Elkashlan, M. (2017). Joint subchannel and power allocation for
NOMA enhanced D2D communications. IEEE Transactions on Communications ,65(11), 5081 /C05094.
Zheng, B., Wang, X., Wen, M., & Chen, F. (2017). NOMA-based multi-pair two-way relay networks with rate
splitting and group decoding. IEEE Journal on Selected Areas in Communications ,35(10), 2328 /C02341.43 ReferencesThis page intentionally left blankCHAPTER
3HetNet/M2M/D2D communication in
5G technologies
Ayaskanta Mishra1, Anita Swain1, Arun Kumar Ray1and Raed M. Shubair2
1School of Electronics Engineering, Kalinga Institute of Industrial Technology, Deemed to be University,
Bhubaneswar, Odisha, India2Department of Electrical and Computer Engineering, New York University (NYU)
Abu Dhabi, Abu Dhabi, United Arab Emirates
3.1 Introduction
Modern day digital networks are a complex mesh of interconnected devices for the transmission of
data bits using various network standards, technologies, and protocols. This makes the data commu-nication network infrastructure heterogeneous in nature. Heterogeneity is imperative in modern daypacket-switched data networks. The primary challenge is to have seamless integration and interop-erability of such heterogeneous network clusters. Such heterogeneity across network is predomi-nantly contributed by the access networks (PHY-MAC layer). As per TCP/IP stack: physical andlink layer specifications are different for different access networks. Physical Media Dependent
Protocol (PMDP) and Physical layer Convergence Protocol (PLCP) of Layer-1 of OSI is necessary
to access network specifics and differences, so an added layer of complexity in design and deploy-ment of data network infrastructure is needed for cross-technology integrated networks. Such net-works are referred to as Heterogeneous Networks or HetNets. Apart from layer-1 even the MediaAccess Control (MAC) is dissimilar across multiple network technologies. Thanks to IEEE 802.1Logical Link Control (LLC) for proving a convergence to Network layer/ Internet Protocol (IP)layer for interoperability across multiple underlying Layer-1 & MAC sublayer. For example, a seg-ment of network may be having application-specific requirements of Wireless Personal Area
(WPAN) for any Wireless Sensor Network (WSN) application hence an IEEE 802.15.4 Zigbee
might be a viable option for the particular cluster of networks. However, other parts of the networkmay have different sets of application specific network requirements for, say, a Wireless LocalArea Network (WLAN) and therefore may be IEEE 802.11 Wi-Fi standard on Layer-I and MACwould be suitable for its deployment. Now the question is the “Internet” (IP-based Global data net-work infrastructure) is full of such use-cases where the heterogeneity is imperative and a necessary
evilto accommodate multiple access network underlying standards. The perspective of mobile cel-
lular networks (MCN), such as Long-Term Evolution (LTE), that is, 4th generation (4G) Mobile
communication standards, shows that the network is completely packet-switched with absolution of
circuit-switching from 3rd generation (3G) mobile communication standards. Hence the 5th genera-tion mobile communication standard New-Radio (5G-NR) is also a completely packet-switcheddata network with more optimized Cloud based Radio Access Network (C-RAN) with other key
455G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00002-4
©2022 Elsevier Inc. All rights reserved.enabling technologies like Network Functions Virtualization (NFV) and Software defined
Networking (SDN). 5G-NR standard is a packet-switch data network having basic requirements toseamlessly integrate with other existing access network technologies (ANT) like IEEE 802.11 andIEEE 802.15.4 etc. Such interoperability of 5G-NR with other underlying technologies like otherWPAN, WLAN, and WAN ANT are provisioned using an extra middle-layer and edge/gateway
devices. The primary functionality of such gateway device is to do the required protocol level trans-
lation of the Protocol Data Unit (PDU). Another key aspect of such heterogeneous networks(HetNet) is “clustering.” Clustering is the concept of dividing the network into multiple workingnetwork segments or Autonomous System (AS). These Autonomous Systems are managed indepen-dently as per their access network standards. However, there is always a requirement of having aninteroperability device (Gateway device) to make any two dissimilar clusters in an op-operativecommunication model. The edge nodes between two dissimilar clusters are gateways in HetNetsand they are the major technology enablers to seamless interoperability across multiple dissimilar
network segments or clusters. Several popular HetNets Radio Access Technologies (RATs) Wi-Fi/
LTE/5G-NR are used in H-CRAN of such 5G HetNets. Interoperability across HetNet clusters isachieved using gateway devices (GWs). GWs are instrumental in providing seamless integrationand interoperability across dissimilar networks and protocols. The primary function of gatewaynode is providing a seamless integration between to dissimilar network clusters. This function isachieved through various strategies like (1) Dual stack, (2) Header Translation and (3) Tunneling.Dual stack is a strategy implemented on the Edge of two heterogonous networks. The Edge device(Gateway) is having both the connected cluster stack implemented, which enables interoperability
as the gateway device can process PDUs of both the HetNet clusters. Being equipped with both the
protocol stack the gateway device can process PDUs of both the HetNet clusters. Secondly,the Header translation strategy is also a viable option to be implemented on gateway devices at theEdge of two dissimilar network clusters in HetNets. Here the Gateway device would process theHeader of the PDU of one side of the network and translate to the required parameters and formatof the other side and vice-versa. This method would be computationally intense and put morecomputational overhead of the gateway device, hence gateway devices demand relatively highcomputational capabilities and resources with higher CPU, RAM, and faster buffer or data memory
access. Lastly, we have a tunneling approach where a typical sandwiched HetNet scenario would
take much benefit. For example, if there is a 5G-NR network cluster in between two IEEE 802.11network clusters, then the 5G-NR network may encapsulate the IEEE 802.11 frames in 5G-NRLink-layer PDU. The encapsulation into the tunnel would be done when the PDU has an entrythrough gateway device of 5G-NR and the de-capsulation would be done when the PDU exit the5G-NR network cluster through the existing gateway device. Apart from these three strategies forhandling seamless integration of multiple HetNets, there is always a technical challenge to dealwith various aspects of heterogeneity and interoperability issues of HetNets.
One such predominant application use-case using HetNets would be the “ smart healthcare sys-
tem.” The vision of Industry 4.0 includes a holistic system with smart healthcare Edge devices
equipped with biomedical sensors with key features like connect, compute and communicate. Thesesystems are referred as Internet of Healthcare Things (IoHT). For an example: An IoHT system isdesigned to acquire sensor data from a cluster/Wireless Body Area Network (WBAN) of low-power sensor motes which are reduced functional devices (RFDs) with limited computationalresource for sensor data preprocessing and signal conditioning. For these scenario-specific46 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesrequirements of Wireless Personal Area Network (WPAN) is befitting, hence Bluetooth Low
Energy (BLE) or IEEE 802.15.4/Zigbee network are used in the WSN/ Body Area Network (BAN)part of a SDN-based 5G Heterogenous-Cloud-Radio Access Network (H-CRAN). However, there isa requirement of Traffic/data aggregation when numerous such sensor nodes have to send datausing mMTC framework to the centralized cloud/server for data storage or analytics.
Fig. 3.1 depicts a generic HetNet in the context of 5G and other allied access network
technologies.
As discussed, for any practical use-case deployment scenario of heterogeneous network; we
may have multiple network clusters each managed and administrated by a separate underlying net-work technology (Physical and MAC layer). As illustrated in
Fig. 3.1 , we can have a WSN/Zigbee
(IEEE 802.15.4), Wi-Fi (IEEE 802.11), satellite and Optical (SONET/SDN) network cluster along-side the 5G network. All dissimilar access network clusters are integrated together to a HetNetgateway integrator through their respective Edge interface gateway devices. These gateway devices
are designed and deployed based on any one of the three methods disused prior or may be a combi-
nation of two or more methods making it a hybrid gateway as per scenario-specific requirements of
FIGURE 3.1
HetNet in the context of 5G.47 3.1 Introductionthe connecting clusters in HetNet. The integrator provides a seamless connectivity to the IP Core.
This provides a convergence for HetNet for providing service point addressing by transport layerSocket ,ip_address:port_no .for connection control based on application-specific requirements
of respective application servers. Transmission Control Protocol (TCP) for a connection-orientedhand-shaking based approach for reliable and guaranteed service framework and User Datagram
Protocol (UDP) for a connection-less and best-effort service framework. The application layer of
TCP/IP stack is complete software implementation based on Application Programming Interface(API) for providing service to the end-user like conventional Hypertext Transfer Protocol (HTTP),File Transfer Protocol (FTP), Telnet, Domain Name System (DNS), and even some IoT specificapplication protocols like Message Queue Telemetry Transport (MQTT) and Constraint ApplicationProtocol (CoAP).
3.2 Heterogenous networks in the era of 5G
In the era of 5th generation (5G) Mobile, that is,. from 3GPP release 14 (R14) onwards has some
distinct features to support massive Machine Type Communication (mMTC) framework much
required for massive deployment of Machine-to-machine (M2M) communication scenario for futureInternet of Things (IoT) applications. Most of the deployment using 3GPP R14 has been done byvarious pilot projects since 2020. One such use-case scenario of the mMTC is Smart City Bussanin South Korea. The 5G mobile standard suggest a minimum of 10Gbps data-rate using enhancedMobile Broad-band (eMBB), enhanced Mobility support (up to 500 km/hr.) with Ultrareliable Low-Latency Communication (URLLC) infrastructure (within 1 ms End-2-End delay). These features of5G have enabled service and business potential for many resource demanding applications like 3D
Video, HD/4K Video content and streaming applications, Augmented Reality (AR), Virtual Reality
(VR), Industrial Automation and Control (IAC), Self-driving Vehicles enabled smart transportationsystems. Ultrahigh data-rate and low-latency smart healthcare applications, smart home, smart city,smart agriculture, smart grids, etc., mMTC supports massive deployment up to 1 million IoT-enabled smart devices (equipped with application-specific sensors and actuators) per square kilome-ter. It is imperative to have network heterogeneity in deployment of such IoT-based massive M2Mcommunication as the Radio Access Network (RAN) is comprising off multiple-heterogeneousunderlying technologies like IEEE 802.11 (Wi-Fi), IEEE 802.15.1 (BLE), IEEE 802.15.4 (Zigbee),
Wi-MAX, LTE, 5G-NR and even Wired Network access like IEEE 802.3 Ethernet and optical net-
works like SONET, SDH. The challenge is seamless interoperability of Heterogeneous Networks(HetNets) in the deployment of such IoT-based mMTC using technologies like M2M and Device-to-Device (D2D).
Fig. 3.2 shows the 5G M2M Heterogonous Network framework with a functional
layered architectural overview. Li, Da Xu, and Zhao (2018) .
3.2.1 5G mobile communication standards and enhanced features
1.Logically independent Network: 5G-NR is having a logically independent Network framework
for application-specific requirements.48 Chapter 3 HetNet/M2M/D2D communication in 5G technologies2.Cloud RAN: C-RAN supporting massive connections of mMTC framework with multiple
standard underlying network technologies (HetNets) support.
3.Simple core network architecture: 5G Core Network Architecture with support for on-demand
networking and computing resource allocations/ configuration using Network FunctionVirtualization (NFV).
4.High data-rate: Application support for HD/AR/VR. (at least 10Gbps on RAN)
5.High scalability: 5G Network with support for NFV can have fine-grained front-haul and
network decomposition.
6.Low-latency: 5G supports interactive communication framework for various applications like
AR, Video-games, 5G-IoT for low latency applications (smart healthcare).
7.Reliable and resilient: Improved convergence, hardened efficiency with support for
interoperability amount HetNets.
8.Security and privacy: Improved security model for applications like digital payment.
9.Long battery life: Improved Energy Efficiency (EE), Low-power, Low-cost, 5G/NB-IoT
devices support.
10. High connection density: Support for millions of M2M communications using mMTC
framework.
11. Mobility: Enhanced mobility model and hand-off management.
Table 3.1 . shows the Heterogeneous Underlying Network technologies for IoT.
FIGURE 3.2
5G M2M Heterogonous Network framework ( Li et al., 2018 ).49 3.2 Heterogenous networks in the era of 5G3.2.2 5G heterogeneous network architecture
5G network architecture support Heterogeneous Radio technologies for seamless interoperability
with the help of Heterogeneous Cloud Radio Access Network (H-CRAN) framework. The func-tional splitting of the User plane from the control plan helps in achieving such complex networkoperation. Coordinated Multipoint (CoMP) transmission and reception mechanism is used for inte-gration of User and Control plane. In 5G network architecture the features like Software DefinedNetwork (SDN) comes handy in dealing with radio resource management in the Cloud-RAN (C-
RAN). The Cooperative Signal Processing and Networking functionalities are viable by using the
virtualization and cloud-based systems implemented on the C-RAN. The Network FunctionVirtualization (NFV) is implemented for on-demand resource management. Resources such as radioresources and computing resources, are managed using various cloud computing models: Software-as-a-Service (SaaS) and Platform-as-a-Service (PaaS). The User plan is deployed using Short-rangecommunication (Small cells: Pico/femto) using the Remote Radio Heads (RRHs). The RRHs dealswith the high-volume data-traffic for the user equipment (UE) with eMBB framework of high data-rate service. These RRHs are allocated with Base-band Units (BBUs) using SDN-based flexible
front-haul on-demand. The control plane works separately using the High-Power Nodes (HPNs) rel-
atively longer-range macrobase stations (MBSs). The primary function of these MBSs is to provideonly control signals for UEs. These control signals are mostly a low data-rate signals for UE con-trol like Cell association, Resource allocation, mobility management (hand-off), etc., hence MBSsdoes not have any high payload requirement. In short, the MBSs present in the control plane arelow-data rate, long-range trans-receivers where the RRHs deal with high volume user data (Userplane) for quite a small cell-size hence short-range communication. The splitting of Control andTable 3.1 Heterogeneous underlying network technologies for Internet of Things.
Type of
radio accessnetworksPeripherals
communication wirelesspersonal area networkHome and campus
communication local areanetwork (LAN) Wide area network (WAN)
Standards Bluetooth low Energy
(BLE)
IEEE 802.15.1Zigbee (IEEE 802.15.4)Wireless LAN (Wi-Fi)
IEEE 802.11GSM, LTE, LTE-A, 5G-NR,
LoRaWAN, SIGFOX
Range Less than 30 feet Less than 300 feet 10 /C030 km
Features Low-power
Low data-rate
Enhanced battery LifeLow range20/C040 MHz Channel
bandwidth
Moderate /C0Power
High data-rateModerate rangeHeterogeneous Bandwidth and
channel allocation across
multiple-technologiesLong range
Applications Wireless sensor Network
(WSN)Body area network (BAN)Smart home, monitoring,
energy, security andsurveillanceIndustry 4.0
Smart citySmart industrial IoTSmart agriculture
Smart transportation and
logistics management50 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesUser plane in 5G network architecture has enabled the H-CRAN to be deployed with multitier het-
erogeneous underlying radio access network (like Wi-Fi, LTE, 5G-NR, etc.) technologies. Each ofthe RRHs can belong to different underlying network technologies but still can be managed andconfigured through the SDN-based Cloud-RAN using centralized resource management using NFV.This architecture helps in achieving seamless interoperability between H-CRAN clusters using
appropriate Gateway devices (GW). Software Defined RAN (SD-RAN) framework manages the
on-demand resource allocation and the traffic-shaping is done based on the application-specificQuality of Service (QoS) requirements using OpenFlow and SNMP in the User/Data plane. Thecontrol plane is being handled by the Network Controller (NC) equipped with custom applicationsand management tools.
Fig. 3.3 shows the Heterogeneous Cloud-RAN (H-CRAN)-based 5G
Network Architecture and its functional components.
FIGURE 3.3
Heterogeneous Cloud-RAN (H-CRAN)-based 5G Network Architecture.51 3.2 Heterogenous networks in the era of 5G5G H-CRAN architecture supports multitechnology access network integration though on-
demand radio resource allocation using CoMP to accommodate user-control plan splitting mecha-nism. The heterogenous wireless access network can take its required radio resource from RRHswith the help of MBSs (Control plane). The RRHs can take on-demand network resource from allo-cated BBUs using software defined radio (SDR) technology. Research in the direction of enhancing
coverage and capacity of such 5G HetNets is key to the performance improvements of HetNets in
the context of 5G. A stochastic geometry model based 2-tier intercellular hand-off mechanism isproposed in
Ouamri, Ote¸ steanu, Alexandru, and Azni (2020) to increase the capacity and coverage
with reduced cost of deployment. Spectral/EE in 5G is ten times and the throughput in 5G istwenty-five times than that of 4G/LTE. H-CRAN based wireless access network is the viable solu-tion to deal with the on-demand radio resource requirements with enhanced spectral and EE. 5Gnetwork architecture supports cloud computing with the aid of “Node-C”: a cloud computing capa-ble node. The centralized large-scale co-operative processing helps in decreasing or suppressing
cochannel interference. Node-C has operational capacities to allocate on-demand radio resource
though BBUs to the RRHs. SDN-based H-CRAN deployment in 5G HetNets are instrumental inperformance enhancement by adaptive large-scale cooperative spatial signal processing and co-operative radio resource management using techniques like NFV, self-organized front-haul, con-strained resource optimization and energy harvesting. 5G H-CRAN has the following technical spe-cifications (
Peng, Li, Zhao, & Wang, 2015 ):
1.Ultrasmall cell HetNets (Pico/Femto for User plane) with a large number of low-cost RRHs
based front-haul. Randomly connected with BBU pool.
2.RRHs are close to UE hence Low-Power Node (LPN). Low-range but high capacity (smaller
cells hence a greater number of RRHs in a geographical area increase the capacity).
3.BBU pool supports resource allocation to RRHs via centralized and co-operative processing
(mitigate interference) using techniques like statistical multiplexing.
4.H-CRANs are cost effective and energy efficient but at the same time complex to manage.
5.Splitting of Data and Control plan using CoMP transmission and reception. MBS are higher
power /less bandwidth control plane with RRHs are low-power/high bandwidth data plane.Separative of control and data plane enables gives better network manageability over acomplex SDN based HetNet architecture.
6.Co-ordinated multipoint (CoMP) architecture helps in mitigating intertier and intratier
interference.
7.Improved spectral efficiency (SE) and EE with the help of co-operative signal processing and
NFV on the H-CRAN with LPN/RRHs (for Data) and MBS (for Control).
8.Physical layer of H-CRAN has functionalities of co-operative radio resource management
using spatial signal processing, Multiple Input Multiple Output (MIMO) antenna.
9.H-CRAN has a MAC layer using Node-C (Cloud based MAC functions).
10. SDN and NFV functions are instrumental in achieving state-of-the-art techniques like self-
configuration, self-optimization, self-healing and ultradense machine type communication(mMTC).
11. Application architecture supports anytime, anywhere Gigabit data to UE with a Edge-less
experience with the help of Heterogenous MBSs, ultradense RRHs, cloud computing (Node-C)
for SDN based on-demand resource processing on the air-interface.52 Chapter 3 HetNet/M2M/D2D communication in 5G technologies12. Node-C using cloud computing framework converges all Ancestral Communication Entities
(ACEs) (i.e., MBSs, micro BSs, pico BSs, etc.). Node-C are having powerful computingresource and capabilities for large-scale co-operative signal and network processing for PHY/MAC and upper layer functionalities. C-RAN manages RRHs and resource allocation throughBBU pool. This new approach of cloud computing using Node-C has technical edge over the
traditional Radio network Controller (RNC) and Base-Station Controller (BSC) based network
architecture in previous generation 3GPP standards.
13. Gateway devices in H-CRAN are having two major functionalities: (1) Co-operative Multi-
Radio Resource Management (CM-RRM) and (2) Media Independent Hand-over (MIH).
14. SDN-NFV based H-CRAN architecture using Node-C provides 5G HetNet the advantage of
self-organizing and plug-and-play capability to dynamic ACE nodes.
15. 5G HetNets using IP network integration over SDN using Node-C for realizing the NFV.
IEEE 802.11 ac/ad with mmWave 5G-NR can be an example of 5G HetNet use-case.
Fig. 3.4 shows the NFV framework used in H-CRAN for 5G HetNets for resource management
and Network function virtualization. The Node-C and computationally intensive nodes cable of run-ning multiple virtual machines on the cloud to emulate the ACEs on a live network deployment.
FIGURE 3.4
Network functions virtualization framework in H-CRAN for 5G HetNet ( Peng et al., 2015 ).53 3.2 Heterogenous networks in the era of 5GNFV framework in H-CRAN for 5G HetNet is having four functional layers. For top-to-bottom
approach:
1.Inter-RAN Virtualization: This layer has multiple gateway devices each catering a service to a
heterogenous set of network segments like 3G, LTE, Wi-Fi and 5G-NR etc. Virtual servers aredeployed for NFV functions required for interoperability among HetNets.
2.Virtual Infrastructure: Node-C-based cloud computing approach for Virtual infrastructure.
Instead of having dedicated functional nodes, this layer uses virtualization of Network functionsdeployed over distributed Virtual machines (VMs) with the help of a generic/converging controlplane.
3.Intra-RAN Radio Resource Virtualization: This layer deals with the Radio resource
virtualization inside a RAN. Resources of a RAN are managed as two separate entities: (1)Network resource like RRHs, BBU Pool management and Radio resource management and (2)Computing resource like CPU, RAM, storage, process and threads.
4.Resource slicing: This is the bottom most layer mostly dealing with the real physical resource
sliced into Network and Computing resources. (1) Network physical resource like bandwidth,frequency, switching and transmission. (2) Computing resource of device, hardware like CPU,RAM, storage, etc.
3.2.3 Intelligent software defined network framework of 5G HetNets
The intelligent SDN framework of H-CRAN architecture of 5G HetNets where Node-C based cloud
computing-enabled nodes are deployed to harness the NFV using SDN-controllers and OpenFlow(ONF) switches with the help of co-operative multipoint communication between RRHs in dataplane and MBSs in control plane (
Sun, Gong, Rong, & Lu, 2015 ). In 5G H-CRAN architecture all
of the network functions and signal processing functions are done in software over generic cloudcomputing platform (with the help of VMs) rather than dedicated hardware nodes. In prior 3GPP
standards in 3G and 4G/LTE networks the network architecture has functional dedicated hardware
nodes like MME, EPC/CN, S/P-GW connected to E-UTRAN with eNodeBs. However, the H-CRAN architecture in 5G has no dedicated functional nodes but rather software level network func-tion virtualization over Node-Cs. Most of the network and signal processing functions are handledby software deployed over H-CRAN and this provides a huge advantage over the prior 3GPP stan-dards as it provides are extra flexibility and programmability of resource management in both com-munication resource like bandwidth, frequency, channel, switching, signal processing andcomputing resource like CPU, RAM, storage etc. in CRAN. All Node-C-based devices are widely
customizable in terms of communication and computing resource providing a high level of scalabil-
ity and adaptability to change in network hence easier to deploy and manage in a very complex setof network attributes.
3.2.4 Next-Gen 5G wireless network
Next-Gen 5G-NR based wireless networks have new functionalities to accommodate IoT, Internet
of Vehicles (IoV) using state-of-the-art technologies like mMTC and M2M and D2D communica-tion. In physical layer MIMO, Smart antenna Beamforming, mmWave technologies and in upper54 Chapter 3 HetNet/M2M/D2D communication in 5G technologieslayer C-RAN, NFV, SDN framework and CoMP based Network splitting (Data and control plane
separate) are the key technology enablers for future wireless communication paradigm. The 5G-NRH-CRAN based architecture are having numerous design advantages over traditional Base Station(BS) centric networks. Small cells (Pico/femto), Low-power RRHs with enhance network capacity,high device (UE) density for mMTC are key enablers for IoT based network deployment. Better
spectral and EE with intelligent antenna technology like MIMO, mmWave, beamforming creates
opportunities for IoV and D2D communication. Deployment of H-CRAN based intelligent SDNbased network has improved seamless integration of HetNets (
Agiwal, Roy, and Saxena, 2016 ).
Accommodating heterogenous networks is itself a challenge for seamless integration and
enhanced performance. A 3GPP-inspired HetNet model using passion cluster for improved down-link coverage is proposed in
Saha, Afshang, and Dhillon (2018) . The complexity of a HetNet is
huge overhead for any real-world deployment scenario. A stochastic geometry-based modelingapproach is proposed for mobile UEs and base Station gNBs for radio resource management in a
complex HetNet deployment scenario. This helps with better resource management of HetNets and
deployment planning with enhanced downlink coverage.
SDN-based C-RAN in 5G has enormous potential in handling crucial network functions of mod-
ern mobile communication. We will discuss a practical case study of one such SDN-based platformfor better understanding. 5G-EmPower is a SDN platform for 5G RAN is proposed in
Coronado,
Khan, and Riggio (2019) . 5G-EmPower is a flexible, programmable, open-source SDN platform for
5G HetNets C-RAN. 5G-EmPower is developed as an open-source platform suing APACHE 2.0license. The platform has three main functional modules. (1) Active Network Slicing, (2) Mobility
Management and (3) Load-balancing. The platform uses key 5G functionalities like mMTC,
URLLC, HetNet C-RAN, NFV and SDN-OpenFlow. 5G active network slicing supports a mixed/hybrid network with small (pico/femto) cells (for data plane) and macro cells (for control plane).5G-EmPower has three software modules: (1) Radio Access Agnostic API (Data/control plane), (2)Software agents- operate on several Radio access networks (HetNets) e.g., Wi-Fi (IEEE 802.11 ac/ad), 4G/LET and 5G-NR, and (3) SDN-controller. The EmPower platform is deployed as a com-plete operating System (OS) on the cloud with two interfaces: (1) North bound: Operation, Adminand Management and (2) South bound: HetNet Access Agents for Wi-Fi, LTE and 5G-NR. The
North bound interface connected to the management plane having key functions like mobility man-
agement, load balancing. The EmPower OS is the control plane having functionalities like globalnetwork view, Web services, topology discovery, device management. Toward the south boundinterface, a hardware abstraction layer is designed for convergence of HetNet with the help ofAgent Wi-Fi, LTE and 5G-NR. The platform uses slicing resource management framework with itsown proposed PDU with the help of an appropriate layer (PHY, MAC, RLC, PDCP)-specific wrap-per with payload and control field structure in its proposed design.
3.2.5 Internet of Things toward 5G and heterogenous wireless networks
IoT having tremendous potential in the recent times. IoT in the era of 5G is having many key tech-
nical advantages like high data-rate, more bandwidth, UR-LLC, better QoS, low interference usingbeamforming, MIMO, mmWave, eMBB, mMTC, M2M, D2D.A generic service-orient architecturemay hold good for all IoT applications in the context of 5G and HetNets. The 5G-IoT architecturehas the following key functional layers: (
Chettri & Bera, 2020 )55 3.2 Heterogenous networks in the era of 5G1.IoT sensor layer: Physical devices, smart sensors and actuators
2.Network layer: LPWAN, SigFox, LoRaWAN, Zigbee, NB-IoT
3.Communication layer: backbone ip
4.architecture layer: cloud, big-data analytics
5.Application layer: IoT application (smart home/city/grid/transportation/agriculture, Industrial
IoT, etc.)
Every layer has its own specific functions that are customizable and flexible in intralayer attri-
butes. The modular structure of the layered architecture makes it generic with scope of massivedesignability, programmability and flexibility in every functional layer as per the scenario-specific
requirements of IoT networks.
The IoT network uses multiple ANT based on the scenario-specific requirements of frequency,
range, data-rate, channel bandwidth, signaling, and modulation, etc. Each of the IoT applicationshas its own specific set of design attributes and specification based on the deployment scenario.
Table 3.2 below shows some of the most used wireless access network technology and their com-
parative overview based on the above discussed technical specifications. It is imperative to takethese design specifications into consideration in deploying wireless network using 5G HetNets forany IoT application-based network. HetNet considered here in the context of IoT mostly belongs to
the wireless network under the Low Power-Wireless Personal Area Network (LP-WPAN) category.
Some of the most popular wireless network technologies in purview of our study here are SigFox,LoRa, Wi-Fi, Zigbee, and NB-IoT. Most of the IoT applications demand tiny constraint devicecommunication using mMTC framework with Cyber-Physical systems (using smart sensor/actuatorcontrol applications). These sensor/actuator-based communications mostly use low-power embed-ded systems as the UEs hence the packet size / PDU being small in size but large in device number,hence making huge numbers of small block data traffic using massive M2M type communication.
Table 3.2 IoT network technologies and their technical specifications ( Chettri & Bera, 2020 ).
Technology Frequency Range Data rateChannel
bandwidth Modulation Standard
SigFox 868 /C0915/C0928 MHz 20 1km 100 kbps 250 /C0500 kHz BPSK ETSI
LoRa 915 /C0928 MHz 15 km 50 kbps 100 kHz CSS LoRa
Alliance
Zigbee 902 /C0928 MHz /
2.4GHz,1 km 250 kbps 2 MHz BPSK/
QPSKZigbee
Alliance
Wi-Fi 2.4GHz/ 5GHz 100 m 10—
150 Mbps
(MIMO)20/C040 MHz DSSC IEEE
802.11
NB-IoT LTE
(carrier inbound/
guard-band
mode)700, 800, 900 MHz 1 km
(Urban)
10 km
(Rural)200 kbps 200 kHz QPSK 3GPP56 Chapter 3 HetNet/M2M/D2D communication in 5G technologies3.2.6 5G-HetNet H-CRAN fronthaul and TWDM-PON backhaul: QoS-aware
virtualization for resource management
An optimum resource management technique using QoS-aware virtualization in H-CRAN of 5G-
HetNet is proposed in Zhang, Huang, Zhou, and Chen (2020) . The authors have proposed a Particle
Swam Optimization (PSO)-based QoS mapping algorithm for Time Wavelength DivisionMultiplexing- Passive Optical Network (TWDM-PON) used in 5G-HetNet backhaul. The objectiveof this work is to optimize resource management by improving QoS. 5G virtualization and load-balancing mechanism based on QoS mapping algorithm in H-CRAN and backhaul is used toimprove performance of 5G-HetNets. Large scale co-operative signal processing and network func-tions is used in the H-CRAN based front-haul comprising of RRHs and MBSs. However, the back-haul of 5G-HetNets comprised of BBU pools are the new bottleneck for resource allocation. The
challenge is that the backhaul must keep up with the resource demand of H-CRAN in terms of high
capacity, low latency, network availability, high energy, and cost efficiency. This potential changeof improving QoS and reducing bottleneck in the backhaul is addressed using the proposed QoS-aware algorithm. The performance of TWDM-PON-based backhaul is improved by optimization ofresource allocation using Network Virtualization (NV) and load balancing strategy aided with PSObased QoS mapping algorithm for better service provisioning (SP). Co-operative MultiPoint(CoMP) communication helps in reducing interference hence improving QoS. There are two morepractical deployment challenges: (1) Use of multiple SPs using the same backhaul where the
resource management is complex and (2) Inconsistent QoS-based priority queue (PQ) deployment
between TWDM-PON (backhaul) and H-CRAN (fronthaul). To address the technical challengesthe proposed NV based resource management architecture has two functional approach: (1) Wave-length allocation based on load balancing and (2) QoS mapping algorithm to mitigate QoS mis-match between TWDM-PON and H-CRAN.
Fig. 3.5 shows the 5G HetNets architecture based on
TWDM-PON (backhaul) and H-CRAN (fronthaul) for optimum resource management.
FIGURE 3.5
5G HetNets architecture based on TWDM-PON (backhaul) and H-CRAN (fronthaul) ( Zhang, Huang, et al.,
2020 ).57 3.2 Heterogenous networks in the era of 5G3.2.7 Spectrum allocation and user association in 5G HetNet mmWave
communication: a coordinated framework
5G-HetNets architecture enables traffic off-loading with dense deployment of RRHs based small
cells (data plane) operating on different frequency band based-on multiple radio ANT (heteroge-
nous network) like 5G-NR, 4G/LTE, Wi-Fi, NB-IoT etc. Multitier HetNets share the microwavespectrum. Coordinated heuristic noncooperative game theory (state-of-the-art) method is proposedfor user association and spectrum allocation in 5G-HetNets (
Khawam, Lahoud, El Helou, Martin,
& Feng, 2020 ). The parameters considered in the proposed methods are: mmWave density, cell
load, user distribution, massive MIMO with high heterogeneous network settings. In the proposedwork, a 6GHz spectrum is considered for allocation in small-micro cell deployment based on cellload, user distribution, spectrum efficiency and frequency reuse. Noncooperative game theory uses
Nash-equilibrium to attain the best convergence for optimum cell density, cluster size, user density,
traffic distribution; those are the global objectives of any 5G-HetNet deployment scenario.
3.2.8 Diverse service provisioning in 5G and beyond: an intelligent self-
sustained radio access network slicing framework
Network slicing for concurrent SP of HetNets for QoS sensitive networks in the era of 5G and
beyond is the need of the hour especially keeping in mind the heterogeneity in RAN and diverseSP requirements of HetNet without compromising with QoS. The challenge is to effectively slicethe RAN with diverse QoS requirements (for multiple concurrent SP) and dynamic heterogeneitywith the use of different wireless access technologies at RAN. However, a self-sustained RAN slic-
ing framework (self-managed Network) using adaptive control strategy is proposed in
Mei, Wang,
and Zheng (2020) . The proposed framework has intelligent features like self-organizing/optimiza-
tion, self-slicing control based-on the most recent approach of Self-Learning Networks (SLN). TheRAN slicing method proposed in this work is hierarchical in structure. This multitier RAN slicinghas three levels: (1) Network-level for RAN and radio resource management, (2) gNodeB-level forcell management and (3) packet-scheduling-level for resource management. This approach enablesmore granularity in terms of resource management strategy giving more fine adaptive control overthe QoS on a very diverse scenario of multiple SP requirements over 5G HetNets. The proposed
transfer learning approach using AI-based algorithms helps in migrating from model-based learning
to autonomic self-learning network (SLN)-based RAN slicing for QoS enhancement in diverse SPin 5G HetNets.
3.3 Device-to-Device communication in 5G HetNets
3GPP standard for 5G mobile communication provides an ad hoc communication framework for
direct D2D communication between UEs without relying on the gNodeB (gNB) (does not requireRRHs in data-plane and MBS in control plane). This D2D mechanism of handling local traffic(UE-to-UE traffic inside close proximity radio coverage area) helps in traffic off-loading andreduces bottlenecks in 5G RAN fronthaul as well as backhaul. The D2D mechanism particularlyimproves the performance of the network in mMTC type communications with massive UE density58 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesin a geographical radio coverage area. 5G architecture supports millions of UEs in a radio coverage
area using mMTC framework. However, this creates a huge overhead on resource allocation in the5G RAN infrastructure. Most of the mMTC traffic creates a challenging resource allocationrequirement of huge number of small data-blocks as the UEs in this scenario would be millions ofresource constraint embedded devices particularly of IoT and Machine-to-Machine (M2M) type
communication. The D2D communication strategy being deployed the UEs would communicate
directly with their peers hence the load on 5G RAN infrastructure would drastically reduce withimprovements in overall QoS in the network.
Fig. 3.6 shows the D2D communication in 5G
HetNets. The close radio proximity UEs can communicate with their neighboring UEs (as shown inthe figure) on-demand over a D2D ad hoc radio channel instead of depending on the 5G infrastruc-ture. Only the remote node traffics (external traffic not local) are communicated via the gNB (basestation) using existing 5G RAN infrastructure. This creates a hybrid communication architecturewith ad hoc D2D communication (UE-UE) and infrastructure-based communication (UE /C0gNB).
FIGURE 3.6
Device-to-device communication in 5G HetNets.59 3.3 Device-to-Device communication in 5G HetNetsThis mechanism of dealing with Local traffic (UE-to-UE traffic inside close proximity radio
coverage area) by sending directly from UE to UE in an ad hoc communication without relying onan gNodeB/gNB in a 5G RAN drastically reduces the overall load on the front-haul as well asback-haul of 5G-NR and would help in reducing the bottleneck at gNB due to large number of UEsespecially for mMTC. However, deploying D2D communication framework in a Cell would have
its own set of challenges. The major technical challenges in D2D based ad hoc communications are
as follows:
1.Multiple wireless hops.
2.High latency.
3.High complexity in resource allocation for a hybrid scenario where both ad hoc (UE-UE) as
well as infrastructure (UE-gNB) communication with separate radio links takes place.
4.Computational overhead.
5.Mitigation of potential interference between two links (1. UE-UE: ad hoc D2D and 2. UE-gNB
infrastructure). However, 5G-NR standard proposes beamforming technique to deal withinterference in this case but the process is complex and would create a substantialcomputational overhead on UE for D2D communication.
6.Vulnerable to eavesdropping as several wireless broadcast links are established without
centralized network control which is a huge network security challenge in D2D communication.
A D2D-enabled resource management of 5G and beyond HetNets is proposed in
Irrum et al.
(2021) . The proposed system has a secured framework of communication with optimum resource
management strategy deployed for D2D communication in 5G HetNets and beyond. The proposed
secured communication framework considered both the type of communication scenario (1)Cellular users (CUs)—infrastructure-based communication and (2) D2D Users (DUs)—ad hoc com-munication. The authors have proposed a mixed integer nonlinear programming (MINLP)-basedapproach to improve secrecy rate, throughput, EE over separate channels for CUs and DUs. Theproposed system achieved promising results in maximum secrecy rate with physical layer modelingusing MINLP.
D2D communication technique in 5G HetNet would be instrumental in off-loading of local traf-
fic from the 5G H-CRAN based fronthaul as well as optical backhaul by optimizing resource allo-
cation. D2D communication would substantially improve the overall network performance as thelocal traffic would be handled separately over ad hoc wireless links mitigating severe bottleneck onH-CRAN with massive machine type communication environment. However, for enabling suchD2D communication many challenges have to be addressed appropriately. Some of the major tech-nical solution required for optimized deployment of D2D communication framework on 5G HetNetcan be summarized as below:
1.UEs must be computationally regressive enough to handle complexity of multiple ad hoc
wireless links as per the scenario-specific requirement of on-demand and dynamic local trafficin a radio coverage area. Devices should have minimum commuting resources to handle thecomplex signal processing required for D2D communications.
2.UEs must have smart antenna technologies and D2D communication reply mostly on complex
beamforming techniques to mitigate potential interference from separate wireless links as the60 Chapter 3 HetNet/M2M/D2D communication in 5G technologiescommunication is hybrid having ad hoc D2D wireless channel and UE-gNB (base-station)
infrastructure based wireless channel.
3.The resource allocation strategy must be optimized by focusing on complex hybrid
communication paradigm as well as scenario-specific requirement of massive D2D typededicated wireless channels which might be the case for mMTC/ M2M based local traffic for
over millions of device deployment scenario.
3.4 Machine-to-Machine communication in 5G HetNets
3GPP standard R14 and newer has proposed mMTC for 5G-NR architecture supporting millions of
devices in the CRAN through gNodeBs. This is technically viable through mmWave communica-tion with a huge number of channels using millions of narrow-band (NB)-IoT devices for realizingmMTC in true sense. 5G provides URLLC for low-latency ( ,1 Ms in RAN) to make this
Machine-to-Machine (M2M) communication viable. M2M communication has native support in
5G-NR enabling millions of NB-IoT devices to communicate over CRAN. The SDN architectureprovides the flexibility to support large number of software defined channels to accommodate mil-lions of devices over RAN. Even the flexibility of data-control plane splitting and CoMP communi-cation is key to attainment of flexibility much needed for mMTC framework in 5G-NR. CognitiveRadio technology driven Smart Objects (CRSOs) are instrumental in the deployment of M2M com-munication for a huge number of devices over a same radio coverage area with spectrum constrainsespecially when there are large number of narrow-band channels to be associated with gNB (in a
Cell).
However, there are quite a few challenges with the deployment of mMTC. A few of the pre-
dominant challenges in M2M deployment are as follows:
1.Legacy systems are designed only for few hundred devices per BS and not millions.
2.Achieving low latency on a live network is a challenge especially for huge number of devices
mMTC hence critical low-latency application like smart healthcare is a challenge.
3.The M2M communication has a small payload size. This means millions of small data-blocks
for diverse devices is a challenge to allocate real-time network resource pool.
4.Control and channel estimation in M2M communication is a huge overhead.
3.4.1 Machine-to-Machine communication in 5G: state of the art architecture,
recent advances and challenges
Unlike human-to-human communication, M2M has distinct features as listed below:
1.mMTC type large number of small data packets massive number of device deployment in a
geographical radio coverage area.
2.Need for interoperability as the devices (hardware and software), networks are dissimilar and
heterogenous.
3.Provision for support of HetNets as both wired and wireless scenarios of deployment may be
feasible as per scenario-specific requirements of M2M application.61 3.4 Machine-to-Machine communication in 5G HetNets4.5G integration is key to achieve enhancement in M2M deployment.
5.Guaranteed QoS-based service is required for M2M communication.
6.Exponential growth and market potential of M2M/IoT application in recent times.
7.Legacy systems like 4G and Wi-Fi based networks does not have sufficient resource to handle
mMTC type communication hence 5G HetNet integration is imperative as 5G has deign
advantages over older legacy systems with SDN-based C-RAN resource management
flexibility, Cognitive Radio, mmWave and MIMO technologies.
8.5G network architecture supports massive M2M (mMTC), enhances data-rate (eMBB), less
End-to-End latency (URLLC), improved EE over small cells, increased battery life for M2Mdevices and increased network capacity to accommodate massive number of devices per cell.
9.Different Standard Development Organizations (SDOs) like 3GPP, ETSI, IEEE, W3C are active
in the development of cutting-edge solutions in the IoT and M2M market space.
M2M communication in 5G has three domains of operations as described below (
Mehmood,
Haider, Imran, Timm-Giel, & Guizani, 2017 ):
1.M2M device domain: sensor/actuators equipped low-power embedded devices forward data to
the nearest nodes and gateways (multihop ad hoc networks) are part of the device domain.
2.M2M communication domain: data forwarding nodes like M2M gateways (M2M-GWs), Rode
Side Units (RSUs), gNBs, RRHs, MSBs, BBU pool, WAN, xDSL, Optical backhaul all can bepart of communication domain.
3.M2M server/ application domain: M2MMiddleware, service-layer and business provisioning
layers are part of server/application domain.
Technical advantages of enabling 5G technology for M2M communication are listed below:
1.Improved core network/ backhaul: 5G network with TWDM-PON based backhaul and NFV has
improved core network resource and QoS-based performance enhancements.
2.Improved radio access/ RAN technologies: H-CRAN based HetNet support, SDN, D2D, data
and control plane splitting and enhanced radio resource allocation.
3.Improved trans-receivers: technologies like mmWave, MIMO, beamforming and smart antenna
have substantially improved the Trans-receivers in 5G networks.
4.Enhanced radio interface: 5G provides enhanced radio interface with more spectral radio
resource and operational in mmWave frequency range ( .30GHz).
5.Cell enhancement: 5G network has small (pico/femto) cells in data-plane to enhance device
density and network capacity and MBSs for control-plane.
3.4.2 Recent advancement in the Internet of Things related standard: oneM2M
perspective
In the era of IoT mMTC based communication framework is imperative for integration of millions
smart devices (Sensor/actuator control) to the 5G HetNet H-CRAN for diverse SP. IoT-based net-work deployment demands a single converged communication framework for seamless integrationand interoperability especially when different (heterogenous) wireless access technologies are usedfor scenario-specific requirement of massive M2M communication. oneM2M is a unified62 Chapter 3 HetNet/M2M/D2D communication in 5G technologiescommunication framework for massive M2M type communication with 5G and heterogenous net-
works. IoT and mMTC-based oneM2M frameworks provide a collaborative approach for interoper-ability among heterogenous devices and networks. oneM2M provides an abstract semantic basedunified layered architecture for seamless communication among heterogenous devices in the era of5G5G HetNets. A semantic-enabled IoT platform would help in understanding IoT data in a stan-
dard and generic perspective for all types of dissimilar devices for achieving seamless interoperabil-
ity. OneM2M interoperability specifications guarantee IoT devices of different vendors tocommunicate with each other over a common M2M framework. OneM2M architecture has specificthree functional layers, distinct node types and their functionals attributes (
Park, Kim, Joo, & Song,
2016 ). The three-layer architecture of oneM2M framework as given below:
1.Application entity (AE): application layer
2.Common service entity (CSE): service layer. There are twelve CSE entities proposed by
oneM2M given in Table 3.3 below:
3.Network service entity (NSE): network layer
Reference points: Three reference points (interfaces) are provisioned for interlayer communica-
tion. “ Mca” interface between AE /C0CSE, “ Mcn” interface between CSE-NSE and “ Mcc” interface
between multiple CSE-CSE.
oneM2M Nodes types: There are three major types of nodes in oneM2M architecture:
1.Application service node (ASN) or application dedicated nodes (and): M2M end devices
2.Middle node (MN): M2M gateways
3.Infrastructure node (IN): M2M servers and platforms with support for big-data analytics and
cloud-based SP
Unified service oriented M2M functional Architecture: an overview of oneM2M reference
model is presented for interoperability in 5G5G HetNets for massive machine type communication.
Fig. 3.7 shows the oneM2M architecture and its functional components.
OneM2M Release-1 (2014) has proposed a set of Common Service Functions (CSFs). The
important CSFs are given in Table 3.4 .
OneM2M Release-2 (2015) has proposed some advancement over the oneM2M-R1. The main
functions of oneM2M-R2 are as follows:
1.Platform/internetworking functions : Supports five types of internetworking; (a) AllJoyn, (b)
OIC, (c) Light-weight M2M, (d) Generic and (e) 3GPP Release-13.
2.Home domain support : oneM2M-R2 has support for home appliances information model.
3.Industry domain support : oneM2M-R2 has support for time-series data for industrial IoT
applications.
Table 3.3 Common service entities proposed by oneM2M.
1. Registration 5. Data management repository 9. Communication management
2. Discovery 6. Subscription and notification 10. Network service exposure3. Security 7. Device management 11. Location
4. Group management 8. Application and service management 12. Service charging and accounting63 3.4 Machine-to-Machine communication in 5G HetNets4.Semantics : oneM2M-R2 has specific three sets of semantic support for semantic-based
interoperability; (a) oneM2M base ontology, (b) Semantic discovery and (c) Semanticdescription.
FIGURE 3.7
OneM2M architecture and its functional components.
Table 3.4 Common service functions in oneM2M R1 (2014).
Common service functions (CSFs) Basic functionalities
Communication management and delivery
handling (CMDH)Connection, communication and scheduling
Data management and repository (DMR) Data—storage, mediation, aggregation, formatting
Device management (DMG) Device (ASN, MN, IN) capabilities managementDiscovery (DIS) Text-matching, information searching of attributesGroup management (GMG) Management of group requestsNetwork service exposure, service execution,
and triggering (NSSE)Communication management with underlying 3GPP, 4G-
LTE, 5G5G-NR Zigbee, etc.
Registration (REG) Service registration for AEs and CSEs
Security (SEC) Access Control. Authentication/ authorization
Subscription and notification (SUB) Tracks resources based on subscription and then notifies64 Chapter 3 HetNet/M2M/D2D communication in 5G technologies5.Security : oneM2M-R2 has two approaches for resilient network security model in place; (a)
End-to-End security and (b) Dynamic authorization.
OneM2M provides service layer interoperability over multiple proprietary systems. The chal-
lenges over a legacy heterogenous systems are: (1) inflexibility, (2) lack of modularity and capabili-ties of service extension, and (3) lack of provision for integration of new services. The solution forall these challenges is oneM2M which is global framework and would work with all legacy andproprietary systems with a common service layer functional architecture (
Swetina, Lu, Jacobs,
Ennesser, & Song, 2014 ). CAPEX and OPEX for proprietary solutions are always more as com-
pared to a single, common, open, and generic one like oneM2M. OneM2M provides a horizontally
common platform for multiple M2M type industrial verticals. The oneM2M provides (1) architec-
ture, (2) candidate protocols, (3) security aspects, (4) device management, and (5) abstractiontechnologies.
OneM2M standard has been developed by a consortium for various standardization organization
/ Standard Development Organizations (SDOs) and more than 270 companies across the globe.oneM2M developer consortium includes eight major SDOs: (1) Association of Radio Industries andBusinesses (ARIB), Japan; (2) Telecommunication Technology Committee (TTC), Japan; (3)Alliance for Telecommunications Industry Solutions (ATIS), United States; (4)
Telecommunications Industry Association (TIA), United States; (5) China Communications
Standards Association (CCSA), China; (6) European Telecommunications Standards Institute(ETSI), Europe; (7) Telecommunications Technology Association (TTA), South Korea; (8)Telecommunications Standards Development Society (TSDSI), India.
oneM2M alliance has three major industry forms: (1) Open-Mobile Alliance (OMA); (2) Broad-
band Form (BBF); (3) Home Gateway Initiative (HGI).
There are five working groups (WGs) associated with oneM2M are: (1) WG1- Requirements;
(2) WG2-Architecture; (3) Protocols; (4) WG4-Security; (5) WG5-Management, Abstraction and
Semantics.
3.4.2.1 Advantages of oneM2M
1.Boost M2M economy by reducing time to market.
2.Simplify development by common APIs
3.Leverages existing world-wide network with enhancements in protocols, services and business
operations.
4.Provides evolution, interoperability, and standard functions.
3.4.2.2 OneM2M protocols
1.Service layer protocols: RESTful, URI, APIs based protocol design for oneM2M.
2.Encapsulation in existing protocols: Legacy TCP/IP stack is used for oneM2M PDUs when
required.
3.Binding with underlying protocols: OneM2M binds with LoRa, Zigbee, BLE, SigFox, NB-IoT
based ANT through ADN/ASN—MN (M2M gateways).
4.Interworking with non-M2M systems: Interoperability with legacy application layer protocols
like MQTT, CoAP, XMPP etc.65 3.4 Machine-to-Machine communication in 5G HetNets5.Security: Lightweight Datagram Transport Layer Security (DTLS) over CoAP in implemented
in oneM2M.
6.Working mechanism: KNX based abstraction and semantic interoperability.
3.4.2.3 OneM2M standard platform: a unified common service-oriented communication
framework
1.End-to-End platform with common service capabilities.
2.Semantic interoperability using common APIs.
3.Seamless integration over heterogenous networks and legacy systems.
4.oneM2M software framework is especially designed for IoT and M2M traffics providing a
single a common convergence platform for interoperability of various types on hardware,software, storage, communication and data.
5.Supports required M2M functionalities like security (authorization and encryption), service
scheduling, and notifications.
6.Provides co-operative intelligence over devices, gateways, and clouds.
7.oneM2M is a like a generic operating system (OS) compatible for sensors, devices, gateways,
and clouds.
8.Common, generic, and global in standard hence provides a seamless interoperability for all
M2M based nodes.
Fig. 3.8 shows oneM2M as a common service-oriented platform providing global interoperabil-
ity for dissimilar devices and network entities hence making seamless integration of heterogenoussystems possible for M2M communication (
Gezer & Ta¸ skın, 2016 ).
The OneM2M architecture has three operational layers:
1.Connectivity layer: IoT devices using mMTC type communication to connect to the HetNet
gateways (GWs) which has traffic aggregation (TA) and seamless interoperability to the IP-based network backbone. Various dissimilar devices and network segments like NB-IoT,Zigbee, SigFox, LoRa etc. can be integrated to a common IP-based network.
2.Service layer: OneM2M unified common service layer provides semantic interoperability to
various types of heterogenous device and data traffic to application layer interface through
common Application Programming Interfaces (APIs).
3.Application layer: Scenario-specific applications like Smart home/cities, smart grid, Industrial
IoT, smart healthcare and medical application etc. are supported by the application layer usingAPPs and specific functional APIs.
3.4.3 M2M traffic in 5G HetNets
M2M traffic is comprised of low data-rate, small packets PDU with correlative transmission. A
two-phase traffic control (2PTC) architecture is proposed in Li, Rao, and Zhang (2016) for opti-
mum M2M traffic management in M2M deployment scenario in 5G5G HetNets. The proposed mul-tipath Traffic Engineering (TE) model has Virtual Service Gateway (V-SGW) to aggregate trafficfrom multiple machines (ADNs/ ASNs) and provide data to the sink. TA mechanism is used at theV-SGW for multiplexing large number of small sized data to a single logical data resource block to66 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesleverage the 5G5G based H-CRAN resource block in optimized approach. Fig. 3.9 shows the two-
phase traffic control (2PTC) for M2M communication.
Phase-1: Machine-to-Gateway (M2GW) traffic: Low data-rate, small packet distinct data-
traffics are forwarded form machines (End-Nodes/ASNs/ADNs) through respective wireless accesspoints via the intermediate routers to the Virtual Service Gateway (V-SGW). This phase-1 commu-
nication is called machine to gateway. Phase-2: Gateway-to-Sink (GW2S) traffic: TA takes place at
the V-SGW and the cumulative data traffic is sent to the designation or sink node. This phase-2communication is called gateway to sink. The proposed mixed integer programming (MIP) based2PTC based gateway solution has following challenges in any practical deployment scenario: (1)Optimum Machine-Gateway association; (2) Minimize association cost; (3) Minimize TA/Gatewaycounts. The V-SGW/TA alleviates large quantity of mMTC type traffic (small packets) challenge.5G5G enabled M2M communication framework has following technical advantages:
1.Flexible and scalable SDN based H-CRAN in 5G5G has granularity in resource management.
2.Better resource manageability over separate control plane (using MBSs -macrocells) and data
plane (using RRHs—pico/femto cells) in 5G5G.
FIGURE 3.8
OneM2M communication framework.67 3.4 Machine-to-Machine communication in 5G HetNets3.Agile network architecture using Node-C (Cloud Nodes) and SDN controllers (SDN-C) using
OpenFlow for better traffic management. Using better data-flow management and resourceallocation using SDN-based framework provides improvement in required QoS parameters withresource constraints.
3.4.4 Distributed gateway selection for M2M communication cognitive 5G5G
networks
M2M communication supports large number of devices (mMTC) with minimum to no human inter-
action. Cognitive 5G5G networks provides high spectrum efficiency (SE) and high bandwidth
(spectrum resource) using mmWave technology. Multiple gateways (GWs) deployment for M2M
communication is imperative to increase throughput, coverage, EE (smaller cells), and capacity.However, a single channel Carrier Sense Multiple Access (CSMA)-based MAC protocol would cre-ate challenge of interference from primary/secondary M2M devices. The solution for this challengeis a distributed gateway selection mechanism proposed in
Naeem et al. (2017) using Decentralized
Multiple Gateway Assignment Protocol (DGAP). A lightweight variant of the same called LowOverhead DGAP (Lo-DGAP) is used utilizing a multichannel CSMA-based MAC protocol for
FIGURE 3.9
2PTC for M2M communication ( Li et al., 2016 ).68 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesM2M communication over cognitive 5G5G networks. To minimize the interference the GW device
would transmit to worst primary M2M device rather than to all neighbors as per the proposed pro-tocol. The proposed Lo-DGAP increases throughput, reduces header size (protocol overhead) andimproves EE. The architecture of Lo-DGAP has a cloud computing based Central Control Unit(CCU) which facilitates the following:
1.Lo-DGAP requires less control traffic for its network operation hence increases EE
substantially.
2.Lo-DGAP has a reward function mechanism to assign weights to the co-operating GW devices.
This mechanism would be helpful in load-balancing across multiple GWs and hence improves
throughput, capacity and spectrum efficiency.
3.Lo-DGAP is a light-weight protocol with less header size hence improves protocol efficiency.
4.Lo-GGAP required less control traffic hence improves network efficiency.
3.4.5 Algorithm for clusterization, aggregation, and prioritization of M2M
devices in 5G5G HetNets
There are quite a few challenges in deployment of massive M2M communication due to very large
number of devices to be connected to the C-RAN of 5G5G HetNets. The major challenges of M2Mcommunication over a 5G-HetNet legacy system are as follows:
1.Huge number of M2M devices, and therefore a large amount of control traffic.
2.Complexity in resource management in 5G5G HetNets as centralized pre-synchronized devices
are forming network cluster hence allocation of data transmission blocks to gateway is achallenge.
3.Data aggregation, classification (Group management) is a challenge due to massive number of
traffic profiles.
Due to above challenges it is imperative to device a mechanism or deploy algorithm to address
such challenges raised due to mMTC type large number of M2M devices and traffic. A FuzzyLogic System (FLS) based algorithm is proposed in
Klymash, Beshley, Seliuchenko, & Beshley
(2017) . The FLS-based proposed system has distinct attributes to solve above challenges by reduc-
ing signal load on Base Stations (BSs) by reducing control traffic. Secondly, the proposed system
is promising in its results for better radio resource management with more energy efficient design
hence increasing the battery life of M2M devices. The proposed cluster-based network architecturehas Master and Slave M2M devices. Each master device cater service as a cluster head providingTA features on the cluster and multiple clusters communicate to a centralized M2M-Gateway(M2M-GW). The M2M-GW sends the aggregated data to the gNB of 5G5G H-CRAN in a HetNetdeployment scenario. The FLS-based algorithm provides the following functions:
1.Clusterization: The system uses a Master-Slave communication model for M2M devices, where
master device is acting as a cluster head and all the participating slave devices connected tomaster act as member M2M device nodes of the respective cluster.
2.Data aggregation: All the data from Slave nodes are sent to the master node, and at the master
node, all M2M device traffics are aggregated and forwarded to M2M-GW.69 3.4 Machine-to-Machine communication in 5G HetNets3.Classification of data and prioritization: The proposed system has QoS Class Identifier (QCI) a
metric in a scale ( Li et al., 2018; Ouamri et al., 2020; Peng et al., 2015; Sun et al., 2015 ) using
Type of Service (ToS) fields of IP packets for classification data traffic and based on the QCIpriority to data traffic can be assigned.
3.5 Heterogeneity and interoperability
The IoT is described as a complicated heterogenous network where things (devices) are integrated
among themselves with wired or wireless medium to exchange information. Hyung-Jun Yim et al.described the “thing” as data, events and services (
Yim et al., 2017 ). The things have the capabili-
ties of data sensing, data capture and storage, data processing and communicating. The features of
things including location, address and control perform through several communication protocols
such as ZigBee, Wi-Fi, Bluetooth, 6LoWPAN etc. However, IoT devices demand interoperable pro-tocols for seamless integration of intelligent devices to the network. To establish a smart environ-ment using IoT (such as smart home, smart healthcare, smart agriculture, etc.), there is a need toidentify the device ID. The identification system in a single IoT platform is same where as it ischallenging to identify and operate in between different devices in different IoT platforms. At thispoint, it is required interoperability between different heterogenous devices in different IoT plat-forms. Some of the IoT platforms that are in developing stages are oneM2M, Apple’s HomeKit,
Google’s BrilloWeave, OCF’s IoTrivity, Samsung’s ATIK, GS1’s Oliot and AllSeen Alliance’s
AllJoyn. For Industrial and commercial utilization of the IoT device services there is requirementof global identification and semantic information integration of IoT devices. However, the interop-erability between IoT devices with other devices and with users faces some challenges such asextensive coordination, universal heterogeneity, unspecified IoT device configuration and semanticconflicts. Moreover, the aim of the requirement of interoperable protocol is to develop IoT networkwithout modifying its system and maintaining stability and high performance.
Various literatures described about the two type of interoperability problem, that is, (1) user
interoperability problem and (2) device interoperability problem. The user interoperability problem
arises between the user and the device (
Jardim-Goncalves, Grilo, Agostinho, Lampathaki, &
Charalabidis, 2013; Panetto & Cecil, 2013 ). The device interoperability problem arises between
two devices ( Kim, Lee, Park, Moon, & Lim, 2007 ). Few researches focused on global middleware
in heterogenous network to handle the device interoperability problem ( Moon, Lee, Lee, & Son,
2005 ).
3.5.1 User interoperability
To resolve the issues of user interoperability, the following cases need to be solved.
1.Locating devices
2.Interconnection with devices
The problem of finding devices as per the requirement of the user is solved by using IPv6 by
setting a link to the device and accepting an electronics product code (EPC) scheme for70 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesidentification of that device ( EPC Tag Data Standard, 2014 ). However, the issues of categorization
of devices into existing and new catalog to avoid semantic ambiguity arise in locating a device.Considering the case of interconnection with devices after its discovery, the device needs to be in asupportive format (syntax and semantics) to the user. At this point, two problems emerge such assyntactic device interoperability and semantic device interoperability. In syntactic device interoper-
ability, the instruction from user is executed by the device and the messages from the device are
executed by the user’s system. In semantic device interoperability the information from the deviceto the user is explained by the user and the information from the user to the device is explained bythe device.
3.5.1.1 Locating the device through identification and classification
For identification of every device connected in IoT platform, EPC code standard plays an importantrole to accept the static data of a device. EPC code standard is developed by GS1 AISBL in uni-form resource identifier (URI) format (
EPC Tag Data Standard, 2014 ). Its unique identification sys-
tem can indicate the static device object belongs to a particular product. The disadvantages of EPCare the ambiguities created in identification process which need to be taken as challenge for furtherresearch. The IP address and the URI can be taken as unique identifier. Considering the IP address,
the IPv6 can be used by the device to sense, actuate and communicate (
Dunkels & Vasseur, 2008 ).
This IP address identifier can only be applied for the object in static position but for moving androaming there must be created a unique device ID. Devices having in built web server can beaccessed by URI. In
Guinard and Trifa (2009) , the authors stated about RESTful resources (web
service API) to access the internet-based objects through URI. Different devices have differentdevice ID; therefore users face difficulties to remember every device ID. For classification ofdevice IDs two standards introduced in IoT, that is,. UNSPSC and ecl@ss (
Xiao, Guo, Da, &
Gong, 2014 ).
3.5.1.2 Syntactic and semantic interoperability for interconnecting devices
To handle the syntactic heterogeneity by exchanging the messages between the device and thedevice users without having any communication problems, few approaches have been adopted.
According to various research articles, some of the technologies are introduced to remotely access
the devices such as Web services (
Guinard, Trifa, Karnouskos, Spiess, & Savio, 2010 ), open and
close standard protocols ( Baronti et al., 2007; Callaway et al., 2002 ), RESTful ( Lanthaler & Gu ¨tl,
2012 ), and SOC ( Guinard & Trifa, 2009 ). Along with this, few exiting heterogeneous standards,
including IEEE 802.15.4 and ZigBee, are unable to satisfy the interoperability among heterogenousdevices. Therefore to manage the above issues of interoperability among heterogenous devices, themiddleware technologies concept has come to light though various IoT research articles (
Gama,
Touseau, & Donsez, 2012; Kim et al., 2007; Moon et al., 2005 ). Semantic interoperability is con-
sidered as important issues in device interconnection as here the device and user have to understand
the exact meaning of the sent and received messages from and to them. To set up a semantic inter-operability between device and user in IoT platform, ontology plays a key role to maintain a rela-tionship between the objects (
Wang, De, Toenjes, Reetz, & Moessner, 2012 ). However, ontology
gives a benefit to the device situated in the range of same context but it has limitations for crosscontext devices. To control this problem, collaborating on the conceptual theory has been proposedwhere things are considered as signs as semiotics and when a device and a user from different71 3.5 Heterogeneity and interoperabilitycontext connected they first collaborate and built semantic signs ( Guo, 2009 ). Moreover, the inter-
operability problems still need further research for smooth integration of device and users betweendifferent contexts in IoT environment.
3.5.2 Device interoperability
Device interoperability is needed to establish a seamless communica tion between different heterogenous
devices in IoT in a particular defined domain or differe nt domain. It requires the integration of different
heterogenous device (smart device) that may be h igh end or low end. The advantages of high end IoT
devices consisting of sufficient resources and high co mputation capabilities and the limitations of low end
IoT devices such as less resource (energy and power) a nd lack in communication capabilities encourages
to integrate between devices to provide better perfo rmance in terms of services. Some of the existing pro-
tocols and standards to support various applications , that is, for e-healthcare the medical device support
ANT1standard and for the wearable device it is NFC and Bluetooth SMART. For the smart environ-
ment, the IoT-based sensor is connected through ZigB ee. Along with this standard, some of the nonstan-
dards communication medium including LoRa, SIG FOX also support sensing and communicating. The
main characteristics of device inter operability are to exchange informa tion between diff erent heterogenous
devices and protocols and to develop capabilities of integration o f new devices in to the IoT environment.
Comparative details are provided in Table 3.5 .
3.6 Research issues and challenges
The increased requirement of mobility and connectivity of mobile devices in anytime and anywhere
increases the deployments of wireless networks to fulfill the demands of wireless communication
Table 3.5 Classification of interoperability in Internet of Things.
Types of
interoperability Supporting technology developed Research issues and findings
User
interoperability
(syntactic andsemantic)WSDL, RESTful, SOC, open and closed
standard protocolLocating devices and interconnecting
them.
Device
interoperabilityANT1standard, Wi-Fi, Bluetooth SMART,
ZigBee, NFC, Wireless HART, 3G/4G, Z-
Wave, LoRa and SIGFOX.Exchange of information between
heterogenous devices and integration of
new device to IoT platform.
Network
interoperabilityWired and wireless communication standards. Exchanging information in different
networks by solving the issues of QoS,
resource management, routing, security,
mobility, and addressing.
Platform
interoperabilityApple’s Homekit, Google’s Brillo, Amazon
AWS, Contiki, TinyOS, RIOT, and Open WSN.Cross platform interoperability need
further research to support smart IoT
application.72 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesservices capabilities. At this point, the large volumes of data-oriented services and applications
play a key role for the emerging of 5G5G communication technology. The 5G5G new radio systemis going to establish the connectivity among people and other equipment, vehicles and machinethrough cellular network, IoT, V2V and M2M respectively (
Camacho, C ´ardenas, & Mun ˜oz, 2018 ).
3.6.1 Resource allocation
The process of resource allocation includes the shar ing of network resources i n wireless communications
by maximizing the transmitted data to the users. Th e conventional resource allocation method is ineffi-
cient to meet the demand of large volume of data-oriented applications and services. Therefore efficientradio resource management technique is needed for proper utilization of spe ctrum. However, various
researchers have devised different radio resource techniques to optimize the system parameters such assystem throughput, SE, fairness, Q uality of Service (QoS) and Quality of Experience (QoE). The system
throughputs can be measured by considering the total d ata rate passed through a device in a network, that
is, bits per second (
Chen, Li, Chen, Lin, & Vucetic, 2016; Liu, Chen, Yu, & Li, 2019 ). Few literatures
proposed some of the resource allocation techniques wh ich is going to be helpful in solving further issues.
InAlnoman, Ferdouse, and Anpalagan (2017) , the authors proposed a user association and bandwidth
allocation technique for heterogen ous network using fuzzy logic contro llers. According to user demand
and bandwidth availability, the controllers manage t he bandwidth allocation to each user by maximizing
the throughput in the network. At this point, the approa ch shows enhancement in dat a rate, resource utili-
zation, and number of offloaded users.
InWang, Chen, Tang, and Wu (2017) , author considered an approach using user association
and power allocation to resolve optimization issues. Here, the author considered two sub problemsfor solving the resource allocation and power allocations problem using two methods, that is, bygraph theory using Hungarian algorithm and difference convex function approximation method.This technique improved the overall system throughput but have limitations in proving services forUEs in bad channel conditions.
Feng, Mao, and Jiang (2018) proposed a technique of joint resource
allocation considering centralized and distributed types. Here, the author integrates resource alloca-tion with user association and frame design in a HetNet with MIMO. The result shows that the cen-
tralized technique has significant improvement in system throughput and robust toward different
network settings than the distributed technique. In
Yuan, Xiao, Bi, and Zhang (2017) author gave a
game theory-based approach considering joint resource and power allocation framework to fit intoa large coverage of network having a greater number of base stations (BSs). Some more literaturestates about the resource allocation with cognitive HetNet, multiband mmWave HetNet to solve theoptimization and improve the system throughput (
Ji, Jia, & Chen, 2019; Xu, Hu, Chen, Chai, & Li,
2017; Zhang, Wei, Wu, Meng, & Xiang, 2019 ).
Few studies have focused on resource allocation techniques considering another performance
parameter SE. In Ye, Dai, and Li (2018) , the authors proposed a model by applying the hybrid-
clustering game algorithm and combining the auction method to reduce the cross-tier and cotierinterference for improving the SE. But the limitations include priority and QoS. However, in
Ya,
Feng, Zhou, and Qin (2018) , the authors considered RAT in HetNet to increase the network
throughput and maintaining QoS by using multiagent reinforcement learning technique. The pro-posed model gave better results as compared to other scheduling methods in terms of optimizationand system throughput. Researchers also proposed resource allocation techniques using game73 3.6 Research issues and challengestheory, binary search algorithm and macro cells to enhance the SE and guarantee QoS of the net-
work ( Manishankar, Srinithi, & Joseph, 2017; Xie, Zhang, Hu, Wu, & Papathanassiou, 2018 ).
Along with QoS, another factor QoE (Quality of Experience) plays a key role in satisfying the userrequirements, like performance factors in QoS such as delay, jitter, and throughput and packet loss.
Wang, Fei, and Kuang (2016) considered a model for QoE in HetNet using macro cell and pico
cell for sharing of spectrum in transmission process. However, the disadvantage of this proposed
model is that it is limited to pico cell and increasing coverage area (cell size) worsens the QoE.Fairness is also an important parameter in resource allocation as there is need of equal distributionof resources according to the QoS requirement of the network. The studies in
Gures, Shayea,
Alhammadi, Ergen, and Mohamad (2020) ;Tang, Zhou and Kato (2020) focused on the fairness in
the resource allocation process by taking into consideration of different methods such as controllingthe sharing process in a centralized way, adjusting between priority and throughput using differentutility function weights and weighted α-fairness based optimization technique to maintain a trade-
off in performance of network fairness and system throughput.
3.6.2 Interference management
Interference management is considered as one of the important research issues as there may be chances of
interference between transmitted signals from comm unicating devices present in same or closer frequency
bands. There are two types of interfer ence present in multitier HetNets su ch as (1) cross tier and (2) cotier.
Cross tier interference is existing between the users pr esent in different network tiers whereas cotier exist
between the users in the same network tier. In Yang et al. (2018) , authors proposed a join interference
management and resource allocation technique with D2D based HetNet to reduce cross tier and cotierinterference. When an alternate method for joint allocation problem compared with conventional DL/ULtechnique, it is found that the delay in overall system process increased. The literature in
Xie et al. (2018)
described FFR-based methods where reduction in cross tier and cotier interfere nce were achieved along
with increased throughput and higher capacity. In Huang, Zhou, Luo, Yang, and He (2017) , authors pro-
posed a model using Logarithmic an d K-means clustering techniques to reduce the cross tier and cotier
interference and achieve higher throughput by consid ering frequency reuse and fairness. But it has the
limitation of increased in energ y consumption. Researchers in Kaneko, Nakano, Hayashi, Kamenosono,
and Sakai (2016) implemented the technique s of CSI overhearing, Norma lized PF scheduling (PFS), and
iterative water filling algorithms to a void the cross-tier interference to not to have additional control over-
head from MBS, and it can be applied in any type of channel quality aware scheduler. However, the
cotier interference is not consid ered for this model. The paper ( Xu, Hu, Chen, Song, & Lai, 2017 ) consid-
ered two techniques, that is, for MUEs, it is improv ed simulated annealing algorithm and for SUEs, it is
maximize minimum distanc e algorithm to manage cross tier inter ference and high throughput and having
issues of no service for SINR users. Lv, Liu, and Gao (2017) proposed a method using geometric pro-
gramming and convex relaxation approach for managin g cross tier interference and throughput to achieve
QoS of MUEs but didn’t consid ered the cell edge users.
3.6.3 Power allocation
Power allocation in 5G5G plays a key role as incoming communication need green technology for
less consumption of energy. Distribution of power according to the requirement of users can reduce74 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesinterference. Various literature describes the challenges and issues still exist in allocating power. In
the paper ( P´erez-Romero, S ´anchez-Gonz ´alez, Agustı ´, Lorenzo, & Glisic, 2016 ), the authors imple-
ment Q-learning and Soft-Max decision making and adoption of the logarithmic cooling techniqueto reduce power consumption. The fairness issue arises as it is not considered. However, the paper(
Lohani, Hossain, & Bhargava, 2017 ) considered four techniques such as discrete binary PSO, dual
decomposition method, dynamic programming, and greedy technique to reduce power consumption
and achieve higher throughput having limitations in high computational complexity. Niknam,
Nasir, Mehrpouyan, and Natarajan (2016) proposed a model using dual decomposition method, sub
gradient method, sub optimal complexity greedy algorithm to improve system throughput andreduce power consumption but having issues of high implementation complexity. Authors in
Munir, Hassan, Pervaiz, Ni, and Musavian (2016) considered noncooperative game and the dual
Lagrangian decomposition method for improvement of EE. At this point, this proposed model hav-ing the issues of lack of interference management. Authors in
Naqvi et al. (2018) implemented the
technique such as Hungarian algorithm, Weighted Tchebycheff and dual decomposition method to
get the advantages of real blockage effect and environmental geometry but having limitation in fair-ness. Another implementation considered in
Wang, Zheng, Jia, and You (2017) used sum rate max-
imization problem and first order approximation based iterative algorithm and get fast convergencewith improved throughput with fairness issues.
3.6.4 User association
User association is required to find the best available network for each and every user in HetNet.
The user association is dependent on the request, range of base station (BS) from user and qualityof the channel. Some of the current studies focus on issues of user association considering the per-formance matrices such as QoS, EE, power consumption and network utility. In
Zhou, Huang, and
Yang (2016) , the author proposed a method considering the technique of nonlinear mixer /C0integer
optimization and primal dual interior point method to improve EE, load balancing and throughputwith a limitation of high-power consumption with less UEs. The author in
Luo (2017) proposed a
model using BCD, ADMM, MF techniques to gain low complexity having issues of interference.In the paper (
Kim & Chang, 2017 ), the authors implement a model using the technique cost based
approach, relaxation and decomposition method, distributed power update method to get fast con-vergence behavior on large scale network with issues of fairness.
3.6.5 Computational complexity and multiaccess edge computing
Computational complexity is considered as an important issue in the resource allocation process to
establish a trade-off between the required processing time and the increasing hardware cost of the
machine. To maintain an equilibrium state between resource allocation and power allocation, the
algorithms based on game theory approaches must be solved through machine learning (ML) tech-niques as the future wireless communication having voluminous data demands artificial intelligence(AI) to be incorporated as RRM complexity in increasing (
Elsayed & Erol-Kantarci, 2019 ). In Lv
et al. (2017) , authors used the technique of dynamic programming and divide into two phases, that
is, planning and implementation to and generate resource allocation lookup table. Here, greedyalgorithm is used to reduce the complexity but as the number of users increases the complexity75 3.6 Research issues and challengesalso increases. Le, Sinh, Lin, and Tung (2018) gave an idea to study the complex traffic pattern
using data mining but different types of traffic increase the processing time. To achieve the lowlatency and high mobility in 5G5G communication system, multiaccess edge computing (MEC) hasbeen introduced to reduce the computational load. It computes the large volume of data and sendsit to the cloud with in the RAN through UEs. The deployment of MEC with cloud RAN can be
benefited from the network function virtualization (NFV) but faces an issue of network manage-
ment in HetNets (
Chabbouh, Rejeb, Choukair, & Agoulmine, 2016; Siddavaatam, Woungang, &
Anpalagan, 2019; Zhang et al., 2015; Zhang, Gui, et al., 2020 ).
3.6.6 Current research in HetNet based on various technologies
In 5G5G communication system, the compute intensive applications and services demand various
networks such as M2M communications, D2D communications, vehicle to vehicle (V2V) commu-
nications, IoT, WSN, mobile edge computing (MEC), cloud radio access network (CRAN) and
cloud computing to integrate among themselves to satisfy the technological requirements. To sup-port this type of integration of networks, the heterogenous network (HetNet) is evolved from theconventional homogeneous network. To enhance the quality of service of users, the HetNets sup-port four types of cell network such as macro-cell network, micro-cell network, pico-cell networkand femto-cell network. Along with different cell types, HetNets provide different communicationscenarios using various technologies such as conventional HetNet, OFDMA based HetNet, NOMAbased HetNet, Cooperative HetNet, H-CRAN, Multi antenna HetNet. In conventional network the
communication supports all types of cell network, that is, macrocell, microcell, pico cell, and femto
cell. The base station (BS) and the UE exchange the signal transmission through a single antennawithout any relay. Although this network able to provide high throughput and improve QoS to theusers but femto-cell interference still exists. To overcome the issues of interference between differ-ent subchannels, orthogonal frequency division multiple (OFDM) has been introduced. In
Rezvani,
Mokari, Java, & Jorswieck (2019) ;Song, Min-Jae, and Won-Chang (2020) , the authors proposed
two delay-based fairness techniques known as proportional fairness (PF) and min max fairness(MMF) to minimize the latency. The OFDMA based HetNets has been given preference (used
orthogonal sub carriers) over the conventional HetNets as it can mitigate the multiple access inter-
ference among users in same cell (
Bogale & Le, 2016; Ghosh & De, 2020 ).
Although there is negligible interference in OFDMA-based HetNets, there is a scarcity of
orthogonal resources. To overcome this limitation, NOMA-based HetNets is proposed for managinglarge number of users and achieving high throughput (
Moltafet, Azmi, Javan, Mokari, & Mokdad,
2019 ). In Nasser, Muta, Elsabrouty, and Gacanin (2019) , the authors described the interference
management technique and resource block allocation by proposing R-WFISTA algorithm and com-pared with traditional OMA HetNets and Conventional NOMA HetNets to improve the sum rate
and outrage probability. To increase the coverage area, the relays are introduced in HetNets to
become heterogeneous relay network (
Omran, Sboui, Rong, Rutagemwa, & Kadoch, 2019 ). The
integration of cloud computing and HetNets provide wide coverage area, improvement in SE, high-er throughput and reduce energy consumption. Along with this, use of multiantenna over single inHetNets improves communication, system capacity and SE. Therefore MIMO channels are pre-ferred to support multi users by designing appropriate transmitting beam-forming vectors for miti-gation of interference and better system performance (
Bogale & Le, 2016 ).76 Chapter 3 HetNet/M2M/D2D communication in 5G technologies3.7 Smart healthcare using 5G5G Inter of Things: a case-study
Integration of network makes an open opportunity in health care sector, where the sensors and other
devices attached to a patient’s body can transmit vital data to doctor’s or hospital attendant’smobile phone for immediate action. It helps in other applications of health care sector such as med-ical supply chain, medical device maintenance, etc. The concept of smart health care system using
WSN-MCN convergence network is going to be a reality due to this machine to machine communi-
cation system. WSN plays an important role in data collection from patient’s body and ready totransmit onwards for processing. The Mobile Cellular Network (MCN) component with 5G5G tech-nology, receive the data from WSN directly or through its mobile terminal (
Zhang, Shan, Hu, &
Yang, 2012 ). This convergence network creates a mutual associative network to support smart
health care system. Simple IoT based health care system also enable machine to machine communi-cation possible but due to small coverage, less mobility and weak terminal, it fails to make a robustsystem. Therefore MCN with IoT (5G5G network) is converged with WSN to provide larger cover-
age area, high capacity terminal to support the proposed health care system.
3.7.1 Mobile cellular network architecture: 5th generation
In the field of mobile communication, the forthcoming 5G5G technology is going to create a revo-
lution. The features and compatibility will make it more user friendly for every automation inaspects of human life. Because of high speed data communication in the cellular network, it is
going to merge the old technologies within it (
Zhang et al., 2012 ). 5G5G is going to facilitate
broadband connectivity, gaming options, quick response, minimum latency, wide coverage, highdefinition video and sound quality etc.
3.7.1.1 5G5G system architecture
5G5G possesses an advanced architecture with upgraded network elements to cope with new tech-nologies. The service providers use these advance features to provide value-based services. The
main feature of 5G5G is inclusion of cognitive technology which is able to identify its geographical
parameters such as temperature, weather, location, etc. By using this technology, the 5G5G term-inals act as transceiver by responding to radio signals of local environment and continue to providequality of service (QoS). The system model of 5G5G technology is totally based on IP for all wire-less technologies. The 5G5G system consist of two components: (1) User Terminal (cell phone) (2)Radio access technologies which are independent and autonomous.
These radio technologies establish paths as IP links for the public domain or internet world.
With the IP technology, the data routing is managed and control with respect to specific application
or session established between the clients present here and the server somewhere else in the inter-
net. For smooth routing, packets routing needs to be fixed according to the application policy.
3.7.1.2 Master core technology
5G5G master core is the key convergence point for other technologies such as WSN and existingmobile cellular network (MCN). Master core design is able to work in parallel with all IP basednetwork modes, that is, WSN and 5G5G network modes (MCN). In this multimode 5G technology,77 3.7 Smart healthcare using 5G5G Inter of Things: a case-studyit controls the RAN and other access network ( Chabbouh et al., 2016 ). Due to compatibility charac-
teristics of this technology, other new converging technologies are more efficient, powerful andless complicated. The World Combination Service Mode (WCSM) allows ultraspeed, for example,text written on a board can be seen at a remote place, displayed on real time apart from sound andvideo. 5G network in a slice format: eMBB, URLLC and mMTC. Theses slices allow the access of
5G between traditional and core network. The specific network functions can be involved for spe-
cific characteristics of the network.
In WSN, the routing algorithms used must be capable of location finding system so that the
accurate location can be estimated for completing the self-reorganizing process. As the nodes arebattery operated, power is a scarce resource in this system. Therefore energy aware MAC protocolclustering process is developed for achieving better life time. For this purpose, most of the nodesenter sleep mode when no transmission of received work is going on and adjusts the reservedpower.
3.7.2 ZigBee IP
ZigBee sensor nodes are with less energy, less computing power, and low range to participate in
global network. ZigBee IP is the upgraded version of ZigBee where the concept of TCP/IP protocolis implemented in WSN. The physical and MAC layer structures of ZigBee are redefined with IPbased system. The physical layer and data link layer are based on original IEEE 802.11.4. IPv6,ICMP and RPL are included in network layer. ZigBee is IP enabled due to RPL which is associatedwith 6LowPAN that allow exchange of data packets using IPv6 over IEEE 802.11.4. Transportlayer may adapt TCP or UDP for data transmission, while 5G terminals have good storage space,more bandwidth for communication, more computing power and IP enabled which supports the
ZigBee network for a better data communication. The 5G MCN terminals can join the ZigBee net-
work whenever they are in the range of ZigBee network.
3.7.3 Healthcare system architecture using wireless sensor network and mobile
cellular network
In this section, we propose a smart health care system based on WSN-MCN convergence network
as shown in Fig. 3.10 . The proposed system has two components: (1) WSN based on ZigBee-IP,
(2) MCN based on 5G technology. The forthcoming 5G cellular communication technology is
equipped with smart devices which are capable of participating with existing ZigBee network andalso support M2M communication (
Lone et al., 2020 ). In this context, our proposed architecture for
smart health care system works as follows:
The hospital area can be considered to be covered under the geographical region of 5G MCN.
1.Patients present on the hospital bed are equipped with smart sensors to measure biological
parameters of the body. These sensors are connected with each other with IEEE 802.15.4ZigBee-IP network and form a cluster. Data collected in the cluster locally from sensor nodesin ZigBee network are passing through the 5G terminal. However, it is not possible for a patientto carry the entire heavy weight wired instrument for everyday monitoring. Therefore lightwearable devices are developing with advanced sensor technology for continuous monitoring of78 Chapter 3 HetNet/M2M/D2D communication in 5G technologiespatient in hospital as well as in home. Table 3.1 presents different wearable devices collected
through various research article and projects.
2.There are two ways of data communication exist. First, the cluster transmits all the collected
data in wireless mode to the 5G cellular network which is coordinated by a 5G terminal.
Second, the data from the cluster will transmit to the 5G terminal (mobile phone) presentnearby, so that further transmission to the 5G cellular network is possible. This enables the dualmode communication of 5G terminals.
3.The patient data from 5G MCN are sent to the analyzing and decision-making server that can
be accessed or intimated to doctors or hospital for further action.
3.7.3.1 System protocol
The protocol convergence can happen in two ways (Case-A and Case-B) can be implemented intwo ways. In case-A, the 5G terminals itself participates in WSN by accepting spectrum from WSNwith ZigBee IP. Therefore data from WSN can be transmitted by 5G terminal as a part of MCN. Incase-B, the 5G terminal from MCN will act as a router for data transmission from WSN. The datapackets of ZigBee-IP in WSN converted to payload of MCN as per the frame structure.
3.7.3.2 Data transmission by 5G terminal in ZigBee network
Initially, patient’s sensor network (ZigBee) and 5G cellular network both operates in two differentchannels and not aware of each other’s presence. First, the 5G terminal starts channel detection forthe existing ZigBee network id and available channels. After detection, if it finds a busy channel,then the information is restored. The 5G terminal broadcasts its existence and working band. If aZigBee node receives the frame to participate in the MCN, it synchronizes with the broadcastedband to transmit the data. The announcement procedure repeats if another ZigBee network findaround its coverage. In the protocol convergence, WSN use the IP core of 5G MCN protocol to
FIGURE 3.10
Health care system based on WSN-MCN network.79 3.7 Smart healthcare using 5G5G Inter of Things: a case-studytransmit sensory data. The 5G MCN has compatible core to recognize sensor nodes as shown in
Fig. 3.11 .
3.7.3.3 Data transmission through 5G terminal by ZigBee network
The 5G terminal acts as a router for passing the ZigBee data to MCN. Initially, the 5G terminal
start detecting any ZigBee WSN, if detected, it broadcasts its ID and channel. If a ZigBee WSNreceives this frame and found that the new location address is closer than the current 5G terminal
FIGURE 3.11
Protocol convergence architecture for case-A.
FIGURE 3.12Protocol convergence architecture for case-B.80 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesin which it is connected, then it updates and broadcasts its updated address. This process is fol-
lowed by all 5G terminal and ZigBee units. The protocol defined for case-B is shown in Fig. 3.12 .
The WSN data generated from WSN-IPv6 are transmitted to 5G MCN. But when data travelled inreverse, that is, from MCN to WSN, a compatible format is generated by extracting extra header ofIPv4 and 5G payloads so that it is recognized by WSN.
A survey on different types of devices in healthcare systems is presented in
Table 3.6 .
Table 3.6 A Survey on types of devices in healthcare system.
Types of wearable
devices Research papers and projects Applications
Wrist-worn device AMON ( Anliker et al., 2004 ) High risk cardiac and respiration
Vivago Wrist Care ( Mattila et al.,
2008 )Skin temperature and movement
Health Monitoring Watch ( Mahesh
et al., 2013 )Continuously measure the vital parameter such as
heart rate, temperature and oxygen content inblood.
Sense Wear Armband (
Liden et al.,
2002 )Heat flow
Smart phone and
mobile-baseddeviceSmart phone healthcare (
Fang, Hu,
Wei, Shao, & Luo, 2014 )Health monitoring
Heart Togo ( Jin, Oresko, Huang, &
Cheng, 2009 )Individualized remote CVD detection
Personal Health Monitor ( Leijdekker
& Gay, 2008 )Heart attack self-test for CVD patient
Bluetooth enabled sensor with smart
phone ( Moro et al., 2007 )Long time monitoring
Textile based
deviceWEALTHY ( Paradiso, Loriga, &
Taccini, 2005 )Monitoring of elderly patient and chronic
diseases
BIOTEX ( Coyle et al., 2010 ) Personal health monitoring
MagIC ( Meriggi et al., 2010 ) Daily life cardiorespiratory signal
Smart vest ( Pandian et al., 2008 ) Remote health monitoring
Smart shirt ( Lee & Chung, 2009;
Tada, Amano, Sato, Saito, & Inoue,
2015 )Measurement of ECG for continuous real time
health monitoring
Biosensing textile-based patch ( Morris
et al., 2009 )Textile based fluid handling platform for sweat
monitoring
My heart ( Habetha, 2006 ) Prevention and monitoring of cardio vascular
diseases
Microcontroller
board-based deviceProe-tex ( Curone et al., 2010 ) Real time monitoring in harsh environment
Ambulatory stress monitoring ( Choi,
Ahme, & Gutierrez-Osuna, 2011 )Monitor a number of physiological correlates of
mental stress
LiveNet ( Sung, Marci, & Pentland,
2005 )Parkinson symptom and epilepsy seizures
detection81 3.7 Smart healthcare using 5G5G Inter of Things: a case-study3.8 Conclusions
We have presented heterogenous network (HetNet) architecture for 5G and allied technologies like
M2M and D2D in this chapter. Emerging applications like IoT and IoV demand scenario-specificdeployment environment and hence a HetNet with dissimilar clusters of access network is the mostviable one. However, these HetNets must be seamlessly interconnected with each other with anagile communication framework to make networks and protocols globally interoperable.
Furthermore, with the development of 3GPP, a standard 5G mobile communication portfolio with a
resilient architecture and advanced features like SDN, NFV, co-operative communication usingH-CRAN-based fronthaul and TWDM-PON based backhaul has been realized. Many recent state-of-the-art communication frameworks like M2M and D2D are coming to aid for a more compre-hensive technology-enabler for IoT and smart communication applications. Integration of HetNets,M2M, and D2D technologies with 5G network standards with future-ready features and functionali-ties (eMBB, URLLC, mMTC, mmWave, MIMO, Cloud-RAN, agile SDN and NFV) is the ultimategoal for today’s smart communications prospects and its business potential in the near future.
References
Agiwal, M., Roy, A., & Saxena, N. (2016). Next generation 5G wireless networks: A comprehensive Survey.
IEEE Communications Surveys & Tutorials ,18(3), 1617 /C01655. Available from https://doi.org/10.1109/
COMST.2016.2532458 .
Alnoman A., Ferdouse L., Anpalagan A., (2017). Fuzzy-based joint user association and resource allocation in
HetNets. In Proceedings of the IEEE eighty-sixth vehicular technology conference , VTC-Fall, pp. 1 /C05.
Anliker, U., Ward, J. A., Lukowicz, P., Troster, G., Dolveck, F., Baer, M., ...Belardinelli, A. (2004). AMON:
A wearable multiparameter medical monitoring and alert system. IEEE Transactions on Information
Technology in Biomedicine ,8(4), 415 /C0427.
Baronti, P., Pillai, P., Chook, V. W., Chessa, S., Gotta, A., & Hu, Y. F. (2007). Wireless sensor networks: A
survey on the state of the art and the 802.15. 4 and ZigBee standards. Computer Communications ,30(7),
1655 /C01695.
Bogale, T. E., & Le, L. B. (2016). Massive MIMO and mmWave for 5G wireless HetNet: Potential benefits
and challenges. IEEE Vehicular Technology Magazine ,11(1), 64 /C075.
Callaway, E., Gorday, P., Hester, L., Gutierrez, J. A., Naeve, M., Heile, B., & Bahl, V. (2002). Home network-
ing with IEEE 802.15. 4: A developing standard for low-rate wireless personal area networks. IEEE
Communications Magazine ,40(8), 70 /C077.
Camacho, F., C ´ardenas, C., & Mun ˜oz, D. (2018). Emerging technologies and research challenges for intelli-
gent transportation systems: 5G, HetNets, and SDN. International Journal on Interactive Design and
Manufacturing (IJIDeM) ,12(1), 327 /C0335.
Chabbouh O., Rejeb S. B., Choukair Z., Agoulmine N., (2016). A novel cloud RAN architecture for 5G
HetNets and QoS evaluation. In Proceedings of the international symposium on networks, computers and
communications , ISNCC, May 11, pp. 1 /C06.
Chen, Y., Li, J., Chen, W., Lin, Z., & Vucetic, B. (2016). Joint user association and resource allocation in the
downlink of heterogeneous networks. IEEE Transactions on Vehicular Technology ,65(7), 5701 /C05706.
Chettri, L., & Bera, R. (2020). A comprehensive survey on Internet of Things (IoT) toward 5G Wireless sys-
tems. IEEE Internet of Things Journal ,7(1), 16 /C032. Available from https://doi.org/10.1109/
JIOT.2019.2948888 .82 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesChoi, J., Ahme, B., & Gutierrez-Osuna, R. (2011). Development and evaluation of an ambulatory stress moni-
tor based on wearable sensors. IEEE Transactions on Information Technology in Biomedicine ,16(2),
279/C0286.
Coronado, E., Khan, S. N., & Riggio, R. (2019). 5G-EmPOWER: A software-defined networking platform for
5G radio access networks. IEEE Transactions on Network and Service Management ,16(2), 715 /C0728.
Available from https://doi.org/10.1109/TNSM.2019.2908675 .
Coyle, S., Lau, K. T., Moyna, N., O’Gorman, D., Diamond, D., Di Francesco, F., ...Taccini, N. (2010).
BIOTEX—Biosensing textiles for personalised healthcare management. IEEE Transactions on Information
Technology in Biomedicine ,14(2), 364 /C0370.
Curone, D., Lecco, E., Tognetti, A., Loriga, G., Dudnik, G., Risatti, M., ...Magenes, G. (2010). Smart gar-
ments for emergency operators: The ProeTEX project. IEEE Transactions on Information Technology in
Biomedicine ,14(3), 694 /C0701.
Dunkels, A., & Vasseur, J. (2008). Ip for smart objects aliance. Internet Protocol for Smart Objcts (IPSO)
Alliance White Paper ,2.
Elsayed, M., & Erol-Kantarci, M. (2019). AI-enabled future wireless networks: Challenges, opportunities, and
open issues. IEEE Vehicular Technology Magazine ,14(3), 70 /C077.
EPC Tag Data Standard, (2014). GS1 Standard Version 1.8.
Fang D., Hu J., Wei X., Shao H., Luo Y., (2014). A smart phone healthcare monitoring system for oxygen sat-
uration and heart rate. In Proceedings of the international conference on cyber-enabled distributed comput-
ing and knowledge discovery , Oct 13, pp. 245 /C0247.
Feng, M., Mao, S., & Jiang, T. (2018). Joint frame design, resource allocation and user association for massive
MIMO heterogeneous networks with wireless backhaul. IEEE Transactions on Wireless Communications ,
17(3), 1937 /C01950.
Gama, K., Touseau, L., & Donsez, D. (2012). Combining heterogeneous service technologies for building an
Internet of Things middleware. Computer Communications ,35(4), 405 /C0417.
Gezer C., Ta¸ skın E., (2016). An overview of oneM2M standard. In Proceedings of the twenty-fourth signal
processing and communication application conference , SIU, pp. 1705 /C01708, doi: 10.1109/
SIU.2016.7496087.
Ghosh S., De D., (2020). Dynamic antenna allocation in 5G MIMO HetNet using weighted majority coopera-
tive game theory. In Proceedings of the IEEE first international conference for convergence in engineer-
ing, ICCE, Sep 5, pp. 21 /C025.
Guinard D., Trifa V., (2009). Towards the web of things: Web mashups for embedded devices. In Proceeding
of the international world wide web conference, Madrid, Spain , Apr, pp. 8.
Guinard, D., Trifa, V., Karnouskos, S., Spiess, P., & Savio, D. (2010). Interacting with the soa-based internet
of things: Discovery, query, selection, and on-demand provisioning of web services. IEEE Transactions on
Services Computing ,3(3), 223 /C0235.
Guo, J. (2009). Collaborative conceptualisation: towards a conceptual foundation of interoperable electronic
product catalogue system design. Enterprise Information Systems ,3(1), 59 /C094.
Gures, E., Shayea, I., Alhammadi, A., Ergen, M., & Mohamad, H. (2020). A comprehensive survey on mobil-
ity management in 5G heterogeneous networks: Architectures, challenges and solutions. IEEE Access ,8,
195883 /C0195913.
Habetha J., (2006). The MyHeart project-fighting cardiovascular diseases by prevention and early diagnosis. In
Proceedings of the international conference of the IEEE engineering in medicine and biology society , Aug
31, pp. 6746 /C06749.
Huang, J., Zhou, P., Luo, K., Yang, Z., & He, G. (2017). Two-stage resource allocation scheme for three-tier
ultra-dense network. China Communications ,14(10), 118 /C0129.
Irrum, F., Ali, M., Naeem, M., Anpalagan, A., Qaisar, S., & Qamar, F. (2021). D2D-enabled resource manage-
ment in secrecy-ensured 5G and beyond heterogeneous networks. Physical Communication ,45, 101275,
ISSN 1874 /C04907. Available from https://doi.org/10.1016/j.phycom.2021.101275 .83 ReferencesJardim-Goncalves, R., Grilo, A., Agostinho, C., Lampathaki, F., & Charalabidis, Y. (2013). Systematisation of
interoperability body of knowledge: The foundation for enterprise interoperability as a science. Enterprise
Information Systems ,7(1), 7 /C032.
Ji, P., Jia, J., & Chen, J. (2019). Joint optimization on both routing and resource allocation for millimeter wave
cellular networks. IEEE Access ,7, 93631 /C093642.
Jin Z., Oresko J., Huang S., Cheng A. C., (2009). HeartToGo: A personalized medicine technology for cardio-
vascular disease prevention and detection. In Proceedings of the IEEE/NIH life science systems and appli-
cations workshop , April 9, pp. 80 /C083.
Kaneko, M., Nakano, T., Hayashi, K., Kamenosono, T., & Sakai, H. (2016). Distributed resource allocation
with local CSI overhearing and scheduling prediction for OFDMA heterogeneous networks. IEEE
Transactions on Vehicular Technology ,66(2), 1186 /C01199.
Khawam, K., Lahoud, S., El Helou, M., Martin, S., & Feng, G. (2020). Coordinated framework for spectrum
allocation and user association in 5G HetNets with mmWave. IEEE Transactions on Mobile Computing
(01). Available from https://doi.org/10.1109/TMC.2020.3022681 , 1-1.
Kim, D., Lee, C. E., Park, J. H., Moon, K., & Lim, K. (2007). Scalable message translation mechanism for the
environment of heterogeneous middleware. IEEE Transactions on Consumer Electronics ,53(1), 108 /C0113.
Kim, T., & Chang, J. M. (2017). QoS-aware energy-efficient association and resource scheduling for HetNets.
IEEE Transactions on Vehicular Technology ,67(1), 650 /C0664.
Klymash M., Beshley H., Seliuchenko M., Beshley M., (2017). Algorithm for clusterization, aggregation and
prioritization of M2M devices in heterogeneous 4G/5G network. In Proceedings of the fourth international
scientific-practical conference problems of infocommunications. science and technology, PIC S&T ,
pp. 182 /C0186, doi: 10.1109/INFOCOMMST.2017.8246376.
Lanthaler M., Gu ¨tl C., (2012). On using JSON-LD to create evolvable RESTful services. In Proceedings of the
third international workshop on RESTful Design , Apr 17, pp. 25 /C032.
Le L. V., Sinh D., Lin B.S., Tung L. P., (2018). Applying big data, machine learning, and SDN/NFV to 5G
traffic clustering, forecasting, and management. In Proceedings of the fourth IEEE conference on network
softwarization and workshops, NetSoft , Jun 25, pp. 168 /C0176.
Lee, Y. D., & Chung, W. Y. (2009). Wireless sensor network based wearable smart shirt for ubiquitous health
and activity monitoring. Sensors and Actuators B: Chemical ,140(2), 390 /C0395.
Leijdekker P., Gay V., (2008). A self-test to detect a heart attack using a mobile phone and wearable sensors.
InProceedings of the twenty-first IEEE international symposium on computer-based medical systems , June
17, pp. 93 /C098.
Li, S., Da Xu, L., & Zhao, S. (2018). 5G Internet of Things: A survey. Journal of Industrial Information
Integration ,10,1/C09, ISSN 2452-414X. Available from https://doi.org/10.1016/j.jii.2018.01.005 .
Li, X., Rao, J. B., & Zhang, H. (2016). Engineering Machine-to-Machine traffic in 5G. IEEE Internet of
Things Journal ,3(4), 609 /C0618. Available from https://doi.org/10.1109/JIOT.2015.2477039 .
Liden, C. B., Wolowicz, M., Stivoric, J., Teller, A., Kasabach, C., Vishnubhatla, S., ...Boehmke, S. (2002).
Characterization and implications of the sensors incorporated into the SenseWear armband for energy
expenditure and activity detection. Bodymedia Inc. White Papers ,1,7 .
Liu, R., Chen, Q., Yu, G., & Li, G. Y. (2019). Joint user association and resource allocation for multi-band
millimeter-wave heterogeneous networks. IEEE Transactions on Communications ,67(12), 8502 /C08516.
Lohani, S., Hossain, E., & Bhargava, V. K. (2017). Joint resource allocation and dynamic activation of energy
harvesting small cells in OFDMA HetNets. IEEE Transactions on Wireless Communications ,17(3),
1768 /C01783.
Lone, T. A., Rashid, A., Gupta, S., Gupta, S. K., Rao, D. S., Najim, M., ...Singhal, A. (2020). Securing com-
munication by attribute-based authentication in HetNet used for medical applications. Eurasip Journal on
Wireless Communications and Networking (1), 1 /C021.
Luo, X. (2017). Delay-oriented QoS-aware user association and resource allocation in heterogeneous cellular
networks. IEEE Transactions on Wireless Communications ,16(3), 1809-1022.84 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesLv T., Liu C., Gao H., (2017). Novel user scheduling algorithms for carrier aggregation system in heteroge-
neous network. In Proceedings of the IEEE wireless communications and networking conference , WCNC,
2017 Mar 19, 1 /C06.
Mahesh K. C., Shriharsha S., Seema G. S., Smitha P. V., Radhika S., Appaji M. A., Mishra G. (2013).
Wearable wireless intelligent multi-parameter health monitoring watch. In Proceedings of the Texas
Instruments India Educators’ conference , Apr 4, pp. 61 /C064.
Manishankar S., Srinithi C. R., Joseph D., (2017). Comprehensive study of wireless networks qos parameters and
comparing their performance based on real time scenario. In Proceedings of the international conference on
innovations in information, embedded and communication systems , ICIIECS, Mar 17, pp. 1 /C06.
Mattila E., Korhonen I., Merilahti J., Nummela A., Myllymaki M., Rusko H., (2008). A concept for personal
wellness management based on activity monitoring. In Proceedings of the second international conference
on pervasive computing technologies for healthcare , pp. 32 /C036.
Mehmood, Y., Haider, N., Imran, M., Timm-Giel, A., & Guizani, M. (2017). M2M communications in 5G:
State-of-the-art architecture, recent advances, and research challenges. IEEE Communications Magazine ,
55(9), 194 /C0201. Available from https://doi.org/10.1109/MCOM.2017.1600559 .
Mei, J., Wang, X., & Zheng, K. (2020). An intelligent self-sustained RAN slicing framework for diverse ser-
vice provisioning in 5G-beyond and 6G networks. Intelligent and Converged Networks ,1(3), 281 /C0294.
Available from https://doi.org/10.23919/ICN.2020.0019 .
Meriggi P., Castiglioni P., Lombardi C., Rizzo F., Mazzoleni P., Faini A., ... Parati G., (2010).
Polysomnography in extreme environments: The MagIC wearable system for monitoring climbers at very-
high altitude on Mt. Everest slopes. In Proceedings of the computing in cardiology , Sep 26,
pp. 1087 /C01090.
Moltafet, M., Azmi, P., Javan, M. R., Mokari, N., & Mokdad, A. (2019). Optimal radio resource allocation to
achieve a low BER in PD-NOMA /C0based heterogeneous cellular networks. Transactions on Emerging
Telecommunications Technologies ,30(5), e3572.
Moon, K. D., Lee, Y. H., Lee, C. E., & Son, Y. S. (2005). Design of a universal middleware bridge for device
interoperability in heterogeneous home network middleware. IEEE Transactions on Consumer Electronics ,
51(1), 314 /C0318.
Moro M. J., Luque J. R., Botella A. A., Cuberos E. J., Casilari E., Dı ´az-Estrella A., (2007). J2ME and smart
phones as platform for a Bluetooth Body Area Network for Patient-telemonitoring. In Proceedings of the
twenty-ninth annual international conference of the IEEE engineering in medicine and biology society ,
Aug 22, pp. 2791 /C02794.
Morris, D., Coyle, S., Wu, Y., Lau, K. T., Wallace, G., & Diamond, D. (2009). Bio-sensing textile based patch
with integrated optical detection system for sweat monitoring. Sensors and Actuators B: Chemical ,139(1),
231/C0236.
Munir H., Hassan S. A., Pervaiz H., Ni Q., Musavian L., (2016). Energy efficient resource allocation in 5G
hybrid heterogeneous networks: A game theoretic approach. In Proceedings of the IEEE eighth-fourth
vehicular technology conference , VTC-Fall, pp. 1 /C05.
Naeem, M., et al. (2017). Distributed gateway selection for M2M communication in cognitive 5G networks.
IEEE Network ,31(6), 94 /C0100. Available from https://doi.org/10.1109/MNET.2017.1700017 .
Naqvi, S. A., Pervaiz, H., Hassan, S. A., Musavian, L., Ni, Q., Imran, M. A., ...Tafazolli, R. (2018). Energy-
aware radio resource management in D2D-enabled multi-tier HetNets. IEEE Access ,6, 16610 /C016622.
Nasser, A., Muta, O., Elsabrouty, M., & Gacanin, H. (2019). Compressive sensing based spectrum allocation
and power control for NOMA HetNets. IEEE Access ,7, 98495 /C098506.
Niknam, S., Nasir, A. A., Mehrpouyan, H., & Natarajan, B. (2016). A multiband OFDMA heterogeneous net-
work for millimeter wave 5G wireless applications. IEEE Access ,4, 5640 /C05648.
Omran A., Sboui L., Rong B., Rutagemwa H., Kadoch M., (2019). Joint relay selection and load balancing
using D2D communications for 5G HetNet MEC. In Proceedings of the IEEE international conference on
communications workshops, ICC Workshops , May 20, pp. 1 /C05.85 ReferencesOuamri, M. A., Ote¸ steanu, M. E., Alexandru, I., & Azni, M. (2020). Coverage, handoff and cost optimization
for 5G heterogeneous network. Physical Communication ,39, 101037, ISSN 1874 /C04907. Available from
https://doi.org/10.1016/j.phycom.2020.101037 .
Pandian, P. S., Mohanavelu, K., Safeer, K. P., Kotresh, T. K., Shakunthala, D. T., Gopal, P., & Padaki, V. C.
(2008). Smart Vest: Wearable multi-parameter remote physiological monitoring system. Medical
Engineering & Physics ,30(4), 466 /C0477.
Panetto H., Cecil J., (2013). Information systems for enterprise integration, interoperability and networking:
Theory and applications ,1/C06.
Paradiso, R., Loriga, G., & Taccini, N. (2005). A wearable health care system based on knitted integrated sen-
sors. IEEE transactions on Information Technology in biomedicine ,9(3), 337 /C0344.
Park, H., Kim, H., Joo, H., & Song, J. (2016). Recent advancements in the Internet-of-Things related stan-
dards: A oneM2M perspective. ICT Express ,2(3), 126 /C0129, ISSN 2405 /C09595. Available from https://doi.
org/10.1016/j.icte.2016.08.009 .
Peng, M., Li, Y., Zhao, Z., & Wang, C. (2015). System architecture and key technologies for 5G heteroge-
neous cloud radio access networks. IEEE Network ,29(2), 6 /C014. Available from https://doi.org/10.1109/
MNET.2015.7064897 .
P´erez-Romero, J., S ´anchez-Gonz ´alez, J., Agustı ´, R., Lorenzo, B., & Glisic, S. (2016). Power-efficient resource
allocation in a heterogeneous network with cellular and D2D capabilities. IEEE Transactions on Vehicular
Technology ,65(11), 9272 /C09286.
Rezvani, S., Mokari, N., Java, M. R., & Jorswieck, E. A. (2019). Fairness and transmission-aware caching and
delivery policies in OFDMA-based HetNets. IEEE Transactions on mobile computing ,19(2), 331 /C0346.
Saha, C., Afshang, M., & Dhillon, H. S. (2018). 3GPP-Inspired HetNet Model using Poisson cluster process:
Sum-product functionals and downlink coverage. IEEE Transactions on Communications ,66(5),
2219 /C02234. Available from https://doi.org/10.1109/TCOMM.2017.2782741 .
Siddavaatam, R., Woungang, I., & Anpalagan, A. (2019). Joint optimisation of radio and infrastructure
resources for energy-efficient massive data storage in the mobile cloud over 5G HetNet. IET Wireless
Sensor Systems ,9(5), 323 /C0332.
Song H. K., Min-Jae P. A., Won-Chang K. I., (2020). Inventors; Sejong University Industry-Academy
Cooperation Group, assignee, MIMO-OFDM-based cooperative communication system for interferencemitigation between cells in heterogeneous network and cooperative communication method using the
same, United States patent U.S. Patent No. 10,862,547. 8 Dec.
Sun, S., Gong, L., Rong, B., & Lu, K. (2015). An intelligent SDN framework for 5G heterogeneous networks.
IEEE Communications Magazine ,53(11), 142 /C0147. Available from
https://doi.org/10.1109/
MCOM.2015.7321983 .
Sung, M., Marci, C., & Pentland, A. (2005). Wearable feedback systems for rehabilitation. Journal of neuro-
engineering and rehabilitation ,2(1), 17.
Swetina, J., Lu, G., Jacobs, P., Ennesser, F., & Song, J. (2014). Toward a standardized common M2M service
layer platform: Introduction to oneM2M. IEEE Wireless Communications ,21(3), 20 /C026. Available from
https://doi.org/10.1109/MWC.2014.6845045 .
Tada, Y., Amano, Y., Sato, T., Saito, S., & Inoue, M. (2015). A smart shirt made with conductive ink and con-
ductive foam for the measurement of electrocardiogram signals with unipolar precordial leads. Fibers ,3
(4), 463 /C0477.
Tang, F., Zhou, Y., & Kato, N. (2020). Deep reinforcement learning for dynamic uplink/downlink resource
allocation in high mobility 5G HetNet. IEEE Journal on Selected Areas in Communications ,38(12),
2773 /C02782.
Wang, F., Chen, W., Tang, H., & Wu, Q. (2017). Joint optimization of user association, subchannel allocation,
and power allocation in multi-cell multi-association OFDMA heterogeneous networks. IEEE Transactions
on Communications ,65(6), 2672 /C02684.86 Chapter 3 HetNet/M2M/D2D communication in 5G technologiesWang N., Fei Z., Kuang J., (2016). QoE-aware resource allocation for mixed traffics in heterogeneous net-
works based on Kuhn-Munkres algorithm. In Proceedings of the IEEE international conference on commu-
nication systems , ICCS, Dec 14, pp. 1 /C06.
Wang W., De S., Toenjes R., Reetz E., Moessner K., (2012). A comprehensive ontology for knowledge repre-
sentation in the internet of things. In Proceedings of the eleventh international conference on trust, security
and privacy in computing and communications , Jun 25, pp. 1793 /C01798.
Wang, X., Zheng, F., Jia, X., & You, X. (2017). Resource allocation in OFDMA heterogeneous networks for
maximizing weighted sum energy efficiency. Science China Information Sciences ,60(6), 062304.
Xiao, G., Guo, J., Da, Xu. L., & Gong, Z. (2014). User interoperability with heterogeneous IoT devices
through transformation. IEEE Transactions on Industrial Informatics ,17(2), 1486 /C01496.
Xie, B., Zhang, Z., Hu, R. Q., Wu, G., & Papathanassiou, A. (2018). Joint spectral efficiency and energy effi-
ciency in FFR-based wireless heterogeneous networks. IEEE Transactions on Vehicular Technology ,67(9),
8154 /C08168.
Xu Y., Hu Y., Chen Q., Chai R., Li G., (2017). Distributed resource allocation for cognitive hetnets with
cross-tier interference constraint. In Proceedings of the IEEE wireless communications and networking
conference (WCNC) , Mar 19, pp. 1 /C06.
Xu Y., Hu Y., Chen Q., Song T., Lai R., (2017). Robust resource allocation for multi-tier cognitive heteroge-
neous networks. In Proceedings of the IEEE international conference on communications , ICC, May 21,
1/C06.
Ya, M., Feng, G., Zhou, J., & Qin, S. (2018). Smart multi-RAT access based on multiagent reinforcement
learning. IEEE Transactions on Vehicular Technology ,67(5), 4539 /C04551.
Yang, C., Xiao, J., Li, J., Shao, X., Anpalagan, A., Ni, Q., & Guizani, M. (2018). DISCO: Interference-aware
distributed cooperation with incentive mechanism for 5G heterogeneous ultra-dense networks. IEEE
Communications Magazine ,56(7), 198 /C0204.
Ye, F., Dai, J., & Li, Y. (2018). Hybrid-clustering game Algorithm for Resource Allocation in Macro-Femto
HetNet. TIIS,12(4), 1638 /C01654.
Yim, H. J., Seo, D., Jung, H., Back, M. K., Kim, I., & Lee, K. C. (2017). Description and classification for
facilitating interoperability of heterogeneous data/events/services in the Internet of Things.
Neurocomputing ,256,1 3/C022.
Yuan, P., Xiao, Y., Bi, G., & Zhang, L. (2017). Toward cooperation by carrier aggregation in heterogeneous
networks: A hierarchical game approach. IEEE Transactions on Vehicular Technology ,66(2), 1670 /C01683.
Zhang, H., Huang, C., Zhou, J., & Chen, L. (2020). QoS-aware virtualization resource management mechanism
in 5G backhaul heterogeneous networks. IEEE Access ,8, 19479 /C019489. Available from https://doi.org/
10.1109/ACCESS.2020.2967101 .
Zhang, J., Shan, L., Hu, H., & Yang, Y. (2012). Mobile cellular networks and wireless sensor networks:
Toward convergence. IEEE Communications Magazine ,50(3), 164 /C0169.
Zhang, N., Cheng, N., Gamage, A. T., Zhang, K., Mark, J. W., & Shen, X. (2015). Cloud assisted HetNets
toward 5G wireless networks. IEEE Communications Magazine ,53(6), 59 /C065.
Zhang, Q., Gui, L., Hou, F., Chen, J., Zhu, S., & Tian, F. (2020). Dynamic task offloading and resource
allocation for mobile-edge computing in dense cloud RAN. IEEE Internet of Things Journal ,7(4),
3282 /C03299.
Zhang, W., Wei, Y., Wu, S., Meng, W., & Xiang, W. (2019). Joint beam and resource allocation in 5G
mmWave small cell systems. IEEE Transactions on Vehicular Technology ,68(10), 10272 /C010277.
Zhou, T., Huang, Y., & Yang, L. (2016). Energy-efficient user association in downlink heterogeneous cellular
networks. IET Communications ,10(13), 1553 /C01561.87 ReferencesThis page intentionally left blankCHAPTER
4An overview of low power hardware
architecture for edge computingdevices
Kushika Sivaprakasam1, P. Sriramalakshmi1, Pushpa Singh2and M.S. Bhaskar3
1School of Electrical Engineering, Vellore Institute of Technology, Chennai, Tamil Nadu, India2Department of
Computer Science & Information Technology, KIET Group of Institutions, Delhi-NCR, Ghaziabad, Uttar Pradesh,
India3Renewable Energy Lab, Department of communication and Networks Engineering, College of Engineering,
Prince Sultan University, Riyadh, Saudi Arabia
4.1 Introduction
In edge computing, the computational power lies closer to the source thereby reducing latency. The
idea is to optimize the edge such that data can be processed closer to its source, which is critical tomany services especially in the 5G networks. This feature of edge computing devices acts as a cata-lyst in the process of deployment of 5G networks. An ultra-low latency network is exactly what 5Gtechnology requires, and edge computing can provide it (
Hassan et al., 2019 ). Not only it promotes
5G networks, but edge computing also promotes various other applications such as high quality vid-
eoconferencing, lag-free online gaming, a wide range of IoT applications ( Singh & Agrawal, 2021 ),
Virtual Reality, Augmented Reality, Big Data Analytics and Crypto currency trading.
Recently vast data are generated by IoT and other connected devices. The generated data need
to be stored, maintained, managed, backed up and made accessible to users through the Internet.Cloud, fog, and edge computing infrastructures permit organizations to access gain of a variousrange of computing resources and data storage. Cloud, fog and edge computing looks like similar,but they are different from each other. Cloud computing viewed as storing and accessing data andprograms over the Internet (third party), rather than on the local hard drive of your single computer.
To access data, users need to create account with the associated cloud service providers (
Indu
et al., 2018 ). IoT and Machine Learning based applications such as smart healthcare, smart agricul-
ture, smart transportation system and smart city deal with massive amount of data ( Mahdavinejad
et al., 2018 ).
The data processing in the cloud service increases the cost of the IoT solution. Though cloud
computing remains the first choice for most of the companies, for real time applications, companiesare transforming their systems to edge and fog computing. The main aim of knowing conceptsabout edge and fog is not to replace the cloud completely but to separate sensitive information
from the generic one and hence to analyze the data economically in the safe manner. The difference
between fog and cloud computing is that cloud is a centralized system whereas fog computing sup-ports distributed environment. Fog computing is an intermediary between hardware and remote
895G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00004-8
©2022 Elsevier Inc. All rights reserved.servers. It controls the information which needs to be transferred to the cloud server and to be pro-
cessed locally. Hence to analyze data that leads to latency issues. This issue is resolved with fogcomputing, where data is processed within a fog node or IoT Gateway which is placed within theLocal Area Network (LAN). The overall Industrial IoT framework consists of cloud, fog and edgecomputing as represented in
Fig. 4.1 . Topmost layer is cloud layer which can have big data proces-
sing, data warehouse and business logic. Fog computing layer acts as a middleware service and
without it, the cloud communicates directly with the devices which is a costly affair. In edge com-puting, the data is processed and analyzed on the device or sensor itself, without being relocatedanywhere. Edge computing allows processing the data remains on the device itself or on the IoTGateway in order to reduce the network traffic. Edge computing drives the artificial intelligence(AI), and communication abilities of an application directly into devices such as EPICs (EdgeProgrammable Industrial Controllers), PACs (Programmable Automation Controllers), and PLCs(Programmable Logic Controllers). Edge computing keeps users’ personnel data on edge devices
rather than keeping it on fog or cloud, hence, decreases the risk of network data leakage and pro-
vides data security and privacy (
Shi et al., 2019 ).
Recently, where each and every object also referred as “things” can have embedded processor
and can connect billions or trillions of processors through the Internet. The conventional cloud-based model processes the data, which demands limited wireless networks. Low or Ultra LowPower (ULP) hardware architecture for edge computing devices is the only possibility that canmeet these demands (
Brooks & Sartori, 2017 ). Hence with fruitful-looking prospects, edge
FIGURE 4.1
Industrial Internet of Things framework with cloud, fog and edge computing ( Singh & Agrawal, 2021 ).90 Chapter 4 An overview of low power hardware architecturecomputing infrastructure systems need to be easy to deploy and at the same time, should be effi-
cient. In edge computing, the computational technique falls nearer to the source and hence itreduces the latency. It can very well optimize the edge, which is critical to many services especiallyin 5G networks. Hence edge computing devices facilitate the process of deployment of 5G net-works and a wide range of cutting-edge technologies. This paper focusses on the hardware infra-
structure of edge computing devices and different ways to enhance its efficiency. For this purpose,
the paper is divided into different sections to present a clear picture.
Section 4.2 explains the basic concepts of cloud, fog and edge computing infrastructure.
Section 4.3 emphasizes the low power hardware architecture for edge computing devices with
detailed analysis and study of different components of the hardware architecture and is divided intosections:
Section 4.1 describes the objectives of hardware development in edge computing clearly
defining the direction of this research. Section 4.2 gives an overview of the system architecture in
an abstractive manner. Each component of the system architecture is elaborated in further sections.
Section 4.3 is dedicated to the Central Processing Unit (CPU) architecture and Section 4.4 explains
the input /C0output architecture. Section 4.5 is crucial as it deals with power consumption and man-
agement. The aim is to also touch on algorithmic optimizations which are dealt with in Section 4.6
along with the explanation of the data processing unit.
Section 4.4 consists of examples of edge gateways and devices that are based on edge comput-
ing. It draws a pragmatic picture of the theory discussed in the previous sections. Section 4.5 dis-
cusses the edge computing for intelligent healthcare applications. It elaborates the advantages ofedge computing for healthcare applications, implementation challenges of edge computing in
healthcare systems and applications of edge computing based smart healthcare systems.
Section 4.6
projects the possible impact of edge computing, IoT and 5G on healthcare industries. The paper is
concluded with Section 4.7 , which consists of a summary and the future scope of research concern-
ing the discussed topics.
4.2 Basic concepts of cloud, fog and edge computing infrastructure
Most of the power processing is carried out in the cloud. In edge computing, the real time data pro-
cessing is performed on the premises. There are numerous advantages of edge computing technolo-gies such as real time data analytics based on AI, less operating costs, enhanced applicationperformance reduced downtime. Fog computing was introduced by Cisco systems to bring thecloud nearer to the end users. The processing is preferably performed in the LAN. It is not carriedout at nodes or in gateway. The mobile edge computing (MEC) is more suitable for server-to-clientapplications (
Torre et al., 2020 ) since the intelligence and management are located in the Radio
Access Network (RAN).
Fog computing is performed in an abstraction layer whereas MEC and cloudlets do not need
this due to its dedicated connections. Multiaccess edge computing MEC, cloudlets, micro data cen-ters, Cloud of Things (CoT) will be the main players in the upcoming network infrastructures(
http://www.lanner-america.com ). Multiaccess edge computing enables the processing and compu-
tation within network which is termed as the RAN. The MEC allows operators to deal with thisexcessive traffic and resource demands more intelligently. It lays the foundations for future91 4.2 Basic concepts of cloud, fog and edge computing infrastructureintelligent and next-generation networks. MEC provides enhanced location, augmented reality and
IoT services support 5G networks. In reality, the edge and fog computing have similar objectives.A minor difference is that fog computing includes running intelligence on the end device and ismore of IoT focused (
http://www.lanner-america.com ). An edge based IoT is shown in Fig. 4.2 .
Fog computing describes a decentralized computing infrastructure. It is the extension of cloud
computing (data center) to the edge of a network and helps in computational and storage applica-
tions efficiently. It is generally located between the cloud and the origin of the data. Fog nodeswhich are computing entities link the edge devices with the cloud. These fog nodes are capable ofprocessing and sensing. The ultimate aim of fog computing is to reduce the massive amount of datatransferred to the cloud for storage and processing. The data acquired from the sensors are pro-cessed at the edge of a network. Cloudlets are used to reduce the latency and improve the mobileapplications. It helps to eradicate the latency delays usually with Wide Area Network (WAN) cloudcomputing. A data center in small scale which is formed in the cloud consists of trusted computing
devices near to the locality of mobile user. In addition it can perform the process combined with
the local network connection. The cloudlet is the major technological enabler for mobile cloudcomputing. It is the convergence of mobile cloud computing and cloud computing. It has the virtualmode of architecture which can be accessed by the users within their locality (
Garcı ´a-Valls et al.,
2018; Satyanarayanan et al., 2015 ).
FIGURE 4.2
Edge based Internet of Things ( https://cognitechx.com/what-is-edge-computing-mean-in-iot/ ).92 Chapter 4 An overview of low power hardware architectureMoreover, mobile users can utilize it. The cloudlets virtual machines can run the required soft-
ware applications closer to the mobile devices which can be used by the mobile users. Hence thelatency issues can be solved by shifting the virtual machine near to the locality of the mobile users.The mobile users need to rely on the network service providers to deploy cloudlets into the LAN net-work for the mobile devices to use them (
Verbelen et al., 2012 ). Cloudlets can assist mobile devices
for computations, and they cannot be used as permanent storage of data. The concept of cloudlet
between cloud and mobile computing is further discussed in ( Satyanarayanan et al., 2015; Verbelen
et al., 2012 ). A mobile device is able to connect to a cloudlet and upload data to the cloudlet.
4.2.1 Role of edge computing in Internet of Things
The significant advancements in embedded systems-on-a-chip with efficient operating systems have
extended the potential of the IoT. The excellent features of devices with advanced controllers not
only collect and send the data to the cloud, also carry out complicated computations on the premises
which are led to the concept of edge computing. It can provide the services offered by the cloud com-puting very nearer to the edge of the network to support various applications (
Hassan et al., 2018 ).
T h ee x t e n s i b l ee d g es e r v e ra r c h i t e c t u r ei ss h o w ni n Fig. 4.3 , which exhibits that the edge computing
comprises itself multiple technologies such as IoT, c loud computing and grid computing. The additional
layer between connected devices and th e cloud brings the end device closer ( Gezer & Ruskowski, 2017 ).
There are various service-level objectives are needed for edge computing in the context of IoT
such as minimal latency, managing network, optimized cost, energy management, resource manage-
ment, data management and data types. The property of latency minimization can definitely fulfill
the quality of service for smart transportation which is delay sensitive based IoT application suchas unmanned vehicles and vehicle accident prevention (
Anawar et al., 2018 ). The role of edge ser-
vers for various domains is presented in Fig. 4.4 . There are few laws can be called for edge based
IoT. The law of physics is applied for latency minimization which is essential for time sensitive
FIGURE 4.3
Extensible edge server architecture ( Gezer & Ruskowski, 2017 ).93 4.2 Basic concepts of cloud, fog and edge computing infrastructurebased applications. The law of economics is applied where we need to minimize the cost. The law
of land is involved where the data need high security and required to be processed locally ( https://
www.xenonstack.com/blog/edge-computing/ ). The edge computing based IoT must have the fea-
tures of mobility, interoperability, reliability, latency and real time interactions. Data acquisition,inferential control, data analysis, decision making are the few major roles of edge computing inIoT. In addition, the technologies adopted by edge based systems need to meet certain standards(
Khan et al., 2020 ). The software must be modified to fulfill the real time implementations ( Liao
et al., 2019; Rostedt, 2013 ). The server placed in the edge level must be flexible enough to incorpo-
rate plug-and-play features to add much more advanced functionalities through placing softwareand hardware blocks. High rate of data transfer is essential to transfer the large amount of data
sensed from various devices, for example, large size of data acquired during surgical operations in
healthcare applications to edge clouds (
Yin et al., 2017 ).
4.2.2 Edge intelligence and 5G in Internet of Things based smart
healthcare system
The primary characteristic of 5G is faster data output which helps gigantic machine communica-
tions. In addition, machine to machine communication without human interaction is possible withhigh reliability and low latency. The vast advancements in cloud computing technology drove acentralized approach to the system managements. Computing toward a distributed architecture isdriven by IoT, mobile computing and software as a service models. The combinations of above two
FIGURE 4.4
Role of edge servers for various domains ( Gezer & Ruskowski, 2017 ).94 Chapter 4 An overview of low power hardware architectureapproaches which improve the system performance to the higher level are obtained with 5G and
edge computing techniques ( Hassan et al., 2019 ). 5G is foreknown to provide support with low
latency for high interactive IoT use cases. Edge computing uses a distributed architecture modelwhich brings the computing capabilities nearer to the premises and so it reduces latency. Edge com-puting unloads a gigantic amount of data from user equipment (UEs) to edge clouds (
Zhang et al.,
2018 ).
Edge computing technique attempts to integrate various types of edge devices and servers. They
can perform collaboratively for the efficient processing of locally generated healthcare sensor data.In addition, edge intelligence tries to move toward smart healthcare frameworks by employing AItechniques and cognitive intelligence related to human behavior into edge architectures. Edge intel-ligence is applied to smart devices that are attached with sensor. These devices are available atgateways closer to the smart sensors and gateway devices which can serve as edge nodes.
Most of the healthcare systems do not consider the emergency situations of patients, and not
providing any personalized resource service for users. To overcome this drawback, the Edge-
Cognitive-Computing-based (ECC-based) smart healthcare system is proposed (
Chen et al., 2018 ).
This system could monitor and analyze the physical health of users using cognitive computing(
Chen et al., 2019; Wan et al., 2020 ). The ECC-based system can adjust the computing resource
allocation of the whole edge computing network according to the health-risk grade of patients.
There is various new intelligent detection techniques using deep learning and edge computing
are coming up. The sensor senses the human electroencephalogram (EEG) signal and sends to anearby edge server. The edge server introduces various preprocessing steps and assigns them to
available edge devices. Consequently, the large size advanced signals are transferred to the cloud
server. Edge Learning as a Service (EdgeLaaS) framework is also proposed to process health super-vision data on premise itself. Edge learning nodes assist patients to choose better suggestions fromthe medical persons in real time whenever emergency situations occurs (
Zhang et al., 2020 ).
The telemedicine field requires an advanced network such as 5G and 6G that offers support in
real time, providing high-quality video communication without slowing down the facility network.Also the network must have standard protocols and more secured mechanisms to address securitychallenges not only to follow security-by-design but also security-by-operations rules (
Hameed
et al., 2021 ). Hence edge could enable more secured remote healthcare facility by assuring security
standards ( https://enterprise.verizon.com/resources/articles/s/future-of-healthcare-technology-5g-edge-
computing/ ).
4.3 Low power hardware architecture for edge computing devices
4.3.1 Objectives of hardware development in edge computing
IoT systems perform a particular function in four main processes namely data collection, data pro-
cessing, data transmission and actuation. Of the above-stated processes, data transmission is themost power-consuming process and the main aim of edge computing devices is to minimize thedistance between the source and the destination of transmission (
Saeed et al., 2018 ). As electronic
devices and embedded systems are growing pervasively, most IoT systems are battery-powereddevices. This means that the underlying hardware architecture must be power-efficient. At the same95 4.3 Low power hardware architecture for edge computing devicestime, they must also be scalable, cheap and easy to manufacture. Therefore a clear set of objectives
are defined for edge computing hardware architecture. MOSFETs which form the lowest level ofthe sensor nodes have been scaled down in size over the past decade to achieve a smaller chip areaor more functionality in the same chip area. The smallest MOSFETs manufactured are about 5 nmin size and widely used today. However, to meet the power efficiency criteria and the cost criteria,
older silicon technology processes of about 130 /C0200 nm can be used. It is important to note that,
systems performing more complex functionalities will still need smaller-sized nodes. Hence, basedon the functionality of the device an optimized architecture is to be developed considering thepower constraints, cost-effective and various other factors.
4.3.2 System architecture
The main component of a Microcontroller unit (MCU) is the CPU and its architecture plays an
important role in determining how efficient the system is ( Saeed et al., 2018 ). The peripherals
include the input /C0output architecture which facilitates connections with sensors to collect data.
Power consumption is the pivotal point of this analysis, hence different power management techni-ques are analyzed and explained. Data processing is an important function of edge computing
devices and enabling MCUs to process data will in turn enable edge computing. Each of the above-
mentioned components is explained in detail in the upcoming sections. The general system archi-tecture is shown in
Fig. 4.5 (Capra et al., 2019; Low Power Hardware Techniques; Overview of
Computer ).
4.3.3 Central processing unit architecture
Many IoT devices demand high computational power, and this can be achieved using multicore
CPU architecture. Multicore processors are CPUs which have several independent processors on asingle chip. In multicore processors, the workload is distributed between several cores which allowsparallel computing and lower power consumption which is depicted in
Fig. 4.6 (Overview of
Computer; Low Power Hardware Techniques; Ermi¸ s&C ¸ atay., 2017 ). These multicores are made
heterogeneous—in which different cores are assigned to different functionalities and will be
FIGURE 4.5
System architecture ( Zhang et al., 2020 ).96 Chapter 4 An overview of low power hardware architecturestructured according to the function they perform. For instance, one large core will be used for the
high computational function while a smaller core in the same chip will be used for peripheralrequests. This technique increases the power efficiency because each of the cores handles differenttasks and can be turned off separately when that particular task is not required to be performed.Hence, multicore CPU architecture facilitates high performance with lower power consumption(
Overview of Computer; Low Power Hardware Techniques; Ermi¸ s&C ¸ atay., 2017 ).
The power consumption of CMOS microcontrollers is of two types: dynamic power and static
power. The power consumed when the microcontroller is performing its programmed tasks is calleddynamic power whereas the power consumed when not running code and that occurs only byapplying a voltage to the device is called static power. In battery-operated systems, static power isof greater significance as most of them will spend significant portions of the application lifetime insleep mode. in a multicore processor, dynamic voltage and frequency scaling (DVFS) can be per-formed on each core, enhancing power savings by decoupling their working points. An optimalconfiguration can be achieved by operating each core independently, as well as shutting down them
selectively.
Larger process technologies(130 /C0200 nm), have considerably lower leakage current but at the
cost of higher dynamic power. This means that when larger process technologies are used, the staticpower decreases while the dynamic power increases. Therefore larger process technologies may bea good solution for battery-operated devices that do not require high computational power, sincestatic power dominates the dynamic power. However, in cases which require large computationalpower, dynamic power dominates. When trying to optimize both dynamic and static power, a criti-cal trade-off becomes apparent. As a result of this trade-off, it can be difficult to determine which
is more important to reduce power consumption for a system. To select the correct MCU, which
will minimize power for a particular system, it is important to know whether dynamic or staticpower has a more significant impact on the system, and this can be done by power budgeting.
FIGURE 4.6
Central processing unit architecture ( Ermi¸ s&C ¸ atay., 2017 ).97 4.3 Low power hardware architecture for edge computing devicesDifferent CPU architectures will consume different amounts of energy in a single clock cycle.
Hence, it is important to take the Instruction Set Architecture (ISA) into consideration. Cycles perinstruction, clocking schemes and the instructions have a major impact on the performance of thedevice, which directly affects the power consumed in one cycle, and therefore the overall powerconsumption. Single core and multicore architecture are depicted in
Fig. 4.7 .
4.3.4 Input /C0output architecture
Peripherals which constitute the input /C0output architecture of a microcontroller play an important
role in reducing power consumption. There are two factors which should be considered when com-paring different peripherals- the current and the time taken to send a given amount of data.A trade-off is achieved by multiplying both these quantities, which results in the total charge con-sumed. This in turn determines the power consumption. This trade-off is important to achieve
because it could be deceiving to consider the factors individually. For instance, the universal asyn-
chronous receiver-transmitter (UART) has a smaller current than the serial peripheral interface(SPI) and may seem like a low power module. However, it takes more time than the SPI to transferdata. While the SPI may consume more power during the transaction, after the transaction is com-pleted, the device can go to sleep and eliminate the power consumption of the CPU which is thehighest source of power consumption.
IoT nodes need to be specifically altered with respect to the application they perform and need
to be optimized with low power and high-performance characteristics. To achieve these optimal
standards, an alliance has been formed which works on several multimedia protocols. This alliance
FIGURE 4.7
Single core and multicore architecture ( Johnson & Dinyo, 2015 ).98 Chapter 4 An overview of low power hardware architectureis called the MIPI (mobile industry processor interface) alliance. The aim is to realize optimized
devices with low power, high bandwidth and low electromagnetic interferences. The low powerMIPI design—I3C interface has been optimized for a high clock speed, has high power efficiency,and is much faster than its previous model—I2C interface (
MIPI Alliance Specifications
Overview ).
4.3.5 Power consumption
The lifetime of any embedded system primarily depends on the lifetime of the battery; hence opti-
mizing the power dissipation is crucial to battery-powered systems ( Shin et al., 2010 ). As the MCU
serves as the brain of the system, it consumes the most power and also has control over the systempower consumption. Since edge computing moves the processing power from the cloud to a pointcloser to the user, the main goal is to achieve a balance between high processing power and lowpower consumption. One of the widely used techniques to achieve this goal is turning off the partsof the circuit that are not performing any task. Clock gating and power gating are popular methodsto perform this technique. In clock gating, logical OR and AND gates usually stop the clock fromthe part of the circuit that is not required in the current task, which in turn results in a significant
reduction of the dynamic power consumption. However, a static leakage power persists but it can
be reduced by combining clock gating and dynamic voltage scaling. In power gating, the supplyvoltage is disconnected from the circuit using MOS transistors, which reduces both the dynamicpower and the static power thereby serving as a better solution than clock gating. The downside tothis technique is that it is complex to design due to insertion of current switches which have to besettled by a combination of extra circuitry and customized tools and methodologies. Nevertheless,the technique should be selected based on the constraints of the application designed. Other basicMCU-level considerations can be made to achieve a low power consuming circuit. The fundamen-
tal rules to design low power MCUs are to increase impedance in current paths and reduce imped-
ance in high-speed switching paths. Hence, leakage currents and operating duty cycles are reduced.
Another approach is to make the MCUs operates at the source voltage closer to the threshold
value of the transistors. Conventionally, the MCUs work in regions more than the threshold levelof transistors. Scaling the voltage so that the transistors can operate in regions near their thresholdcan significantly reduce the power dissipation. This could give rise to performance degradationissues which need to be compensated by a technique known as PVT (process, voltage, and tempera-ture) compensation. Special circuits such as Canary circuits and Razor flip-flops are used for PVT
compensation by controlling the voltage supply.
4.3.6 Data processing and algorithmic optimization
Parallel computing plays a key role in processing data at the edge. In parallel computing, different
tasks are broken down into smaller independent tasks and are executed simultaneously by multipleprocessors. Different processors communicate with each other and the memory unit via sharedmemory. The results of each independent smaller task are combined upon completion as part of anoverall algorithm. Instruction-level parallelism and multistage instruction pipelines are a part of allmodern processors and enable the execution of more than one instruction per clock cycle. Task par-allelism can be achieved by using multicore processors to execute different threads (or processes)99 4.3 Low power hardware architecture for edge computing deviceson the same or different data. It emphasizes on the distributed nature of the processing (concurrent
processing of threads). By running many threads at once, these applications can tolerate the highamounts of peripheral data and memory system latency. For instance, if one thread is delayed wait-ing for memory access, other threads can do useful work without being interrupted by the delayedtask.
Increased bandwidth is another important feature of edge computing devices. While collecting
real time data from the sensors, the information contains some redundant data, which need not tobe processed. Transmitting this data to the cloud can consume an unnecessary amount of band-width. Edge computing prevents this unnecessary usage of bandwidth with the help of an algorith-mic optimization technique known as compressive sensing. In compressive sensing, the datacollected from the sensors is transmitted to a local processing unit that has got an ample amount ofspace and computing capacity. Only the important information and features that are required are fil-tered out and transmitted to the cloud which in turn significantly reduces the network bandwidth.
4.4 Examples of edge computing devices
One of the main applications of edge computing is human machine interface (HMI) products which
serve as the ideal choice for IoT edge applications in factory, marine and building automation.EXOR’s eX Series 700 HMI products are IoT edge devices that perform powerful controlling and
networking operations. The design is very user-friendly with a capacitive touchscreen and
high-resolution display. The eX715 is powered by a rechargeable Lithium battery and an ARMCortex-A9 quad-core 800 MHz CPU. The CPU consists of four cache-coherent cores. The CPUarchitecture supports four main modes namely run mode, standby mode, dormant mode and shut-down mode. The run mode is the normal mode of operation, where all of the functions of the pro-cessor are available. During the standby mode, clock pulses are not gated but logic is powered up.Dormant mode enables the processor to be powered down while leaving the caches powered upand maintaining their state. Shutdown mode shuts down the entire device. These different modes
play a major role in optimizing the power dissipation of the device. This device supports serial
communication through SPI and contains three Ethernet ports. Ethernet can be a better option whencompared to Wi-Fi as it is a faster and more reliable option with lesser interference.
Another example of edge computing devices is edge gateways which accelerate IoT at the edge
by providing fast and responsive data solutions. Dell Edge Gateway 5100 is one such edge gatewaysolution. It runs on a dual-core processor. It supports SATA (Serial Advanced TechnologyAttachment) and UART communication standards. The CPU adopts the Intel Hyper-ThreadingTechnology (Intel HT Technology) which provides dual processing threads per physical core
and ensures parallel computing. Intel Virtualization Technology (VT-x) is another technology
that is incorporated which supports task parallelism by isolating computing activities into separatepartitions (
https://www.intel.in/content/www/in/en/gaming/resources/hyper-threading.html ). The
Enhanced Intel SpeedStep Technology is also supported which is an advanced method of enablinghigh performance and meeting the power-conservation constraints of the system. Based on the pro-cessor load and the Enhanced Intel SpeedStep, the traditional Intel SpeedStep technology transitsboth voltage and frequency between high and low levels.100 Chapter 4 An overview of low power hardware architecture4.5 Edge computing for intelligent healthcare applications
Edge computing is a remarkable technology that allows processing at the end of the device edge. It
hypothesizes that computation should always take place near a data source ( Intelligent & Cloud
Computing, 2021 ). Edge computing models are things that act as data consumers but can also act
as data producers. Edge computing includes data transfer, data storage, processing, and delivery
from the cloud to the user. Edge-of-Things (EoT)-based healthcare services are providing patient-
care amenities related to autonomic and persuasive healthcare in which EoT broker plays the roleas a middleman between the Healthcare Service Consumers (HSC) and Computing ServiceProviders (CSP). The data acquired by the sensor from a patient’s Body Area Networks (BAN)are very sensitive. They should be stored and analyzed in secured environment. EoT based patientmonitoring systems and edge computing based real time analysis is more secured (
Golam Rabiul
Alam et al., 2019 ). Edge computing as a service (ECaaS) and Cloud Computing as a Service
(CCaaS) providers are more economical, so that the broker can deliver smart healthcare services to
consumers with optimized cost. Edge computing plays an important role in the smart health care
system ( Gupta & Khamparia, 2020 ). It is very difficult for the traditional healthcare systems to
maintain the scalability to meet the rising number of patients and to provide the best healthcaresolutions. With the evolution of IoT and edge based systems, traditional healthcare systems intosmart health systems. Mobile healthcare is also termed as tele-healthcare used to provide healthcaresolutions to patients at the remote area. Edge computing is used to enhance the managing systemand quality level of the healthcare system. In addition, it assists the system for better utilization ofits resources and needs to reduce its cost (
Oueida et al., 2018 ).
The consumable and nonconsumable resources of the healthcare system are very well monitored
with the help of edge computing based network. The human resource and employing this resourcein an effective way is very essential and which can be implemented with the help of edge comput-ing effectively. It is the need of the hour to identify certain novel approaches to monitor the health-care system economically and effectively (
Kim, 2018 ). The wireless body area networks can be
improved for the deployment of healthcare applications ( Otoum et al., 2015 ). The massive amount
of data generated in this field must be stored and analyzed very carefully and handled cautiously(
Aloqaily et al., 2016 ). The features of the IoT and fog computing are highlighted and illustrated in
(Gia et al., 2015 ). The difficulties in adopting novel technologies and the advantages of shifting
from traditional healthcare system to cloud, fog and edge based smart healthcare system are dis-cussed in (
Kuo, 2011 ).
4.5.1 Edge computing for healthcare applications
A huge amount of data is generated by healthcare applications and there is a dire need for a reliable
and secure system for smart health applications. As the healthcare sector deals with critical applica-tions, the migration from the conventional system to a smart system has to be planned very meticu-lously. Edge computing accelerates this migration as it forms a system in which the data ismanipulated at its source, hence is secure and faster than regular servers. Smart healthcare applica-tions are broadly classified into two primary classes: patient-related class and process-related class.The patient-related class applications include wearable devices and embedded sensors that collect101 4.5 Edge computing for intelligent healthcare applicationshealth data. The process-related class applications include various healthcare policies that can be
automated or improved with technology. Some examples are scheduling of resources, service qual-ity, and utilization factor of resources. This chapter demonstrates an example of an applicationfrom one of the healthcare departments and the importance of edge computing in enhancing thedepartment, has been highlighted. Emergency departments are the most time-critical systems of
the healthcare department and require foremost planning and systemization. One of the main pro-
blems of these departments is the congestion and overcrowding due to the presence of a large num-ber of patients at a given time. This in turn leads to a complex workflow and resourcemanagement. Hence edge computing can be used to optimize the existing models and can be usedfor managing the processes at emergency departments. These models used to represent distributedsystem. The main idea of edge computing is to spread the computation tasks to different layers ofthe network. A part of the work is distributed to the edge rather than computing all the informationin the cloud.
In this model, the scheduling algorithm is stored in the cloud while every healthcare resource
will have an edge node that communicates the fulfillment of an assigned task. Then, the schedulingalgorithm from the cloud reassigns another task to the edge node. Hence the task allocations andinformation regarding completed tasks are sent from the edge nodes to the cloud and the cloudstores the database and workflow software. Every healthcare resource has a smart device, such as amobile phone, tablet, smart-watch that behaves as an edge node. After the task is completed, thenode notifies the cloud that the resource is available and ready to be reassigned. Thus schedulingoccurs in the cloud while the assignment is distributed to the edge (
Oueida et al., 2018 ). A system-
atic cycle is formed that has a significant impact on the management of resources in emergency
departments.
4.5.2 Advantages of edge computing for healthcare applications
The advantages of edge based healthcare system are illustrated in Fig. 4.8 . The adaptation of edge
computing for healthcare system finds wide application in healthcare systems. Even after the vast
development and innovations in healthcare systems, there are numerous challenges in providing
high quality healthcare to rural areas. Due to the lack of Internet connectivity and access, medicalproviders find difficulty in providing the quality healthcare to the people who are far away fromhealth centers. All the above-mentioned issues can be alleviated with the help of Medical Internetof Things (MIoT) and edge computing applications. It is quite possible to generate, collect, storeand analyze very critical data without having consistent network connectivity. It is very simple andfast to diagnose the patients with smart medical devices and the data can be easily sent to the cloudservers or to the central servers with the network. In addition, the massive data collected by medi-
cal devices such as blood glucose monitor, wearable sensors and healthcare apps make everything
feasible for the healthcare professionals to diagnose the patients especially with chronic issues(
Suresh & Paiva, 2021 ). Thus edge computing has the potential to expand the quality healthcare
systems to the remote areas. Generally, the massive data collected which may be structured orunstructured from the patients are stored in the cloud server. These data must be analyzed in a waythat can be used to obtain the inference by using some effective analytical tool. By that time thehealth condition would have been changed. Edge computing applications can be used effectively to102 Chapter 4 An overview of low power hardware architectureovercome the aforementioned issues. The critical analysis can be performed on the edge devices
located on the edge of the network ( http://www.vxchange.com ).
Real time analytics on the premises can easily predict the anomalies and healthcare profes-
sionals can take immediate actions during medical emergencies. The data which are not critical canbe stored in the cloud server and data collected over the period can be segregated and analyzed byadopting certain machine learning algorithm in the remote data centers. There are numerous bene-
fits when edge solution is employed properly in healthcare systems. This includes data proximity,
integration of IoT data and health care provider system, medical care with lowest cost, patientrecord keeping, operating rooms leveraging advanced robotics and video equipment, patient moni-toring leveraging medical devices such as insulin pumps, smart lenses and pacemakers, wearablesand connected apps that track various health metrics, such as heart rate, count steps and hydration,facility utilization where sensors and data analytics help make the most efficient use of clinicalfacilities. Smart devices facilitate people to check in for appointments (
https://blog.apc.com/2017/
12/12/edge-computing-iot-healthcare/ ). Smart medical devices are the key edge computing use
cases that completely transform the healthcare industry’s customer experience. Edge computing
companies can be associated with healthcare professionals. Many hospitals provide interactive edu-cational contents to patients. Edge data centers can provide this content and make it available tothe patients with reduced latency. IoT healthcare supply chain innovations offer an opportunity togain operational efficiencies on the margins and represent one of the more compelling edge com-puting use cases.
FIGURE 4.8
Advantages of edge based healthcare systems ( Naveen & Manjunath, 2019 ).103 4.5 Edge computing for intelligent healthcare applications4.5.3 Implementation challenges of edge computing in healthcare systems
Adequate bandwidth is the foremost challenge that is fundamental for the implementation of 5G
wireless networks. Security aspects of medical devices and interoperability are some other chal-lenges. Cost is an essential factor in procuring sufficient bandwidth. Distributed edge computingarchitecture is not so expensive since the data is stored on premise and so data need not be trans-mitted to a long distance where the cloud server is located. But it is not effective to store largeamounts of data. Edge computing can provide better safety since the data is not transmitted too far
as in cloud computing (
Varghese et al., 2016 ).
The challenges of edge based healthcare system are presented in Fig. 4.9 , and there is less
chance for system wide malfunctioning of the edge architecture. But edge computing is not reliablefor storing large amounts of data. Selecting the appropriate edge computing tool for various health-care scenarios is one of the most important challenges. Due to privacy concerns, it is difficult todeploy open-source software immediately into the healthcare system. Medical professionals shouldbe encouraged to embrace innovation. Healthcare specialists need to be prepared to approve newsensors that can provide information in real time. Edge computing in healthcare is domain-specific
and requires support from healthcare organizations. The approval from the medical experts is the
major challenge. The medical experts have to give creative ideas and helps in designing edge baseduse cases for real time smart healthcare system.
4.5.4 Applications of edge computing based healthcare system
By 2025, 75% of data is expected to be generated at the edge and several edge computing health-
care use cases will be implemented such as point-of-care management and monitoring, chronic dis-ease management, remote assisted living, wellness and preventive care, drug management andhospital operations and logistics. With an increased number of connected devices in the healthcareecosystem, some of the major problems faced by traditional digital healthcare systems are latency
FIGURE 4.9
Challenges of edge based healthcare system ( Naveen & Manjunath, 2019 ).104 Chapter 4 An overview of low power hardware architectureand data security. Edge computing plays a role in resolving these issues as the data is processed
closer to where it is generated. The distributed nature of computation of edge computing systemsalso enables simpler and quicker data management. In smart healthcare systems, there is a continu-ous flow of critical data and an efficient and practical management policy is required to be devised.
Tele-health tools such as video check-ups and appointments with medical professionals
will require seamless data transfer which can be enabled using edge computing. With enhanced
connectivity and saved bandwidth, plenty of healthca re applications can be practically implemented and
improvised (
http://www.cbinsights.com/res earch/internet-of-medical-thing s-5g-edge-computing-chang-
ing-healthcare/ ;https://9to5mac.com/202 0/03/08/apple-watch-bl ood-oxygen-saturation/ ). Remote diag-
nosis is one of the primary use cases of edge compu ting in smart healthcare ap plications. Increased
bandwidth and data security are critical for remote d iagnosis. Portable devices and wearables will be
able to work faster and are more sensitive with th e application of edge computing as the nodes are
decentralized. With the rise in robo tic surgery technology, there is a dire need for seamless data trans-
mission for the reliable operation of such systems. E dge computing facilitates tele-health and continuous
lag-free real time patient monitoring making it a much more reliable and efficient system. Timing is themost crucial aspect of emergency wards and intensiv e care units in hospitals. Managing the operations
of such time-critical units will require highly relia ble and secure network gat eways which can be pro-
vided by edge computing gateways.
An enormous amount of real time data is continuously generated in healthcare applications
and managing this data is an ordeal. Since the processes of data storage and data processing aredecentralized and performed closer to the source of the data, data management is much easier and
quicker. Each data source may have its own data processing unit, where a reduced amount of data
is quickly processed and anomalies are detected immediately when generated. This mechanism canbe used in emergency and time-critical situations. With an overall faster response in thehealthcare ecosystem, the data lifecycle management is simplified to a great extent thereby reduc-ing costs and human effort and increasing efficiency. Processing and storing medical data in thecentralized edge network of the hospital can overcome the privacy challenges of smart healthcaresystems. The data is not stored in a third-party cloud and hence is not vulnerable to data privacyand security threats. Processing data at the edge can also ensure the protection of sensitive data
transmitted between patient and healthcare provider. A combination of edge computing and cogni-
tive computing (computing algorithms for simulation of human thought processes) can be used topersonalize the user experience by altering the resources according to the health data collectedfrom each user.
4.5.5 Patient data security in edge computing
Maintaining the patient’s records is a key issue in the medical world. Hence the experts in the
medical field are looking for technological solutions. Edge computing assists in speed up the pro-
cess in transferring the data without delay. So it helps to make decisions quickly. Edge-enabledtools are helpful in data management. In addition, the APIs created by companies enable medicalprofessionals to know the patient’s current condition and to know the patient’s history and take pre-ventive measures in real time. Healthcare providers can securely store patient data inaccordance with the Health Insurance Portability and Accountability Act (HIPAA) (
https://www.
ncbi.nlm.nih.gov/books/NBK500019/ ). All data can be stored securely in a location called mini105 4.5 Edge computing for intelligent healthcare applicationscenters. Companies can work in conjunction with cyber security and focus on IoT security
features and external networks ( Jurcut et al., 2020 ). MIoT technology and edge-integrated AI can
be expanded to make use of these resources. 5G and edge computing help surgeons to use divingtechnology for training and planning. A kidney injury monitoring program was developed byPotrero Medical. Forecast analytics analyzes patient data in real time with the help of edge comput-
ing and alerts medical professionals automatically before any issues are significant (
https://potrer-
omed.com/ ).
4.6 Impact of edge computing, Internet of Things and 5G on smart
healthcare systems
The technologies associated with of IoT are enormous. The connected devices count is expected to
increase to 3.2 billion by 2023 ( https://www.cisco.com/c/en/us/solutions/collateral/executive-per-
spectives/annual-internet-report/white-paper-c11-741490.html ). One of the most important factors
for this massive growth is the development of 5G networks. The upcoming 5G mobile communica-tion technology is great news for the Internet of Tings world. Since the connected devices finds theimproved performance and reliability due to these advancements in the mobile communicationtechnology. The 5G technology is going to be 10 times faster than current LTE networks. It enablesthe IoT devices to communicate and share data faster. With 5G, there is considerable increase in
data transfer speeds. Nearly all IoT devices will benefit from greater speeds including those with
healthcare and industry applications. For any IoT application, reliable and stable network conditionsare very essential and especially for the applications that depend on real time updates. Reliabilityand performance of connected devices increases with 5G network. The 5G network communicationwhich has the high-speed connectivity, reduced latency, and greater coverage will be the key forthe IoT world (
Liu & Jiang, 2016 ).
With 5G, smart healthcare systems can handle telemedicine appointments which can also
support real time video to get transferred. The f iles and videos used by the medical experts will
be of massive data set and it is very difficult to re view process at the medi cal professional end.
With the help of edge computing, the doctors and a ssociated members can c ollaborate easily. It
can reduce pain and anxiety for terminally ill pat ients in hospice by providing calming, distract-
ing content. But these data consume more time to get transferred if the network bandwidth islow or sometimes may not be sent successfully. T he patients need to wait longer for treatment
and more patients are not attended. Moreover due to the delay, the condition of the patient might
get changed. Both 5G and edge computing technolo g i e sa r er e q u i r e dt ob ec o m b i n e dt oa c h i e v e
ultra-low latency for use cases like healthcar e, remote tele-surgery and autonomous drones.
Additionally, wearables are predicted to decrea se hospital costs by 16% over the next five years
(
https://www.healthcareexecutiv e.in/blog/wearable-technology ). By adding 5G network which is
super-fast to existing architectures increase the s peed and reliable transport of massive data files
of medical imagery, which can improve both ac cess to care and the quality of care. Most of the
key healthcare systems started to use AI to diagno se the patients and prov ide the prescription to
the concern patient depend upon the specific treatment plan ( Singh et al., 2020; Singh & Singh,
2020 ).106 Chapter 4 An overview of low power hardware architecture4.7 Conclusion and future scope of research
This chapter proposes an overview of the low power hardware architecture for edge computing
devices and also presents a few examples of such devices toward smart healthcare systems. With theproliferation of IoT devices, large amounts of data are created. It is crucial to save bandwidth by dis-tributing the processing load and data to points closer to the source. Moving toward 5G, low latency
is the need of the hour and is critical for real time systems. With so much data being sent to the cloud
and the digitalization era, the security and privacy of user data is a huge problem. Edge Computingcan be a reliable solution for all the above problems associated with smart healthcare system.However, the pitfall is that they require more storage space and demand more investment and mainte-nance costs. Nevertheless, trade-offs can be made for cost-effective and scalable solutions. It isimportant to take into consideration that an IoT system consists of both hardware and software partsand optimizing both components is important for a fully-optimized low power system. This chapteremphasizes on the hardware architecture and the different techniques used to reduce the power con-
sumption, advantages of edge computing for smart healthcare, challenges in implementation and
applications of edge computing in smart healthcare system. The optimizations of software side suchas code optimization, conditional code execution etc. are not the focus and can be an interesting topicfor future research. Moreover, this paper presents only an overview of the power management techni-ques, an in-depth analytical investigation can be performed in the future.
References
Alam, M. G. R., Munir, M. S., Uddin, M. Z., Alam, M. S., Dang, T. N., & Hong, C. S. (2019). Edge-of-things
computing framework for cost-effective provisioning of healthcare data. Journal of Parallel and
Distributed Computing ,123,5 4/C060.
Aloqaily, M., Kantarci, B., & Mouftah, H. T. (2016). Multiagent/multiobjective interaction game system for
service provisioning in vehicular cloud. IEEE Access ,4, 3153 /C03168.
Anawar, M. R., Wang, S., Azam Zia, M., Jadoon, A. K., Akram, U., & Raza, S. (2018). Fog computing: An
overview of big IoT data analytics. Wireless Communications and Mobile Computing, 2018 .
Brooks, D., & Sartori, J. (2017). Ultra-low-power processors. IEEE Micro ,37(6), 16 /C019.
Capra, M., Peloso, R., Masera, G., Ruo Roch, M., & Martina, M. (2019). Edge computing: A survey on the
hardware requirements in the Internet of Things World. Future Internet ,11(4), 100.
Chen, M., Li, W., Fortino, G., Hao, Y., Hu, L., & Humar, I. (2019). A dynamic service migration mechanism
in edge cognitive computing. ACM Transactions on Internet Technology (TOIT) ,19(2), 1 /C015.
Chen, M., Li, W., Hao, Y., Qian, Y., & Humar, I. (2018). Edge cognitive computing based smart healthcare
system. Future Generation Computer Systems ,86, 403 /C0411.
Ermi¸ s, G., & C ¸ atay, B. (2017). Accelerating local search algorithms for the travelling salesman problem
through the effective use of GPU. Transportation research procedia ,22, 409 /C0418.
Garcı ´a-Valls, M., Dubey, A., & Botti, V. (2018). Introducing the new paradigm of social dispersed computing:
Applications, technologies and challenges. Journal of Systems Architecture ,91,8 3/C0102.
Gezer, V., & Ruskowski, M. An Extensible Real-Time Capable Server Architecture for Edge Computing.
Gia, T. N., Jiang, M., Rahmani, A. M., Westerlund, T., Liljeberg, P., & Te nhunen, H. (2015, October). Fog com-
puting in healthcare internet of things: A case study on ecg feature extraction. In 2015 IEEE international con-
ference on computer and information technology; ubi quitous computing and communications; dependable,
autonomic and secure computing; pervas ive intelligence and computing (pp. 356 /C0363). IEEE.107 ReferencesGupta, D., & Khamparia, A. (Eds.). (2020). Fog, Edge, and Pervasive Computing in Intelligent IoT Driven
Applications. John Wiley & Sons.
Hameed, K., Bajwa, I. S., Sarwar, N., Anwar, W., Mushtaq, Z., & Rashid, T. (2021). Integration of 5G and
Block-Chain Technologies in Smart Telemedicine Using IoT. Journal of Healthcare Engineering , 2021.
Hassan, N., Gillani, S., Ahmed, E., Yaqoob, I., & Imran, M. (2018). The role of edge computing in internet of
things. IEEE communications magazine ,56(11), 110 /C0115.
Hassan, N., Yau, K. L. A., & Wu, C. (2019). Edge computing in 5G: A review. IEEE Access ,7,
127276 /C0127289.
Indu, I., Anand, P. R., & Bhaskar, V. (2018). Identity and access management in cloud environment:
Mechanisms and challenges. Engineering science and technology, an international journal ,21(4),
574/C0588.
Intelligent and Cloud Computing, Springer Science and Business Media LLC, (2021).
Johnson, O., & Omosehinmi, D. (2015). Comparative Analysis of Single-Core and Multi-Core Systems.
International Journal of Computer Science & Information Technology (IJCSIT) ,7(6), 117 /C0130.
Jurcut, A. D., Ranaweera, P., & Xu, L. (2020). Introduction to IoT security. IoT security: Advances in authen-
tication ,2 7/C064.
Khan, L. U., Yaqoob, I., Tran, N. H., Kazmi, S. A., Dang, T. N., & Hong, C. S. (2020). Edge-computing-
enabled smart cities: A comprehensive survey. IEEE Internet of Things .Journal ,7(10), 10200 /C010232.
Kim, J. (2018). The effect of patient participation through physician’s resources on experience and wellbeing.
Sustainability ,10(6), 2102.
Kuo, M. H. (2011). Opportunities and challenges of cloud computing to improve health care services. Journal
of medical Internet research ,13(3), e67.
Liao, C. C., Chen, T. S., & Wu, A. Y. (2019). Real-time multi-user detection engine design for IoT applica-
tions via modified sparsity adaptive matching pursuit. IEEE Transactions on Circuits and Systems I:
Regular ,Papers, 66 (8), 2987 /C03000.
Liu, G., & Jiang, D. (2016). 5G: Vision and requirements for mobile communication system towards year
2020. Chinese Journal of Engineering ,2016 (2016), 8.
Low Power Hardware Techniques: ,http://ww1.microchip.com/downloads/en/appnotes/01416a.pdf ..
Mahdavinejad, M. S., Rezvan, M., Barekatain, M., Adibi, P., Barnaghi, P., & Sheth, A. P. (2018). Machine
learning for Internet of Things data analysis: A survey. Digital Communications and Networks ,4(3),
161/C0175.
MIPI Alliance Specifications Overview: https://www.mipi.org/specifications .
Naveen, S., & Kounte, M. R. (2019). Key technologies and challenges in IoT edge computing. In 2019 Third
international conference on I-SMAC (IoT in social, mobile, analytics and cloud)(I-SMAC ) (pp. 61 /C065).
IEEE.
Otoum, S., Ahmed, M., & Mouftah, H. T. (2015). Sensor Medium Access Control (SMAC)-based epilepsy
patients monitoring system. In 2015 IEEE 28th Canadian conference on electrical and computer engineer-
ing (CCECE) (pp. 1109 /C01114). IEEE.
Oueida, S., Kotb, Y., Aloqaily, M., Jararweh, Y., & Baker, T. (2018). An edge computing based smart health-
care framework for resource management. Sensors ,18(12), 4307.
Overview of Computer Architecture: https://www.cise.ufl.edu/Bmssz/CompOrg/CDAintro.html .
Rostedt, S. (2013). Intro to Real-Time Linux for Embedded Developers. Linux Foundation Blog .
Saeed, F., Gazem, N., Mohammed, F., & Busalim, A. (Eds.). (2018). Recent Trends in Data Science and Soft
Computing: Proceedings of the 3rd International Conference of Reliable Information and Communication
Technology (IRICT 2018) (Vol. 843). Springer.
Satyanarayanan, M., Simoens, P., Xiao, Y., Pillai, P., Chen, Z., Ha, K., & Amos, B. (2015). Edge analytics in
the internet of things. IEEE Pervasive Computing ,14(2), 24 /C031.108 Chapter 4 An overview of low power hardware architectureShi, W., Pallis, G., & Xu, Z. (2019). Edge computing [scanning the issue]. Proceedings of the IEEE ,107(8),
1474 /C01481.
Shin, Y., Seomun, J., Choi, K. M., & Sakurai, T. (2010). Power gating: Circuits, design methodologies, and
best practice for standard-cell VLSI designs. ACM Transactions on Design Automation of Electronic
Systems (TODAES) ,15(4), 1 /C037.
Singh, P., & Agrawal, R. (2021). An Overloading State Computation and Load Sharing Mechanism in Fog
Computing. Journal of Information Technology Research (JITR) ,14(4), 94 /C0106.
Singh, P., & Singh, N. (2020). Blockchain With IoT and AI: A Review of Agriculture and Healthcare.
International Journal of Applied Evolutionary Computation (IJAEC) ,11(4), 13 /C027.
Singh, P., Singh, N., Singh, K. K., & Singh, A. (2021). Diagnosing of disease using machine learning. In .
Machine Learning and the Internet of Medical Things in Healthcare (pp. 89 /C0111). Academic Press.
Suresh, A., & Paiva, S. (Eds.) (2021). Deep learning and e dge computing solutions for high performance computing.
Springer International Publishing .
Torre, R., Doan, T., & Salah, H. (2020). Mobile edge cloud. In .Computing in Communication Networks
(pp. 77 /C091). Academic Press.
Varghese, B., Wang, N., Barbhuiya, S., Kilpatrick, P. & Nikolopoulos, D. S. (2016). Challenges and opportu-
nities in edge computing. In 2016 IEEE International Conference on Smart Cloud (SmartCloud)
(pp. 20 /C026) IEEE.
Verbelen T., P. Simoens, F. Turck, B. Dhoedt. (2012). Cloudlets: bringing the cloud to the mobile user. In
Proceedings of the third ACM workshop on mobile cloud computing and services . (pp. 29 /C036).
Wan, S., Gu, Z., & Ni, Q. (2020). Cognitive computing and wireless communications on the edge for health-
care service robots. Computer Communications ,149,9 9/C0106.
Yin, H., Zhang, X., Liu, H. H., Luo, Y., Tian, C., Zhao, S., & Li, F. (2016). Edge provisioning with flexible
server placement. IEEE Transactions on Parallel and Distributed Systems ,28(4), 1031 /C01045.
Zhang, J., Xia, W., Yan, F., & Shen, L. (2018). Joint computation offloading and resource allocation optimiza-
tion in heterogeneous networks with mobile edge computing. IEEE Access ,6, 19324 /C019337.
Zhang, Y., Chen, G., Du, H., Yuan, X., Kadoch, M., & Cheriet, M. (2020). Real-time remote health monitoring
system driven by 5G MEC-IoT. Electronics ,9(11), 1753.109 ReferencesThis page intentionally left blankCHAPTER
5Convergent network architecture of
5G and MEC
Ayaskanta Mishra1, Anita Swain1, Arun Kumar Ray1and Raed M. Shubair2
1School of Electronics Engineering, Kalinga Institute of Industrial Technology, Deemed to be University,
Bhubaneswar, Odisha, India2Department of Electrical and Computer Engineering, New York University (NYU)
Abu Dhabi, Abu Dhabi, United Arab Emirates
5.1 Introduction
5G IoT applications, such as smart healthcare, smart home, smart city, smart farming, smart retail, etc.,
can be enabled by multi-access edge computing (MEC). However our focus is on “ Smart healthcare ”
application due to its significant impact on human lif e. Such 5G-enabled, IoT-based smart healthcare sys-
tems need scenario specific designs attributes espe cially in the context of MEC. Convergence of 5G with
MEC is mission critical (MC) in design and deploymen t of such 5G-enabled IoT-based smart healthcare
systems those deal with critical healthcare data. A s the smart healthcare market matures, it needs to
implement massive sensor based machine to machine (M2M) communication devices for medical service
such as data transmission between bio-medical sensor s network data acquisition systems, telemedicine
based framework for communication of huge amount of critical medical data with medical practitioner,
patient and hospital management system and even a resil ient, ultra-high bandwidth, ultra-low latency for
remote robotic surgeries. These remote surgeries may r equire real-time live high definition (HD) or ultra-
high definition (UHD)/4K video feed and ultra-low la tency actuator control of surgery robotic system. To
support this type of priority service, the Tactile 5G-bas ed data network infrastructure is instrumental for
high throughput, ultra-reliability, and low latency co mmunication. Nevertheless, all these objectives have
to be achieved with a cost effective solution. An ultra-re silient accurate convergent network infrastructure
is the need of the hour for realizing the envisioned digital health framework considering the scenario spe-
cific design requirements of future smart healthcar e systems. Researchers and scientific community is
constantly improvising the current communication sta ndards and technologies to address the shot-falls in
existing communication framework towards more c omplex and dynamic need of digital healthcare
domain for practically realizable sm art healthcare infrastructure of future. Wireless network and sensing
technologies for digital healthcare system are rapi dly growing. The objective is small system with high
performance accuracy; prolong battery life and low cost to make the healthcare smarter. The mobile ter-minal of 5G can act as both sensor nodes for patient to sense and collect the medical data and also as
g a t e w a yt ot r a n s m i tt h ei n f o r m a t i o nt or e q u i r e dd e s t i nation (Medical practitione r, hospital data server).
Therefore, by comparing various communication s tandards, Bluetooth low energy (BLE) is found to be
more suitable due to its low cost and easy availability in almost every mobile terminal. Along with it, toincrease the smartness of the system, Edge AI, Emb edded AI, AI PaaS, and Explainable AI will play a
1115G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00003-6
©2022 Elsevier Inc. All rights reserved.key role to assist and manage different resources wi th rapid decision making cap abilities. The authors
also discuss about the research directions in this area.
Fifth-generation mobile communication was init ially suggested by International Telecommunication
Union’s IMT-2020 standard ( International Telecommunication Union, 2021 ). IMT-2020 suggests a peak
download speed of 20 Gbps and upload speed of 10 Gbps with some additional requirements. The 3rd
generation partnership project (3GPP) with their rel ease 15 onwards proposed the 5G NR (new radio) stan-
dard together with long term evolution (LTE) for achieving the IMT-2020 standards technology require-ments. 3GPP release 14 was the last specifications for 4 th generation (4G) LTE networks. Fifth-generation
(5G) mobile communication as standardized by 3GPP release 15 (2019) onwards to provide enhancedmobile broadband (eMBB), ultra-reliable low late ncy communications (URLLC) and massive machine
type communications (mMTC). All these three application areas of 5G has three distinct flavors of 5Gcommunication: (1) high data-rate up to 20 Gbps (per de vice), (2) resilient ultra-low latency network for
MC applications and (3) massive M2M communication framework to support billions of Internet of
Things (IoT) enabled smart devices. 3GPP release 15 (2019) (
3GPP the mobile broadband standard, 2021 )
standard suggested 5G phase-1, NR, mMTC/IoT, vehicle -to-everything (V2X), MC interworking with leg-
acy system, Network slicing, API exposer—third party access to 5G services, service based architecture,further improvements to LTE and mobile communication system for railway (FRMCS). 3GPP release 16(2020) (
3GPP the mobile broadband standard, 2021 ) standard suggests key features like 5G phase-2 with
support for V2X phase-3, industrial IoT, URLLC , NR based access in Unlicensed spectrum (NR-U),
increased 5G efficiency with interference mitigation, SON, enhanced multiple-input multiple-output, loca-
tion, positioning, power efficiency and mobility enha ncement, integrated access and backhaul, enhanced
common API framework for 3GPP northbound APIs (eCAPIF), satellite access in 5G and FRMCS
(phase-2). Apart from providing 20 Gbps in downlink and 10 Gbps in uplink the 5G provides ultra-lowlatency for MC applications with support for billions of IoT and M2M device communication.
European Telecommunications Standards Institu te (ETSI) has proposed a mobile/MEC standard
(
Mobile edge computing—A key technology towards 5G, 2021 ) for 5G. In their recommendations,
ETSI has proposed a decentralized and distributed fog computing (FC) framework for 5G networks.
The 5G network support application programming interface (API) based on some software centric
approach like software defined network (SDN) and net work functions virtualization (NFV). The distrib-
uted computing approach brings the cloud towards the edge devices by bring decentralized storage and
computing platforms towards rad io access network (RAN). In the context of 5G, the FC platforms are
strategically placed near the gNodeB (RAN-access po ints/trans-receivers). A dapting the decentralized
Edge-cloud (distributed computin g) approach would be helpful in achieving various technical goals
like: (1) Computational resource and load-balancing, (2) avoiding bottle neck at a centralized cloud, (3)
minimizing latency for low-latency and mission criti cal applications like image processing, deep learn-
ing and augmented/virtual reality (AR/VR) applicatio ns. In recent times, there is substantial develop-
ment in ultra large scale integra tion (ULSI) based system-on-chip (SoC). These micro-chips possess
various design attributes like low-power design, e nhanced computing capabilitie s and hence are instru-
mental in designing edge computing platforms ne ar to the user device in RAN. The distributed FC
approach is enabled using these enhance d SoCs realizing the MEC design concept.
In the era of 5G mobile communication, heterogeneous technologies integration and their con-
vergence is a major challenge especially in the paradigm of mobile/MEC approach. In this chapter,we are presenting different technical key-aspects related to network and data communication andcomputation framework and architecture pertaining to integration and convergence aspects of 5G112 Chapter 5 Convergent network architecture of 5G and MECand MEC. As per the 3GPP and ETSI standards LTE and 5G technologies and beyond is the era
for complete packet switch data network which is a complete internet protocol (IP) based network.Even the subscriber’s voice calls are processed over voice over internet protocol (VoIP) over LTEand 5G making the technologies voice over LTE (VoLTE) for 4G and voice over new radio(VoNR) or voice over 5G (Vo5G) for 5G networks. As the architecture is based on packet switched
data networks all the user data is quantized as distinct packets using the TCP/IP protocol layered
architecture. All the fundamental networking techniques would come into play in these networksallowing it to be seamlessly integrated to the Internet (global packet switched network). When wetalk about Internet, it is the conglomerate of multiple technologies, protocols and standards makingit heterogeneous network (HetNets) architecture. In the context of heterogeneity comes the conceptof interoperability and convergent network architecture. The Convergent network architecture hasto be adapted for such HetNets to make all functional elements interoperable. Recently numerousresearch works propose different strategies providing interoperability in the context of HetNets in
the era of 5G and MEC (Gur et al., 2020; Chih-Lin et al., 2020). Most of the research works in
these directions propose some clustering based network architecture supported by gateway devicesbased network design approach. The gateway or edge routers are deployed in such HetNets. Thegateway is a software implementation on the edge of two dissimilar network clusters. For anyexample IEEE 802.11 Wireless fidelity (Wi-Fi) network cluster with 5G network cluster. As per3GPP release 16 /C017(draft), support integration with unlicensed ISM band (2.4 and 5 GHz) spec-
trum using IEEE 802.11 wireless LAN access network. The framework and convergent networkarchitecture for such HetNet cluster are pretty much crucial in Next-Gen Wireless network where
some predominant wireless network technologies would be working in collaboration with each
other. In this case, a 5G-NR network cluster with Wi-Fi network (IEEE 802.11 standard) cluster.On the RAN both the technologies support multiple-input-multiple-output (MIMO) technology forenhanced data-rate by spatial channel multiplexing concept. These similarities in functional attri-butes of network can be harnessed using interoperable network framework using edge gatewaydevices. The gateway devices are meticulously designed to translate the required protocol stackimplementations by means of techniques like header translation, dual stack, tunneling approachesfor Protocol Data Unit based data payload delivery in HetNets. This convergent architecture makes
a seamless data communication framework for dissimilar and heterogamous networks, hence pro-
viding a complete interoperability support to the access networks for payload delivery. One moreuse case for such convergent network architecture would be the integration of 5G network withWireless Sensor Networks (WSN) for massive-M2M communication model for future IoT-basedapplication network deployment scenario. The WSN access technologies like Zigbee (IEEE802.15.4), Long Range Wide-Area Network (LoRaWAN), IEEE 802.15.1 BLE are integrated with5G-NR networks using clustering and gateway devices to provide seamless data transfer from bil-lions of smart IoT devices and sensor motes and even can control and drive actuator based applica-
tion by providing the distinct flavors of 5G network attributes and advanced MEC based
infrastructural support. Convergent network architecture for Next-Generation Wireless (NG-W) net-works in the era of IoT, M2M and 5G would be useful for many mission-critical applications likeSmart healthcare, smart-city, AR/VR based smart life-style, smart home automation, assisted living,smart agriculture, vehicular networks, logistic tracking and management network and many more.
Internet of healthcare things (IoHT) and telemedicine applications in future would be getting
major boost with these convergent network technologies with the help of 5G and MEC integration113 5.1 Introductionwith IoT and massive M2M communication framework. Needless to say, with HetNet technologies
coming closer for cooperative network deployment and seamless data integration would be enablingfuture smart medical devices to sense, connect and compute in the paradigm of bio-medical sensordata acquisition, data-storage, data-processing, data-extraction and data-visualization for humaninferable data for MC medical applications. Further, 5G and MEC technologies in conjunction with
IoT, M2M and WSN networks would be instrumental in remote low-latency medical applications
like remote surgery and actuator controls for medical instruments and obviously doing real-timedata-acquisition from bio-medical sensors for critical health applications using IoT.
Artificial intelligence (AI) techniques like deep learning, neural networks along with machine
learning (ML) techniques are quite popular in recent times for solving many engineering problems,especially where a large data-set is involved. With advancement in enhanced Microprocessors andAI accelerators it is now technically feasible to implement AI, deep learning and ML models onreal-time and a large amount of data can be processed using these intelligent algorithms with very-
less processing delay. We are witnessing a huge boom in last 5 years in AI and ML use cases
because of this advanced Microprocessor Units (MPUs) and Graphics Processing Units (GPUs)with help of ULSI and modern fabrication techniques. The major industry players in these segmentslike Samsung, NVIDIA, AMD have achieved 7 nm chips successfully. Samsung and TaiwanSemiconductor Manufacturing Company (TSMC) are ambitious for 5 nm fabrication technique fortheir newer chips. Hence the recent advancements in chip industry would provide better computa-tional platform and hence has a huge potential to support AI and ML software using newer MPUs,GPUs and accelerators. These enhanced computational platforms would be the technology enabler
for future 5G and MEC based network architecture having capabilities to support more computa-
tionally regressive AI and ML application framework on real-time or with very minimal computa-tional latency. Future looks very bright with some other Nonconventional computing approach likeQuantum-Computing. Companies like ColdQuanta, IBM, Honeywell and Google and many morehave already developed and implemented working models of Quantum Computers and in comingfuture more and more advancement are expected in this field to support more computational regres-sive applications like advanced AI and computational intelligence models. All these technologieswill be instrumental for future data communication networks like 5G and beyond with the help of
MEC approach with enhanced application support.
In
Section 5.1 of the chapter, we present introduction to convergent network architecture of 5G
and MEC with a technical overview. Rest of the chapter is organized as follows: Section 5.2
focuses on technical overview on 5G Network with MEC. In Section 5.3 , we discuss on convergent
network architecture for 5G with MEC. Section 5.4 deals with current research in 5G with MEC.
We present challenges and issues in implementation of 5G with MEC in Section 5.5 and finally
Section 5.6 concludes the chapter.
5.2 Technical overview on 5G network with MEC
5G network infrastructure provides ultra-low latency high bandwidth communication. The URLLC
framework using technologies like millimeter wave, massive-MIMO (m-MIMO) based 5G NR withadditional enhancements in technologies like SDN and NFV.114 Chapter 5 Convergent network architecture of 5G and MEC5G NR provides improved RAN with the help of m-MIMO antenna system. m-MIMO technol-
ogy helps in improving the data-rate with spatial multiplexing.
Concept of reduced range is to support more network capacity with the help of technologies
like pico/femto/micro cell. The cell size in 5G is small; hence the transmission power gNodeB(Base Station (BS)) is less in each cell. This small size of cell makes more number of cells in a
geographic location. Increased number of cells helps in increasing the network capacity to accom-
modate more users and devices. This would be ideal for IoT and M2M deployment scenario.
Very high frequency operation ( B50 GHz) in the range of millimeter wave (mm-wave) communica-
tion is an essential technological aspect to increase the channel bandwidth hence increasing the effective
data-rate of communication. As very high-frequen cy is used in 5G even it’s inevitable to reduce the
transmission power to make the power spectral densi ty in a technically feasible and practicable range
and also to limit the permissible specific absorptio n rate threshold for safe use for living beings mostly
in an urban deployment scenario where the population is exposed to so many 5G antennas.
5G silent features
 Date-rate: up to 20 Gbps (per device)
 Latency: ,1 ms in RAN
 Mobility: up to 500 km/h
 Localization: 1 m (accuracy)
 Service continuity: trains, spare and dense areas Scale: 20 million user devices (UD) and more tha n trillion IoT/M2M devices with high reliability
5G key technological aspects
 Virtualization/programmability of network services
 Emerging technology support for SDN, NFV, MEC and FC Migration network/cloud
5.2.1 5G with multi-access edge computing (MEC): a technology enabler
Advancements in Embedded Systems and Energy efficient MPU based SoC have opened up newer
avenues in computing approach in today’s era. Unlike the centralized cloud based architecture, nowit is very much technically feasible to have computation at the edge devices. The Edge is referredto the mobile terminals /smart phones and the Base-Station gNodeB (5G terminology) in the RAN.
The classical approach of Cloud computing has an issue of bottle-neck at the centralize server
because all the computation and data processing is done at the single node. However in MECapproach the computation has been distributed using a task scheduler technique. This methodwould be helpful in reducing the computational load on the centralized cloud infrastructure.
The 5G standard proposed in 3GPP Revision (
3GPP the mobile broadband standard, 2021 ) and
the MEC framework for 5G networks proposed in ETSI recommends few key technical designaspects of the 5G-NR based RAN and deployment of edge computing platform for incorporatingthe MEC into 5G network. Some of the predominant techniques suggested by latest 5G and MEC
standards can be summarized below:
 5G-NR based RAN with enhanced computational resources on the Edge of the network which
enables MEC features115 5.2 Technical overview on 5G network with MEC Ultra-low latency (also referred to as “Zero Latency”) in the RAN for supporting real-time
applications on the edge
 Software Defined Radio (SDR) in RAN and SDN in backbone network infrastructure NFV for creating a generic communication infrastructure and virtualizing the network functions
on it using SDR and SDN techniques
 High data-rate (typical 20 Gbps per device) on the RAN to support real-time data intensive
application framework. This can support rich application content like UHD (4K) video streamsand Smart Vehicular applications and also applications like AR, VR, AI, deep learning and MLcan be implemented on the RAN using techniques like MEC
 A hybrid application framework which works on distributed computing approach. A Task
Scheduler for splitting the applications between the Cores centralized Cloud and Edge devices.This would be instrumental in providing a optimum on-demand computing approach and wouldbe helpful in reducing load from the centralized cloud server and hence be effective in
mitigating the bottleneck on the centralized cloud server. This is the key feature towards MEC
in the era of 5G network technology and beyond.
Fig. 5.1 shows the MEC framework in 5G networks. The 5G-MEC framework as suggested by
3GPP and ETSI standards support major communication and computation functionalities through
some functional elements. We have presented the same with the functional division of the system
through control and data plane.
Control plane : The control plane would deal with the control functionalities of 5G and MEC
network and RAN aspects.
Resource configuration of RAN with NFV and SDR/SDN approach
Edge-cloud task manager and scheduler to spilt the application in edge computing (distributed
computing) and centralized cloud computing. The Task manager would be able to take decisionbased on the application. Some part of the application should be executed on the Edge-deviceand some data intensive part of the application should be executed on the centralized cloud
Control over different functional blocks of the 5G and MEC system for communication and
computation framework support
Virtual Machine support for virtualizations of functions and operation over NFV and SDN
technique.
Data plane : The data plane would support the transportation of data (packet switched data net-
work) from node to node delivery model using TCP/IP stack implementation in 5G.
Next generation (NG) core data network for switching, forwarding and routing of data packets
through the network
Centralized data center and cloud server to provide the central cloud computing and storage
functionalities.
Some of the highlights of MEC approach in 5G technology can be summarized as below:
 Reduce load from the centralized cloud infrastructure
 Bringing cloud near to the User device (into the RAN) to reduce the latency (theoretically zero
latency) to support applications like AR/VR/Image Processing applications on the go on mobiledevices116 Chapter 5 Convergent network architecture of 5G and MEC Computing, storage and networking resources are integrated with gNodeB (base-Station)
 Compute intensive and latency sensitive applications framework can be deployed on the edge
devices and RAN
 Computational platform and storage resources like Platform-as-a-Service (PaaS), Software-as-a-
Service (SaaS) Infrastructure-as-a-Service (IaaS) using the hybrid edge-cloud computingapproach on the network edge with close proximity to the user devices.
5.2.2 Application splitting in MEC
Application splitting is the strategy used in 5G and MEC based communication and computation
framework. The Fig. 5.2 shows the framework for application splitting for MEC based architecture
in 5G. The task scheduler/manager would take decision based on the application. A part of theapplication which has low-latency requirement and real-time executional component should be exe-cuted on the edge computing infrastructure of 5G RAN and the part of the application which is
FIGURE 5.1
Multi-access edge computing (MEC) framework in 5G network.117 5.2 Technical overview on 5G network with MECdata intensive and need access to the Big-data of centralized cloud server would be executed cen-
trally. Finally at the user device it application can be integrated using distributed computing
approach. This approach is efficient in managing an application in the paradigm of MEC but at the
same time it is one of the major technical challenge to distribute subtasks (Task Manager) andcoordinate millions and billions of tasks and should be addressed by service provider and technol-ogy and computational framework developers.
Key-technical aspects of MEC
 Commercial off-the-shelf (COTS) application servers are integrated to gNodeB (base-Stations)
of 5G networks
 Tailored services/Apps for an enhanced user experience (UX)
Functionalities of MEC Server
 Computation and storage on the Edge
 Connectivity and communication in RAN
 Access user traffic
 RAN functions using SDR and NFV
FIGURE 5.2
Application splitting framework using MEC in 5G network.118 Chapter 5 Convergent network architecture of 5G and MEC Network information and management
 Query to response in less than ,100 ms latency
 Real-time traffic analytics and Machine Intelligence Real-time processing of data on the Edge in the RAN using gNodeB Virtualization: MEC Virtualization Manager
Functionalities of Centralized/hybrid Cloud Server with collaborative computing approach with
MEC
 Offline-batch processing tasks
 High latency tasks Data intensive tasks (big-data and data mining applications) Larger (Data Centers) and centralized cloud computing approach High computational power
5.2.3 Layered service oriented architecture for 5G MEC
Fig. 5.3 depicts the layered architecture for 5G and MEC. The 5G communication infrastructure
provides platform low-latency and high data-rate for Communicate. Additionally compute andsense/actuate (control) billions of smart devices are feasible using technologies like IoT and M2M.The Fog/Distributed computing approach provide flexibility in computing. The layering is abstractconcept suggested by differently modular and functional blocks. The service oriented layering
architecture discussed here is in abstract conceptual level (not any standard yet). This service ori-
ented modular architecture is suggested here with the vision of 5G and MEC integration aspectssuggested by ETSI and 3GPP release 16 onwards.
Functions of modular service oriented architectureDevice access management: All the User devices are connected to 5G RAN. All the data and
control for management and content delivery of User devices in RAN is handled by the MECdevice access and management layer (MDAML).
Process scheduler—virtualization : Process Scheduler—Virtualization Layer (PSVL) is for
Centralized Cloud and MEC based hybrid computing platform. MDAML is connected to the PSVL
through an interlayer communication (Logical connection) with resource management framework.The PSVL would be allocating and managing the hybrid computational platform integrationaspects. The some application modules which require a decentralized approach of computing (EdgeComputing/Fog) would be assigned MEC framework by PSVL and application modules whichwould require a centralized and data intensive cloud-based computational resource would beassigned the centralized cloud infrastructure by the PSVL. Point to be noted the application frame-work support for both Edge Computing/Fog and Cloud computing is done on the application virtua-
lization layer and splitting concept.
Edge computing framework: The distributed MEC servers at gNodeB in RAN are part of the
edge computing framework. Low-latency and less computationally regressive applications wouldbe handled by the MEC infrastructure of RAN in 5G-NR. Application like AR/VR with accesslayer computational requirement would be processed by the MEC computational framework.
Cloud computing framework: The centralized cloud based computational framework would be
processing all data intensive applications where extensive big-data support and more computationally119 5.2 Technical overview on 5G network with MECregressive algorithms are needed to be implemented. The centralized data servers with extensive stor-
age requirements are key aspects of cloud computing based architecture. The Cloud computing basedApplication programming Interfaces (APIs) are enabled for the processes to run with high resource
requirement of data, RAM and CPU. Application like data analytics, ML, deep learning, data mining,
multimedia with audio and video content streaming based applications would be more suitable forthis centralized cloud infrastructure based cloud computing platform.
Advanced application with high demand for computation with IoT and 5G technologies
will lead to unprecedented traffic requirement. This encourages the shifting of centralized cloud
FIGURE 5.3
Layered service oriented architecture for 5G MEC.120 Chapter 5 Convergent network architecture of 5G and MECcomputing to edge computing. The new technology such as MEC will enable a paradigm shift in
the way we envisage cloud computing as a part of RAN. The important characteristic of MEC is toperform the task of computing, controlling and storage at the edge of the network. MEC servers aredeployed to execute delay sensitive and context aware applications in the close proximity of endusers. The MEC offers large bandwidth, low latency, highly efficient network operation and service
delivery to the end user. Although IoT is an integral part of 5G but the devices have limited storage
and processing capabilities. Therefore, to fulfill the requirement of new compute-intense applica-tions of 5G IoT, it intuitive to converge with MEC for smooth processing of large data trafficbefore sending to the cloud (Zhang, Leng, He, Maharjan, & Zhang, 2018). The successful realiza-tion of convergence of MEC with 5G requires research and development from academic and indus-tries communities.
The standardization of MEC was provided by European Telecommunication Standard Institute
(ETSI) and Industry Specification Group (ISG). MEC is also recognized by European 5G
Infrastructure Public Private Partnership as a primary emerging technology for 5G networks. ETSI
defined MEC as “Multi-access edge computing provides an IT service environment and cloud com-puting capabilities at the edge of the mobile network (RAN) and in close proximity to mobile sub-scribers.” Deployment of MEC at the BS will enable the end user to access the cloud services withenhanced computation and less system failure. The characteristics of MEC include the following:
 On premises- MEC can work isolated from other networks by having access to only local
resources which makes it less vulnerable.
 Proximity- The deployment of MEC in the nearest location helps to analyze big data and give
benefit to the computation demanded applications such as AR/VR, analytics, etc.
 Low Latency- less than 1 ms. High Bandwidth- Download data-rate of 20 Gbps for each device. Location Awareness- MEC can be aware about the location of the devices through the
information received from the edge distributed devices within the local access network.
 Network Context Information- The context information and real-time radio network benefit the
service experience and utilization of network resources to handle the voluminous data traffic byoptimizing network operation.
 Mobility Support-Enhanced mobility support up to 500 km/h. Security and Privacy- Advanced Encryption and authentication framework support.
Mobile cloud computing (MCC), cloudlet and FC were in demand for computation purposes
before MEC. However, FC presents a layer consisting of devices such as the M2M gateway andwireless routers known as fog computing nodes (FCNs) between the cloud and the end user. Thesedevices compute and store the data locally before sending to the cloud. The FCNs behaves as het-erogeneous combination of nodes and support every device including non-IP based technologies to
fit into every protocol layer and it is hidden from the end devices due to creation of Fog abstraction
layer. MCC is the combination of cloud computing and mobile computing and creates a virtualizedenvironment in the cloud consisting of resources such as compute, storage and communicationwhich can be accessed by the end users remotely. Here the resources are placed inside the cloud.
The drawbacks of MCC attract the researchers to propose the edge cloud concept. In Cloudlets,
the devices have similar capabilities like data center but in a lower scale by distributing computa-tions to the cloudlet devices. A Cloudlet is called as “data center in a box” handling a virtual121 5.2 Technical overview on 5G network with MECmachine providing resources for end devices and users over WLAN network. Cloudlets consist of
three layers such as (1) component layer (2) node layer (3) Cloudlet layer. One of the use-caseimplementation of Cloudlet Architecture for cognitive assistance applications involving the primaryvirtual machine along with other virtual machines provides cognitive functionalities in the Cloudletto serve the request. However, the Cloudlet has the limitation to access only in Wi-Fi which cover
small range of area and unable to provide ubiquitous computing.
Table 5.1 shows a comparison of different computing technologies in the context of MEC,
MCC, Cloudlet and FC. We have presented all these different approach of computing with respectto their advantages, limitations and challenges with key parameters like origin, context awarenessand latency. The detailed comparison shows the technical advantages of both centralized computingapproach and the distributed computing. Each of the computational models has their own trade-offin dealing with the computational requirements. The cloud computing approach would be moresuitable for data intensive centralized computing architecture. However the MEC, Fog or distrib-
uted computing approach would be more preferable for applications with low latency requirements.
The comparison can be summarized as the best use-case implementation as a hybrid computationalmodel with application splitting approach with both edge as well as cloud computing modules inplace in the real-world deployment in the context of 5G and MEC.
To solve the above mentioned limitations/challenges of the similar technology, MEC is pro-
posed. MEC provides the platform to converge the cloud computing capabilities within the RANnearer to the mobile users (
Sodhro, Luo, Sangaiah, & Baik, 2019 ). The early definition of MEC is
to offload the computation intensive task to the BSs from the end devices. Later on, in 2014 the
MEC ISG and ETSI started the standardization of MEC to encourage the growth of edge cloud
computing in mobile networks. MEC provide an open environment to locate multivendor cloudplatforms at the edge of RAN to overcome the challenges faced by centralized cloud computingparadigm for giving higher speed and low latency. Taking this into consideration, in 2017 forexpanding the advancement of MEC application to HetNet, that is, for 4G, 5G, Wi-Fi, and fixedaccess technology, the word “Mobile” is removed from Mobile Edge Computing and ETSI ISGrenamed MEC as Multi-access Edge Computing (
Dong et al., 2020 ). The diversification of MEC
helps its server to deploy at different locations within RAN set aside with different component of
network edge such as BSs, that is, eNB for 4G and gNB for 5G, optical network unit, radio network
controller sites and Wi-Fi access points. At this point, the intelligence is transferred from center tonetwork edge which facilitates communication along with computation; caching and service controland satisfy its name as MEC. Since last few years, researchers concentrate on integration of MECin 5G communication technology and its applications.
5.3 Convergent network architecture for 5G with MEC
In today’s era 5G and beyond network deployment we may encounter heterogeneity of network
technologies and even protocol stack implementation. The heterogeneity in communication technol-ogies are attributed by various access networks like 5G-NR, Wi-Fi/Wireless LAN (IEEE 802.11),BLE (IEEE 802.15.1), Zigbee (IEEE 802.15.4), LoRaWAN even satellite access networks for vari-ous application-specific access network deployment scenarios. Convergent network architecture122 Chapter 5 Convergent network architecture of 5G and MECprovides a seamless operation in HetNet deployment scenario. A convergent network is one which
provides framework for seamless interoperability and required translation at the gateway devices.In a nutshell, Convergent network architecture provides required communication framework to inte-grate multiple HetNet clusters (segments) into a single functional network to operate seamlessly.Different network technologies are integrated together to work as a single logical as well as opera-tional network realizing a converged network.Table 5.1 Comparison of different computing technologies.
Technology Advantages Limitations/challengesIntroduced
byContext
awareness Latency
MEC  It gives benefit to
mobile network
operator (MNO),
application serviceprovider (ASP) andthe end user by
providing
computationaloffloading, lowlatency, storage,
energy efficiency and
high bandwidth. Security and privacy,
resource optimization,
robustness of MEC
servers, compatibilityissue of web interface,transparency in
migration of user
applications.ETSI
(2014)High About
1m s
MCC  Resource
management on
demand such as
network, server,application, storageand computation in a
mobile environment. The farthest distance
of centralized cloud
servers from the end
user increases thedemand for acompute-intensive
environment.
 Increase latency and
network disconnection.No 30 /C0100 ms
Cloudlet  Overcome latency
and cellular energy
consumptionproblem.
 Provide cloud
technology nearer to
the end user. Privacy and security
issue of data.Carnegie
Mellon
University(2013)Low .100 ms
Fog
computing It allows the single
processing devices to
collect data from
different sensors andtake appropriateaction.
 It provides low
latency ascomparison to cloud
computing. Dependency on
wireless connection.
 Limitations in
performing complexaction.Cisco
(2011)Medium ,30 ms123 5.3 Convergent network architecture for 5G with MECFig. 5.4 depicts a technical visualization of convergent network architecture with 5G and MEC.
The main idea behind convergence is to provide interoperability of cooperative communicationframework for multiple dissimilar network clusters with heterogeneity in network technologies andprotocol stack (TCP/IP) implementation.
In a real-world network deployment scenario, it is inevitable to have multiple HetNet clusters. As we
can see in
Fig. 5.4 , different applications require different kind of access technology (Physical as well as
Data-Link layer). Specifically, if we analyze the T CP/IP stack implementation of the Physical Media
dependent protocol (PMDP) and physical layer convergence protocol of Layer-I along with the MediaAccess Control sublayer of Layer-II, we observe that d ifferent access technologies like IEEE 802.11 (Wi-
Fi), 802.15.1 (Bluetooth), 802.15.4 (Zigbee), LoRaWAN, LTE/5G-NR are needed. Many IoT-based appli-cation use IEEE 802.11 as access network. WSNs a pplications uses Zigbee (IEEE 802.15.4) access
FIGURE 5.4
Convergent network architecture of 5G and MEC.124 Chapter 5 Convergent network architecture of 5G and MECnetwork. Many smart healthcare wearable devices use BLE IEEE 802.15.1 access network as Wireless
Personal Area Network (WPAN). Vehicular Network is often implemented using IEEE 802.11p WirelessAccess in Vehicular Environments (WAVE) and al so cellular VANET implementation. Some of the IoT
and M2M applications are popularly using Long-R ange Wide-Area Network (LoRaWAN) by harnessing
low data-rate and long range communication design ed for IoT and M2M communication. Even satellite
communication based access network can be used in remote areas where cellular or other network infra-
structure is unavailable. All these application-specif ic network clusters can be integrated together and work
like a converged network with TCP/IP stack and gateway devices. The gateway and clustering techniquesare proven to be a successful strategy in dealing with heterogeneity and interoperability. All the different
segment/cluster of access network are integrated with existing 5G cellular network using IP-based gate-ways and WAN access routers making a convergent architecture for 5G with MEC enabled services.
5G Communication is based on eMBB, URLCC and mMTC not delivery (4G) functions (
Li,
Huang, Li, Yu, & Shu, 2019 ). It provides advanced communication platform for many advance
applications to be developed such as VR/AR, autonomous mobile, Tactile IoT. To meet the com-
munication as well as computation demands, several additional technologies have been developedfor 5G including radio access, network resource management, network architecture and applicationsscenarios, energy saving and system performance. The above technologies includes dense HetNets,cloud radio access network (C-RAN), nonorthogonal multiple access (NOMA), unmanned aerialvehicle (UAV), wireless power transfer (WPT), energy harvesting (EH), IoT and ML.
With the increased use of mobile devices and IoT, data traffic on communication network expo-
nentially increases. Hence, to optimize the network use and minimize device numbers, convergence
is an emerging solution. However, MEC can be used in three categories such as for consumer ori-
ented services, third party operator’s services and QoE improvement of the network. MEC platformshould create a supporting environment for all these services at the edge of the network. The userscan be benefited through the computation offloading at the edge. Therefore, user-centric applica-tions such as AR\VR, gaming, big data, location tracking, security and privacy, latency sensitiveservices (Remote surgery through tactile internet) and QoE (network performance) in 5G need con-vergence with MEC to support new architecture and advancement in emerging technologies. The5G and MEC convergence scenarios includes: NOMA, EH and WPT, UAV Communications, IoT,
H-CRAN, ML, VM, SDN, NFV and network slicing. Apart from these technologies, block-chain
can be integrated with MEC to give various benefits related to context of cloud of thing.
In the
Fig. 5.4 , we have considered the convergence of 5G with MEC, where the BS of 5G and
Mobile Internet world are merged and the edge servers are located in such a way that BS candirectly control it and simultaneously BS can access the internet data and the IoT sensors data.
5.4 Current research in 5G with MEC
Advancements in the field of 5G and MEC have been phenomenal since last few years. In this sec-
tion, we discuss about the research perspectives on the same. Table 5.2 shows a detailed analysis
on the current research work on various technical aspects of 5G and MEC. Here, we present anextensive study on few prominent research works pertaining to some emerging field of researchwhich are relevant with 5G, MEC, and Convergent networks. This study focuses on these following125 5.4 Current research in 5G with MECTable 5.2 Current surveys and research works.
Technology References Authors contributions Findings Research issues
MEC in
healthcareSodhro et al.(2019) Window based ratecontrol algorithm(w-RCA) is proposed to
develop the medical
quality of service(m-QoS) in MEC basedhealthcare.From the comparison of
performance evaluationwith battery smoothing
(BSA) algorithm, w-
RCA shows betterperformance.Steady and inefficient
feature of traditionalmethod unable to
optimize QoS properly
for healthcareapplication.Rate controlalgorithm is only used
for multimedia and big
data application andthere is delay in w-RCA.
Dong et al.(2020) The two subparts ofIoMT, that is, (1) intraWBANs is modeledusing bargaining
game to handle the
wireless channelresource allocationissues and in
(2) faraway WBANs
the offloadingdecision issue
developed game.Investigation of edge
computing orientedstructure in IoMTs withminimizing system wide
cost.Machine learning and
block-chain basedelectronic health datafor maintenance of
privacy and distributed
intelligent spectrumallocation.
Li et al.(2019) Proposed a safe andsystematic informationmanagement systemcalled “EdgeCare” for
mobile medical systems.
A hierarchicalarchitecture is designed
to implement EdgeCare.EdgeCare provide
solution for protectionof medical informationand support well
organized data trading.Confidentiality,
integrity andauthentication,traceability, user centric
access control.
Ray, Dash,
and De (2019)Provide taxonomicalclassification andreview of IoT andedge computing, use
cases for urban smart
living problems andedge-IoT-based
architecture proposed
for healthcare.Minimization of
dependency on IoTcloud analytics.Edge gateway, Edge
analytics, edgearchitecture, sensoractuator integration,
context awareness and
unified framework ofintegration.
Hartmann,Hashmi, and
Imran (2019)Survey of latest and
appearing edge
computing architectures
and methodology forhealthcare applicationwith requirements and
challenges.A detailed survey on
edge computing data
functioning.Low latency
communication of
voluminous data with
maintaining a trade-offbetween the accuracyand complexity of AI.
(Continued)126 Chapter 5 Convergent network architecture of 5G and MECTable 5.2 Current surveys and research works. Continued
Technology References Authors contributions Findings Research issues
Oueida, Kotb,
Aloqaily,Jararweh, and
Baker (2018)An integrated
architecture of resourcepreservation net (RPN)
framework and edge
computing has beenproposed for emergencydepartment in
healthcare.Improvement in
performance measuressuch as patient length of
stay (LoS), resource
usage and patientwaiting time.New privacy policies
should be introducedand quality of
experience (QoE) of
service should beconsidered.
Yu and Choi(2020) Proposed a landmark-free and pose-estimation-free frontal-
face synthesis system.Pose-unconstrained face
recognition is improvedusing MEC for human
identification in medical
systems.Improvement in
accuracy.
Integration
of MEC
and 5GHu, Patel,
Sabella,
Sprecher, and
Young (2015)The scientific andtechnical advantages
from mobile edge
computing.Proposed proof-of-
concept (PoC) program
to exhibit the feasibility
of MEC performance.Internet of things
gateway, augmented
reality, intelligent video
acceleration,deployment framework.
Kekki et al.(2018) Presents the benefit ofdeploying MEC on the
N6 reference point of
the 5G system.The user plane function
(UPF) remain is same
and independent of the
deployment of MEC andit integrates the MEC
application traffic in the
5G bearer stratum.Standardization of the
application
programming interfaces
(APIs) to expose therequired 3GPP system
capabilities.
Tomaszewski,
Kukli ´nski, and
Kołakowski
(2020)5G slicing architecture
and MEC integration.Proposed architecture
with 5G, APIs andMEC.MEC using services of
APIs, for mobility androaming application in
5G.
Lin, Hu, Gao,
and Tang(2019)Optimize the end to endlatency with authenticityof user’s priorities.The proposed algorithm
reduces end to end delayby allocating
communication and
computation andresources in healthcareapplication.Network delay and
users priority inauthenticity.
Gavrilovska,Rakovic, andDenkovski(2018)Resource scaling inMEC as a favorableperspective for 5G.Container based
virtualization isappropriate for resourcescaling more
suitable for deployment
of 5G-MEC.Latency, reliability and
throughput.
Tran,Hajisami,
Pandey, and
Pompili(2017)Studied three use cases,that is, (1) mobile edge
orchestration, (2)
collaborative caching andprocess, (3) multilayerinterference cancellation.Describe the benefits of
the proposed approaches
for the evolution of 5G.Resource management,
interoperability, service
discovery, mobility
support, fairness andsecurity.
(Continued )127 5.4 Current research in 5G with MECTable 5.2 Current surveys and research works. Continued
Technology References Authors contributions Findings Research issues
Sarrigiannis
et al. (2019)Latency basedembedding mechanismand an online
scheduling algorithm
evaluated through aMEC enabled 5Gplatform.Maximizes the number
of users by utilizing theonline allocation of
edge and core resources.Virtual Network
Function (VNF)allocation and
placement, overhead
due to live migrationprocess.
Pham et al.
(2020)Surveyed an overviewof use cases and
application of MEC andoutline researches in
integration of MEC and
5G technology.Summarized test beds
and experimental
evaluation.Machine learning based
framework
implementation inMEC, federated
learning and application
for MEC.
Integration
of MEC,
5G and IoTLiu, Peng,
Shou, Chen,
and Chen
(2020)Analyzed the mainfeatures of integration
of MEC, 5G and IoT.Described various
technologies including
SDN/NFV, virtual
machines (VM),network slicing, cloudcomputing and
computation offloading.Mobility management,
edge intelligence,
pricing, MEC servers’
deployment, greenMEC, security andprivacy.
Redondi,Arcia-Moret,and Manzoni
(2019)IoT in 5G using pub/
subarchitecture withsystem design and
optimization.Discussed MQTT 1
protocol supporting IoTapplication in MEC
based 5G networks.Orchestration strategies
create issues in loadbalancing and resource
allocation process.
Sanchez-
Iborra et al.(2019)Slicing framework forIoT application of in5G-based MEC.Quality of service
(QoS) and highscalability.Security and privacy
policy.
Farris, Orsino,
Militano, Iera,
and Araniti(2018)Investigate mobile-IoT-
federation-as-a-service
(MIFaaS) model in 5Gtechnology to satisfy thelow latency
requirement.To handle large data
traffic, the combination
of NB-IoT and LTE isneeded to get high datarate and low latency.Prediction of mobility
pattern, power
consumption andtrustworthy interactionto build a secure
system.
Hsieh, Chen,
andBenslimane
(2018)5G technology is
implemented usingvMEC, SDN and NFV
technologies support
various IoT devices toobtain low latency andhigh bandwidth.The traffic control
strategies decrease inIoT delay, increase QoE
and less network
congestion.Latency reduction of
IoT application servicesand QoS.
Porambage,Okwuibe,Liyanage,Ylianttila, and
Taleb (2018)A survey of MEC
technology supportingIoT applications.The role of IoT towards
5G communicationsystem and technicalaspect of empowering
MEC in IoT and other
various integrationtechnologies.Scalability, mobility
management, security,privacy, computationoffloading and resource
allocation.
(Continued)128 Chapter 5 Convergent network architecture of 5G and MECareas: (1) MEC in healthcare, (2) integration of MEC and 5G, (3) Integration of MEC, 5G and IoT
and (4) MEC, 5G and IoT in Healthcare.
Fig. 5.5 shows the year-wise research trend over the last 6 years (2015 /C020) as per Google
Scholar in various key-technical aspects like 5G, MEC, and Convergence network such as HetNets,IoT, and M2M communication. These charts give a better understanding of the research focus inkey areas related to 5G MEC and convergent network. Number of research papers in 5G, HetNet,IoT are consistent throughout last 6 years. Number of research papers on convergence and M2M is
gradually decreasing however, there is a substantial growth in number of publications in MEC.
MEC being quite popular among the research community to adapt as a computational technologyespecially in today’s era 5G and beyond for IoT enabled heterogonous networks (
Ray et al., 2019 ).
5.5 Challenges and issues in implementation of MEC
There is a paradigm shift in the computing scenario after introduction of 5G and associated conver-
gence with MEC. Edge technology is the key to user specific computing along with cloudTable 5.2 Current surveys and research works. Continued
Technology References Authors contributions Findings Research issues
MEC, 5G
and IoT inHealthcareBraeken andLiyanage(2020) Architecture of remotemonitoring system using5G technology
integration with IoT
devices and MECnodes.Established an
authentication phasebetween MEC nodes
and IoT devices to
provide requiredsecurity features.Handover mechanism
between MEC nodesprovide dynamicity and
low latency for IoT
devices and highcomputation requiredsecurity.
Ning et al.
(2020)To set up a MEC based5G system for IoMT
home health monitoring.Performance evaluation
indicated the efficacy of
the proposed algorithmfor cost effective and
patient benefit using
MEC.Medical data
management complexity
and power consumption.
Hewa,Braeken,
Ylianttila, and
Liyanage(2020)MEC and block-chainbased architecture using
ECQV (Elliptic Curve
Qu Vanstone) tomaintain security andprivacy of data between
IoT, MEC and cloud.Performance evaluation
demonstrate that the
system support
voluminoustransmission of datathrough MEC nodes and
optimized the block-
chain storage byoffloading.Scalability and on chain
secure multiparty
computation.
Zhang et al.(2020) MEC and artificialintelligence technique
inspired medicale-healthcare system.Improved patient
treatment and cost
effective.Low latency and high
accuracy.129 5.5 Challenges and issues in implementation of MECFIGURE 5.5
Research tread in various key-technical aspects: (A) 5G, (B) MEC, (C) convergence, (D) HetNet, (E) IoT and
(F) M2M from 2016 to 2020 (past 6 years)/C3Data acquired from Google Scholar with keyword search.130 Chapter 5 Convergent network architecture of 5G and MECcomputing. The convergence with 5G and the recent developments in association with NFV, ICN
(Information Centric Networks) and SDN form the basis of MEC implementation ( Hartmann et al.,
2019; Oueida et al., 2018 ). Implementation of 5G and MEC for different applications raises many
challenges ( Yu & Choi, 2020 ). We discuss the challenges and issues for MEC and 5G so far as
their architecture, communication and computing scenarios are concerned.
Use of MEC is becoming increasingly important as there is a steady rise in the host of IoT and
mobile applications. These applications need edge specific computations with MEC making cloudcomputation secondary (
Yu & Choi, 2020 ). MEC servers take on the local computations thereby
reducing computational load on cloud servers. In this chapter, we focus on different applicationswhere MEC has taken a key role in enhancing the performance of 5G network. However, we dis-cuss the challenges and issues associated with each of these.
5.5.1 Communication and computation perspective
The use of MEC in 5G is primarily based on communication and computation models. MEC is usu-
ally positioned by between the mobile devices and the cloud servers to leverage the device specific
computations. The communication model deals with various research issues ( Shafi et al., 2017;
Zhang et al., 2016 ).
5.5.1.1 MEC service orchestration and programmability
It should be performed simultaneously by cooperating with each other considering the allocation ofVNF and service across the edge cloud platform. The edge cloud platform across various adminis-trative domains creates more challenges for resource aggregation and service mapping. Various ser-vice aspects of edge cloud orchestration and programmability includes:
 Resource allocation, service placement, selection of platform and reliability.
 Continuous service and mobility in the edge cloud platform. Joint optimization of VNFs and MEC should be performed to execute proper resource
utilization and cross layer optimization in edge cloud services and network resources.
The integration of various technologies such as SDN\NFV creates opportunities for deployment
of MEC orchestrator by enabling the edge cloud resources control. Currently, different APIs anddata models are under development in ETSI MEC for assisting discovery process of millimeterwave (mmWave) and provide RAN and network information for visibility of network towards dif-ferent applications.
5.5.1.2 MEC service continuity and mobility and service enhancements
It is considered as one of the most important challenging aspect and need further research for ser-
vice continuity for high speed mobile users. Integration of MEC with MCC encourages the utiliza-
tion of mobile resources for communication and computation services is considered to bechallenging for service orchestration. The multiconnectivity supporting multipath and potentiallystreaming among various MEC platform encourage further research considering the network scal-ability and mobile user performance. Quality of Experience (QoE) and Resiliency add direction tothe researchers for further research to increase the efficiency and QoE of user. To develop servicesand application such as resiliency to provide backup features in case of a state of critical parameter131 5.5 Challenges and issues in implementation of MECand analyze the QoE and other performance related to that state. MEC upgrade the experience of
location services, intelligent proximity services by integrating user context information, big dataand social applications. From the resource optimization viewpoint it should be studied further tocombine network and cloud resources with various services offering QoE. To enhance the effi-ciency of MEC, the concept of HAEC (Highly Adaptive Energy-efficient Computing) should be
considered for further research (
Chen & Hao, 2018; Sabella, Vaillant, Kuure, Rauschenbach, &
Giust, 2016; Sun & Ansari, 2016 ). The limitation in the range of cell in a small cell network
increases the importance of mobility which need more research exploration for fast migration.Service Discovery is an important research issue where discovery mechanism is introduced to findproper nodes that can be leveraged in a decentralized system. The automatic monitoring and accu-rate synchronization for multiple devices should consider as important aspect for further research.
5.5.1.3 MEC security and privacy
It is considered very important in MEC as it is deployed at BS and at the areas where it is exposedto physical attack. Therefore, MEC having significant security risk than traditional cloud comput-ing. MEC need more stringent security policies as the third party stake holders can access the infor-
mation related to user proximity and radio analytics. Hence, authentication needs to be considered
for accessing the MEC platform. Isolation between the hosted applications should be taken as chal-lenging issue as security attack on particular application will not affect the other running applica-tions. Therefore, proper encryption technique needs to be studied for secure collaboration andinteroperability between heterogeneous resources and different operational parties. Different intru-sion detection techniques in cloud computing for large scale geo distributed environment need fur-ther exploration.
5.5.1.4 Standardization of protocols
It requires standardized organization to provide a set of universally accepted rules for edge comput-
ing in 5G technology. However, the challenges arise to accept the standard due to its flexibility and
diversified customization provided by various vendors. At this point, various heterogeneous UEscommunication with edge cloud through different interfaces. Addressing heterogeneity and comput-ing technologies in 5G communication faced difficulties in portability across various environments.Programming based models may give a solution to edge node for execution of workload simulta-neously in multiple hardware levels (
Al-Shuwaili & Simeone, 2017; Dong et al., 2020; Nunna
et al., 2015 ). The parallelism of data and task divide the workload into independent and smaller
task to be executed in parallel across various hardware and edge cloud layers.
5.5.1.5 MEC service monetization
It plays an important role for mobile operators to monetize the combined cloud and network
resources with third parties. Therefore, resource brokering solution needs further investigation as
tariff planning service usage such as video analytics or optimization is becoming a challengingissue on demand. More dynamic pricing models need to be introduced for requirement of advancedaccounting and monitoring. Techniques need to be for solving potential economic conflict betweenvarious parties for smooth MEC system operation. The collaboration between different networkproviders for MEC needs common collaboration protocols to access network and context informa-tion regardless of their deployment location.132 Chapter 5 Convergent network architecture of 5G and MEC5.5.1.6 Edge cloud infrastructure and resource management
The infrastructure of MEC includes the combination of Telco and IT cloud capabilities within the
RAN. For smooth working in highly difficult environment, the edge cloud infrastructure needs tobe designed. Web Interface has not been designed for mobile devices so facing compatibility issuesand this web interface is not suitable for MEC and arise overhead problem. However, standard pro-tocols require for smooth communication among users, MEC and the cloud (Ren et al., 2019).
It is essential for MEC platform as there is limited computing and storage resources to support
constrained number of applications. However, MEC can be considered as a service where opera-
tor’s resources can be accessed by service provider according to the need. There should a trade-offbetween fair resource sharing and load balancing to achieve an increased efficiency. It is veryimportant to migrate transparently the user applications to MEC for execution.
5.5.1.7 Mobile data offloading
The consistency in user experience and service continuity should be independent of user locationand edge service delivery location creates challenges in mobile data offloading. Another issue isflexibility in diverse forwarding function. Computation offloading for offloading at different levelof mobile terminal and edge cloud following changes needed:
 Change in network bandwidth to achieve appropriate latency.
 Requirement of minimal communication overhead. Need minimum effort from software developer for rapid development with reduced time and
cost.
5.5.2 Application perspective
Convergence of 5G and MEC assures eMBB, URLLC and mMTC along with enhanced perfor-
mance and scalability. Efficient resource management in the convergence network will enable con-
sumers and network providers to develop applications that require intensive processing closer to thedevice (data source). This reduces latency for real-time and computation intensive applications. Forexample, the quality of voice and video over mobile internet will have a enhanced experience (
Bai
et al., 2020; Giust et al., 2018; Ksentini & Frangoudis, 2020; Ma, Zhao, Gong, & Wang, 2017;Pham et al., 2020
). MEC with Intelligent IoT will have computations at edge for variety of devices
and applications. IoT-based smart health care will enable improved hospital management and MCapplications. Application specific implementations and development on new applications will lead
to challenges and issues as discussed in this chapter in details.
5.5.2.1 Industrial IoT application in 5G
5G is capable of providing technological requirement to the modern industries through robotics.
The three important factors of 5G such as eMBB, URLLC and MMTC can enhance the perfor-mance of industries 4.0. Industry 4.0 deals with the latest automation and data exchanging processin manufacturing sector. IoT is one of the important research domains of industry 4.0. The smartfactory concept arises from the industry 4.0 but for 3G and 4G it has some limitations such as endto end delay, energy consumption, reliable wireless communications and to support voluminousdevices. However, 5G overcome all these limitations and give high latency and high bandwidth.133 5.5 Challenges and issues in implementation of MECAt this point, 5G also provides the advancement in time-critical and reliable processes, Nontime-
critical communication, and remote control of factories ( Althebyan, Yaseen, Jararweh, & Al-
Ayyoub, 2016; Ning, Wang, & Huang, 2018 ).
5.5.2.2 Large scale healthcare and big data management
There is a need of edge computing enabled large scale healthcare system as most of the researchrelated to edge computing healthcare applications are for small scale environment. Althebyan et al.proposed architecture to be fitted for large scale healthcare application (
Abdellatif, Mohamed,
Chiasserini, Tlili, & Erbad, 2019; Zhang, Weiliang, Fengyi, & Zhouyun, 2018 ). In this paper, the
author consider a large number of user with a decision making model for health worker to observethe trends in disease spread. As in hospitals, large number patients need to be treated and their use-ful medical data to be recorded and to handle such a large number of data and users, edge enabledlarge scale healthcare system is required with new analysis technique.
5.5.2.3 Integration of AI and 5G for MEC enabled healthcare application
Scalability encourages converging 5G communication with edge computing and shifts centralizedcomputation to the edge. The deployment of large scale MEC enabled healthcare system need inte-gration of AI to get accurate and in time services by analyzing the factor such as mobility of user
and device usage pattern, monitoring medical records. Recently, ML and deep learning are enabling
the advancement in medical diagnosis which requires voluminous processing, computation and stor-age. However, the centralized cloud platform is shifting to the edge keeping URLLC as one of itskey factor which requires edge enabled ML trained model for localized data to provide low costand low latency. The ML categories include (1) supervised learning, (2) unsupervised learning and(3) reinforcement learning and can be used for AI and 5G integrated and MEC enabled healthcareinfrastructure deployment. AI gives importance to the context aware health care system to personal-ize the patient data such as age, gender and recent medical condition which can be taken as input
and helps doctor for diagnosis. Considering one AI based model in where the lung cancer has been
diagnosed with higher precision than the simple threshold method. Some other research includesvoice disorder analysis, prediction of emotion of patient which helps the hospital caretaker to givemore attention to the patient (
Bellavista, Chessa, Foschini, Gioia, & Girolami, 2018; Bruschi,
Bolla, Davoli, Zafeiropoulos, & Gouvas, 2019; Mitra & Agrawal, 2015; Zhang, Di, Wang, Lin, &Song, 2020
). However, there are some features such as energy efficiency, computation need further
exploration in research through various edge ML training models to get higher system performance(
Agiwal, Roy, & Saxena, 2016; Li et al., 2019; Zhang, Wu, Xie, & Yang, 2018 ).
5.6 Conclusions
MEC and edge computing approach are instrumental in developing application framework in 5Gtechnology and beyond. Further, convergent network architecture is vital in dealing with heterogene-ity and interoperability for any real-world deployment scenario using multiple heterogeneous accessnetwork technologies with communication stack and computational platforms. In this chapter, wehave discussed various key technical aspects of 5G and MEC for convergent networks. The 5G com-munication standards (3GPP release 15 and beyond) with newer technologies enabled distributed134 Chapter 5 Convergent network architecture of 5G and MECcomputing (Fog/Edge/MEC) approach are the key technology enabler for a holistic convergent net-
work deployment. For New-gen communication and computation framework, there is the need formodern applications like AR/VR, IoT, M2M, and ultra-reliable and low-latency applications.
Additionally, the hybrid computational approach evolved for proper amalgamation of decentralized
edge computing (MEC) and centraliz ed cloud computing would be a smart tailored solution for heteroge-
neity in computational req uirement across differen t applications scenarios. 5G technology and MEC with
integration for HetNet and communication protocols (Convergent Network architecture) would provide a
holistic multimodal framework of sens e, connect and compute infrastruct ure required for modern era IoT
and Cyber-Physical Systems (CPS). It is needless to m ention that these technologies would shape the
future beyond imagination with a lot of business pote ntial. IoT-based a pplications like S mart healthcare
would be bolstered using fusion of technologies lik e 5G and MEC with convergent network architecture.
IoHT would harness the technological benefits of 5G netw ork (low-latency, ultra- reliable, high data rate,
enhanced mobility support) for bio-m edical sensor data acquisition over “ internet” (global seamless packet
switched data network infrastructure) powered b y the hybrid computational model of MEC and cloud
computing platforms for better ser vices custom made for edge applica tion and data intensive cloud based
analytics engine. Further, emerging technologies like AI, Neural networks, and ML would provide the
machine intelligence (per ception) for intelligent solutions. We h ave already discussed 5G and MEC based
convergent network technologies, which would provide the required technical feas ibility for realizing such
intelligent systems. We have also presented a compara tive analysis of different computational platforms
like MEC, Mobile Cloud Computing (M CC), Cloudlet, and FC and in a nut shell, none of the techniques
show technical supremacy over others. Rather, it sh ows that a perfect blend of different computational
approaches harnessing the technical advantages an d mitigating the limitations would be the smarter solu-
tion to provide a hybrid computational framework for f uture IoT- and CPS-based systems especially with
the aid of communication infrastructure like 5G and beyond. Nevertheless, scenario-specific requirementsof data networks always show heterogeneity with ac cess network technology as well as protocol stack
implementation in Physical as well as link layer.
We have also presented a convergent network perspective for 5G and MEC with technical overview
on heterogeneity and interoperability with the help of c lustering and gateways technologies. Further, we
have presented a current research perspective on s ome emerging field of research which are relevant
with 5G, MEC and Convergent networks like MEC in healthcare, integration of MEC and 5G, integra-
tion of MEC with 5G and IoT also MEC, 5G and IoT in Healthcare. We have also discussed some key-emerging areas of research in this context of HetNet/Convergent network architecture for 5G and MECwith keeping in mind the IoT and M2M application framework. We also mention the challenges and
issues in the implementation of 5G with MEC. We ha ve discussed various technical challenges with
communication and computational perspective as w ell as the application perspective. This would be
helpful for researchers working on these technical domains of 5G, MEC, and convergent network archi-
tecture to focus on challenges to select research objectives and direction and contribute to the scientific
community and society at large with their t echnical innovations and contributions.
References
3GPP the mobile broadband standard. 2021. ,https://www.3gpp.org/release-15 .Accessed 25.03.20.
3GPP the mobile broadband standard. 2021. ,https://www.3gpp.org/release-16 .Accessed 25.03.20.135 References3GPP the mobile broadband standard. 2021. ,https://www.3gpp.org/release-17 .Accessed 25.03.20.
Abdellatif, A. A., Mohamed, A., Chiasserini, C. F., Tlili, M., & Erbad, A. (2019). Edge computing for smart
health: Context-aware approaches, opportunities, and challenges. IEEE Network ,33(3), 196 /C0203.
Agiwal, M., Roy, A., & Saxena, N. (2016). Next generation 5G wireless networks: A comprehensive survey.
IEEE Communications Surveys & Tutorials ,18(3), 1617 /C01655.
Al-Shuwaili, A., & Simeone, O. (2017). Energy-efficient resource allocation for mobile edge computing-based
augmented reality applications. IEEE Wireless Communications Letters ,6(3), 398 /C0401.
Althebyan, Q., Yaseen, Q., Jararweh, Y., & Al-Ayyoub, M. (2016). Cloud support for large scale e-healthcare
systems. Annals of telecommunications ,9, 503 /C0515.
Bai, T., Pan, C., Deng, Y., Elkashlan, M., Nallanathan, A., & Hanzo, L. (2020). Latency minimization for
intelligent reflecting surface aided mobile edge computing. IEEE Journal on Selected Areas in
Communications ,38(11), 2666 /C02682.
Bellavista, P., Chessa, S., Foschini, L., Gioia, L., & Girolami, M. (2018). Human-enabled edge computing:
Exploiting the crowd as a dynamic extension of mobile edge computing. IEEE Communications Magazine ,
56(1), 145 /C0155.
Braeken, A., & Liyanage, M. (2020). Highly efficient key agreement for remote patient monitoring in MEC-
enabled 5G networks. The Journal of Supercomputing ,1/C024.
Bruschi, R., Bolla, R., Davoli, F., Zafeiropoulos, A., & Gouvas, P. (2019). Mobile edge vertical computing
over 5G network sliced infrastructures: An insight into integration approaches. IEEE Communications
Magazine ,57(7), 78 /C084.
Chen, M., & Hao, Y. (2018). Task offloading for mobile edge computing in software defined ultra-dense net-
work. IEEE Journal on Selected Areas in Communications ,6(3), 587 /C0597.
Chih-Lin, I., Kuklinski, S., Chen, T. C., & Ladid, L. L. (2020). A perspective of O-RAN integration with
MEC, SON, and network slicing in the 5G era. IEEE Network ,34(6), 3 /C04.
Dong, P., Ning, Z., Obaidat, M. S., Jiang, X., Guo, Y., Hu, X., ...Sadoun, B. (2020). Edge computing based
healthcare systems: Enabling decentralized health monitoring in Internet of medical Things. IEEE
Network ,34(5), 254 /C0261.
Farris, I., Orsino, A., Militano, L., Iera, A., & Araniti, G. (2018). Federated IoT services leveraging 5G tech-
nologies at the edge. Ad Hoc Networks ,68,5 8/C069.
Gavrilovska, L., Rakovic, V., & Denkovski, D. (2018). Aspects of resource scaling in 5G-MEC: Technologies
and opportunities. In: Proceedings of the IEEE Globecom Workshops (GC Wkshps) , December 9, 2018,
pp. 1 /C06.
Giust, F., Verin, G., Antevski, K., Chou, J., Fang, Y., Featherstone, W., ...Purkayastha, D. (2018). MEC
deployments in 4G and evolution towards 5G. ETSI White paper 24, pp. 1 /C024.
Gu¨r, G., Porambage, P., & Liyanage, M. (2020). Convergence of ICN and MEC for 5G: Opportunities and
Challenges. IEEE Communications Standards Magazine ,4(4), 64 /C071.
Hartmann, M., Hashmi, U. S., & Imran, A. (2019). Edge computing in smart health care systems: Review,
challenges, and research directions. Transactions on Emerging Telecommunications Technologies , e3710.
Hewa, T., Braeken, A., Ylianttila, M., & Liyanage, M. (2020). Multi-access edge computing and blockchain-
based secure telehealth system connected with 5G and IoT. In: Proceedings of the 8th IEEE International
Conference on Communications and Networking (IEEE ComNet’) .
Hsieh, H. C., Chen, J. L., & Benslimane, A. (2018). 5G virtualized multi-access edge computing platform for
IoT applications. Journal of Network and Computer Applications ,115,9 4/C0102.
Hu, Y.C., Patel, M., Sabella, D., Sprecher, N., & Young, V. (2015). Mobile edge computing—A key technol-
ogy towards 5G. ETSI white paper 11 (11), pp. 1 /C06.
International Telecommunication Union. (2021). ,https://www.itu.int/rec/R-REC-M.2083-0-201509-I/en .
Accessed 25.03.20.136 Chapter 5 Convergent network architecture of 5G and MECKekki, S., Featherstone, W., Fang, Y., Kuure, P., Li, A., Ranjan, A., ...Wen, K.W. (2018). MEC in 5G net-
works, ETSI white paper, pp. 1 /C028.
Ksentini, A., & Frangoudis, P. A. (2020). Toward slicing-enabled multi-access edge computing in 5g. IEEE
Network ,34(2), 99 /C0105.
Li, X., Huang, X., Li, C., Yu, R., & Shu, L. (2019). EdgeCare: Leveraging edge computing for collaborative
data management in mobile healthcare systems. IEEE Access ,7, 22011 /C022025.
Lin, D., Hu, S., Gao, Y., & Tang, Y. (2019). Optimizing MEC networks for healthcare applications in 5G
communications with the authenticity of users’ priorities. IEEE Access ,7, 88592 /C088600.
Liu, Y., Peng, M., Shou, G., Chen, Y., & Chen, S. (2020). Toward edge intelligence: Multiaccess edge com-
puting for 5G and internet of things. IEEE Internet of Things Journal ,7(8), 6722 /C06747.
Ma, X., Zhao, J., Gong, Y., & Wang, Y. (2017). Key technologies of MEC towards 5G-enabled vehicular net-
works. In: Proceedings of the International Conference on Heterogeneous Networking for Quality,
Reliability, Security and Robustness , pp. 153 /C0159.
Mitra, R. N., & Agrawal, D. P. (2015). 5G mobile technology: A survey. ICT Express ,1(3), 132 /C0137.
Mobile edge computing—A key technology towards 5G. (2021). ETSI white paper. ,https://www.etsi.org/
images/files/ETSIWhitePapers/etsi_wp11_mec_a_key_technology_towards_5g.pdf .Accessed 25.03.20.
Ning, Z., Dong, P., Wang, X., Hu, X., Guo, L., Hu, B., ...Kwok, R. Y. (2020). Mobile edge computing
enabled 5G health monitoring for Internet of medical things: A decentralized game theoretic approach.
IEEE Journal on Selected Areas in Communications ,1/C06.
Ning, Z., Wang, X., & Huang, J. (2018). Mobile edge computing-enabled 5G vehicular networks: Toward the
integration of communication and computing. IEEE Vehicular Technology Magazine ,14(1), 54 /C061.
Nunna, S., Kousaridas, A., Ibrahim, M., Dillinger, M., Thuemmler, C., Feussner, H., & Schneider, A. (2015).
Enabling real-time context-aware collaboration through 5G and mobile edge computing. In: Proceedings of
the 12th International Conference on Information Technology-New Generations , pp. 601 /C0605.
Oueida, S., Kotb, Y., Aloqaily, M., Jararweh, Y., & Baker, T. (2018). An edge computing based smart health-
care framework for resource management. Sensors ,18(12), 4307.
Pham, Q. V., Fang, F., Ha, V. N., Piran, M. J., Le, M., Le, L. B., ...Ding, Z. (2020). A survey of multi-
access edge computing in 5G and beyond: Fundamentals, technology integration, and state-of-the-art.
IEEE Access ,8, 116974 /C0117017.
Porambage, P., Okwuibe, J., Liyanage, M., Ylianttila, M., & Taleb, T. (2018). Survey on multi-access edge com-
puting for internet of things realization. IEEE Communications Surveys & Tutorials ,20(4), 2961 /C02991.
Ray, P. P., Dash, D., & De, D. (2019). Edge computing for Internet of Things: A survey, e-healthcare case
study and future direction. Journal of Network and Computer Applications ,140,1/C022.
Redondi, A.E., Arcia-Moret, A., & Manzoni, P. (2019). Towards a scaled IoT pub/sub architecture for 5G net-
works: The case of multiaccess edge computing. In: Proceedings of the IEEE 5th World Forum on Internet
of Things (WF-IoT) , pp. 436 /C0441.
Ren, J., Zhang, D., He, S., Zhang, Y., & Li, T. (2019). A survey on end-edge-cloud orchestrated network com-
puting paradigms: Transparent computing, mobile edge computing, fog computing, and cloudlet. ACM
Computing Surveys (CSUR) ,52(6), 1 /C036.
Sabella, D., Vaillant, A., Kuure, P., Rauschenbach, U., & Giust, F. (2016). Mobile-edge computing architec-
ture: The role of MEC in the Internet of Things. IEEE Consumer Electronics Magazine ,5(4), 84 /C091.
Sanchez-Iborra, R., Covaci, S., Santa, J., Sanchez-Gomez, J., Gallego-Madrid, J., & Skarmeta, A.F. (2019).
MEC-assisted end-to-end 5G-slicing for IoT, Proceedings of the IEEE Global Communications Conference
(GLOBECOM) , December 9, pp. 1 /C06.
Sarrigiannis, I., Ramantas, K., Kartsakli, E., Mekikis, P. V., Antonopoulos, A., & Verikoukis, C. (2019).
Online VNF lifecycle management in an MEC-enabled 5G IoT architecture. IEEE Internet of Things
Journal ,7(5), 4183 /C04194.137 ReferencesShafi, M., Molisch, A. F., Smith, P. J., Haustein, T., Zhu, P., De Silva, P., ...Wunder, G. (2017). 5G: A tuto-
rial overview of standards, trials, challenges, deployment, and practice. IEEE Journal on Selected Areas in
Communications ,35(6), 1201 /C01221.
Sodhro, H., Luo, Z., Sangaiah, A. K., & Baik, S. W. (2019). Mobile edge computing based QoS optimization
in medical healthcare applications. International Journal of Information Management ,45, 308 /C0318.
Sun, X., & Ansari, N. (2016). EdgeIoT: Mobile edge computing for the Internet of Things. IEEE
Communications Magazine ,54(12), 22 /C029.
Tomaszewski, L., Kukli ´nski, S., & Kołakowski, R. (2020). A new approach to 5G and MEC integration. In:
Proceedings of the International Conference on Artificial Intelligence Applications and Innovations , Jun 5,
pp. 15 /C024.
Tran, T. X., Hajisami, A., Pandey, P., & Pompili, D. (2017). Collaborative mobile edge computing in 5G net-
works: New paradigms, scenarios, and challenges. IEEE Communications Magazine ,55(4), 54 /C061.
Yu, W., & Choi, J. (2020). Human identification in health care systems using mobile edge computing.
Transactions on Emerging Telecommunications Technologies ,31(12), e4031.
Zhang, J., Weiliang, X. I., Fengyi, Y. A., & Zhouyun, W. U. (2018). 5G mobile/multi-access edge computing
integrated architecture and deployment strategy. Telecommunications Science ,34(4), 109.
Zhang, J., Wu, Z., Xie, W., & Yang, F. (2018). MEC architectures in 4G and 5G mobile networks. In:
Proceedings of the 10th International Conference on Wireless Communications and Signal Processing
(WCSP) , October 18, pp. 1 /C05.
Zhang, K., Mao, Y., Leng, S., Zhao, Q., Li, L., Peng, X., ...Zhang, Y. (2016). Energy-efficient offloading for
mobile edge computing in 5G heterogeneous networks. IEEE Access ,4, 5896 /C05907.
Zhang, Y., Di, B., Wang, P., Lin, J., & Song, L. (2020). HetMEC: Heterogeneous multi-layer mobile edge
computing in the 6 G era. IEEE Transactions on Vehicular Technology ,69(4), 4388 /C04400.
Zhang, Y., Chen, G., Du, H., Yuan, X., Kadoch, M., & Cheriet, M. (2020). Real-time remote health monitoring
system driven by 5G MEC-IoT. Electronics ,9(11), 1753.
Zhang, K., Leng, S., He, Y., Maharjan, S., & Zhang, Y. (2018). Cooperative content caching in 5G networks
with mobile edge computing. IEEE Wireless Communications ,25(3), 80 /C087.138 Chapter 5 Convergent network architecture of 5G and MECCHAPTER
6An efficient lightweight speck
technique for edge-IoT-based smarthealthcare systems
Muyideen AbdulRaheem1, Idowu Dauda Oladipo1, Alfonso Gonz ´alez-Briones2,3,4, Joseph Bamidele
Awotunde1, Adekola Rasheed Tomori5and Rasheed Gbenga Jimoh1
1Department of Computer Science, University of Ilorin, Ilorin, Kwara State, Nigeria2Research Group on Agent-
Based, Social and Interdisciplinary Applications, (GRASIA), Complutense University of Madrid, Madrid, Spain
3BISITE Research Group, University of Salamanca, Salamanca, Spain4Air Institute, IoT Digital Innovation Hub,
Salamanca, Spain5Computer Services and Information Technology, University of Ilorin, Ilorin, Kwara State, Nigeria
6.1 Introduction
The high expenses of healthcare, the increase in the number of new diseases, and the imbalance in
population growth compared with the rate of medical professionals create a wide gap in the use ofthe conventional healthcare model for the treatment of patients. This necessitates the use of smartsensor Internet of Things (IoT) for diagnosing, monitoring, and treatment of some diseases. Theconcept of IoT is the global interconnection and integration of every device on the Internet networkfor the possible exchange of information among the spectrum of heterogeneous devices. This con-
cept has been applied in various fields of human life such as Smart Energy Network, Smart Home,
Smart Building, Smart Healthcare, Smart City among others (
Beheshti-Atashgaha, Reza Aref,
Barari, & Bayat, 2020 ).
Smart healthcare is emerging as a solution for he althcare challenges in which patients need not
travel to the health center to visit a doctor for medical health care services, but rather communicatewith a distant physician using IoT devices and advanced features of Information and CommunicationTechnology (ICT). The physicians then remotely an alyze the patients’ data and recommend drugs and
treatments accordingly. Patients need not travel ou tside their community for m edical treatment in expec-
tation of the right application of smart healthcare. S mart healthcare provides b enefits capable of turning
around the challenges of healthca re services, especially for those people who have uneasy access to
healthcare facilities as well as aged peoples. Though s mart healthcare shows prom ising potentials, suc-
cessful implementation is a source of concern. Seve ral challenges require atte ntion to fully utilize the
potential of smart healthcare. Sma rt healthcare being a networked s ystem with sensitive data that must
not be mishandled requires as the fo remost priority, the security, con fidentiality, and privacy of patients’
data as the major concerns. Smart healthcare profes sionals, heavily rely on the patients’ digital health
data for diagnosis, therefore the integrity and relia bility of this information are essential for error-free
medical analysis. Smart healthcare networked syst ems are vulnerable to a di fferent form of security
threats that exist in every stage of a smart healthcare system, such as, data collection, transmission, stor-
age, and access levels.
1395G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00005-X
©2022 Elsevier Inc. All rights reserved.One of the main objectives of IoT technology in the healthcare system is to improve the quality
of service provided for patients. Various healthcare devices like smart wearables are worn to moni-tor the vital organs of the human body, such as the kidney, heart, or lung, and once any dangeroussymptom of a person’s health is present, a report of the danger is generated and communicated tothe relevant health authorities. This in turn provides significant usage of health information dissem-
ination for improved, correct, and timely diagnosis. However, such a report is mostly personal
information and findings of the patient and needs to be secured against several security loopholesand various malicious attacks on confidentiality, integrity, etc. In the smart healthcare realm, thesmart sensor devices used are resources constraint with short battery life, low processing power andsmall storage capabilities hence conventional security protocols are not efficient to secure such anenvironment (
Noshina, Ayesha, Muhammad, & Farrukh, 2020 ).
Edge computing is playing a vital role in mining valued and suitable information out of these
data for healthcare professionals to take an efficient and effective decision. The edge computing
realm, in combination with 5G speeds and contemporary computing procedures, are tending to
meet with an energy-efficient and latency requirement for timely collection and analysis of healthdata (
Hartmann, Hashmi, & Imran, 2019 ), however, security remains an issue. Smart Healthcare or
s-Healthcare requires the broad deployment of healthcare services connected across large domainsand communicate through networked sensors, controllers’ platforms, and servers between patients,physicians, and other healthcare providers. Wireless Sensor Networks (WSNs) structure enablestransportation of health data accurately among participating entities thus providing varied series ofapplications such as patient monitoring, emergency, and immediate response intervention. These
WSN technologies such as Radio Frequency Identification (RFID), Bluetooth, LoRa, etc., are
required to be equipped with necessary encryptions to secure the health data transported among theentities.
Lightweight encryption is a type of cryptography strictly for resources constrained devices and
WSNs. Thus resource-constrained devices are limited in power, energy, and memory as a result oftheir small size. These devices communicate in an insecure channel in a difficult and critical envi-ronment. Conventional cryptography algorithms such as Rivest /C0Shamir /C0Adleman (RSA) and
Advanced Encryption Standard (AES) use a lot of power and energy not available in smart sensor
devices. Therefore a need for new alternatives, referred to as lightweight encryption (
Ayo,
Folorunso, Abayomi-Alli, Adekunle, & Awotunde, 2020 ), is to provide security for resource
restricts devices. Generally, encryption is divided into two categories: symmetric and asymmetric.
In symmetric encryption, the identical key is shared for both encoding and decoding of data.
However, in asymmetric encryption, two different but related keys are used for encryption anddecryption of the codes (
Jana, Bhaumik, & Maiti, 2013; Mohd, Hayajneh, & Vasilakos, 2015 ). In
encryption operation, a single key is used as a public key available to everyone while the privatekey is used for decryption operation known secretly to the user that generates it. Lightweight sym-
metric ciphers are further divided into two categories: block and stream ciphers. In this paper, an
asymmetric lightweight block cipher is proposed for encryption of medical data for smart senordevices in the healthcare system. The remainder of this work is organized as follows. Section tworeviews related work, section three discusses the model while section four explains the result, andsection five presents the conclusion.
Fig. 6.1 displayed the architecture of a smart healthcare
system.140 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedThe architecture of IoT-enabled wearable health care has basic four layers. The foremost layer
of the IoT architecture is the layer referred to as the device layer or the sensing layer. It provides,depending on the application, physical objects and sensor devices such as wireless sensors, RadioFrequency Identification (RFID), barcode, and infrared. These devices identify objects and gatherdesired valuable information in the form of orientation, vibration, location, chemical changes,acceleration, humidity, temperature, and emotional condition with the help of sensors. The informa-
tion collected is transmitted securely to the communication layer. The communication layer also
referred to as the transmission layer processes and transmits the sensor data gathered from the sen-sor devices using the wireless or wired medium of transmission based on the available technologiessuch as Wi-Fi, 4G, Zigbee, infrared, and Bluetooth. The data processing layer also called the “mid-dle-ware layer” analyzes and processes the information gathered from the communication layer.The layer establishes and manages the connection with the database using the background technol-ogy to process a huge bulk of data in the database. Relying on the knowledge obtained from thethree layers, the application layer provides judgment and suggestions on actions to be taken.
6.2 The Internet of Things in smart healthcare system
The modern wave of information technology such as artificial intelligence, big data, the IoT, and
cloud computing is turning the conventional medical system into a smart healthcare system that is
Smart Healthcare 
System
Smart 
infrastructureEmergency 
response
Smart hospitalNurseDoctor
EHR database
Patient with 
sensorsSmart telehealth
FIGURE 6.1
The architecture of a smart healthcare system.141 6.2 The Internet of Things in smart healthcare systemmore convenient, efficient, and more interactive ( Hassanalieragh et al., 2015 ). Smart healthcare is
an infrastructure of health service that uses technologies such as IoT, mobile Internet, and wearabledevices to access information dynamically, link individuals, healthcare-related materials, and orga-nizations, which then intelligently handle and respond effectively to the needs of the medical sys-tem (
Bhatt & Bhatt, 2017 ).
Smart healthcare fosters participation in the healthcare system among the stakeholders, ensures
that participants access the needed treatment required, assists medical personals to make aninformed decision, and promote the responsible distribution of resources. In this way, smart health-care is a higher level of information building in the health sector (
Tian et al., 2019 ). Smart health-
care consists of various users, like medical professionals, health facilities, and medical researchorganizations that integrate and provides different services, including prevention and control of dis-eases, diagnosis and recovery, administration of health facilities, decision-making on wellbeing,and medical science (
Ullah, Shah, & Zhang, 2016 ).
The components of information technology like 5G, microelectronics, IoT, mobile Internet,
cloud computing in addition to modern biotechnology form the foundation of intelligence broadlyapplied in all areas of smart healthcare (
Hassani et al., 2020 ). Information technology provides
patients with technologies such as wearables, to from time to time, tracking their health status,seeking electronic emergency care, and integrating smart healthcare remote facilities from theirrespective home (
Majumder et al., 2017 ).
To assist and optimize diagnosis, health providers use a range of intelligent clinical decision
support tools. Utilizing an interconnected knowledge network such as the Laboratory Information
Management System (LIMS), Picture Archiving and Communication Systems (PACS), Electronic
Medical Record (EMR), and other knowledge-based networks, health officers can therefore handlemedical information in smart healthcare (
Fernandes et al., 2020 ). More effective surgery can be
performed with smart healthcare using surgical robotics and mixed-reality technologies.Radiofrequency identification (RFID) technologies can be used from the viewpoint of smart health-care hospitals to control staffing, materials, and the supply chain, to gather information, and assistin decision-making with automated management platforms (
Jokanovi ´c, 2020 ).
From the perception of scientific research agencies, the use of health platforms will improve the
experience of patients, using methods like machine learning rather than manual drug monitoring,
and using big data for identifying ideal topics ( Roski, Bo-Linn, & Andrews, 2014 ). Smart health-
care assists to effectively minimize the expense and risk of medical operations with the usage ofthese tools, increase the quality of the use of medical equipment, facilitate exchanges and collabo-ration in various areas, foster the growth of telemedicine and personal medical care, and eventuallymake customized medical services everywhere (
Zeadally & Bello, 2019 ).
6.2.1 Support for diagnosis treatment
In line with the application of technology such as artificial intelligence, surgical robotics, and tele-
presence in smart healthcare to develop the clinical decision support architecture, the treatment anddiagnosis of diseases have turned smarter. Important outcomes have been achieved with the suc-cessful diagnosis and treatment of such diseases (
Islam, Kwak, Kabir, Hossain, & Kwak, 2015 ).
Machine-based computing algorithms have been found to function more effectively than trained142 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedphysicians and have correctly outperformed the findings of seasoned doctors as a consequence of
artificial intelligence diagnosis ( Froomkin, Kerr, & Pineau, 2019 ).
Physicians may offer expert guidance based on the application of the medical decision support
system to increase the accuracy of diagnosis, minimize the rate of failed biopsy and medicationerrors, and allow patients to obtain prompt and effective medical care (
Saposnik, Redelmeier, Ruff,
& Tobler, 2016 ). Experts confirmed that the state of the patient and disease status was defined
more reliably using technologies to better create a customized care plan and the method of therapyis being more specific (
Monje, Foffani, Obeso, & S ´anchez-Ferro, 2019 ). Where the surgeon is
allowed greater versatility and consistency with technology and easy deployment, the surgeon hasreported high performance relative to the conventional method (
Peters, Armijo, Krause,
Choudhury, & Oleynikov, 2018 ).
6.2.2 Management of diseases
chronic diseases contribute to the continuum of human illness and eventually become a new phe-
nomenon with a long path of disease that is incurable and expensive to support; thus it is especiallynecessary to control and give adequate attention to it (
Franceschi et al., 2018 ). The conventional
paradigm of practitioner- and hospital-centered systems of managing health, however, continues tobe unable to handle the growing number of patients and diseases adequately. One way out of thissituation is the advent of implantable and portable smart health information systems, smart homes,
and mobile devices related through Technology in the IoT.
More focus has been paid to patient self-control with the modern health management paradigm
of smart healthcare. It stresses self-monitoring of patients in real-time, direct clinical data input,and prompt medical action (
Brew-Sam, 2020 ). Wearable and implantable systems with sophisti-
cated sensors, microprocessors, and wireless technologies intelligently identify and monitor numer-ous physiological metrics in patients on an ongoing basis, minimizing power usage, improvingconvenience, and making data accessible for tracking health information (
Majumder et al., 2017 ).
This method requires a switch to simultaneous understanding and coordinated treatment from con-
tingency tracking. The related complications posed by the disease are further minimized thus mak-
ing it possible for healthcare centers to track the disease’s survival rate ( National Academies of
Sciences, Engineering, & Medicine, 2020 ). For tracking purposes, the advent of mobile phones,
smartwatches, etc., offers a new tool, that attempts to incorporate nanomaterials into smartphonesto enhance portability further. With this, medical personnel can more effectively track patientsglobally and their bodies using a super high smartphone (
Tian et al., 2019 ).
Smart healthcare supports the aged and the disabled with health aid in smart homes. Smart
homes are individual dwellings or buildings with smart objects in the apartment buildings facilities
that track the physical signs and atmosphere of the occupants ( Mshali, Lemlouma, Moloney, &
Magoni, 2018 ). Smart homes further carry out events that enhance the performance of living in
smart healthcare. The function of artificial intelligence incorporated in smart healthcare is seg-mented into two facets: home integration and health surveillance (
Pramanik, Lau, Demirkan, &
Azad, 2017 ). When gathering health data, these tools offer certain basic resources, allowing indivi-
duals who need treatment to lessen their dependency on healthcare professionals and alleviateanguish at home.143 6.2 The Internet of Things in smart healthcare systemBy way of software and a health information portal, patients can self-manage their disease using
a wearable medical sensor provided by the Stress Detection and Alleviation system to constantlytrack levels of patient body pressure and give assistance to relieve the body stress automatically(
Greiwe & Nyenhuis, 2020 ). To generate a hierarch for a patient decision support structure that
would allow full utilization of the data collected for successful illness analysis, health data from
different handheld systems may also be integrated into a hospital information system ( Abdel-
Basset, Manogaran, Gamal, & Chang, 2019 ). It will forecast possible issues concern patients and
providing advice across the simulator cloud and data analytics ahead of time while helping clinicaldecision-making (
Galetsi, Katsaliaki, & Kumar, 2020 ).
One other proposal is to create a conducive medical health system that encourages medical sta-
keholders through lowering entry barriers, to meet and exchange ideas with colleagues. This helpspatients to receive advisory services quickly from healthcare services, while physicians can trackpatient’s condition continuously with professional consultants and analysts’ support (
Vutha, 2020 ).
Mobile technologies like m-Health help mitigate patient complications, eliminate issues with emer-
gency care, enhance the timeliness of surgical procedures, and offer an economic alternative tohealth services (
Albahri et al., 2020 ).
6.2.3 Risk monitoring and prevention of disease
Health authorities’ effort yields the standard disease risk prediction to compile patient data, equate
the recommended data from the authorizing organization, and eventually issue the findings of theprediction public. This technique has a certain time delay and this does not offer adequate informa-tion to the professionals (
Ayo, Awotunde, Ogundokun, Folorunso, & Adekunle, 2020 ). Disease
severity forecasting is complex and specific under smart healthcare. However, it helps patients andclinicians to engage, control the risk of infection proactively, and perform prevention based on its
effects of testing (
Albahri et al., 2020 ).
The new cardiovascular disease predictive algorithm collects data through the use of smart apps
wearable devices, automatically syncs it through a network to the cloud, and evaluates the out-comes using analytics data algorithms to provide users the forecast results in no time throughinstant messages (
Dias, Marques, & Bhoi, 2021 ). It has been shown that these interventions are suc-
cessful. They support physicians and patients at all times to reconsider their habits of treatment andways of life and even give assistance to health managers in implementing policies relating to com-munity health to achieve the goal of lowering the risk of disease (
Poitras et al., 2020 ). For example,
scientists employed algorithms to combined blood glucose parameters, dietary patterns, anthropom-
etry, physical exercise, intestinal microbiota, and other variables to successfully predict changes inglycemic resistance in a diabetes prevention study estimating the glycemic response after monitor-ing 800 volunteer glucose levels resistance for 46,898 meals per week (
Marques, Bhoi,
Albuquerque, & Hareesha, 2021 ).
6.2.4 Virtual support
Robotic assistance is considered an algorithm instead of being an object. Using methods like recog-
nition of voice, mobile apps can connect users, depend solely on big data technology to obtaindata, and respond after measurements based on user needs or desires (
Abiodun et al., 2021 ). Digital144 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedsupports generally function as a gateway to link smart healthcare and medical institutions, patients,
and physicians ( Rahmani et al., 2018 ). Healthcare facilities are easily accessible by digital assis-
tants. Medical terms can be translated to familiar, ordinary language for patients using a digitalassistant through the mobile device to seek the required medical treatment more precisely (
Farahani
et al., 2018 ).
For health personnel, the digital assistant responds instantly to identified details of patient his-
tory, thus ensuring adequate and effective treatment of patients and allowing physicians to plan forsurgical treatments to save time for other assignments (
Bhoi, Sherpa, & Khandelwal, 2018 ). The
deployment of digital support system significantly improve manpower and material costs of medi-cal facilities and adapt more appropriately to the challenges of both parties (
Marques et al., 2020 ).
The Nuance technologies are also used to create a conversation between various virtual assistants,in particular between common assistants and highly skilled assistants, thus significantly enhancingthe knowledge of stakeholders in the healthcare treatment (
Mishra, Mallick, Tripathy, Bhoi, &
Gonz ´alez-Briones, 2020 ). Digital assistants may be employed also to help with the management of
illnesses, such as using digital assistants to enhance individual mental health, and therefore improvethe limited availability of human psychotherapists to provide more patients with emotional wellbe-ing (
Ebert et al., 2018 ).
6.2.5 Smart healthcare hospitals support
Smart healthcare components comprise of three main items: clinic, geographical, and community.
To optimize practices of current patient care and implementation of new features, smart healthcare
focuses on environments driven by information and communication technology, concentrating pri-marily on effective and efficient IoT technology and process automation (
Papa, Mital, Pisano, &
Del Giudice, 2020 ). In smart hospitals, there are three fundamental categories of facilities: medical
personnel, patient, and admin services. In-hospital management decisions support systems, it is par-amount that these healthcare categories be adequately addressed (
Jamil, Ahmad, Iqbal, & Kim,
2020 ).
Mobile computers, smart cities, and personnel are interconnected in-hospital administration by
the digital network that integrates many systems of digital IoT devices. This system makes possible
the identification and documentation of patients in hospitals, regularly oversee medical staff, andtrack equipment and biotic samples (
Oniani, Marques, Barnovi, Pires, & Bhoi, 2021 ). In the phar-
maceutical sector, intelligent healthcare is also being utilized for manufacturing and distribution ofthe drug, control of warehouse, anticounterfeiting, and additional required procedures. To realize areliable, secure, safe, and efficient distribution of hospital materials utilizing RFID technology,a specific RFID tag allocation for each individual can be issued and the data can be transmitted toa device that easily tracked and retrieved the data via the portable apps (
Mishra, Tripathy, Mallick,
Bhoi, & Barsocchi, 2020 ).
The creation of an operational control framework assists in carrying out tasks such as resource
selection, quality monitoring, and success analysis in terms of decision-making, and can minimizepatient costs, optimize resource usage, and help hospitals make recommendations to management(
Pramanik, Pradhan, Nandy, Bhoi, & Barsocchi, 2021 ). Patients can control various features in
terms of healthcare services, such as physical exam services, digital consultations, and doctor-patient experiences. Such computerized systems allow the medical care procedures of patients145 6.2 The Internet of Things in smart healthcare systemacceptable and more straightforward. In a timely way, patients wait and receive increasingly lifelike
care. In summary, the outcome of intelligent healthcare is transformation, development, and mod-ernization (
Marks, 2020 ).
6.3 Application of edge computing in smart healthcare system
The Artificial-IoT (AIoT) systems, along with e dge computing, forward to the cloud servers data for
deciding on smart healthcare. Quality of Life (QoL) is thus enhanced, protected, and effective with the
provision of healthcare services and recorded growth in popularity after combining Intelligence edge com-puting and IoT-enabled heterogeneous networks for reliable and timely delivery of medical data
(
Atitallah, Driss, Boulila, & Gh ´ezala, 2020 ). The progress of the implemen tation of diverse networks IoT-
enabled mobile technology, AI, edge computing, and I oT has aroused the atten tion of scientists and
industrialists in such a way that edge computing was identified as one of the most common technologies
currently used in recent times and research effort s are more intensive in this field of research ( Faheem
et al., 2018 ). For the science community, edge computing in healthcare networks is very important to
addressing big challenges at a global level ( Greco, Percannella, Ritrovato, Tortorella, & Vento, 2020 ).
AIoT increases healthcare’s operating productivity by reducing the medical professional
nontechnical activities, thus healthcare personnel provided with more time to attend to patients in apatient-centric environment as needed (
Luk, 2018 ). Edge computing technology is smart enough to
understand the sharing of real-time services, supplying medical practitioners with continuous computing
services, and improving the quality of experience (QoE) of smart healthcare professionals ( Chen, Li,
Hao, Qian, & Humar, 2018 ). In smart healthcare, edge computing offers cognitive health monitoring
with simple patient data and knowledge on the extent of disease risk in real-time for urgent attention. Italso generates complex network resource knowledge for resource allocation in a cost-constraint settingand applies the best network resources to the user with the highest priority which then makes the fulledge computing resources accessible to patients with the highest risk of disease and thereby increasesthe patient’s health probability (
Omoniwa, Hussain, Javed, Bouk, & Malik, 2018 ).
Advanced data processing functions required in the data sources and the cloud are implemented
by edge computing. It combines clinical and nonmedical data from various sources, processes thedata collected on the network, classifies and notifies emergencies, gathers the required information,and transmits the processed data or the data extracted to the cloud (
Abdellatif, Mohamed,
Chiasserini, Tlili, & Erbad, 2019 ). Remarkably, with edge computing management of long-term
chronic diseases, different healthcare-related apps can be implemented which actively assistpatients to participate in administering their treatment and to communicate with their physicianseverywhere and at any time (
Aceto, Persico, & Pescap ´e, 2018 ).
Also, with edge computing, a specialized context- aware processing application can be run with mul-
tiple connected data sources and use d to easily manage the patient, wh ile optimizing context-based data
delivery, such as data type, supported applic ation, and overall conditions of the patient ( Porambage,
Okwuibe, Liyanage, Ylianttila, & Taleb, 2018 ). The well-known category of remote monitoring apps is
heart tracking apps that detect v ital signs associated with the hea rt to reveal multiple types of heart
attacks, such as cardiac arrhythmia, chronic hea rt disease, ischemia, and myocardial infarction ( Ayo,
Ogundokun, Awotunde, Adebiyi, & Adeniyi, 2020 ).Bansal et al. (2015) introduce a real-time heart146 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedwearable sensor device capable of t ransmitting extracted patients’ medical data through Bluetooth to an
Android-based listening port with onward transm ission to a web server the data for analysis. Table 6.1
shows the basic features of IoMT-enabled edge computing in smart healthcare system.
Edge and cloud computing work together to provide a scalable solution focused on each organiza-
tion’s data collection and analysis re quirements. For certain workloads, the edge is suitable for real-time
selection and analysis. Simultane ously, the cloud can serve as a centra lized place for large-scale analyt-
ics. They have real-time and long-term perspectives into efficiency and power interventions such as ML
and asset performance management when they work to gether. Rising computing power at the edge lays
the groundwork for autonomous systems, allowing smart healthcare system to boost operational effi-
ciency while freeing up physicians to c oncentrate on higher-value activities.
Edge computing is critical because it gives smart healthcare system new and better ways to
increase operating efficiency, enhance performance and safety, automate all core medical processes,and ensure “always on” availability. It is a leading method for achieving digital transformation ofthe healthcare operations. To generate actionable healthcare intelligence, edge computing optimizesdata collection and analysis at the edge. Edge computing makes technology, systems, and smarthealthcare model processes more versatile, scalable, stable, and automated.
Hence, to enable remote cardiac control utilizes a s martphone Android to capture a patient’s details
captured by wearable sensors and forward it to a web server. The mobile, in this case, is only used, as
a contact pivot in the networks, to transmit captured data to the cloud (
Shah, Bhat, & Khan ). As a result
of the heavy energy workload it requires, constant da ta transfer seems infeasible. Therefore proposed
Multiaccess Edge Computing (MEC) ME C architecture in such systems closely monitoring the operat-
ing condition of the equipment and its data transmis sion at the Mobile/Infrastructure Edge Node (MEN)
to substantially impr oved energy saving ( Mehrabi et al., 2019 ). The combination of derivingTable 6.1 Basic features of IoMT-enabled edge computing in smart healthcare system.
Requirements IoMT-based edge computing
Mark user Mobile users
Location of servers Edge nodesService type Localized information servicesGeographical distribution DistributedDistance between client and server Single hop
Delay jitter Low
Latency LowData processing FastReliability LowType of connectivity WirelessLocation awareness YesServer nodes LargeComputing cost Low
N/W bandwidth Low
Reply time Milliseconds, subsecondsSecurity Very secure147 6.3 Application of edge computing in smart healthcare systembackground knowledge and localiza tion strategies can be effectivel y and efficiently used by a comput-
ing edge to connect the geographical location of t he patient with the closest suitable caregivers for
emergency attention ( Oladele, Ogundokun, Awotunde, Adebiyi, & Adeniyi, 2020 ).
One of the rapidly increasing ways to collect physiological signals un-interrupting the behavior
of a patient is to calculate the heart rhythm using digital camera sensors from facial videos.
However, it is not advisable to move big data generated using traditional cloud-based architecture
from these camera sensors and some of these implementations could be deemed impractical consid-ering the restricted availability of bandwidth (
Abdellatif et al., 2019 ). The quantity of digital infor-
mation obtained from a small standard camera will exceed 40 GB in a single day. Reading,compressing, and removing the most relevant information from the data obtained by edge comput-ing dramatically decreases the volume of data to be transmitted to the server, hence the use ofbandwidth, and even allows local processing of the data (
Kazeem Moses, Joseph Bamidele,
Roseline Oluwaseun, Misra, & Abidemi Emmanuel, 2021 ).
One of the successes of smart hea lthcare is predictive surveillance o f patients at high risk. To incor-
porate prevention interve ntions to minimize morbidity and morta lity associated with patients at high
risk, A l ie ta l .( 2 0 2 0 ) proposed to enhance emergency prediction/detection technique of optimal
machine learning model. The fast transfer of data to the server is a must in real-time prediction/detec-
tion systems. In certain cases, this means that res ults be processed and even a diagnosis is made as
closely as possible to the patient. IoT devices a re developed with AI embedded with improved edge
computing to provide IoT devices with information ( Calo, Touna, Verma, & Cullen, 2017 ). The use of
AI in IoT-enabled networks at the network edge is mad e better since IoT device-generated data empow-
ers AI embedded in the IoT devices and edge servers to improve knowledge.
In IoT-enabled healthcare networks, before data tra nsfer, IoT devices need to be validated, next, for
fast processing, the sensor data needs to be downlo aded to high configuration edge computing. With
the system, which can build complete network pro grammability, the offload to edge should be per-
formed intelligently. In terms of re source utilization and load balancing, information needs to satisfy
the criteria of edge computing, while security is supported by a lightweig ht encryption scheme ( Li
et al., 2020 ). Data processing, availability, edge virtuosity, a nd fast and highly efficient data transfer are
the essential characteris tics of a healthcare system ( Dhayne, Haque, Kilany, & Taher, 2019 ).
Edge computing which is close to data sources is designed with the ability to transport, easy
process, and store data generated by numerous wireless sensors, cameras, and controllers at the net-work edge (
Khan, Parkinson, & Qin, 2017 ). In addition to offering low latency, reduced energy
usage for battery-operated systems, and network bandwidth saving, edge computing assists smarthealthcare devices with automated diagnosis tracking, proper drug administration to patients, andintensive real-time surveillance of vital signs and early warning of clinical degradation of patents(
Pisani et al., 2019 ).
6.4 Application of encryptions algorithm in smart healthcare system
In smart healthcare-based applications, the main concern is ensuring improved and secured data
security and confidentiality in the transmission. Prevention of illegal access to information andcyber-attacks on data security remains the major challenge in the IoT healthcare systems. Although148 Chapter 6 An efficient lightweight speck technique for edge-IoT-basednumerous methods have been proposed to secure the health data in the smart healthcare environ-
ment, ensuring the safety of information, and cyber-attacks protection against medical data is a vitalproblem concern that requires adequate attention.
Adeniyi, Ogundokun, and Awotunde (2021) pro-
posed a Random Coefficient Selection and Mean Modification Approach (RC-SMMA) for a strongmultilevel security solution that focused on hiding information and chaotic theory to protect data
privacy and secrecy in an insecure multimedia exchange environment among dual IoT devices.
The approach indicates a promising solution for entire signal processing and the region-oriented
attacks, acting as standalone or in a network. As a result of timely communication and the abilityof integrated IoT sensors to decide into day to day activities, smart sensors are becoming more sig-nificant. Therefore securing the sensors is one of the major concerns as a result of the universalstate of the sensors and the high sensitivity of health data transmitted. Also, the power-constrainedstatus of the sensors stresses the need for lightweight security that needs to be tailored to the con-straint resource requirements need of the sensors. Sankaran,
Abikoye, Ojo, Awotunde, and
Ogundokun (2020) propose a lightweight security framework with the development of a level-wise
security architecture for IoT devices and further build protocols for secured communication of IoTsusing identity-based cryptography. The proposed mechanism was assessed using simulation toolsand proved to reduce overhead.
To provide end-to-end secured devices communication,
Abdulraheem, Awotunde, Jimoh, and
Oladipo (2021) proposed lightweight asymmetric encryption to ensure the protection of IoT gate-
way services via Asymmetric Key Encryption (AKE) to session share key among the devices andused lattice encryption with low energy consumption sensors to protect broker gateways devices
and cloud services. Distributed Denial of Service Attacks Protection (DDSAP), attack by eaves-
droppers, and Quantum algorithm attacks were achieved. Varshney & Gupta (
Varshney & Gupta,
2017 ) proposed a blockchain backbone technique to protect and manage data securely among smart
devices on the cloud. The technique achieved a scalable and robust result that addressed the iden-tity and security concerns of IoT devices.
Al-Muhtadi et al. (2020) gave an account of the main cyber-attacks in smart environments and
proposed a secured lightweight encryption framework to maintain and authenticate the senders andreceivers in the cloud environment. The framework was implemented for authentication, role
assignment, and access control at the user layer and evaluated for efficiency, sustainability, and
secure communication.
Beheshti-Atashgaha et al. (2020) proposed an efficient lightweight security
authentication framework to protect patients’ privacy, identity, and information about their health-care records and satisfies essential security requirements.
Using the Two-Fish (TF) method of symmetric key encryption,
Deebak & Al-Turjman (2020)
proposed routing and monitoring protocol to expose and guard against eavesdroppers in the wire-less sensor network. The proposed method demonstrates a high percentage of success compared toother existing techniques. Lightweight cryptography (LWC) is a new cipher whose main purpose is
to satisfy the criteria provided for the use of restricted objects. The term "lightweight" is for a fam-
ily of smaller code size cryptographic ciphers, low processing capacity, and low energy consump-tion. As a result of these resource constraints’ difficulty, security solutions focused on LWC builtaccording to IoT specifications are increasingly required (
Tausif, Ferzund, Jabbar, & Shahzadi,
2017 ).
Efforts have been intensified to propose numerous lightweight security algorithms and protocols
for smart healthcare support. In general, cryptography is used to safeguard networks with149 6.4 Application of encryptions algorithm in smart healthcare systemauthentication, secrecy, the integrity of knowledge, and control of access. However, standard cryp-
tographic protocols may not be appropriate in IoT system environments because of the resourcerestriction design of IoT devices (
Meneghello, Calore, Zucchetto, Polese, & Zanella, 2019 ). IoT
incorporates a huge variety of devices that communicate with one another and collect a large quan-tity of data with the availability of powerful technologies, creating the need for IoT imperative pro-
tection (
Tariq et al., 2019 ).
By gathering different data from a patient’s body, continuous transmitting is infeasible on power
constraint AIoT systems. To address these inadequacies, it is preferred to use edge computing anda lightweight authentication device to process data at the edge of the network, near the wearables(
Ren, Zhang, He, Zhang, & Li, 2019 ). The effectiveness of cryptographic algorithms is concerned
with LWC, and it is a relatively recent subfield connecting cryptology, computer science, and com-puter engineering (
Al-Garadi et al., 2020 ). The constrained resource design of IoT devices and indi-
vidual security are the basic problems for most IoT-based applications. Hence, AIoT applications
are becoming common by using the limited resources of IoT devices safely and effectively, espe-
cially in healthcare ( Hassan, 2019 ). IoT sensors continuously track patients in healthcare systems
and constantly relay the necessary data that needs to be shielded from harmful activities ( Majumder
et al., 2017 ).
Services such as medical identification, healthcare personnel, medication inventory, manage-
ment of drugs, electronic health control, timely monitoring, and interpretation are provided byLWC. Real-time smart healthcare tracking and analysis involve constant data transfer and protec-tion that are crucial for stakeholders, so lightweight authentication mechanisms are the perfect way
to protect a low-energy networked enable IoT (
Pramanik, Upadhyaya, Pal, & Pal, 2019 ).
Healthcare is among how IoT technology is used to deliver benefits to people with health condi-tions in which the health of the patient is tracked at all times and everywhere without the patient’sneed to see the doctor at all times. In the same way, for data protection adversaries, smart healthcare is now one of the targeted sectors (
Kadhim, Alsahlany, Wadi, & Kadhum, 2020 ).
The result of targeting medical sensors is extremely risky and life-threatening, leading to dan-
gerous issues with life and death. The problem of medical fraud may lead to incorrect care thatcauses loss of life and even fraudulent rises in financial costs (
Kadhim et al., 2020 ). Any of the
main security problems with Medical IoT include allowing an unauthorized party to access the con-
fidential records, medication details, and other details of the patient, such as prescription detailsthat make it vulnerable for patients and even mishandle the patient data (
Hossain, Islam, Ali,
Kwak, & Hasan, 2018 ).
6.4.1 Speck encryption
Speck belongs to a family of lightweight symmetric block ciphers having a pair of varying block
and key sizes. A block cipher is an encryption algorithm that enables users to have a common keysecretly and securely encrypt/decrypt blocks of data. Lightweight encryption is built for efficientimplementation of extremely constrained platforms, such as RFID tags, smart sensors, microcon-trollers, and other resource-limited devices.
The term “lightweight” does not mean how secured the algorithm is, but rather refers to its suit-
ability for use on highly constrained devices. Thus a secure lightweight block cipher having a given150 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedblock and key size pairs offers the equivalent level of security as any other secure block cipher
with that same block and key size.
Lightweight encryption addresses the request to secure resource constraint devices which the
conventional encryptions could not adequately attend to, using algorithms and protocols designedto perform well on platforms for devices with resources constrained.
Speck has support for a block of various sizes such as 32, 48, 64, 96, and 128 bits, and up to
three different key sizes to go along with each size of a block. Speck family has ten different algo-rithms with various block and key sizes, as shown in the table. All values are in bits (
Table 6.2 ).
Speck has operations that are highly efficient for software platforms of varying requirements.
There are a quite few numbers of the operation such as NOT, OR, AND, XOR, rotations, modularaddition, and subtraction use to achieve nonlinearity as against S-Boxes used in conventionalencryptions. Since Speck is not using S-Boxes, then it is not Substitution-Permutation Networks(SPNs). Rather than being SPNs, Speck is an Add /C0Rotate /C0XOR (ARX) cipher with Feistel
Structure round functions, which provides an adequate equilibrium between operations of nonlinear
confusion and linear diffusion. Using the bit permutation rotation operation may not achieve suffi-cient diffusion, but with additional functional rounds, an adequate level of security is achieved likein the S-Boxes permutation of SPN. Speck nonlinearity is achieved with modular addition. Thus itsfunctions are secured cryptographically and effectively suitable for IoT constraint devices softwareimplementations.
The proposed modelThe notations used in the definition of the proposed model are given in the table below
n A word size
2n A block size
m Number of words
R Number of rounds
PT 2n-bit input plaintextCT 2n-bit output ciphertextki n-bit round subkey for round i
Table 6.2 Speck parameters.
Block size Key sizes nm
32 64 16 4
48 72 24 3
48 96 24 464 96 32 364 128 32 496 96 48 296 144 48 3128 128 64 2128 192 64 3
128 256 64 4151 6.4 Application of encryptions algorithm in smart healthcare systemK mn-bit Master key from which round subkeys are generated
" Bitwise exclusive OR operation
Sj Left cyclic shift by nbits
S-j Right cyclic shift by n bits
1 Mod 2n addition operation
Speck block cipher denoted as Speck2n has an n-bit word, where n represents either 16, 24, 32,
48, or 64. Speck2n having an m-word (mnbit) key is denoted as Speck2n/mn. For instance,Speck64/96 denotes the Speck version having plaintext blocks of 64-bit and with a key having 96-bit (n532 and m53) (
Fig. 6.2 ).
The round function of Speck2n encryption is denoted by the mapping
Rkx;yðÞ5ðS2αx1y ðÞ "k;Sβy"S2αx1y ðÞ "kÞ;
where xandyaren-bit halves of 2n-bit plaintext, and k is round key.
x5RCS(x, α) Right shift x by αand assign the result to x
x5x1y modulo 2n addition of x and y and assign the result to x
x5x"k XOR xand round key kand assign the result to x
y5LCS(y, β) Left shifty by βand assign the result to y
y5y"x XOR y and x and assign the result to y
For decryption, the inverse of the round function is used with modular subtraction instead of
modular addition and is given by
R21kx;yðÞ5ðSαððx"kÞ2S2βðx"yÞÞ;S2βðx"yÞÞ:
The parameters αandβare 7 and 2 respectively, for Speck32/64 and they are 8 and 3 for other
variance of Speck.
Key scheduleSpeck uses key schedules of 2-, 3-, and 4-word depending on Speck variance. Key schedules
for Specks use around function, as given below.
Suppose mis the number of words for a key, and the key K can be written as (lm 22, l0, k0).
Then, to generate sequences of two words ki and li by
li1m215ki1S2αli ðÞ "i
and
ki115Sβki"li1m21:
The value ki is the ith round key, for i $0.
The table below shows the Speck rounds and the required parameters ( Table 6.3 ).
The Speck key schedules receive an input key K and produce from it a sequence of T key words
k0, kT21, where Tis the number of rounds. Encryption is the composition RkT213/C1/C1/C13Rk13
Rk0, read from right to left ( Fig. 6.3 ).152 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedFIGURE 6.2
Speck round function.
Table 6.3 Speck rounds.
Block size 2n nM key size mn Speck rounds αβ
32 16 4 64 22 7 2
48 24 3 72 22 8 348 24 4 96 23 8 364 32 3 96 26 8 3
64 32 4 128 27 8 3
96 48 2 96 28 8 396 48 3 144 29 8 3128 64 2 128 32 8 3128 64 3 192 33 8 3128 64 4 256 34 8 3153 6.4 Application of encryptions algorithm in smart healthcare systemAlgorithm 6.1 below presents the pseudo-code of the Speck encryption procedure. The key
scheduling function forms the encryption key K 5lm22, l0, k0 and generates the T /C0rounds
keys sequence k0, kT 21 using the procedures presented by Algorithm 6.2 .
6.5 Results and discussion
The proposed encryption method was developed on the microcontroller (MCU) AVR 8-bit RISC
(Reduced Instruction Set Computing) architecture that has programmable on-chip flash memory,
FIGURE 6.3
Speck encryption flow.154 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedSRAM, IO storage space, and EEPROM, as shown in Table 6.1 . Different block size data was pro-
cessed for encryption in KB and our experimental findings are seen in Table II. The execution timetests the amount of encryption and decryption block cycles used. This is the contrast between thebeginning and the end of the process (
Table 6.4 ).ALGORITHM 6.1 Speck Encryption
input: plaintext
split plaintext into n-bit x, y
input: K encryption key
initialize T, β,α
generate round keys k0, kT 21
for i50 to T-1
x5(S2αx1y)"ki
y5Sβy"x
end for loop
ALGORITHM 6.2 Round Key generation
initialize m;
generate lm 22, l0, k0
for I50 to T-2
li1m215(ki1S2αli)"i
ki115Sβki"li1m21
end for loop
Table 6.4 Features of the device.
Name Value
MCU ATmega328PB
Program memory type FlashProgram memory size (KB) 32CPU speed (MIPS/DMIPS) 20SRAM (B) 2048
Data EEPROM/HEF (bytes) 1024
Digital communication peripherals 2-UART, 2-SPI, 2-I2CCapture/compare/PWM peripherals 3 Input Capture, 3 CCP, 10PWMTimers 2 38-bit
Number of comparators 1Operating voltage range (V) 1.8 to 5.5Pin count 32Low power Yes155 6.5 Results and discussionOther block cipher techniques such as AES, DES, SIMON were compared to evaluate the analy-
sis of memory use and speed. Table III presents the findings of the comparison. The findings asshown in this table indicate that the memory use of AES, DES, and SIMON is greater than the pro-
posed memory usage of the proposed system. In comparison, the time to encrypt and decrypt is less
in the proposed system than in most of the lightweight ciphers (
Table 6.5 ).
The table uses various algorithms to display the different encryption times of the same input
data. This illustrates that the suggested encryption solution takes the same amount of time as otherregular block ciphers (
Table 6.6 ).
The proposed system was still tested using the EEG datasets taken from Bonn database a widely
used dataset ( Andrzejak et al., 2001 ). The dataset contains five various datasets labels as A, B, C,
D and E. The dataset A, D, and E was analyzed to measure the latency in both cloud and edge
computing using computational time and transmission delay. Dataset A was taken from aTable 6.5 Performance analysis of the proposed model.
File size (Kb) Encryption time (MM) Encryption time (MM)
0.82 0.121 0.120
1.65 0.216 0.21512.32 0.893 0.89136.50 2.014 2.01250.2 3.142 3.138
100.7 5.461 5.459
Table 6.6 Comparison results of our proposed model with another model.
AlgorithmSpeed (Clockcycle
/bytes)Memory usage
(bytes)Encryption time
(ms)Decryption Time
(ms)
AES 24695 1709 650 124
DES 28401 1608 797 281
SIMON 39313 755 700 300
Speck 30957 1364 719 236
Table 6.7 Coefficients of extracted features for dataset A.
Coefficient Variance Standard deviation Energy
D1 25.2164 5.0216 2.8564e 104
D2 587.553 24.2395 3.0435e 105
D3 5.3957e 103 73.4555 1.4426e 106
D4 9.9058e 103 99.5279 1.9874e 106
A4 1.5439e 104 124.2539 4.0502e 106156 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedstable individual, while dataset D was taken during an interracial situation. During seizure activity,
dataset E was collected from the epileptegenic region (ictal state). Each dataset contains 100 EEGepochs, each of which contains 4097 samples. The EEG data was collected at a sampling rate of173.61 Hz.
Tables 6.7 and 6.8 demonstrate the statistical parameters that were derived from the
subbands ( Sayeed, Mohanty, Kougianos, Yanambaka, & Zaveri, 2018 ). The values of all statistical
parameters are clearly higher for dataset E. The derived attribute estimates for datasets A and D areextremely similar. Dataset A, D, and E versus Dataset E were used to assess identification in this
study.
The SimulinkR was used to calculate the proposed system’s latency. The cloud-based IoT had a
latency of 2.503 seconds, while the latency of 1.432 seconds was generated by the edge-based IoTsystem. Both computational cost and processing delay are included in the latency. In real-worldvarious devices are connected to the cloud server in their millions, the cloud-based IoT applicationsand excess data in the transmission line causes higher network congestion and system latency.Therefore from the edge-based IoT framework there is 46% significant decrease in latency, thushelp in the reduction of latency during computational time and reduced transmission delay during
processing. These features are very critical and crucial in smart healthcare applications (
Table 6.9 ).
6.6 Conclusions and future research directions
The IoT-based system is an upcoming information and communication science technology. A large
number of IoT technologies have been developed in the healthcare field to remotely detect, track,predict, and manage chronic diseases. The data generated by these devices is huge and the proces-sing and transmissions need typical infrastructure to cope with them. Edge computing can be apply-ing to overcome the challenges of IoT-cloud-based systems. The edge computing technology withlower latency, high computational paradigm, and scalability will help IoT-based system devices inTable 6.8 Coefficients of extracted features for dataset E.
Coefficient Variance Standard deviation Energy
D1 1.4426e 103 37.9819 1.8934e 106
D2 6.4382e 104 253.736 4.8707e 107
D3 7.0151e 105 837.560 3.0676e 108
D4 6.9684e 105 834.769 1.887e 108
A4 1.7177e 106 1.310e 103 4.0854e 108
Table 6.9 Comparison of cloud-based and edge-based computational time.
Smart healthcare Latency (s)
Cloud-IoT framework 2.503
Edge-IoT framework 1.432157 6.6 Conclusions and future research directionsreal-time data collection and processing. The deployment of cloud data is far away from the net-
work; this causes the response time delay in real-time. Moreover, the cloud may cause significantoverhead on the backbone network to the user application due to the huge amount of big data sentto the cloud. Hence, the application of edge computing will bring the storage resources and compu-tational closer to the end-user devices, thus reduce the burden on the cloud. The edge computing
architecture is heterogeneous devices, and geographically distributed ubiquitously connected at the
end of a network to provide collaboratively variable and flexible communication, storage services,and computation. The information transmitted by the smart sensor in smart healthcare systems(SHS) is personal, requiring secrecy, trustworthiness, and usability for healthcare practitioners totake timely and accurate decisions, so the data required to be transmitted and stored safely. For theIoT-based Smart Healthcare system, a lightweight ciphering technique was implemented. The pro-posed approach is based on basic operations of ARX Addition, Rotation XORing, swapping, slic-ing, etc. The outcome of the implementation reveals that the memory occupation is limited while
the speed for producing a successful cipher is important. In different constrained devices, the pro-
posed technique can be applied and tested. It is also possible to perform differential and linearcrypto-analysis of this algorithm in the future to ensure the cipher’s robustness.
References
Abdel-Basset, M., Manogaran, G., Gamal, A., & Chang, V. (2019). A novel intelligent medical decision sup-
port model based on soft computing and IoT. IEEE Internet of Things Journal ,7(5), 4160 /C04170.
Abdellatif, A. A., Mohamed, A., Chiasserini, C. F., Tlili, M., & Erbad, A. (2019). Edge computing for smart
health: Context-aware approaches, opportunities, and challenges. IEEE Network ,33(3), 196 /C0203.
Abdulraheem, M., Awotunde, J. B., Jimoh, R. G., & Oladipo, I. D. (2021). An efficient lightweight cryptographic
algorithm for IoT security. Communications in Computer and Information Science ,2021 (1350), 444 /C0456.
Abikoye, O. C., Ojo, U. A., Awotunde, J. B., & Ogundokun, R. O. (2020). A safe and secured iris template
using steganography and cryptography. Multimedia Tools and Applications ,79(31/C032), 23483 /C023506.
Abiodun, M. K., Awotunde, J. B., Ogundokun, R. O., Misra, S., Adeniyi, E. A., Arowolo, M. O., & Jaglan, V.
(2021, February). Cloud and big data: A mutual benefit for organization development. Journal of Physics:
Conference Series, 1767 (1), 012020.
Aceto, G., Persico, V., & Pescap ´e, A. (2018). The role of information and communication technologies in
healthcare: Taxonomies, perspectives, and challenges. Journal of Network and Computer Applications ,
107, 125 /C0154.
Adeniyi, E. A., Ogundokun, R. O., & Awotunde, J. B. (2021). IoMT-based wearable body sensors network
healthcare monitoring system .IoT in healthcare and ambient assisted living (pp. 103 /C0121). Singapore:
Springer.
Albahri, A. S., Alwan, J. K., Taha, Z. K., Ismail, S. F., Hamid, R. A., Zaidan, A. A., ...Alsalem, M. A.
(2020). IoT-based telemedicine for disease prevention and health promotion: State-of-the-Art. Journal of
Network and Computer Applications ,173, 102873.
Al-Garadi, M. A., Mohamed, A., Al-Ali, A. K., Du, X., Ali, I., & Guizani, M. (2020). A survey of machine
and deep learning methods for internet of things (IoT) security. IEEE Communications Surveys &
Tutorials ,22(3), 1646 /C01685.
Ali, F., El-Sappagh, S., Islam, S. R., Kwak, D., Ali, A., Imran, M., & Kwak, K. S. (2020). A smart healthcare
monitoring system for heart disease prediction based on ensemble deep learning and feature fusion.Information Fusion ,63, 208 /C0222.158 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedAl-Muhtadi, J., Saleem, K., Al-Rabiaah, S., Imran, M., Gawanmeh, A., & Rodrigues, J. J. P. C. (2020). A
lightweight cybersecurity framework with context-awareness for pervasive computing environments.
Sustainable Cities and Society , 102610. Available from https://doi.org/10.1016/j.scs.2020.102610 .
Andrzejak, R. G., Lehnertz, K., Mormann, F., Rieke, C., David, P., & Elger, C. E. (2001). Indications of non-
linear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence
on recording region and brain state. Physical Review E ,64(6), 061907.
Atitallah, S. B., Driss, M., Boulila, W., & Gh ´ezala, H. B. (2020). Leveraging Deep Learning and IoT big data analyt-
ics to support the smart cities development: Review and future directions. Computer Science Review ,38, 100303.
Ayo, F. E., Awotunde, J. B., Ogundokun, R. O., Folorunso, S. O., & Adekunle, A. O. (2020). A decision sup-
port system for multi-target disease diagnosis: A bioinformatics approach. Heliyon ,6(3), e03657.
Ayo, F. E., Folorunso, S. O., Abayomi-Alli, A. A., Adekunle, A. O., & Awotunde, J. B. (2020). Network intru-
sion detection is based on deep learning model optimized with rule-based hybrid feature selection.
Information Security Journal: A Global Perspective ,1/C017.
Ayo, F. E., Ogundokun, R. O., Awotunde, J. B., Adebiyi, M. O., & Adeniyi, A. E. (2020, July). Severe acne
skin disease: A fuzzy-based method for diagnosis. Lecture Notes in Computer Science (including subseries
Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12254 LNCS, pp. 320 /C0334.
Bansal, A., Kumar, S., Bajpai, A., Tiwari, V. N., Nayak, M., Venkatesan, S., & Narayanan, R. (2015). Remote
health monitoring system for detecting cardiac disorders. IET Systems Biology ,9(6), 309 /C0314.
Beheshti-Atashgaha, M., Reza Aref, M., Barari, M., & Bayat , M. (2020). Security and Privacy-preserving in e-health:
a new framework for patient. Internet of Things . Available from https://doi.org/10.1016/j.iot.2020.100290 .
Bhatt, Y., & Bhatt, C. (2017). Internet of things in healthcare .Internet of things and big data technologies for
next generation HealthCare (pp. 13 /C033). Cham: Springer.
Bhoi, A. K., Sherpa, K. S., & Khandelwal, B. (2018). Arrhythmia and ischemia classification and clustering
using QRS-ST-T (QT) analysis of electrocardiogram. Cluster Computing ,21(1), 1033 /C01044.
Brew-Sam, N. (2020). App use and patient empowerment in diabetes self-management: Advancing theory-
guided mHealth research . Springer Nature.
Calo, S. B., Touna, M., Verma, D. C., & Cullen, A. (2017, December). Edge computing architecture for apply-
ing AI to IoT. In Proceedings of the IEEE international conference on big data (Big Data)
(pp. 3012 /C03016). IEEE.
Chen, M., Li, W., Hao, Y., Qian, Y., & Humar, I. (2018). Edge cognitive computing based smart healthcare
system. Future Generation Computer Systems ,86, 403 /C0411.
Deebak, D., & Al-Turjman, F. (2020). A hybrid secure routing and monitoring m echanism in IoT-based wireless sen-
sor networks. Ad Hoc Networks ,97, 102022. Available from https://doi.org/10.1016/j.adhoc.2019.102022 .
Dhayne, H., Haque, R., Kilany, R., & Taher, Y. (2019). In search of big medical data integration solutions-a
comprehensive survey. IEEE Access ,7, 91265 /C091290.
Dias, R. M., Marques, G., & Bhoi, A. K. (2021). Internet of Things for enhanced food safety and quality assur-
ance: A literature review. Advances in Electronics, Communication and Computing , 653 /C0663.
Ebert, D. D., Van Daele, T., Nordgreen, T., Karekla, M., Compare, A., Zarbo, C., ...Kaehlke, F. (2018).
Internet-and mobile-based psychological interventions: Applications, efficacy, and potential for improvingmental health. European Psychologist .
Faheem, M., Shah, S. B. H., Butt, R. A., Raza, B., Anwar, M., Ashraf, M. W., ...Gungor, V. C. (2018).
Smart grid communication and information technologies in the perspective of Industry 4.0: Opportunitiesand challenges. Computer Science Review ,30,1/C030.
Farahani, B., Firouzi, F., Chang, V., Badaroglu, M., Constant, N., & Mankodiya, K. (2018). Towards fog-
driven IoT eHealth: Promises and challenges of IoT in medicine and healthcare. Future Generation
Computer Systems ,78, 659 /C0676.
Fernandes, M., Vieira, S. M., Leite, F., Palos, C., Finkelstein, S., & Sousa, J. M. (2020). Clinical decision sup-
port systems for triage in the emergency department using intelligent systems: A review. Artificial
Intelligence in Medicine ,102, 101762.159 ReferencesFranceschi, C., Garagnani, P., Morsiani, C ., Conte, M., Santoro, A., Grignolio, A., ...Salvioli, S. (2018). The contin-
uum of aging and age-related diseases: Common mechanisms but different rates. Frontiers in medicine ,5,6 1 .
Froomkin, A. M., Kerr, I., & Pineau, J. (2019). When AIs outperform doctors: confronting the challenges of a
tort-induced over-reliance on machine learning. Arizona Law Review ,61, 33.
Galetsi, P., Katsaliaki, K., & Kumar, S. (2020). Big data analytics in health sector: Theoretical framework,
techniques and prospects. International Journal of Information Management ,50, 206 /C0216.
Greco, L., Percannella, G., Ritrovato, P., Tortorella, F., & Vento, M. (2020). Trends in IoT based solutions for
health care: Moving AI to the edge. Pattern Recognition Letters ,135, 346 /C0353.
Greiwe, J., & Nyenhuis, S. M. (2020). Wearable technology and how this can be implemented into clinical
practice. Current Allergy and Asthma Reports ,20,1/C010.
Hartmann, M., Hashmi, U. S., & Imran, A. (2019). Edge computing in smart health care systems: Review,
challenges, and research directions. Transactions on Emerging Telecommunications Technologies .
Available from https://doi.org/10.1002/ett.3710 .
Hassan, W. H. (2019). Current research on Internet of Things (IoT) security: A survey. Computer networks ,
148, 283 /C0294.
Hassanalieragh, M., Page, A., Soyata, T., Sharma, G., Aktas, M., Mateos, G., ...Andreescu, S. (2015, June).
Health monitoring and management using Internet-of-Things (IoT) sensing with cloud-based processing:
Opportunities and challenges. In Proceedings of the IEEE international conference on services computing
(pp. 285 /C0292).
Hassani, F. A., Shi, Q., Wen, F., He, T., Haroun, A., Yang, Y., ...Lee, C. (2020). Smart materials for smart
healthcare /C0moving from sensors and actuators to self-sustained nanoenergy nanosystems. Smart Materials
in Medicine .
Hossain, M., Islam, S. R., Ali, F., Kwak, K. S., & Hasan, R. (2018). An internet of things-based health pre-
scription assistant and its security system design. Future Generation Computer Systems ,82, 422 /C0439.
Islam, S. R., Kwak, D., Kabir, M. H., Hossain, M., & Kwak, K. S. (2015). The internet of things for health
care: A comprehensive survey. IEEE Access ,3, 678 /C0708.
Jamil, F., Ahmad, S., Iqbal, N., & Kim, D. H. (2020). Towards a remote monitoring of patient vital signs based
on IoT-based blockchain integrity management platforms in smart hospitals. Sensors ,20(8), 2195.
Jana, S., Bhaumik, J., & Maiti, M. K. (2013). Survey on lightweight block cipher. International Journal of
Soft Computing and Engineering ,3(5), 183 /C0187.
Jokanovi ´c, V. (2020). Smart healthcare in smart cities. Towards Smart World: Homes to Cities Using Internet
of Things, 45.
Kadhim, K. T., Alsahlany, A. M., Wadi, S. M., & Kadhum, H. T. (2020). An overview of patient’s health status
monitoring system based on Internet of Things (IoT). Wireless Personal Communications ,114, 2235 /C02262.
Kazeem Moses, A., Joseph Bamidele, A., Roseline Oluwaseun, O., Misra, S., & Abidemi Emmanuel, A.
(2021). Applicability of MMRR load balancing algorithm in cloud computing. International Journal of
Computer Mathematics: Computer Systems Theory ,6(1), 7 /C020.
Khan, S., Parkinson, S., & Qin, Y. (2017). Fog computing security: a review of current applications and secu-
rity solutions. Journal of Cloud Computing ,6(1), 1 /C022.
Li, J., Cai, J., Khan, F., Rehman, A. U., Balasubramaniam, V., Sun, J., & Venu, P. (2020). A secured frame-
work for SDN-based edge computing in IoT-enabled healthcare system. IEEE Access ,8, 135479 /C0135490.
Luk, C. Y. (2018). The impact of digital health on traditional healthcare systems and doctor-patient relation-
ships: The case study of Singapore .Innovative perspectives on public administration in the digital age
(pp. 143 /C0167). IGI Global.
Majumder, S., Aghayi, E., Noferesti, M., Memarzadeh-Tehran, H., Mondal, T., Pang, Z., & Deen, M. J. (2017).
Smart homes for elderly healthcare—R ecent advances and research challenges. Sensors ,17(11), 2496.160 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedMarks, M. (2020). Emergent medical data: Health Information inferred by artificial intelligence. UC Irvine
Law Review .
Marques, G., Bhoi, A. K., Albuquerque, V. H. C. d., & Hareesha, K. S. (Eds.), (2021). IoT in healthcare and
ambient assisted living . Springer.
Marques, G., Miranda, N., Kumar Bhoi, A., Garcia-Zapirain, B., Hamrioui, S., & de la Torre Dı ´ez, I. (2020).
Internet of things and enhanced living environments: Measuring and mapping air quality using cyber-physical systems and mobile computing technologies. Sensors ,20(3), 720.
Mehrabi, M., You, D., Latzko, V., Salah, H., Reisslein, M., & Fitzek, F. H. (2019). Device-enhanced MEC:
Multi-access edge computing (MEC) aided by end device computation and caching: A survey. IEEE
Access ,7, 166079 /C0166108.
Meneghello, F., Calore, M., Zucchetto, D., Polese, M., & Zanella, A. (2019). IoT: Internet of threats? A survey
of practical security vulnerabilities in real IoT devices. IEEE Internet of Things Journal ,6(5), 8182 /C08201.
Mishra, S., Mallick, P. K., Tripathy, H. K., Bhoi, A. K., & Gonz ´alez-Briones, A. (2020). Performance evalua-
tion of a proposed machine learning model for chronic disease datasets using an integrated attribute evalua-
tor and an improved decision tree classifier. Applied Sciences ,10(22), 8137.
Mishra, S., Tripathy, H. K., Mallick, P. K., Bhoi, A. K., & Barsocchi, P. (2020). EAGA-MLP—An enhanced
and adaptive hybrid classification model for diabetes diagnosis. Sensors ,20(14), 4036.
Mohd, B. J., Hayajneh, T., & Vasilakos, A. V. (2015). A survey on lightweight block ciphers for low-resource
devices: Comparative study and open issues. Journal of Network and Computer Applications ,58,7 3/C093.
Monje, M. H., Foffani, G., Obeso, J., & S ´anchez-Ferro, A ´. (2019). New sensor and wearable technologies to
aid in the diagnosis and treatment monitoring of Parkinson’s disease. Annual Review of Biomedical
Engineering ,21, 111 /C0143.
Mshali, H., Lemlouma, T., Moloney, M., & Magoni, D. (2018). A survey on health monitoring systems for
health smart homes. International Journal of Industrial Ergonomics ,66,2 6/C056.
National Academies of Sciences, Engineering, and Medicine. (2020). Opportunities to improve opioid use dis-
order and infectious disease services: Integrating responses to a dual epidemic .
Noshina, T., Ayesha, Q., Muhammad, A., & Farrukh, A.K. (2020). Blockchain and smart healthcare security:
A survey. Procedia Computer Science .
Oladele, T. O., Ogundokun, R. O., Awotunde, J. B., Adebiyi, M. O., & Adeniyi, J. K. (2020, July). Diagmal:
A malaria coactive neuro-fuzzy expert system. Lecture notes in computer science (including subseries lec-
ture notes in artificial intelligence and lecture notes in bioinformatics) , 12254 LNCS, pp. 428 /C0441.
Omoniwa, B., Hussain, R., Javed, M. A., Bouk, S. H., & M alik, S. A. (2018). Fog/edge computing-based IoT
(FECIoT): Architecture, applications, and research issues. IEEE Internet of Things Journal ,6(3), 4118 /C04149.
Oniani, S., Marques, G., Barnovi, S., Pires, I. M., & Bhoi, A. K. (2021). Artificial intelligence for Internet of
Things and enhanced medical systems .Bio-inspired neurocomputing (pp. 43 /C059). Singapore: Springer.
Papa, A., Mital, M., Pisano, P., & Del Giudice, M. (2020). E-health and wellbeing monitoring using smart
healthcare devices: An empirical investigation. Technological Forecasting and Social Change ,153,
119226.
Peters, B. S., Armijo, P. R., Krause, C., Choudhury, S. A., & Oleynikov, D. (2018). Review of emerging surgi-
cal robotic technology. Surgical Endoscopy ,32(4), 1636 /C01655.
Pisani, F., de Oliveira, F. M. C., de Souza Gama, E., Immich, R., Bittencourt, L. F., & Borin, E. (2019). Fog
computing on constrained devices: Paving the way for the future IoT. Advances in Edge Computing:
Massive Parallel Processing and Applications ,35,2 2/C060.
Poitras, M. E., Hudon, C., Godbout, I., Bujold, M., Pluye, P., Vaillancourt, V. T., ...L´egar´e, F. (2020).
Decisional needs assessment of patients with complex care needs in primary care. Journal of Evaluation in
Clinical Practice ,26(2), 489 /C0502.161 ReferencesPorambage, P., Okwuibe, J., Liyanage, M., Ylianttila, M., & Taleb, T. (2018). Survey on multi-access edge
computing for internet of things realization. IEEE Communications Surveys & Tutorials ,20(4),
2961 /C02991.
Pramanik, M., Pradhan, R., Nandy, P., Bhoi, A. K., & Barsocchi, P. (2021). Machine learning methods with
decision forests for Parkinson’s detection. Applied Sciences ,11(2), 581.
Pramanik, M. I., Lau, R. Y., Demirkan, H., & Azad, M. A. K. (2017). Smart health: Big data enabled health
paradigm within smart cities. Expert Systems with Applications ,87, 370 /C0383.
Pramanik, P. K. D., Upadhyaya, B. K., Pal, S., & Pal, T. (2019). Internet of things, smart sensors, and perva-
sive systems: Enabling connected and pervasive healthcare .Healthcare data analytics and management
(pp. 1 /C058). Academic Press.
Rahmani, A. M., Gia, T. N., Negash, B., Anzanpour, A., Azimi, I., Jiang, M., & Liljeberg, P. (2018).
Exploiting smart e-Health gateways at the edge of healthcare Internet-of-Things: A fog computing
approach. Future Generation Computer Systems ,78, 641 /C0658.
Ren, J., Zhang, D., He, S., Zhang, Y., & Li, T. (2019). A survey on end-edge-cloud orchestrated network com-
puting paradigms: transparent computing, mobile edge computing, fog computing, and cloudlet. ACM
Computing Surveys (CSUR) ,52(6), 1 /C036.
Roski, J., Bo-Linn, G. W., & Andrews, T. A. (2014). Creating value in health care through big data: opportu-
nities and policy implications. Health Affairs ,33(7), 1115 /C01122.
Saposnik, G., Redelmeier, D., Ruff, C. C., & Tobler, P. N. (2016). Cognitive biases associated with medical
decisions: A systematic review. BMC Medical Informatics and Decision Making ,16(1), 1 /C014.
Sayeed, A., Mohanty, S. P., Kougianos, E., Yanambaka, V. P., & Zaveri, H. (2018, December). A robust and
fast seizure detector for IoT edge. In Proceedings of the IEEE international symposium on smart electronic
systems (iSES)(Formerly iNiS) (pp. 156 /C0160). IEEE.
Shah, J. L., Bhat, H. F., & Khan, A. I. (2020). Integration of cloud and IoT for smart e-healthcare. In
Healthcare paradigms in the Internet of Things ecosystem (pp. 101 /C0136). Academic Press.
Tariq, N., Asim, M., Al-Obeidat, F., Zubair Farooqi, M., Baker, T., Hammoudeh, M., & Ghafir, I. (2019). The
security of big data in fog-enabled IoT applications including blockchain: A survey. Sensors ,19(8), 1788.
Tausif, M., Ferzund, J., Jabbar, S., & Shahzadi, R. (2017). Towards designing efficient lightweight ciphers for
internet of things. KSII Transactions on Internet and Information Systems (TIIS) ,11(8), 4006 /C04024.
Tian, S., Yang, W., Le Grange, J. M., Wang, P., Huang, W., & Ye, Z. (2019). Smart healthcare: making medi-
cal care more intelligent. Global Health Journal ,3(3), 62 /C065.
Ullah, K., Shah, M. A., & Zhang, S. (2016, January). Effective ways to use Internet of Things in the field of
medical and smart health care. In Proceedings of the international conference on intelligent systems engi-
neering (ICISE) , (pp. 372 /C0379). IEEE.
Varshney, G., & Gupta, H. (2017). A security framework for IoT devices against wireless threats. In
Proceedings of the second international conference on telecommunication and networks (TEL-NET) .
Vutha, A. Z. (2020). Dealing with performance-based health technology assessment outcomes for medical
devices: A South African perspective . (Doctoral dissertation), University of Pretoria.
Zeadally, S., & Bello, O. (2019). Harnessing the power of Internet of Things based connectivity to improve
healthcare. Internet of Things , 100074.162 Chapter 6 An efficient lightweight speck technique for edge-IoT-basedCHAPTER
7Deep learning approaches for the
cardiovascular disease diagnosisusing smartphone
Abdulhamit Subasi1, Elina Kontio2and Mojtaba Jafaritadi2,3
1Faculty of Medicine, Institute of Biomedicine, University of Turku, Turku, Finland2Faculty of Engineering and
Business, School of Information and Communications Technology, Turku University of Applied Sciences, Turku,
Finland3Faculty of Technology, Department of Computing, University of Turku, Turku, Finland
7.1 Introduction
Machine learning (ML) is an artificial intelligence (AI) subdivision, which allows computer algo-
rithms to learn from input / training data. In the development of ML algorithms in AI, neural net-works (NN) play a crucial role. NNs have been inspired by the function of biological neurons inthe human brain. Each neuron is a part of a processing element, and an interconnection of thoseneurons contributes to tremendous computational capacity that can solve complex problems. Thecomputational model for NN opened the way for studies on NN to follow two paths, one for bio-logical processes in the brain and the other for the use of NN to AI. NN were designed specifically
to solve complex problems, which could not fit with rule-based methods and algorithms. Some
information may be absent in real world problems, or all the rules may not be known. In AI and,thus, in NN, heuristics play a crucial role (
Vasuki & Govindaraju, 2017 ).
NN are mostly utilized for the classification of signals and images and are trained with the
learning capacity to diagnose disease. There is a collection of training data available and relation-ships between training inputs and the desired outputs (or pattern classes) must be fed into the net-work. On these known data, the network is trained and learns to recognize/interpret new data. MLmethods have been developed to make learning occur, with the concept being learned by the algo-
rithm through training. The data or patterns are supplied as inputs, the outputs are defined, and the
algorithm learns to explore the connection between inputs and outputs. More hidden layers areneeded when the problem is complicated, such as in signal and image recognition for disease diag-nosis, and this makes the neural network “deep.” The performance of the classification is enhancedby hundreds of hidden layers and learning becomes “deep learning” (DL) (
Vasuki & Govindaraju,
2017 ).
DL that is a sub-branch of ML, is one of AI’s most commonly employed techniques. Usually,
DL is a technique for extracting informative features automatically by integrating several linear and
nonlinear processing elements in a deep architecture ( Deng & Yu, 2014 ). Artificial neural networks
(ANNs) made quick strides in the last decade and the name of DL began to be employed.Numerous DL models are employed based on the type of application. The leading and most popular
1635G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00010-3
©2022 Elsevier Inc. All rights reserved.architectures in medical image processing are Convolutional Neural Networks (CNNs), which is a
form of DL, and its derivatives ( Kharazmi, Zheng, Lui, Jane Wang, & Lee, 2018; Premaladha &
Ravichandran, 2016; Wang et al., 2018 ). Hybrid deep networks refer to systems, which are built by
merging various DL models to achieve better performance. CNN and its variations, autoencoders,deep belief neural networks, Recurrent neural networks (RNN), and Long Short Term Memory
(LSTM) are the most utilized and popular DL models in the diagnosis and treatment of diseases.
Traditional ML methods are primarily focused on predefined hand-crafted features and are nor-mally developed for particular issues only. However, DL algorithms do not need an explicitdescription of features, but instead employ data and incorporate features that are high-dimensionaland difficult to understand for achieving results. Several approaches, such as k nearest neighbor (k-NN), support vector machines (SVM), random forest, and Gaussian mixture models that are amongthe traditional ML algorithms, were replaced by DL algorithms because of the superior perfor-mance and achievements of DL techniques. These traditional approaches are not adjustable, robust,
and computationally intense, since they are manually developed for the diagnosis and treatment of
diseases (
Pacal, Karaboga, Basturk, Akay, & Nalbantoglu, 2020 ).
The healthcare industry is facing different concerns associated with disease diagnosis and cost-
effective delivery of services ( Awan, Ali, Aadil, & Qureshi, 2018 ). One of the crucial needs of a
healthcare system is to provide the patient with adequate services by the utilization of medicalrecords, lifestyle habits, and any variability of molecular traits. These systems have faced numerouschallenges relevant to data processing, the identification of information, data retrieval and decisionmaking with the rapid growth of emerging technology. Several intelligent instruments have been
developed based on AI and data-driven methods to overcome these challenges. In order to create
mutual information for further exploration and quantitative analysis, these methods include aneffort to connect several data sources. In addition, for particular diseases such as brain tumors andcoronary heart diseases, multiple prediction-based approaches have been designed (
Alanazi,
Abdullah, Qureshi, & Ismail, 2018 ). However, because of their temporal dependencies, inconsisten-
cies, interpretability, high-dimensionality and diversity, there are many difficulties to address in thesense of biomedical data analysis for disease diagnosis and treatment. For disease diagnosis andtreatment, modern and automated technology-enabled Smart Healthcare Platforms have been
employed. Data are relevant to demographic details of patients, laboratory test results, image-based
radiological documents, prescriptions, medication dose records, and dietician appointment recordsin these Smart Healthcare Platforms. Healthcare services are delivered by Smart HealthcarePlatforms utilizing smart phone/watch and connected technologies. Patient information is gatheredin a file for the purpose of disease diagnosis and treatment and offering timely care, in particularfor the areas where urgent intervention is a challenge. Despite the existence of these modern com-munication technologies, because of the scalability and various technology issues, digital develop-ment in the healthcare sector also does not fulfill all requirements. A more efficient Smart
Healthcare infrastructure has to be built to make the service simpler for providers, patients and
healthcare workers. The vast volume of associated medical data is contained in the cloud, so it is amajor challenge to decide if we can utilize those data to achieve better forecasts (
Qureshi, Din,
Jeon, & Piccialli, 2020 ). It is desirable for forecasting models to examine the data and forecast
patient statistics. This chapter introduces and explores the use of AI approaches to detect diseasessuch as heart attacks, brain cancers, diabetes, cardiac disease, and prediction of epileptic seizures inthis perspective.164 Chapter 7 Deep learning approaches for the cardiovascular diseaseIn modern societies, healthcare is one of the most critical issues, since the life quality of people
depends mainly on it ( Bagga & Hans, 2015 ). The healthcare industry, though, is extremely diverse,
broadly spread and decentralized. The provision of sufficient medical care from a professional pointof view needs access to reliable patient records that is rarely accessible when and where it isrequired (
Grimson et al., 2001 ). In addition, the large variance in the test-ordering for diagnostic
aims implies the need for an adequate and suitable range of test data ( Daniels & Schroeder, 1977;
Wennberg, 1984 ).Smellie, Galloway, Chinn, and Gedling (2002) expanded this claim by implying
that the broad discrepancies found in the request for general disease diagnosis and treatment areprimarily due to individual changes in clinical practice and are thus theoretically prone to changesby making doctors’ decisions more reliably and better informed (
Stuart, Crooks, & Porton, 2002 ).
Medical data thus also includes a vast variety of heterogeneous factors, obtained from multipledatabases, such as populations, history of illness, medications, biomarkers, medical records, aller-gies, or genetic markers, each of which gives a distinct partial view of the status of the patient. In
addition, among the above references, statistical properties are fundamentally distinct. This causes
contribute to complications and factual errors in the identification, diagnosis and treatment of thedisease, thereby preventing patients from providing proper care (
Dick, Steen, & Detmer, 1997 ).
There is also a strong need for an efficient and rigorous approach that allows for the timely identifi-cation, diagnosis and treatment of diseases and can be utilized by physicians as a decision supportsystem (
Zhuang, Churilov, Burstein, & Sikaris, 2009 ). The statistical, computational and medical
fields are now facing the challenge of exploring new methods for modeling disease diagnosis andtreatment, as existing paradigms struggle to answer any of this situation (
Huang, Chen, & Lee,
2007 ). This necessity is closely connected with advances in other areas, such as ML, AI or Big
Data. As the quantity of medical data recorded and stored digitally is immense and rapidly expand-ing, data management and interpretation is similarly evolving to transform this massive data intoinformation and knowledge, which assists them to accomplish their goals (
Caball ´e, Castillo-
Sequera, Go ´mez-Pulido, Go ´mez-Pulido, & Polo-Luque, 2020 ).
Advanced methods for explaining these structural variations in data need to be used. Within an
area known as AI, several techniques were invented. AI is a component of computer science whichintends to make computers smarter. Learning is one of the fundamental criteria for any intelligent
behavior. Presently, most scholars believe that without learning, there is no intelligence. A learning
challenge can be identified as the complexity of enhancing certain measures by some level of train-ing experience while performing certain tasks. In turn, ML has appeared within AI as a tool for cre-ating dataset analysis algorithms (
Jordan & Mitchell, 2015 ). Today, ML offers many important
methods for intelligent information processing. Furthermore, its technology is presently welladapted for the study of medical data and, particularly, a wide variety of medical diagnostic workhas been carried out on small-specialized diagnostic problems (
Kononenko, 2001 ), where initial
implementations of ML have been found. ML algorithms, which are helpful instruments in clinical
diagnosis, have been successfully used for the disease diagnosis and treatment ( Sriram, Rao,
Narayana, & Kaladhar, 2016 ). Evidently, several ML methods perform very well on a wide range
of critical issues. Nevertheless, as they get incredibly complicated and the curse of dimensionalityin the data is high, they failed in tackling the key issues in AI. DL then appeared as a special formof ML. The development of DL was therefore inspired and planned to circumvent the inability ofconventional algorithms to deal with high-dimensional data and to learn complex functions in high-dimensional spaces (
Caball ´e et al., 2020; Goodfellow, Bengio, Courville, & Bengio, 2016 ).165 7.1 IntroductionEvery year, cardiovascular d iseases (CVDs) claim more th an 17 million death worldwide
(Cardiovascular diseases, 2021 ), so early detection of CVDs for faster recovery becomes an essential
task. One-lead portable electrocardiograph (ECG) i nstruments such as chest patches and wristbands
were invented, implemented and commonly used to a chieve this role. ECG is also a non-invasive tech-
nique that cardiologists usually use in their clinical practice. It helps individuals to track themselves in
real time, supplying the signal that holds the data about whether or not CVDs are occurring. A natural
challenge then emerges about h ow to recognize heart rhythm ab normalities immediately, because
arrhythmias can need early diagnosis. It has been rece iving more and more scien tific interest in the area
of medical diagnosis due to the high mortality rate of CVDs ( Li et al., 2020 ).
The most frequent cardiac arrhythmia is atrial fibrillation (AFib) ( Colilla et al., 2013 ), but its
paroxysmal nature and low relationship with symptoms make it extremely challenging to diagnoseand treat. Actually, for one-quarter of patients with asymptomatic AFib, stroke is the first clinicalappearance of AFib (
Jaakkola et al., 2016 ). In response, a range of remote tracking systems have
been more commonly used for diagnosis and treatment of AFib. These include wearable heart sen-
sors, medical wearables, direct-to-consumer applications, and smartphone apps ( Hickey, Riga,
Mitha, & Reading, 2018 ).Turchioe et al. (2020) focused on testing free mobile apps so access to
health services are not obstacles to their acceptance and use. There is growing awareness thatphotoplethysmography (PPG) can be used for heart disease tracking, an affordable and readilyavailable technology used in all smartphones. PPG sensors sense variations in the concentration oftissue blood originating from peripheral pulses (
Allen, 2007 ). As a light source flash from a mobile
camera illuminates body tissues in the finger, PPG waveforms are generated inside smartphones,
and a photodetector, which is camera, measures changes in light intensity across the tissue ( Allen,
2007 ). PPG might be utilized to scan big portions of the population for irregular heart rate detection
without any cost ( Turchioe et al., 2020 ).
To avoid cardio-embolic strokes, a prompt diagnosis of AFib is important. Appropriate and
cost-effective screening methods for asymptomatic AFib are yet to be adopted. Mechanical heartactivity with gyroscopes and accelerometers of smartphones can be recorded in mechanocardiogra-phy (MCG). Seismocardiography is a method previously established in which mechanical cardiacactivity is measured with accelerometers (
Inan et al., 2015 ). Gyrocardiography is a novel modality
introduced by Jafari Tadi et al. (2017) in which gyroscopes are utilized to measure mechanical car-
diac signals. Jointly, these two methods constitute the concept of MCG. The potential of anadvanced mobile MCG application to distinguish AFib from sinus rhythm (SR) was tested by
Jaakkola et al. (2018a, 2018b) . From a total of 300 hospitalized subjects (150 in AFib and 150 in
SR), a 3-minute MCG recording was acquired with a smartphone put on their chest during thetracking of rhythm with telemetry electrocardiography (ECG). The rhythm of the MCG recordingsas either AFib or SR was classified by proposed algorithm. Compared to ECG interpretation, theprecision of the MCG algorithm to distinguish AFib from SR was tested by two independent cardi-
ologists. Without any external hardware for accurate and usable AF scanning, the smartphone
MCG accurately detects AFib (
Jaakkola et al., 2018a, 2018b ).
This chapter is organized as follows: disease diagnosis and treatment are discussed in
Section 7.2 .Section 7.3 includes DL approaches for the disease diagnosis and treatment, which
includes a description of the DL algorithms. Section 7.4 delivers details of the case study for detec-
tion of atrial fibrillation using smartphone. Section 7.5 is the discussion. Finally, in Section 7.6 ,
conclusions are given.166 Chapter 7 Deep learning approaches for the cardiovascular disease7.2 Disease diagnosis and treatment
In precision medicine, interpretation of healthcare data can play a crucial role. For instance, by con-
sidering many forms of patient data, including environment, genomics variants, imaging genomics,existing medications, and lifestyle, customized cancer care aims to offer the right treatment for theright patient. New technology such as medical imaging, genomics, and lifetime tracking technolo-gies have created vast amount of healthcare data over the past decade, allowing researchers to pro-vide patients with improved therapies. Our knowledge of diseases, and how we can handle patients,
is still inadequate, considering this massive volume of data. In order to interpret such a big volume
of data, due to the data complexity, the use of ML and AI techniques (
Kalantari et al., 2018 ), like
the DNN, has become more desirable. Particularly, it is a key issue to implement accurate diagnos-tic tools focused on data-driven techniques and ML approaches to define the connections betweenall various forms of patient data. A wide variety of AI and ML methods have been utilized over thepast decade to accurately analyze huge healthcare data. For instance, in cardiac disease detection, alogistic regression-based predictive approach was developed for automatic detection of heart dis-ease (
Kumar & Gandhi, 2018 ). In medical imaging, ML has been introduced to deliver an auto-
mated discovery of object characteristics ( Ravı`et al., 2016 ). DNN-based methods are appealing a
lot of interest among various ML models, particularly in the analysis of large datasets. DLapproaches are feature learning methods where data is filtered through a cascade of multiple layers.As they process large-scale data, DNN models become more and more precise, helping them to sur-pass many traditional models of ML. These methods are becoming stimulating methods to evaluatehealthcare data, provided the success of DL approaches in various fields and their steady continu-ous methodological development. Thorough the utilization of DL approaches on healthcare and bio-medical data, a broad range of projects have been carried out (
Shamshirband, Fathi, Dehzangi,
Chronopoulos, & Alinejad-Rokny, 2020 ).
A CNN for region classification of semantic ally coherent tissu es was designed by Dubrovina,
Kisilev, Ginsburg, Hashoul, and Kimmel (2018) and has been utilized for mass detection of mammo-
grams. The findings suggest the computation and classification’s high accuracy. Jiao, Gao, Wang, and
Li (2018) suggested the classification of DL-masses wher e CNN layers organize simple discriminatory
representation to improve the class ification accuracy. Misclassifie d cases are placed in an “error book”
during each training period. If the v alidation error is not minimized in a loop, a stored example is sent
to the training set. A CNN was also used for oth er breast cancer tomosynthesis studies ( Samala et al.,
2016 ). For brain-MRI using the multi-scale CNN, a ti ssue segmentation appro ach was suggested by
Moeskops et al. (2018) . With varying degrees of abnormality, t he procedure was tested. The findings
indicate that it can corre ctly segment brain tissues. In the clin ical sector, model transparency is an
important topic impacting real world medical decisi on making and patient care in terms of predictions
(Nie et al., 2015 ). A new DL technique for brain tumor segmentation by combining Fully
Convolutional Neural Netwo rks (FCNN) and Conditional Rando m Fields (CRF) was suggested by
Zhao et al. (2018) . The purpose of such integration was to en hance the robustness of the scheme. The
approach includes four phases, which include prepr ocessing, segmenting image slices with embedded
FCNNs and CRF-RNN, feature extraction and classification using DL models. The system of CRF was
also formulated as RNN. In three steps, the model was educated. First, FCNN was educated in the useof image patches. Second, through the use of ima ge slices, CRF-RNN was trained. The image slices
were eventually used to fine-tune the whole network (
Shamshirband et al., 2020 ).167 7.2 Disease diagnosis and treatmentDiabetic retinopathy (Dr) is one of the major reasons of blindness in the working-age popula-
tion. It is one of the diabetes problems that is most anticipated. The fundamental issue of Dr is that,at advanced stages, it becomes incurable, so early diagnosis is critical. However, owing to the num-ber of patients and the limited number of skilled specialists, this involves remarkable complexity inthe health care sector. This motivated the requirement to build an automatic diagnostic tool to help
with early diagnosis of Dr. Several efforts in this direction have been made, and several methods
have been proposed based on hand-crafted features that have revealed encouraging competence inidentifying Dr regions in retinal fundus images. For traditional ML approaches used for Dr diagno-sis, hand-crafted features are widely utilized. Expert experience, however, is a requirement forhand-crafted features, and it involves extensive investigation into different options and exhaustiveparameter adjustments to select the right features. In comparison, approaches built on hand-craftedfeatures cannot generalize well. The existence of massive datasets and the immense computationalpower provided by graphics processing units (GPUs) have inspired research on DL algorithms that
demonstrated excellent success in different computer vision tasks and achieved a significant success
over conventional hand-crafted-based approaches in recent years. Several DL-based algorithmswere also developed to analyze retinal fundus images for different tasks in order to implement auto-mated computer-aided diagnostic systems for Dr (
Asiri, Hussain, Al, & Alzaidi, 2019 ).
Neuroscientists and physicians also use noninvasive imaging methods in current neuroscience
and clinical practice to test hypotheses and DL models, analyze brain functions, and identifybrain disorders. Functional magnetic resonance imaging (fMRI) is one of the most widelyutilized imaging technique, which can be employed to explain the functions of the human brain and
to identify and diagnose brain diseases. In order to better understand fMRI data, developments in
AI and the improvements of DL methods achieved remarkable performance. In order toanalyze fMRI images, DL approaches quickly became the state of the art and have achieved betterperformance in different fMRI applications. DL is typically viewed as an end-to-end learningmechanism, which can mitigate the needs of feature engineering and thereby diminish the require-ments for domain awareness to some degree. fMRI data may be considered as images, time series,or image series within the paradigm of DL. Therefore, it is possible to create different DL models,such as CNNs, RNN, or a mixture of both, to utilize fMRI data for various tasks (
Yin, Li, & Wu,
2020 ).
An ensemble of DL was used by the authors in Moeskops et al. (2018) to classify three types of
cancers, specifically breast invasive carcinoma, stomach adenocarcinoma, and lung adenocarci-noma. They chose the significant genes utilizing distinguished gene expression study to create thismodel. Five CNN classifiers were then used to train these chosen genes and assembled to achievethe final result (
Xiao, Wu, Lin, & Zhao, 2018 ).Sharma, Zerbe, Klempert, Hellwich, and Hufnagl
(2017) recommended a system of automated classification of gastric carcinoma using CNN. Three
multichannel ROI-based deep structured algorithms, specifically DBN, CNN, and stacked denoising
AE for lung cancer diagnosis, were introduced in another study ( Sun, Zheng, & Qian, 2017 ). In
(Timmis et al., 2020 ), in order to analyze encephalogram signals for epilepsy diagnosis, the authors
presented a new computer-aided diagnostic method employing CNN. This CNN approach includes13 layers to construct a complex and powerful seizure prediction models and is compared to otherML models (
Shamshirband et al., 2020 ).Choi, Schuetz, Stewart, and Sun (2017) employed the
most appropriate application of DL in the diagnosis of cardiac disease. They introduced an RNNmodel of gated time-stamped events.168 Chapter 7 Deep learning approaches for the cardiovascular diseaseIn the present research scenario, the incidence of chronic kidney disease (CKD) rises per year.
The CKD prediction is one of the areas needs for more therapy where, due to their high precisionclassification capacity, ML methods become crucial in disease diagnosis. In the last decade, theperformance of prediction models is contingent on the correct usage of feature extraction and selec-tion algorithms to minimize the size of the results.
Ma, Sun, Liu, and Jing (2020) have proposed
the Heterogeneous Modified Artificial Neural Network (HMANN) on the Internet of Medical
Things (IoMT) platform for the quick identification, segmentation, and identification of chronicrenal failure. The developed algorithm operates on the basis of an ultrasound image that is signifiedas a pre-processing phase to segment the region of interest of kidney. The suggested HMANNapproach achieves high precision in kidney segmentation and decreases the time to define the con-tour considerably.
To boost Parkinson’s disease, a DL-based FPCIT SPECT analysis method was suggested by
Choi, Ha, Im, Paek, and Lee (2017) . In the presented research, it was shown that the inter-observer
variability problem can be solved by this approach. Recently, Gunduz (2019 ) employed DL for the
characterization of Parkinson’s disease on the range of vocal (speech) characteristics. Nine-layeredCNN is utilized to solve this problem and achieved reasonable performance.
In order to create a ML platform, which optimize clinical decision frameworks, an unsupervised
deep feature learning algorithm was suggested by
Miotto, Li, Kidd, and Dudley (2016) . To distin-
guish red lesions in fundus images, an ensemble of domain information and DL was offered by
Orlando, Prokofyeva, Del Fresno, and Blaschko (2018) . Hand-crafted features have been fused in
the learning phase of CNN. It was recorded that the efficiency of this feature fusion based classifier
was higher than that of other individual classifiers ( Shamshirband et al., 2020 ).
One of the worst form of cancer is pancreatic cancer and the prognosis in the current scenario is
very low. Computer-aided scanning (CAD), detection and functional analyses of radiology imagessuch as CT and MRI also include automated pancreatic tumor image segmentation. Via theseapproaches, tumor classification will also help to monitor, anticipate and support personalized ther-apy as part of successful treatment without cancer incursions. ANN have provided positive resultsfor correct segmentation of pancreatic images today. For pancreatic tumor identification,
Xuan and
You (2020) suggested a DL-based Hierarchical Convolutional Neural Network (HCNN). A RNN is
given to solve the problem of segmentation of spatial difference across slices of neighboring
images. By optimizing the smoothness and form, the recurrent neural network produces CNN out-comes and fine-tunes its segmentation. In addition, the targets of HCNN configurations and trainingfor the success of pancreatic tumor image segmentation were demonstrated. The experimental find-ings revealed that the suggested solution will increase the classifier’s efficiency and decrease thecost of the network for IoMT.
Automatic diagnosis of cardiac disease is one of the most important and challenging healthcare
issues in the world. Cardiac disease affects the functioning of blood vessels and triggers infections
of the coronary artery that damage the patient’s body, especially in adults and the elderly. In order
to successfully cure cardiac patients before a heart attack or stroke will happen, early detection ofcardiac disease is extremely significant. By performing diagnostic examinations and using wearablemonitors, cardiovascular disorders may be detected. Extracting useful risk factors for cardiac dis-ease from electronic medical tests, however, is challenging when doctors aim to diagnose patientsrapidly and reliably. Wearable sensors are also currently used to monitor regularly the body ofpatient to identify cardiac failure. Therefore, an intelligent framework is required, which can169 7.2 Disease diagnosis and treatmentautomatically combine the information collected from both sensor data and electronic medical
records and which can interpret the data recorded to detect the hidden signs of cardiac failure anddiagnose cardiac failure before a heart attack happens. Now, using AI approaches and hybrid mod-els, numerous schemes have been developed to forecast and diagnose CVD.
Ali et al. (2020) sug-
gested a novel, smart heart disease prediction tracking framework for healthcare utilizing feature
fusion and ensemble DL as a whole.
A simple and accurate procedure for the diagnosis of cardiac arrhythmias is electrocardiograms
(ECGs). In clinical practice, patients with multiple and overlapping arrhythmias diagnosed by ECGare frequently used. ECGs that are inappropriately interpreted can lead to improper clinical deci-sions. Additionally, it is impossible even for an expert cardiologist to correctly interpret the ECGfor a patient with instantaneous arrhythmias. About 300 million ECGs are performed yearlyworldwide and delivering the right diagnosis can be extremely problematic in middle-income andlow-income countries because skilled cardiologists are limited. There is therefore an immediate
requirement for an integrated multilabel ECG based cardiac arrhythmia detection system supported
by AI methods. Several experiments have been aimed at detecting arrhythmias using DLapproaches over the past two years (
Attia et al., 2019a, 2019b; Faust, Hagiwara, Hong, Lih, &
Acharya, 2018; Kamaleswaran, Mahajan, & Akbilgic, 2018; Sannino & De, 2018; Zhu et al., 2020 )
and several benchmark datasets have been revealed to be effective for analyzing DL arrhythmiamodels on the basis of quantitative performance measurements. Researches on heart arrhythmiadetection have demonstrated that complex anomalies, such as atrial fibrillation, may be detected byutilizing a robustly developed DL model (
Attia et al., 2019a, 2019b; Yildirim, 2018; Yıldırım,
Pławiak, Tan, & Acharya, 2018 ).Zhu et al. (2020) attempted to solve the issue related to patients
with more than one heart disease, which is often identified concurrently during the clinical diagno-sis period, by analyzing the ECG diagnosis problem at a multilabel stage. The goal was to establisha general form of automated diagnosis of cardiac arrhythmia disorders using ECG tests and todevelop an AI-based multilabel ECG diagnosis model covering nearly all forms of arrhythmias.
7.3 Deep learning approaches for the disease diagnosis and treatment
DL is a subfield of ML, which requires networks able to learn from unstructured/unlabeled data.
Deep neural learning or deep neural networks is a subclass of ML techniques, which progressivelyextract sophisticated features from the raw data using multiple layers. It is founded on ANNs withlearning from representation. Learning may be unsupervised, semisupervised or supervised. DLdraws from vast amounts of unstructured data. ANN, with neuron nodes joined together, are analo-gous to the human brain. Traditional programs usually interpret data in a linear fashion, but data
can be analyzed in a nonlinear way by DL models. Each level learns to change the input data into
a much more complex presentation through DL. Advances in the field of ML have shown a greatpotential to use DL models in disease diagnosis (
Gao et al., 2019 ).
DL has shown its outstanding success in providing the solution to problems associated with pat-
tern recognition, image processing, disease diagnosis and classification. DL is becoming a criticalfactor in identifying specific patterns in large volumes of gene expression datasets due to the bene-fit of large amounts of RNA-Seq data and freely accessible microarray gene expression.170 Chapter 7 Deep learning approaches for the cardiovascular diseaseClassifying cancer cells and normal cells on the basis of patterns of gene expression is a huge
obstacle. In the field of bioinformatics, supervised ML methods are utilized to solve this problem(
Ahn et al., 2018 ).
7.3.1 Artificial neural networks
A theoretical model, which is inspired by the structure of the human brain is a neural network.
A large amount of nerve cells, or neurons, make up the human brain. Neurons have a basic three-part configuration that consists of a cell body, a series of dendrites called fibers, and a single long
fiber named an axon. The axons and the dendrites derive from the body of the cell, and the den-
drites of one neuron are linked to the axons of other neurons. The dendrites act as input channels tothe neuron and receive signals sent from other neurons along their axons. The axon serves as a neu-ron’s output channel, and thus the signals transmitted along the axon are processed as inputs byother neurons whose dendrites are linked to the axon. The neuron transmits an electrical impulse,termed as action potential, through its axon to the other neurons, which are connected to it, if theincoming stimuli are big enough. Therefore, a neuron functions as an all-or-no switch, which takesin a set of inputs and either outputs an action potential or no output. A substantial simplification of
biological fact is this description of the human brain, but it captures the key points required to
explain the analogy between the human brain structure and the theoretical models called NN.These points of comparison are: (1) the brain consists of a huge number of interconnected basicelements known as neurons; (2) the operation of the brain can be explained as data transmission,encoded as high or low electrical signals, or activation potentials, scattered across the neuron net-work; and (3) each neuron receives a series of stimuli from its neighbors and maps these inputs toeither a low or high value output. All neural network models have these aspects (
Kelleher, 2019 ).
7.3.2 Deep learning
It has been observed that NN are ideally adapted for the application of ML. There is one input layer,
one output layer and two to three hidden layers in conventional neural networks. There is one input
layer, one output layer and several hidden layers for deep neural networks. The more hidden layers
there are, the stronger the network is. The layers are i nterconnected, with the input of the current layer
being the output of the previous layer. The inputs/o utputs are weighted, and the network efficiency is
determined by the weights. The network training inc ludes obtaining the suitable weights for the differ-
ent layers. Deep networks need higher computational power, computation speed, a big repository andparallel processing of the suitable software (
Vasuki & Govindaraju, 2017 ).
Neurons are structured in layers with links between the layers. Signals are passed, after proces-
sing, through the layers. Signals often propagate in the forward direction from the input layer to the
output layer, and such networks are termed feed-forward networks. Signals propagate in the oppo-
site direction in such networks, leading to recurrent networks and the development of the backpropagation algorithm. Both layers of neurons other than input and output are named ’hidden’ and,number of hidden layers varies based on the problem at hand. It becomes a deep neural networkwhen the number of hidden layer is greater than two or three. These deep neural networks can learnhow to solve problems, and the learning is known as DL. These deep neural networks and learningalgorithms are built to solve complex tasks such as machine vision, pattern recognition,171 7.3 Deep learning approaches for the disease diagnosis and treatmentidentification of objects, surveillance, classification of images for disease diagnosis, character rec-
ognition, speech/speaker recognition, etc. ( Vasuki & Govindaraju, 2017 ).
DL has evolved from AI and ML science. The last two subjects are focused on the new field of
ML: machines that learn from examples, and research on the neural network. ML includes design-ing and testing algorithms that allow a computer to extract (or learn) features from a dataset (sets
of examples). We need to know three words to comprehend what ML means: dataset, algorithm,
and function (
Kelleher, 2019 ). DL is an AI subfield, which focuses on developing massive models
of neural networks, which are capable of making correct data-driven decisions. DL is specificallydeveloped for the problems in which the data is challenging, and massive databases are available.DL is employed in the healthcare industry to process diagnostic images such as X-rays, CT, andMRI scans and to diagnose diseases. DL is now at the heart of self-driving vehicles, in which it isutilized for localization and navigation, motion planning and steering, and awareness of the envi-ronment, as well as driver status monitoring (
Kelleher, 2019 ).
A mathematical model that is (loosely) influenced by the structure of the brain is a DL network.
To understand DL, it is also helpful to provide an abstract understanding of what a mathematicalmodel is, how model parameters can be set, how we can merge (or compose) models, and how wecan use geometry to explain how data is interpreted by a model. The term DL defines a family ofneural network models, which have multiple layers of basic data processing units, known as neu-rons, in the network. NN, which contain several hidden layers of neurons are DL networks. Twoare the minimum number of hidden layers expected to be considered deep. Nevertheless, there aremore than two hidden layers in most DL networks. The key argument is that the depth of a network
is measured by the number of hidden layers, plus the output layer (
Kelleher, 2019 ).
7.3.3 Convolutional Neural Networks
CNNs are built to operate with grid-structured inputs that have robust spatial needs in local areas
of the grid. A 2-dimensional image is the clearest example of grid-structured data. Spatial depen-dencies are often demonstrated by this type of data, since neighboring spatial positions in an imagealso have identical pixel color values. The multiple colors are captured by an additional dimension,
which generates a 3-dimensional amount of input. Thus, the properties of a CNN are contingent on
spatial distances between each other. Specific cases of grid-structured data with different kinds ofrelations between adjacent items may also be treated as other types of sequential data such as text,time series, and sequences. The vast majority of CNN applications concentrate on imaging data,while these networks can still be used for all forms of spatial, temporal, and spatiotemporal data.A significant characteristic of image data shows a certain degree of invariance in localization thatis not the case in different grid-structured data types. CNNs, from geographic regions with identicalpatterns, tend to generate similar feature values. One strength of image data is that it is always pos-
sible to explain the effects of particular inputs on the feature illustrations in an intuitive way.
A process that is referred to as convolution is a crucial defining feature of CNNs. A convolutionprocess is a dot-product operation between a weight set structured by a grid and related grid-structured inputs received in the input volume from different spatial locations. This sort of proce-dure is useful for data, such as image data, with a high degree of spatial or other location. CNNsare thus described as networks, which utilize a convolutional operation in at least one layer, eventhough this operation is used by most CNNs in multiple layers (
Aggarwal, 2018 ).172 Chapter 7 Deep learning approaches for the cardiovascular diseaseCNN is composed of an input layer and hundreds of layers for feature recognition. Each of the fol-
lowing three operations is carried out by feature recog nition layers: Convoluti on, Pooling, Rectified
Linear Unit (ReLU). Convolution positions the ima ge in the image by convolution filters, which acti-
vate certain characteristics. In order to minimize t he volume of data to be dealt, pooling conducts non-
linear down sampling. The ReLU pre serves positive values and maps negative values to zero. The
classification layer is the one before the output lay er. It is an N-dimensional output fully connected
layer, N being the number of classes to be classifi ed. This layer generates an N-dimensional vector
with the probability that each component of the ve ctor belongs to one of the N classes of the input
image. To give the categorized output, the Softma x function is employed at the final output layer.
Thousands or millions of images have to be fed into t he network to achieve precise outcomes. With
multiple GPUs running in parallel, hi gher computing power is achieved ( Vasuki & Govindaraju, 2017 ).
7.4 Case study of a smartphone-based Atrial Fibrillation Detection
CVD, also known as heart disease, is the leading reason for health problems, claiming approxi-
mately 30% of total death worldwide ( Timmis et al., 2020 ). The prevalence of heart diseases is ris-
ing, mainly due to the growing population of elderly people and the continued pervasiveness ofcardiovascular risk factors (
Heidenreich et al., 2011 ). Heart diseases cause immense health and eco-
nomic burdens, resulting in high risks of morbidity and mortality worldwide ( Levenson, Skerrett, &
Gaziano, 2002 ). Early detection of critical heart disorders via advanced smart monitoring systems
could be of potential importance in preventing fatal incidents and reduce expensive interventionaltreatments. It will also lead to a better quality of patient care and savings for the healthcare sectorand the entire global society (
Bhavnani, Narula, & Sengupta, 2016 ).
Increased prevalence of CVD incidents has led governments to resume national efforts to rein-
force new accessible and affordable preventive strategies ( Schwalm, McKee, Huffman, & Yusuf,
2016 ). With the new detection and prevention policies, many lives are saved, and the burden of
immense health crises will decline, which will lead to a better economy, life quality, and prosperity.
In addition to preventive screening, novel technologies may provide improved care of existing dis-
eased patients. Therefore, it is a critically important task to seek more effective and innovative dis-ease or patient management solutions by which the quality of patient care will improve, and thecosts of healthcare services minimize (
Chaudhry et al., 2006 ).
Atrial fibrillation (AFib) is a common heart arrhythmia, known for its irregular and randomly
frequent appearances ( Waktare Johan, 2002 ). During the normal rhythm, or SR, the heart beats
with a regular rate ranging from 60 to 100 beats per minute. However, in the AFib condition, theheart muscle beats randomly and in an uncoordinated fashion (
Waktare Johan, 2002 ). AFib is a
chronic heart arrhythmia often diagnosed by interpreting electrocardiography (ECG) signals
(Kleiger & Senior, 1974 ). Prevalence of AFib rises with aging and occurs in almost 3% of
people older than 40 years and 70% of individuals between 65 and 85 years of age ( Chen, Yi, &
Cheng, 2018; Feinberg, Blackshear, Laupacis, Kronmal, & Hart, 1995; Go et al., 2001 ). About
40% of all strokes are due to AFib, and more than 200,000 deaths per year happen in Europe(
Martignani, Massaro, Biffi, Ziacchi, & Diemberger, 2020 ). AFib detection is a challenging task
because it can develop infrequent and asymptomatic. Undiagnosed AFib can cause the formation of173 7.4 Case study of a smartphone-based Atrial Fibrillation Detectiona blood clot(s), potentially leading to acute strokes, heart failure (HF), and other heart-related com-
plications ( Anter Elad & Callans David, 2009 ). Persistent AFib can lead to substantial morbidity,
mortality, and hospitalization costs ( Savelieva & Camm, 2000 ). Active prevention of brain strokes
requires continuous monitoring of atrial fibrillation indications.
Today, remarkable advances have been made in the development of smart monitoring devices
as they can leverage the recent improvements in biosensing developments, embedded or pervasive
health computing, and physiological data analytics ( Onodera & Sengoku, 2018 ). Modern smart-
phone devices can offer an easy-to-access non-invasive sensing solution for unobtrusive recordingof underlying physiological data to diagnose at-risk patients for early intervention. With the gain inpopularity of smartphones, it is possible to deploy the device’s built-in motion sensors for AFibdetection (
Hendriks, Gallagher, Middeldorp, & Sanders, 2018 ).
Recent developments of telecommunication systems, including Internet of Things (IoT) technol-
ogy, allows connections of various smart devices, enabling seamless transfer of data, along with
advanced data analysis and data storage platforms such as cloud computing and fog computing.
Smartphone-based AFib detection is one of the innovative application domains in IoT that inducesimmense interest from the healthcare sector, the research community, and the public division(
Kotecha & Kirchhof, 2017 ). Smartphone AFib apps sought to improve patients care, satisfaction,
and cost-efficiency in the healthcare industry.
Current mobile phone screening solutions are based on ECG ( Lau et al., 2013 ), pulse oximeter/PPG
(Lee, Reyes, McManus, Maitas, & Chon, 2013 ), and microelectromechanical (MEMS) technology
(Jaakkola et al., 2018a, 2018b ), allowing early detection of heart arrhythmia. These modern sensing
technologies intend to simplify and revolutionize personal monitoring by enabling individuals to track
their cardiovascular status thems elves. Recently, diffe rent smartphone and smartwatch modalities such
as AliveCore Kardia ( Albert & Myles, 2015 ), Apple Watch ( Perez et al., 2019 ), Samsung Simband
smartwatch ( Nemati et al., 2016 ), Withings smartwatch ( Tajrishi et al., 2019 ), as well as many others
with similar technologies have been established for AFib detection. These wea rable/mobile modalities
often require separate dedicated software/applicatio ns to facilitate long term or intermittent monitoring.
However, they pose two notable limitations. Such mod alities are high-priced and always require addi-
tional hardware hampering large-scale screening purposes.
CardioSignal (Precordior Oy, Turku, Finland) is a new cardiography platform which deploys
multidimensional built-in inertial measurement unit (IMU) sensors of smartphones for recordingcardiovascular mechanical activity.
Fig. 7.1 shows a CardioSignal app for smartphone detection of
cardiac disorders including heart arrhythmias. User-friendly and cost-effective IMU sensors basedon MEMS technology together with the advanced signal processing and data analytics are used inCardioSignal app to improve prevention of the cardiac disorders through early-stage detection andrapid determination of the characteristics of an acute cardiac event, e.g., heart arrhythmia, withgreater certainty. Measuring precordial vibrations has been explored as a technique for assessing
the mechanical condition of the heart (
Inan et al., 2015 ). The generic motion of the heart consists
of translation of the center of mass in three orthogonal directions and rotation about the center ofmass around three orthogonal axes (
Young & Axel, 1992 ). Small-scale accelerometer and gyro-
scope sensors allow six degree of freedom (6DoF) cardiac motion sensing in the three-dimensionalspace (
Jafari Tadi et al., 2017 ).
In this chapter we present biosignal processing and DL techniques to comprehensively assess
mechanical status of the heart from smartphone-derived data. The focus of this chapter is to174 Chapter 7 Deep learning approaches for the cardiovascular diseaseintroduce data-driven algorithms to decode cardiac motion pattern solely based on multidimensional
IMU data. We present data fusion and AI techniques to characterize and analyze physiological pat-terns derived from the MEMS sensors. The focus is to show the capability of modern data analytictechniques in the context of smartphone and MEMS technology to overcome existing challengesand improve obtained parameters estimated from mechanical cardiovascular monitoring while intro-ducing a new modality based on motion sensors, called mechanocardiography (MCG). The techni-
ques addressed in this chapter may be extensively used in the detection of other heart disorders
such as HF, myocardial infarction, and coronary artery diseases (CAD). Current use case considersAFib detection via retrospective MCG data from MODE-AF study (
Jaakkola et al., 2018a, 2018b ).
We introduced CardioSignal platform as the first medical-grade smartphone MCG product for self-monitoring with the envision that the emergence of 5G telecommunication may foster its widerusability worldwide. The present use case does not reflect the current in use data analytics/proces-sing frameworks for CardioSignal app in any form. The scientific insights proposed in this use caseare purely based on academic investigations in the field of smartphone MCG and data analysis.
7.4.1 Smartphone data acquisition
For smartphone MCG technology, b uilt-in IMUs include micro-sized accelerometers and gyroscopes—
both offering three channels of motion sensing—are used to measure translatio nal and rotational cardiac
movements, typically with a sampling frequency of 200 Hz ( Lahdenoja et al., 2018 ). A dedicated
FIGURE 7.1
CardioSignal smartphone detection of atrial fibrillation.175 7.4 Case study of a smartphone-based Atrial Fibrillation Detectionsmartphone application (app) is re quired, as a user-interface, to collect the raw data, visualize the mea-
surements, and facilitate decision making. Data pr ocessing can be performed either through online
processors—on a cloud server—or o ffline—by smartphones’ processor. CardioSignal is one of the pri-
mary medically approved applications which is comme rcially available for online data processing where
a cloud server is used for data storage, vis ualization, and arrhythmia detection only. Fig. 7.2 shows the
overall pipeline for smartphone MC G starting from data collection, on line/offline data p reparation, and
data-driven analytics based on CNNs. This section d escribes data acquisition p rocedure including col-
lection and placement of the smartphone.
Smartphone’s IMU sensors are sensitive to the lin ear and rotational displ acements and the sensing
mechanism in most of them is capacitance change r elated to the movement of a spring mass in multiple
orientations or axis ( Nihtianov & Luque, 2014 ). The smartphone is placed (longitudinally) on the sub-
ject’s chest (near to the body of s ternum), while the screen is facing upwards and the bottom edge of
the phone at the level of the lower chest. Smartphone -derived acceleration and angular velocity data are
obtained in three different orientations, x-,y-, and z-axis. The x-axis corresponds to the right-to-left lat-
eral, y-axis to the head-to-foot, and z-axis to the dorsoventrally aligned movements (see Fig. 7.2 ).
During the recording, the subjects are aske d to remain calm, silent, and motionless.
7.4.2 Biomedical signal processing
Biomedical signal processing starts with filtering each of the sensors channels, namely 3-axis accel-
erometer and 3-axis gyroscope. MCG signals include various forms of noise from intentional or
Smartphone App
Semi-local data processing and 
storageCardiac Mo/g415on 
Processor/Acquisi/g415on
Smartphone built-in iner/g415al measurement unit
Fail/Noise? AFibSCG/GCG signal Processing
Neural NetworksSignal Processing and AI
SR
Chest Vibra/g415ons
Rota/g415onal and transla/g415onalClassiﬁerXY
Z
FIGURE 7.2
Overall diagram of data acquisition and processing pipeline.176 Chapter 7 Deep learning approaches for the cardiovascular diseaseunintentional movements. Such noise can be discarded by using signal denoising methods to filter
out artifacts and noise. Denoising is one of the key steps of the analysis of MCG signals whichallows successive processes without losing relevant information. Signal denoising allows theremoval of white noise and signals offset, resulting in increased signal-to-noise ratio. Researcherstypically utilize digital filters to remove or decrease the undesirable signal components by trans-
forming the signals into another domain (
Akay, 2012 ).
Another noise removal technique is singular spectrum analysis which decomposes the signal
into its favorable and unwanted components ( Golyandina & Zhigljavsky, 2013 ). This approach is
consisted of two complementary stages, decomposition and reconstruction, and tends to be usefulwhen the frequency spectrum of noise may not be known a priori.
In addition to denoising, the pulse amplitude signal is obtained by computing the envelope of
the accelerometer and gyroscope signals. Several methods based on Hilbert transform (
Cizek,
1970 ), continuous wavelet transform ( Rioul & Duhamel, 1992 ), median filter ( Pratt, 2007 ), moving
average filter ( Akay, 2012 ), and Savitzky /C0Golay ( Orfanidis, 1996 ) filter are used to derived the
envelope of the MCG signals. These smoothing filters allow to discover important rhythm patternsin the signals while leaving out unimportant (i.e., noise) information. The goal of envelope detec-tion is to obtain low frequency pulsatile changes in sensors data so that it’s easier to see rhythmchanges in the MCG signals.
Following filtering and envelope detection, the resulting MCG axes/channels of each sensor are
divided into a shorter sequences of discrete segments. These segments or frames are obtained withoverlapping windows. A Hamming window is commonly used to facilitate the spectral analysis via
fast Fourier transform (FFT), e.g., in terms of spectral leakage, side lobe amplitude, and width of
the central peak (
Prabhu, 2014 ). Segmentation is a particularly important part of the data curation
and choosing the right window length is very critical as shorter or longer segments can result in dif-ferent outcomes. A suitable window length preserves the statistical properties of the signals that arealmost constant over time. The overlapping windowing phase, although bears more processingload, allows a smoother change of the parameters between the signal frames. This will make theprocessing in frequency domain more robust to noise (
Prabhu, 2014 ).Fig. 7.3 shows an example of
the bandbass and envelope filtered MCG signals during AFib mode.
Motion artifact removal in MCG signals is an incredibly challenging task and different methods
have been previously suggested ( Tadi et al., 2016 ) These methods include recognizing distorting
patterns from the MCG signals, either by calculting a power envelope of the signal using the rootmean quare (RMS) or power spectral density features of the signal with FFT and discard the noisycomponents or spectra which exceed the determined threshold automatically.
7.4.3 Prediction and classification
Detection of cardiac arrhythmia from mechanical signals is an innovative technology and is often
challenging due to the motion artifacts including intentional and unintentional movements. Visualcharacterization and interpretation of MCG waveforms are not as straightforward as in ECGs,mainly because mechanical waves are inherently complex, varying and less monomorphic as com-pared to the ECG waves. Advanced analytical techniques are, therefore, essential to thoroughlyexplore pathological patterns from the mechanical signals.177 7.4 Case study of a smartphone-based Atrial Fibrillation DetectionResearch efforts on arrhythmia detection using MCG technology are mainly focused on showing
rhythm irregularities either by rule-based algorithms, e.g., learning recurrent waveforms using auto-correlation (
Hurnanen et al., 2017 ), or ML techniques ( Lahdenoja et al., 2018; Mehrang et al.,
2019, 2020; Tadi et al., 2019 ). The latter considers various hand-engineered features such as heart
rate variability estimations, median absolute deviation of successive intervals, spectral entropy(
Rezek & Roberts, 1998 ), approximate and sample entropy ( Richman & Moorman, 2000 ), Shannon
entropy ( Shannon, 1948 ), turning point ratio, and many other features ( Tadi et al., 2019 ). The fea-
ture engineering is tailored to classic ML classifiers such as SVM with radial basis function ker-nels, random forests (RF) based on Bootstrap-aggregated (bagged) decision trees, XGBoost, robustboosting (RB), multilayer perceptron, and other types of classifiers to determine arrhythmic epi-sodes. These binary classifiers have been used to distinguish AFib from normal SR only; whilenoisy measurements as one of the major causes of false-positive (FP) detection are investigated byDL methods to avoid premature decision making.
To reduce the processing workload and inconclusive assessments caused by FP readings made
by sMCG, data-driven methods based on deep neural networks are proposed to classify SR, AFib,
FIGURE 7.3
smartphone-derived MCG signals. The left-side column shows filtered acceleration and angular velocity
signals while the right-side columns show the corresponding envelope signals.178 Chapter 7 Deep learning approaches for the cardiovascular diseaseand noisy recordings from a short smartphone recording. DNNs include CNN and RNN to identify
dominant spatial and spatiotemporal characteristics of physiological signals mimicking theapproach a trained human would take in identifying irregular cardio-mechanical signals. This chap-ter introduces a DL framework which allows data-driven processing of MCG data, namely accelera-tion and angular velocity signals. This approach extends previous efforts towards efficient and fast
processing of sMCG signals among the full population of the elderly patients.
Deep convolutional-recurrent neural networks are recently developed to characterize arrhythmia
pattern from the MCG biosignals. The artificial neural network takes as input the filtered and enve-lope signals obtained from accelerometer and gyroscope sensors and outputs one prediction. Theneural model takes as input only the sensors data, and no other patient metadata or signal-relatedfeatures. This dual-sensor sequential model concatenates both filtered and envelope signals into oneinput tensor before feeding to the network.
The network architecture shown in
Fig. 7.4 resembles densely connected residual network archi-
tecture inspired from three previously designed networks ( Hannun et al., 2019; He, Zhang, Ren, &
Sun, et al., 2015a, 2015b; Huang, Liu, van der Maaten, & Weinberger, 2018 ). A residual neural
network (ResNet) is an ANN which mimics pyramidal cells in the cerebral cortex. Residual neuralnetworks are based on certain connections, or shortcuts to skip some layers. Deep ResNet modelsare implemented with multiple shortcut connections that contain nonlinearities (ReLU) and batchnormalization in between (
He et al., 2015a, 2015b ). Networks with several densely connected
FIGURE 7.4
Dense residual learning architecture (A) for smartphone MCG data processing. Convolutional-max pool (Conv-
Max) blocks (B) are sequentially deployed following the initial Conv-Max block (C).179 7.4 Case study of a smartphone-based Atrial Fibrillation Detectionshortcut blocks are referred to as DenseNets ( Huang et al., 2018 ). In the context of residual neural
networks, a nonresidual network may be described as a plain network.
The smartphone MCG model of densely connected residual nets has major differences including
a new configuration of shortcut connections between the residual blocks, namely a convolutional-max pool block where 1D convolution and max pool operations are sequentially used (see
Fig. 7.4 B and C). Additionally, densely residual learning is adopted to every three stacked residual
blocks in each stage. The baseline architectures are the same as residual nets, except that convolu-tional layers consider 1-dimensional data plus max pooling, and a shortcut connection is added toevery stack of three residual blocks forming a dense residual function.
This network consists of residual blocks with two c onvolutional layers in each block and one convo-
lution in the shortcut connection. The first block com prises an extra primitive convolution layer as well
as a convolution layer in the shortcut connection. Ba tch normalization and a lea ky rectified linear acti-
vation (ReLU) are also used at the beginning of each c onvolutional layer adopting the preactivation
block design. Dropout w ithin each block and after the nonlin earity with a probability of .3 is also
deployed. Dropout is a common regularization technique for deep neural networks and has proven tobe phenomenally successful by giving 1% /C02% accuracy boost. Batch nor malization, Dropout, and
leaky ReLU allow stable learning process and prevent vanishing/exploding gradient.
Following the convolutional layers, a recurrent network consisting of two LSTM layers are
used. Batch normalization and leaky ReLU are used on the output of each layer too. LSTM net-works are a type of recurrent neural network capable of learning time dependencies. The CNNlayers can be highly effective at automatically extracting and learning hidden features from bio-
medical signal sequence. Convolutional layers can be used in a hybrid mode with recurrent layers
such as LSTMs to interpret subsequences of input signals. This hybrid model is called a CNN-LSTM. The output layer of the LSTM block is then fed into fully connected neural network blockwith four layers. A ReLU activation function is used on the output of each layer. The last fully con-nected Softmax layer calculates a probability distribution over the three output classes, AFib, SR,or noise. Softmax is used as the last activation function of a neural network and returns decimalprobabilities for each class.
The convolutional layers of residual networks are typically initialized with randomly initializa-
tion weights. Glorot and He initialization method are commonly used in DL to alleviate the
unstable gradients problem (
Glorot & Bengio, 2010; He et al., 2015a, 2015b ). Model optimization
for training deep neural network is critical as it can be sometimes slow due to poor customizationof the network. Several ways are then considered to speed up the training phase such as random ini-tialization, proper choice of activation functions, using Batch Normalization, and ad-hoc fine tuningof the hyperparameters. In addition, fast optimizing strategies are important to speed up the trainingphase, resulting in rapid and stable learning processes. The most popular optimization algorithmsused by the DL community are momentum optimization, Nesterov Accelerated Gradient, AdaGrad,
RMSProp, and Adam and Nadam optimization (
G´eron, 2019 ). The smartphone MCG model
deploys Adam ( Kingma & Ba, 2017 ) as the optimizer and considers categorical cross-entropy as
the loss function with a mini-batch size of 16. The categorical cross-entropy is preferred due to thenature of MCG multiclass classification problem and the way the labels are provided to the net-work—e.g., one-hot encoded labeling.
The hyperparameters, such as number of convolutional layers, the size and number of the con-
volutional filters, and optimization algorithm are chosen in a manual tuning manner in the MCG180 Chapter 7 Deep learning approaches for the cardiovascular diseaseneural model. However, there are other automated techniques to explore a search space much
more efficiently than manually. These methods work like zooming into a region of the spacewhich has higher probability of returning better outcomes in a shorter time. Some Python librariesto optimize hyperparameters DL models are Hyperopt, Keras tuner, Scikit-Optimize (skopt),Hyperband, Spearmint, and many other custom-designed tools available in the DL community
(
G´eron, 2019 ).
7.4.4 Experimental data
Experimental data for this use case study is consisted of retrospective (de-identified) measurements
from 300 patients including 150 patients with AFib as the prevalent heart rhythm. An Androidsmartphone dedicated for clincal research was used for the trial and each measurement tookapproximately three minutes. Two sets of measurement senarios including physician-applied and
patient-applied were planned to collect data from the patients. Among them, 182 patients (86 AFib)
proceeded with two recordings, one physician-applied and one patient-applied. The remainingpatients (n 5118) were either nervous, physically in poor condition, or not interested to perform
the self-applied measurements. Those measurements in which either the patient or physician failedto obtain a valid recording, for example, due to intentional or unintentional movements, lack ofconcentration, late/poor placement, and phone drop were labeled as noise. In total, 827 MCG mea-surements, of which 345 recordings labeled as noise, were considered in this use case study. Datacollection was strictly followed according to the Helsinki declaration in all phases and the study
was reviewed by the Ethical Committee of the Hospital District of South-Western Finland before
data collection. A reference ECG (Philips IntelliVue MX40) was also acquired simultaneously withthe sMCG recordings. ECG meaurements allowed accurate assessment of the cardiac rhythm eitheras SR, AFib, or other as defined by two independent cardiologists.
Table 7.1 shows detailed clinical
characteristics of the study patients.
DL projects typically require that the dataset is divided into three subsets, namely train, valida-
tion, and test sets. The train set is used to fit the parameters (e.g., weights) of the model. The modelis trained on the training dataset using a supervised learning method. The validation data is used to
tune the models’ parameters, while the test data is deployed to assess the error rate of the tuned
model at the final stage (
G´eron, 2019 ). Random sampling of the biomedical measurements in only
one of the subsets (train/valid/test) is required to avoid any data leakage, and overfitting. Therefore,each MCG patient’s measurment(s) is put into only one category, regardless of being captured bythe physician or the patient him/her-self. This way of data sampling is a standard strategy to pre-vent overfitting. However, making data subsets will reduce the amount of the labeled training datawhich can consequently influence the ending classification results. Data augmentation is a conve-nient step to improve classification performance when large-scale labeled training dataset is not
available (
Shorten & Khoshgoftaar, 2019 ). Since the availability of labeled MCG signals for train-
ing the DL models is limited to the above retrospective measurements, a data augmentation step isused to leverage data insufficiency by transforming the existing MCG data to create a new one, yetmaintaining the correct labels. Researchers typically consider simple augmentation techniques suchas adding noise, flipping, rotating, and permutating biomedical signals (
Um et al., 2017 ).
Permutation method is a simple form of signal generation in which the temporal location of within-window events is randomly perturbed. Adding Gaussian noise is another technquie which can give181 7.4 Case study of a smartphone-based Atrial Fibrillation Detectionsimilar looking biosignals with extra noise components. Rotation simply inverts the sign of the sen-
sor readings without changing the labels. It is also possible to create a label-preserving transforma-tion by which simultaneous combination of rotation, Gaussian noise, and permutation methods areapplied on the MCG signals (
Um et al., 2017 ). In total, 28408 augmented MCG segments of 10 s
are generated from the two IMU sensors.
7.4.5 Performance evaluation measures
A common way to evaluate the performance of a deep neural network is to look at the error metrics
such as F 1score, accuracy, sensivity, specificity, precision, and other relevant measures. The evalu-
taion metric introduced in this study is based on PhysioNet Challenge 2017 ( Clifford et al., 2017 )
and the goal is to measure how well the model performs in distinguishing biosignals on the testsubset. Classification metrics for the assessment of multiclass learning in MCG AFib detectionstudy are calculated according to
Table 7.2 .Table 7.1 Clinical characteristics of the subjects with sinus rhythm (SR) or atrial fibrillation
(AFib) during the mechanocardiography (MCG) recordings.
Clinical variable SR (n 5150) AFib (n 5150) P-value
Age, y 74.5 (73.0 /C076.1) 75.0 (73.5 /C076.6) .660
Female sex 66 (44.0) 66 (44.0) 1.000Chest circumference, cm 103 (101 /C0104) 105 (103 /C0107) .043
BMI, kg/m
227.5 (26.7 /C028.2) 29.0 (28.1 /C029.9) .013
History of heart failure 24 (16.0) 73 (48.7) ,.001
Hypertension 101 (67.3) 103 (68.7) .902
History of ischemic stroke 12 (8.0) 10 (12.7) 0.255Coronary artery disease 90 (60.0) 54 (36.0) ,.001
History of AMI 68 (45.3) 26 (17.3) ,.001
History of atrial fibrillation 30 (20.0) 150 (100) ,.001
Heart rate, beats/min 70.8 (68.7 /C073.0) 88.0 (84.6 /C091.4) ,.001
Respiratory rate 16.9 (16.2 /C017.5) 19.2 (18.2 /C020.2) ,.001
Systolic blood pressure, mmHg 145 (141 /C0149) 137 (134 /C0141) 0.003
Diastolic blood pressure, mmHg 70.0 (67.7 /C072.2) 80.9 (77.7 /C084.1) ,.001
LBBB configuration in ECG 15 (10.0) 12 (8.0) .687RBBB configuration in ECG 10 (6.7) 10 (6.7) .597Edema in chest X-ray 38 (37.6) 67 (54.5) .015ProBNP (pg/mL)
a1095 [3596] 2965 [5603] ,.001
Patients with SVES during MCG 47 (31.3) 0 (0.0) ,.001
Patients with VES during MCG 52 (34.7) 73 (49.0) .014
AMI , indicates acute myocardial infarction; BMI, body mass index; ECG , electrocardiogram; LBBB , left bundle branch block;
LVEF , left ventricular ejection fraction; proBNP , pro-brain natriuretic peptide; RBBB , right bundle branch block; SVES ,
supraventricular extrasystolia; VES, ventricular extrasystolia.182 Chapter 7 Deep learning approaches for the cardiovascular diseaseF1measure for each class is obtained as an average of:
F1AF523AaPA1Pa
F1SR523SsPS1Ps
F1noise523NnPN1Pn
The final score is calculated as an arithmetic mean of the per-class F 1scores as described in following:
Micro F15F1AF1F1SR1F1noise
3
Researchers often consider two-sided 95% confidence intervals (CI) for an aggregate measure
of the model performance and network stability, and to be more conservative for accuracy. The CIfor the performance metrics is obtained with ntimes replications of the entire train, validation, and
test process. Other metrics such as accuracy, precision, recall, specificity, and Cohen’s kappa coef-
ficient are also calculated to evaluate the performance of the DL algorithm in detection of AFib.These classification metrics in a simple form are calculated by counting the total number of trueand false predictions with respect to the true labels. True positives is counted when the classifiercorrectly predicts a disease group, while true negatives happens when the model predicts healthygroup correctly. If the model wrongly predicted a negative class as positive, a FP is counted and ifpredicted as negative when it is actually positive, a false negative is counted. Interested reader canrefere to (
G´eron, 2019 ) for the detailed information on the model evaluation methods.
7.4.6 Experimental results
The primary goal of this work is to establish a segment-based model for MCG signal classification,
which can be implemented using DNNs in automated detection of heart diseases using smart-
phones. The contribution of this chapter is to find a model which can learn the rhythm and noisecharacteristics of the MCG signals. The performance of the DL methods was evaluaated and theexperimental results are given in the following. The model revealed an overall accuracy of 92%(95% CI: 91 /C093). We primarily considered 10- and 20-s window lengths to evaluate the perfor-
mance variability of the model between the two fragments. However, the longer fragment did notshow any significant improvement in the performance of the deep model. Therefore, we report theTable 7.2 Definition of parameters for scoring.
SR AFib Noise Total
SR S s Sa SnPS
AFib A s Aa AnPA
Noise N s Na NnPN
TotalPsPaPn183 7.4 Case study of a smartphone-based Atrial Fibrillation Detectionmodel performance for only 10-s fragments here. The model successfully classified AFib, SR, and
noise cases resulting in an averaged sensitivity/recall of 87% (95% CI: 86 /C088) and successfully in
an averaged sensitivity/recall of 87% (95% CI: 86 /C088) and specificity of 93% (95% CI: 92 /C094)
for the corresponding segment. The respective micro and macro F 1scores of the algorithm were
87% and 80%, while the precision or positive predictive value was 78% (95% CI: 76 /C079) and the
negative predictive value 92.5% (95% CI: 92 /C093). The kappa coefficient of the method was 78%
showing a substantial agreement in classification between the MCG algorithm and visual interpreta-tions. The performance of current smartphone MCG model-based on deep neuarl networks com-pares favorably to the various ECG (single-lead) algorithms in detection of AFib. In 2017Physionet Challenge (
Clifford et al., 2017 ), where a total of 12,186 ECGs were used, the overall
macro F1score for the top 11 algorithms was 83% 62%. Our study scored an average macro F1
measure of 80%.
7.5 Discussion
With the recent advancements of IoT and smartphon e systems, remote processing of health data has
become more popular than it was in the past. Smart devices are fast becoming available globally, evenamong developing countries, and the number of eld erly users is also growing at a rapid rate in those
countries, where low-cost healthcar e solutions are crucial. A personal smart monitoring program such
as CardioSignal allows seamless as sessment of the health risks by early detection of cardiovascular dis-
orders. The readily available motion sensors in smar t devices provide an unprecedented opportunity for
the cost-effective screening of heart diseases if they can be d iagnosed via early symptoms.
In addition to smartphones, IMU sensors can be embedded into a wearable patch or
implantable system for long-term monitoring. These sensors allow seamless measurement of themyocardium’s mechanical movement and are not subjected to electrical or implantable stimulatinginterventions. This allows continuous cardiac function monitoring via the integration of motion sig-nal processing tools in devices like pacemakers and cardioverter-defibrillators.
Various smartphone and smartwatch-based ECG and/or optical sensor recorders are commer-
cially available to check the heart activity and have shown decent performances to detect irregulari-ties. However, they are often expensive and sometimes inconvenient and with the growingprevalence of AFib in an aging population this is an important usability consideration for healthcaresystems. Rhythm classification using mechanical signals has been the focus of ML investigationsusing a variety of techniques such as linear and quadratic discriminant analysis, support vectormachines, RB, XGboost, and RF. However, most of those studies relied on the derivation of vibra-tional pattern features to classify different rhythm types as well as on feature selection techniques
for further generalization and performance improvements.
A clear advantage of methods based on NN is that they do not necessarily require direct feature
extraction from the physiological signals and perform complex nonlinear operations. One drawbackof DL methods is that they are computationally expensive and require large datasets for training.As with other clinical DL applications, the key challenge for MCG analysis is not only computa-tional resources but also the availability of large-scale annotated (labeled) datasets with the requiredclinical information.184 Chapter 7 Deep learning approaches for the cardiovascular diseaseSmartphone as a new modality demands minimum professional knowledge and coordination so
that people can quickly collect biosignals in an everyday manner. Self-screening allows patients tocollect their vital signals themselves outside the hospital environment. With the development ofsmart health technologies, the self-screening has become more popular in the elderly patients.Smartphone MCG vision is to better serve the society by facilitating early detection and interven-
tions of CVDs. Smartphones can promptly collect and analyze patient’s data, enabling people to
get the same quality of diagnostic medical services without shuttling back and forth between hospi-tals and their homes. Smart healthcare not only reduces the immense burden of social challenges,but it also lowers the economic burden on end users. However, the collection and AI-based proces-sing of massive data still have concerning data privacy risks, which may lead to various ethical pro-blems and endanger the vital interests of users. According to the Ethics Guidelines for TrustworthyAI guidelines, trustworthy AI should be: (1) lawful—respecting all applicable laws and regulations,(2) ethical—respecting ethical principles and values and (3) robust—both from a technical perspec-
tive while considering its social environment (
AI HLE, 2020 ). In this chapter, we explore some of
the key questions prompted by ethical AI in relation to information privacy.
Even though the use of analytical tools in medicine, especially in cardiovascular medicine, is
growing, there still serious challenges about using clinical data and especially protected healthinformation about patients (
Mathur, Srivastava, Xu, & Mehta, 2020 ). In Europe, data privacy laws
become more stringent with the establishment of General Data Protection Regulations (GDPR).GDPR defines a body of legal safeguards on digital data protection and privacy for all individualswithin the European Union and the European Economic Area. The GDPR outlines the principles
associating with the processing of personal data. It defines, for example, the usage and storage prin-
ciples, the lawfulness of data processing, and several special conditions on personal data usage(REGULATION (EU) 2016/679). With the increasing use of remote data processing in health care,especially with AI tools dealing with electronic medical records of the patients, the GDPR is trulyrelevant as it recognizes data concerning health as a special category of data and supplies a defini-tion for health data for data protection purposes.
The foundations introduced by the GDPR are consistent and applicable to health data as well.
However, specific safeguards for personal health data and for a definitive interpretation of the rules
that allow comprehensive protection of such data must yet be addressed promptly. Processes that
accelerate innovation and better quality in healthcare, such as AI or mobile health tools, requirerobust data protections to support the trust and confidence of the users in the rules designed to pro-tect their data.
Under the new laws, personal data extends to the type of data that is collected about people,
including online identifiers, economic, cultural, and health information. As recommended by theGDPR, personal data must continue to be processed according to the six data protection principles:(1) Processed legally, fairly, and transparently, (2) Collected for specific legal goals, (3) Adequate,
relevant, and limited to necessity, (4) Accurate and up to date, (5) Stored if needed and (6) Ensure
security, integrity, availability, and confidentiality (REGULATION (EU) 2016/679).
However, there are several limitations to use AI/ML in healthcare. For example, in cardiovascu-
lar medicine, dichotomic and improper calibration are known problems of AI-based ML methods(
Johnson et al., 2018 ). Furthermore, AI-based systems need to address data privacy concerns
(Johnson et al., 2018; Powles & Hodson, 2017 ) and data integrity to prevent faulty data curation,
selection bias, and historical stereotypes in data analysis.185 7.5 Discussion7.6 Conclusion
The healthcare industry is facing different concerns associated with disease diagnosis and cost-
effective delivery of services ( Awan et al., 2018 ). One of the crucial needs of a healthcare system
is to provide the patient with adequate services by the utilization of medical records, lifestylehabits, and any variability of molecular traits. Several intelligent IoT have been developed basedon AI and data-driven methods to overcome these challenges. For disease diagnosis and treatment,
modern and automated technology-enabled Smart Healthcare Platforms have been employed.
Healthcare services are delivered by Smart Healthcare Platforms utilizing smart phone/watch andconnected technologies. Patient information is gathered in a file for the purpose of disease diagno-sis and treatment and offering timely care, in particular for the areas where urgent intervention is achallenge. This chapter introduces and explores the use of DL approaches to detect diseases suchas heart attacks, brain cancers, diabetes, cardiac disease, and prediction of epileptic seizures in thisperspective.
Moreover, this chapter addresses the globally prominent issues of DL models in detecting
CVDs, for example arrhythmia, with multidimensional motion sensors using a smartphone applica-
tion. Atrial fibrillation is one of the main reasons for heart-related disabilities, and other comorbid-ities such as stroke, HF, and other complications. The results of the presented use case studytogether with a body of earlier investigations in the field acknowledge the clinical potentials of thecardiac motion sensing to reliably find signs of heart disorders, without any external complemen-tary equipment.
Due to the broad accessibility and availability of smart devices, it is possible to develop a uni-
versal diagnosis system as a part of efficient global prevention and detection strategies to deal with
heart diseases. Smartphone MCG devices may offer a reliable and cost-efficient screening and
monitoring possibility for AFib compared with other monitoring modalities.
References
Aggarwal, C. C. (2018). Neural networks and deep learning . Springer.
AI HLE (2020, Jul. 17). Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment.
Shaping Europe’s digital future - European Commission .https://ec.europa.eu/digital-single-market/en/
news/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment . Accessed 15.01.21.
Ahn, T., Goo, T., Lee, C., Kim, S., Han, K., Park, S., & Park, T. (2018). Deep learning-based identification of
cancer or normal tissue using gene expression .In 2018 IEEE international conference on bioinformatics
and biomedicine (BIBM) , 1748 /C01752.
Akay, M. (2012). Biomedical signal processing . Academic Press.
Alanazi, H. O., Abdullah, A. H., Qureshi, K. N., & Ismail, A. S. (2018). Accurate and dynamic predictive
model for better prediction in medicine and healthcare. Irish Journal of Medical Science ,187(2),
501/C0513. Available from https://doi.org/10.1007/s11845-017-1655-3 .
Albert, D.E. & Myles, C.A. (2015, Mar. 26). Smartphone and ecg device microbial shield .
US20150087952A1.
Ali, F., et al. (2020). A smart healthcare monitoring system for heart disease prediction based on ensemble
deep learning and feature fusion. Information Fusion ,63, 208 /C0222.186 Chapter 7 Deep learning approaches for the cardiovascular diseaseAllen, J. (2007). Photoplethysmography and its application in clinical physiological measurement.
Physiological Measurement ,28(3), Art. 3.
Anter, E., Jessup, M., & Callans, D. J. (2009). Atrial fibrillation and heart failure. Circulation ,119(18),
2516 /C02525. Available from https://doi.org/10.1161/CIRCULATIONAHA.108.821306 .
Asiri, N., Hussain, M., Al Adel, F., & Alzaidi, N. (2019). Deep learning based computer-aided diagnosis sys-
tems for diabetic retinopathy: A survey. Artificial Intelligence in Medicine ,99, 101701.
Attia, Z. I., et al. (2019a). Prospective validation of a deep learning electrocardiogram algorithm for the detec-
tion of left ventricular systolic dysfunction. Journal of Cardiovascular Electrophysiology ,30(5), 668 /C0674.
Attia, Z. I., et al. (2019b). An artificial intelligence-enabled ECG algorithm for the identification of patients
with atrial fibrillation during sinus rhythm: A retrospective analysis of outcome prediction. The Lancet ,
394(10201), 861 /C0867.
Awan, K.M., Ali, A., Aadil, F., & Qureshi, K.N. (2018, Feb.). Energy efficient cluster based routing algorithm
for wireless sensors networks. In 2018 International conference on advancements in computational
sciences (ICACS) (pp. 1 /C07). doi: 10.1109/ICACS.2018.8333486.
Bagga, P., & Hans, R. (2015). Applications of mobile agents in healthcare domain: A literature survey.
International Journal of Grid and Distributed Computing ,8(5), 55 /C072.
Bhavnani, S. P., Narula, J., & Sengupta, P. P. (2016). Mobile technology and the digitization of healthcare.
European Heart Journal ,37(18), 1428 /C01438. Available from https://doi.org/10.1093/eurheartj/ehv770 .
Caball ´e, N. C., Castillo-Sequera, J. L., Go ´mez-Pulido, J. A., Go ´mez-Pulido, J. M., & Polo-Luque, M. L.
(2020). Machine learning applied to diagnosis of human diseases: A systematic review. Applied Sciences ,
10(15), 5135.
Cardiovascular diseases. https://www.who.int/westernpacific/health-topics/cardiovascular-diseases . Accessed
18.01.21.
Chaudhry, B., et al. (2006). Systematic review: Impact of health information technology on quality, efficiency,
and costs of medical care. Annals of Internal Medicine ,144(10), 742 /C0752. Available from https://doi.org/
10.7326/0003-4819-144-10-200605160-00125 .
Chen, Q., Yi, Z., & Cheng, J. (2018). Atrial fibrillation in aging population. Aging Medicine ,1(1), 67 /C074.
Available from https://doi.org/10.1002/agm2.12015 .
Choi, E., Schuetz, A., Stewart, W. F., & Sun, J. (2017). Usi ng recurrent neural network m odels for early detection
of heart failure onset. Journal of the American Medical Informatics Association: JAMIA ,24(2), 361 /C0370.
Choi, H., Ha, S., Im, H. J., Paek, S. H., & Lee, D. S. (2017). Refining diagnosis of Parkinson’s disease with
deep learning-based interpretation of dopamine transporter imaging. NeuroImage: Clinical ,16, 586 /C0594.
Available from https://doi.org/10.1016/j.nicl.2017.09.010 .
Cizek, V. (1970). Discrete Hilbert transform. IEEE Transactions on Audio and Electroacoustics ,18(4),
340/C0343. Available from https://doi.org/10.1109/TAU.1970.1162139 .
Clifford, G. D., et al. (2017). AF classification from a short single lead ECG recording: The PhysioNet/com-
puting in cardiology challenge 2017. Computers in Cardiology ,44, Accessed: Jan. 15, 2021. [Online].
Available. Available from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978770/ .
Colilla, S., Crow, A., Petkun, W., Singer, D. E., Simon, T., & Liu, X. (2013). Estimates of current and future
incidence and prevalence of atrial fibrillation in the US adult population. The American Journal of
Cardiology ,112(8), 1142 /C01147.
Daniels, M., & Schroeder, S. A. (1977). Variation among physicians in use of laboratory tests II. Relation to
clinical productivity and outcomes of care. Medical Care , 482 /C0487.
Deng, L., & Yu, D. (2014). Deep learning: Methods and applications. Foundations and Trends in Signal
Processing ,7(3/C04), 197 /C0387. Available from https://doi.org/10.1561/2000000039 .
Dick, R. S., Steen, E. B., & Detmer, D. E. (1997). The computer-based patient record: An essential technology
for health care . National Academies Press.187 ReferencesDubrovina, A., Kisilev, P., Ginsburg, B., Hashoul, S., & Kimmel, R. (2018). Computational mammography
using deep neural networks. Computer Methods in Biomechanics and Biomedical Engineering: Imaging &
Visualization ,6(3), 243 /C0247.
Faust, O., Hagiwara, Y., Hong, T. J., Lih, O. S., & Acharya, U. R. (2018). Deep learning for healthcare appli-
cations based on physiological signals: A review. Computer Methods and Programs in Biomedicine ,161,
1/C013.
Feinberg, W. M., Blackshear, J. L., Laupacis, A., Kronmal, R., & Hart, R. G. (1995). Prevalence, age distribu-
tion, and gender of patients with atrial fibrillation: Analysis and implications. Archives of Internal
Medicine ,155(5), 469 /C0473. Available from https://doi.org/10.1001/archinte.1995.00430050045005 .
Gao, F., et al. (2019). DeepCC: A novel deep learning-based framework for cancer molecular subtype classifi-
cation. Oncogenesis ,8(9), 1 /C012.
G´eron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and
techniques to build intelligent systems (Second edition). Sebastopol, CA: O’Reilly Media, Inc.
Glorot, X. & Bengio, Y. (2010). Understanding the diffficulty of training deep feedforward neural networks
(p. 8).
Go, A. S., et al. (2001). Prevalence of diagnosed atrial fibrillation in adults: National implications for rhythm
management and stroke prevention: The AnTicoagulation and risk factors in atrial fibrillation (ATRIA)
study. JAMA: The Journal of the American Medical Association ,285(18), 2370 /C02375. Available from
https://doi.org/10.1001/jama.285.18.2370 .
Golyandina, N., & Zhigljavsky, A. (2013). Singular spectrum analysis for time series . Springer Science &
Business Media.
Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). 2 Deep learning (vol. 1). Cambridge: MIT
press.
Grimson, J., Stephens, G., Jung, B., Grimson, W., Berry, D., & Pardon, S. (2001). Sharing health-care records
over the internet. IEEE Internet Computing ,5(3), 49 /C058.
Gunduz, H. (2019). Deep learning-based Parkinson’s disease classification using vocal feature sets. IEEE
Access ,7, 115540 /C0115551.
Hannun, A. Y., et al. (2019). Cardiologist-level arrhythmia detection and classification in ambulatory electro-
cardiograms using a deep neural network. Nature Medicine ,25(1). Available from https://doi.org/10.1038/
s41591-018-0268-3 , Art. no. 1.
He, K., Zhang, X., Ren, S., & Sun, J. (2015a, Feb.). Delving deep into rectifiers: Surpassing human-level per-
formance on ImageNet classification . ArXiv150201852 Cs. [Online]. Available: http://arxiv.org/abs/
1502.01852 . Accessed 15.01.21.
He, K., Zhang, X., Ren, S., & Sun, J. (2015b, Dec.). Deep residual learning for image recognition .
ArXiv151203385 Cs. [Online]. Available: http://arxiv.org/abs/1512.03385 . Accessed 15.01.21.
Heidenreich, P. A., et al. (2011). Forecasting the future of cardiovascular disease in the United States: A policy
statement from the American Heart Association. Circulation ,123(8), 933 /C0944. Available from https://doi.
org/10.1161/CIR.0b013e31820a55f5 .
Hendriks, J. M., Gallagher, C., Middeldorp, M. E., & Sanders, P. (2018). New approaches to detection of atrial
fibrillation. Heart (British Cardiac Society) ,104(23), 1898 /C01899. Available from https://doi.org/10.1136/
heartjnl-2018-313423 .
Hickey, K. T., Riga, T. C., Mitha, S. A., & Reading, M. J. (2018). Detection and management of atrial fibrilla-
tion using remote monitoring. The Nurse Practitioner ,43(3), 24.
Huang, G., Liu, Z., van der Maaten, L., & Weinberger, K.Q. (2018, Jan.). Densely connected convolutional
networks . ArXiv160806993 Cs. [Online]. Available: http://arxiv.org/abs/1608.06993 . Accessed 15.01.21.
Huang, M.-J., Chen, M.-Y., & Lee, S.-C. (2007). Integrating data mining with case-based reasoning for chronic
diseases prognosis and diagnosis. Expert Systems with Applications ,32(3), 856 /C0867.188 Chapter 7 Deep learning approaches for the cardiovascular diseaseHurnanen, T., et al. (2017). Automated detection of atrial fibrillation based on time-frequency analysis of seis-
mocardiograms. IEEE Journal of Biomedical and Health Informatics ,21(5), 1233 /C01241. Available from
https://doi.org/10.1109/JBHI.2016.2621887 .
Inan, O. T., et al. (2015). Ballistocardiography and seismocardiography: A review of recent advances. IEEE
Journal of Biomedical and Health Informatics ,19(4), 1414 /C01427. Available from https://doi.org/10.1109/
JBHI.2014.2361732 .
Jaakkola, J., et al. (2016). Stroke as the first manifestation of atrial fibrillation. PLoS One ,11(12), e0168010.
Jaakkola, J., et al. (2018a). Mobile phone detection of atrial fibrillation with mechanocardiography: The
MODE-AF study (Mobile Phone Detection of Atrial Fibrillation). Circulation ,137(14), 1524 /C01527.
Available from https://doi.org/10.1161/CIRCULATIONAHA.117.032804 .
Jaakkola, J., et al. (2018b). Mobile phone detection of atrial fibrillation: The mode-AF study. Journal of the
American College of Cardiology ,71(11S), pp. A410 /C0A410.
Jafari Tadi, M., et al. (2017). Gyrocardiography: A new nnon-invasive monitoring method for the assessment
of cardiac mechanics and the estimation of hemodynamic variables. Scientific Reports ,7(1). Available
from https://doi.org/10.1038/s41598-017-07248-y , Art. 1.
Jiao, Z., Gao, X., Wang, Y., & Li, J. (2018). A parasitic metric learning net for breast mass classification based on
mammography. Pattern Recognition ,75,2 9 2 /C0301. Available from https://doi.org/10.1016/j.patcog.2017.070.008 .
Johnson, K. W., et al. (2018). Artificial intelligence in cardiology. Journal of the American College of
Cardiology ,71(23), 2668 /C02679. Available from https://doi.org/10.1016/j.jacc.2018.03.521 .
Jordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. Science (New
York, N.Y.) ,349(6245), 255 /C0260.
Kalantari, A., Kamsin, A., Shamshirband, S., Gani, A., Alinejad-Rokny, H., & Chronopoulos, A. T. (2018).
Computational intelligence approaches for classification of medical data: State-of-the-art, future challengesand research directions. Neurocomputing ,276,2/C022.
Kamaleswaran, R., Mahajan, R., & Akbilgic, O. (2018). A robust deep convolutional neural network for the
classification of abnormal cardiac rhythm using single lead electrocardiograms of variable length.Physiological Measurement ,39(3), 035006.
Kelleher, J. D. (2019). Deep learning . Mit Press.
Kharazmi, P., Zheng, J., Lui, H., Jane Wang, Z., & Lee, T. K. (2018). A computer-aided decision support sys-
tem for detection and localization of cutaneous vasculature in dermoscopy images via deep feature learn-
ing.Journal of Medical Systems ,42(2), 33. Available from https://doi.org/10.1007/s10916-017-0885-2 .
Kingma, D.P. & Ba, J. (2017, Jan.). Adam: A method for stochastic optimization . ArXiv14126980 Cs.
[Online]. Available: http://arxiv.org/abs/1412.6980 . Accessed 15.01.21.
Kleiger, R. E., & Senior, R. M. (1974). Longterm electrocardiographic monitoring of ambulatory patients with
chronic airway obstruction. Chest ,65(5), 483 /C0487. Available from https://doi.org/10.1378/chest.65.5.483 .
Kononenko, I. (2001). Machine learning for medical diagnosis: History, state of the art and perspective.
Artificial Intelligence in Medicine ,23(1), 89 /C0109.
Kotecha, D., & Kirchhof, P. (2017). ESC apps for atrial fibrillation. European Heart Journal ,38(35),
2643 /C02645. Available from https://doi.org/10.1093/eurheartj/ehx445 .
Kumar, P. M., & Gandhi, U. D. (2018). A novel three-tier Internet of Things architecture with machine learn-
ing algorithm for early detection of heart diseases. Computers & Electrical Engineering ,65, 222 /C0235.
Lahdenoja, O., et al. (2018). Atrial fibrillation detection via accelerometer and gyroscope of a smartphone.
IEEE Journal of Biomedical and Health Informatics ,22(1), 108 /C0118. Available from https://doi.org/
10.1109/JBHI.2017.2688473 .
Lau, J. K., et al. (2013). iPhone ECG application for community screening to detect silent atrial fibrillation: A
novel technology to prevent stroke. International Journal of Cardiology ,165(1), 193 /C0194. Available from
https://doi.org/10.1016/j.ijcard.2013.01.220 .189 ReferencesLee, J., Reyes, B. A., McManus, D. D., Maitas, O., & Chon, K. H. (2013). Atrial fibrillation detection using
an iPhone 4S. IEEE Transactions on Bio-Medical Engineering ,60(1), 203 /C0206. Available from https://
doi.org/10.1109/TBME.2012.2208112 .
Levenson, J. W., Skerrett, P. J., & Gaziano, J. M. (2002). Reducing the global burden of cardiovascular dis-
ease: The role of risk factors. Preventive Cardiology ,5(4), 188 /C0199. Available from https://doi.org/
10.1111/j.1520-037X.2002.00564.x .
Li, Y., et al. (2020). CraftNet: A deep learning ensemble to diagnose cardiovascular diseases. Biomedical
Signal Processing and Control ,62, 102091.
Ma, F., Sun, T., Liu, L., & Jing, H. (2020). Detection and diagnosis of chronic kidney disease using deep
learning-based heterogeneous modified artificial neural network. Future Generation Computer Systems .
Martignani, C., Massaro, G., Biffi, M., Ziacchi, M., & Diemberger, I. (2020). Atrial fibrillation: An arrhythmia
that makes healthcare systems tremble. Journal of Medical Economics ,23(7), 667 /C0669. Available from
https://doi.org/10.1080/13696998.2020.1752220 , Jul.
Mathur, P., Srivastava, S., Xu, X., & Mehta, J. L. (2020). Artificial intelligence, machine learning, and cardio-
vascular disease. Clinical Medicine Insights: Cardiology ,14. Available from https://doi.org/10.1177/
1179546820927404 , p. 1179546820927404, Jan.
Mehrang, S., et al. (2019). Reliability of self-applied smartphone mechanocardiography for atrial
fibrillation detection. IEEE Access ,7, 146801 /C0146812. Available from https://doi.org/10.1109/ACCESS.
2019.2946117 .
Mehrang, S., et al. (2020). Classification of atrial fibrillation and acute decompensated heart failure using
smartphone mechanocardiography: A multilabel learning approach. IEEE Sensors Journal ,20(14),
7957 /C07968. Available from https://doi.org/10.1109/JSEN.2020.2981334 .
Miotto, R., Li, L., Kidd, B. A., & Dudley, J. T. (2016). Deep patient: An unsupervised representation to predict
the future of patients from the electronic health records. Scientific Reports ,6(1), 1 /C010.
Moeskops, P., et al. (2018). Evaluation of a deep learning approach for the segmentation of brain
tissues and white matter hyperintensities of presumed vascular origin in MRI. NeuroImage: Clinical ,17,
251/C0262.
Nemati, S., et al. (2016). Monitoring and detecting atrial fibrillation using wearable technology. Annual
International Conference of the IEEE Engineering in Medicine and Biology Society ,2016 , 3394 /C03397.
Available from https://doi.org/10.1109/EMBC.2016.7591456 .
Nie, L., Wang, M., Zhang, L., Yan, S., Zhang, B., & Chua, T.-S. (2015). Disease inference from health-related
questions via sparse deep learning. IEEE Transactions on Knowledge and Data Engineering ,27(8),
2107 /C02119.
Nihtianov, S., & Luque, A. (2014). Smart sensors and MEMS: Intelligent devices and microsystems for indus-
trial applications . Woodhead Publishing.
Onodera, R., & Sengoku, S. (2018). Innovation process of mHealth: An overview of FDA-approved mobile
medical applications. International Journal of Medical Informatics ,118,6 5/C071. Available from https://
doi.org/10.1016/j.ijmedinf.2018.07.004 .
Orfanidis, S. J. (1996). Introduction to signal processing . Englewood Cliffs, N.J: Prentice Hall.
Orlando, J. I., Prokofyeva, E., Del Fresno, M., & Blaschko, M. B. (2018). An ensemble deep learning based
approach for red lesion detection in fundus images. Computer Methods and Programs in Biomedicine ,153,
115/C0127.
Pacal, I., Karaboga, D., Basturk, A., Akay, B., & Nalbantoglu, U. (2020). A comprehensive review of deep
learning in colon cancer. Computers in Biology and Medicine , 104003.
Perez, M. V., et al. (2019). Large-scale assessment of a smartwatch to identify atrial fibrillation. The New
England Journal of Medicine ,381(20), 1909 /C01917. Available from https://doi.org/10.1056/
NEJMoa1901183 .190 Chapter 7 Deep learning approaches for the cardiovascular diseasePowles, J., & Hodson, H. (2017). Google DeepMind and healthcare in an age of algorithms. Health
Technology ,7(4), 351 /C0367. Available from https://doi.org/10.1007/s12553-017-0179-1 .
Prabhu, K. M. M. (2014). Window functions and their applications in signal processing . Boca Raton,
[Florida]: CRC Press/Taylor & Francis.
Pratt, W. K. (2007). Digital image processing: PIKS scientific inside . Hoboken, NJ: Wiley.
Premaladha, J., & Ravichandran, K. S. (2016). Novel approaches for diagnosing melanoma skin lesions
through supervised and deep learning algorithms. Journal of Medical Systems ,40(4), 96. Available from
https://doi.org/10.1007/s10916-016-0460-2 , Feb.
Qureshi, K. N., Din, S., Jeon, G., & Piccialli, F. (2020). An accurate and dynamic predictive model for a smart
M-Health system using machine learning. Information Sciences ,538, 486 /C0502.
Ravı`, D., et al. (2016). Deep learning for health informatics. IEEE Journal of Biomedical and Health
Informatics ,21(1), 4 /C021.
Rezek, I. A., & Roberts, S. J. (1998). Stochastic complexity measures for physiological signal analysis. IEEE
Transactions on Bio-Medical Engineering ,45(9), 1186 /C01191. Available from https://doi.org/10.1109/
10.709563 .
Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and
sample entropy. American Journal of Physiology—Heart and Circulatory Physiology ,278(6),
H2039 /C0H2049. Available from https://doi.org/10.1152/ajpheart.2000.278.6.H2039 .
Rioul, O., & Duhamel, P. (1992). Fast algorithms for discrete and continuous wavelet transforms.
IEEE Transactions on Information Theory ,38(2), 569 /C0586. Available from https://doi.org/10.1109/
18.119724 .
Samala, R. K., Chan, H., Hadjiiski, L., Helvie, M. A., Wei, J., & Cha, K. (2016). Mass detection in digital
breast tomosynthesis: Deep convolutional neural network with transfer learning from mammography.Medical Physics ,43(12), 6654 /C06666.
Sannino, G., & De Pietro, G. (2018). A deep learning approach for ECG-based heartbeat classification for
arrhythmia detection. Future Generation Computer Systems ,86, 446 /C0455.
Savelieva, I., & Camm, A. J. (2000). Clinical relevance of silent atrial fibrillation: Prevalence, prognosis, qual-
ity of life, and management. Journal of Interventional Cardiac Electrophysiology: An International
Journal of Arrhythmias and Pacing ,4(2), 369 /C0382. Available from https://doi.org/10.1023/
A:1009823001707 .
Schwalm, J.-D., McKee, M., Huffman, M. D., & Yusuf, S. (2016). Resource effective strategies to prevent and
treat cardiovascular disease. Circulation ,133(8), 742 /C0755. Available from https://doi.org/10.1161/
CIRCULATIONAHA.115.008721 .
Shamshirband, S., Fathi, M., Dehzangi, A., Chronopoulos, A. T., & Alinejad-Rokny, H. (2020). A review on
deep learning approaches in healthcare systems: Taxonomies, challenges, and open issues. Journal of
Biomedical Informatics , 103627.
Shannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal ,27(3),
379/C0423. Available from https://doi.org/10.1002/j.1538-7305.1948.tb01338.x .
Sharma, H., Zerbe, N., Klempert, I., Hellwich, O., & Hufnagl, P. (2017). Deep convolutional neural networks
for automatic classification of gastric carcinoma using whole slide images in digital histopathology.
Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging
Society ,61,2/C013.
Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. Journal
of Big Data ,6(1), 60. Available from https://doi.org/10.1186/s40537-019-0197-0 .
Smellie, W., Galloway, M., Chinn, D., & Gedling, P. (2002). Is clinical practice variability the major reason
for differences in pathology requesting patterns in general practice? Journal of Clinical Pathology ,55(4),
312/C0314.191 ReferencesSriram, T., Rao, M. V., Narayana, G., & Kaladhar, D. (2016). A comparison and prediction analysis for the
diagnosis of Parkinson disease using data mining techniques on voice datasets. International Journal of
Applied Engineering Research ,11(9), 6355 /C06360.
Stuart, P. J., Crooks, S., & Porton, M. (2002). An interventional program for diagnostic testing in the emer-
gency department. The Medical Journal of Australia ,177(3), 131 /C0134.
Sun, W., Zheng, B., & Qian, W. (2017). Automatic feature learning using multichannel ROI based on deep
structured algorithms for computerized lung cancer diagnosis. Computers in Biology and Medicine ,89,
530/C0539.
Tadi, M. J., et al. (2016). A real-time approach for heart rate monitoring using a Hilbert transform in seismo-
cardiograms. Physiological Measurement ,37(11), 1885 /C01909. Available from https://doi.org/10.1088/
0967-3334/37/11/1885 .
Tadi, M. J., et al. (2019). Comprehensive analysis of cardiogenic vibrations for automated detection of atrial
fibrillation using Smartphone Mechanocardiograms. IEEE Sensors Journal ,19(6), 2230 /C02242. Available
from https://doi.org/10.1109/JSEN.2018.2882874 .
Tajrishi, F. Z., Chitsazan, M., Chitsazan, M., Shojaei, F., Gunnam, V., & Chi, G. (2019). Smartwatch for the
detection of atrial fibrillation. Critical Pathways in Cardiology ,18(4), 176 /C0184. Available from https://
doi.org/10.1097/HPC.0000000000000192 .
Timmis, A., et al. (2020). European Society of Cardiology: Cardiovascular Disease Statistics 2019. European
Heart Journal ,41(1), 12 /C085. Available from https://doi.org/10.1093/eurheartj/ehz859 .
Turchioe, M. R., Jimenez, V., Isaac, S., Alshalabi, M., Slotwiner, D., & Creber, R. M. (2020). Review of
mobile applications for the detection and management of atrial fibrillation. Heart Rhythm O2 ,1(1), 35 /C043.
Um, T.T. et al. (2017, Nov.). Data augmentation of wearable sensor data for parkinson’s disease monitoring
using convolutional neural networks. In Proceedings of the 19th ACM international conference on multi-
modal interaction (pp. 216 /C0220). Glasgow UK. doi: 10.1145/3136755.3136817.
Vasuki, A., & Govindaraju, S. (2017). Deep neural networks for image classification ,.Deep Learning for
Image Processing Applications (vol. 31, p. 27). IOS Press.
Waktare Johan, E. P. (2002). Atrial fibrillation. Circulation ,106(1), 14 /C016. Available from https://doi.org/
10.1161/01.CIR.0000022730.66617.D9 .
Wang, S.-H., Phillips, P., Sui, Y., Liu, B., Yang, M., & Cheng, H. (2018). Classification of Alzheimer’s dis-
ease based on eight-layer Convolutional Neural Network with leaky rectified linear unit and max pooling.
Journal of Medical Systems ,42(5), 85. Available from https://doi.org/10.1007/s10916-018-0932-7 .
Wennberg, J. E. (1984). Dealing with medical practice variations: A proposal for action. Health Affairs
(Millwood) ,3(2), 6 /C033.
Xiao, Y., Wu, J., Lin, Z., & Zhao, X. (2018). A deep learning-based multi-model ensemble method for cancer
prediction. Computer Methods and Programs in Biomedicine ,153,1/C09.
Xuan, W., & You, G. (2020). Detection and diagnosis of pancreatic tumor using deep learning-based hierarchi-
cal convolutional neural network on the internet of medical things platform. Future Generation Computer
Systems .
Yildirim, O ¨. (2018). A novel wavelet sequence based on deep bidirectional LSTM network model for ECG
signal classification. Computers in Biology and Medicine ,96, 189 /C0202.
Yıldırım, O ¨., Pławiak, P., Tan, R.-S., & Acharya, U. R. (2018). Arrhythmia detection using deep convolutional
neural network with long duration ECG signals. Computers in Biology and Medicine ,102, 411 /C0420.
Yin, W., Li, L., & Wu, F.-X. (2020). Deep learning for brain disorder diagnosis based on fMRI images.
Neurocomputing .
Young, A. A., & Axel, L. (1992). Three-dimensional motion and deformation of the heart wall: Estimation
with spatial modulation of magnetization /C0a model-based approach. Radiology ,185(1), 241 /C0247.
Available from https://doi.org/10.1148/radiology.185.1.1523316 .192 Chapter 7 Deep learning approaches for the cardiovascular diseaseZhao, X., Wu, Y., Song, G., Li, Z., Zhang, Y., & Fan, Y. (2018). A deep learning model integrating FCNNs
and CRFs for brain tumor segmentation. Medical Image Analysis ,43,9 8/C0111.
Zhu, H., et al. (2020). Automatic multilabel electrocardiogram diagnosis of heart rhythm or conduction abnor-
malities with deep learning: A cohort study. Lancet Digital Health .
Zhuang, Z. Y., Churilov, L., Burstein, F., & Sikaris, K. (2009). Combining data mining and case-based reason-
ing for intelligent decision support for pathology ordering by general practitioners. European Journal of
Operational Research ,195(3), 662 /C0675.193 ReferencesThis page intentionally left blankCHAPTER
8Advanced pattern recognition tools
for disease diagnosis
Abdulhamit Subasi1, Siba Smarak Panigrahi2, Bhalchandra Sunil Patil3, M. Abdullah Canbaz4and
Riku Kl ´en5
1Faculty of Medicine, Institute of Biomedicine, University of Turku, Turku, Finland2Department of Computer
Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India3Department of
Mechanical Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India
4Computer Science Department, Indiana University Kokomo, Kokomo, IN, United States5Turku PET Centre,
University of Turku, Turku, Finland
8.1 Introduction
Machine learning (ML) is a subbranch of artificial intelligence (AI) that is extensively employed in
medical image analysis to diagnose diseases. Computer-aided diagnosis (CAD) techniques wereemployed to provide more precise and useful medical imaging interpretation results. In the mid-1980s, CAD algorithms started to evolve and were first employed for mammography and chest
radiography to detect and diagnose cancer (
Giger, Doi, & MacMahon, 1988 ). Other approaches,
such as ultrasound and computed tomography (CT) ( Kawata et al., 1999 ), were then expanded.
CAD algorithms has been generally employed data-driven strategy in the early days, as many deeplearning (DL) algorithms utilized today. But early CAD methods were primarily focused on featureextraction. The development of CAD systems was not helped by feature extraction, as it had manyshortcomings (
LeCun, Bengio, & Hinton, 2015 ). The feature extraction was substituted by transfer
learning and DL to overcome these shortcomings by using a transfer learning approach to boostthe performance of the CAD systems (
Bengio, Courville, & Vincent, 2013 ). DL, a ML subfield,
is a technique widely employed in research areas, including medical images, natural language pro-
cessing, speech recognition, and computer vision. Because of the increase in computing capacitywith the hardware costs reduction and many new datasets, DL has received much interest in thelast decade. In disease detection and diagnosis, as well as in biomedical image segmentation, DLalgorithms are fruitful since high-level features from raw images can be automatically extractedusing DL to assist clinicians. Furthermore, convolutional neural networks (CNNs), which is a formof DL algorithm, have become crucial with the success in medical image analysis (
LeCun et al.,
2015 ). Newly, several studies have begun to utilize DL in new research areas due to the availability
of high performance computing hardware components ( Pacal, Karaboga, Basturk, Akay, &
Nalbantoglu, 2020 ).
Image recognition is one of the most formidable challenges of the algorithms to learn and solve.
The human brain recognizes and categorizes existing as well as new images with almost 100
1955G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00011-5
©2022 Elsevier Inc. All rights reserved.percent accuracy when several images are available. ML techniques have been proposed to mimic
this behavior of the human brain precisely. Whenever the images are taken under various condi-tions, such as shift in lighting, translation or rotation of objects, incomplete or hidden objects, dif-ferent postures in the image, the problem becomes more complicated. These circumstances result innumerous image sets involving the same object, contributing to the recognition or classification
problem’s complexity. In the past, several image recognition techniques have been developed, such
as the k-means clustering, Principal Component Analysis (PCA), Minimum Distance Classifier,Maximum Likelihood Classifier, Bayes Classifier, Support Vector Machines (SVM), etc. The imagerecognition can be object-based or pixel-based. Each pixel’s attributes are extracted in pixel-basedrecognition to denote it as a member of a specific class. In object-based classification, segmentationis performed to extract objects or regions in the image and evaluate their attributes. In order to beclassify medical images in a good manner, image attributes or features must be extracted. The algo-rithm’s efficiency is based on the number of features employed in the procedure. This creates the
“curse of dimensionality” problem. Reducing dimensionality, which is similar to reducing features,
is needed to reduce the computational burden. The more the number of features, the more data pro-cessing and storage is required. This increases the algorithm’s time complexity. Algorithms, whichcategorize objects with a minimum number of features, and less time, are more effective (
Vasuki &
Govindaraju, 2017 ).
Classical ML methods were mainly utilized to make predictions and inferences on data until the
1990s. However, it had many disadvantages depending on handcrafted features, which were con-strained by human-level precision (
Nanni, Ghidoni, & Brahnam, 2017 ). However, in the case of
DL, handcrafted feature engineering is not needed; instead, features are extracted automatically
from the data during training. Moreover, with the help of creative techniques, the computationalpower of modern computers, and the existence of the large volume of datasets, DL can make moreprecise predictions and classifications. Such a DL model’s main capabilities that made them effi-cient and widely used are their feature extraction, massive parallelism, and nonlinearity capabilities(
Goodfellow, Bengio, Courville, & Bengio, 2016 ). There are different DL algorithms such as Long
Short Term Memory (LSTM) ( Sak, Senior, & Beaufays, 2014 ), Recurrent Neural Networks (RNN)
(Mandic & Chambers, 2001 ), CNN ( Ghosh, Sufian, Sultana, Chakrabarti, & De, 2020 ), Generative
Adversarial Networks (GAN) ( Goodfellow et al., 2014 ), etc. Following the success of a CNN-based
model called AlexNet ( Krizhevsky, Sutskever, & Hinton, 2012 ), several DL models, such as
ResNet ( He, Zhang, Ren, & Sun, 2016a ), VGGNet ( Simonyan & Zisserman, 2014 ), DenseNet
(Huang, Liu, Van Der Maaten, & Weinberger, 2017 ), GoogLeNet ( Szegedy et al., 2015 ), ZFNet
(Zeiler & Fergus, 2014 ), MobileNet have been proposed especially for computer vision problems
(Sufian, Ghosh, Sadiq, & Smarandache, 2020; Sultana, Sufian, & Dutta, 2018 ).
Nowadays, cancer has a high death rate and is one of the leading illnesses that has impacted
human health. Cancers are the results of tumors that are malignant. On the other hand, benign
tumors, which can be removed easily, are not cancers and can rarely be harmful. In contrast, malig-
nant tumors or cancers are dangerous when they spread irregularly and uncontrollably. Cancer is amain concern of clinicians and scientists who are involved in this area. The researchers introduceda variety of early cancer detection studies since early cancer diagnosis saves human life and is criti-cal to the fight against the disease (
Schiffman, Fisher, & Gibbs, 2015 ). Medical imaging is a pow-
erful application, which plays an important role in the early detection of cancer ( Abd El-Salam,
Reda, Lotfi, Refaat, & El-Abd, 2014 ). The analysis of the medical imaging data relevant to the196 Chapter 8 Advanced pattern recognition tools for disease diagnosisdisease progress is exhausting and complicated. Furthermore, if the misdiagnosis of physicians is
taken into account, the rate of accuracy declines dramatically, and the early detection can be crucial(
Waite et al., 2017 ). For medical image processing, DL has achieved tremendous progress. In
healthcare systems, such as cancer screening and diagnosis, treatment techniques, and disease mon-itoring, DL practices are utilized. DL is at the forefront of the growing medical data and transform-
ing this data into usable knowledge. In the diagnosis and treatment of colon cancer, the success of
DL is truly exceptional. The success of topics such as pathology image analysis or colonoscopyimage analysis plays a significant role in diagnosing colon cancer (
Pacal et al., 2020 ).
Parkinson’s disease (PD) is a progressive and chronic neurodegenerative disease predominantly
influencing balance and movement. Tremor, postural instability, rigidity, and slow movement arethe key signs of PD (
Durga, Jebakumari, & Shanthi, 2016 ). The dysfunction and death of essential
neurons positioned in the brain cause PD. For contemporary physicians, the exact diagnosis of PDremains a major problem at its early stages. Owing to the similarities of symptoms with other dis-
eases, the challenge of differentiating PD from other neurodegenerative disease is excessive. While
the predominant source of clinical diagnosis is the occurrence of traditional clinical symptoms suchas tremor, bradykinesia, and other main motor characteristics, PD is related to a variety of nonmo-tor symptoms that contribute to the total impairment. Thus timely and precise diagnosis of PD iscrucial for prompt detection and appropriate diagnosis and for initiating neuroprotective therapies.Neuroimaging methods such as single-photon emission computed tomography (SPECT) presentedthe diagnosis even at the early stages of the disorder (
Booth et al., 2015 ,p .1 ; Cummings et al.,
2011 ;Oliveira, Faria, Costa, Castelo-Branco, & Tavares, 2018 ). SPECT images are important
instruments since PD’s progression can be easily seen by the presynaptic dopaminergic deficits in
the striatum ( Prashanth, Roy, Mandal, & Ghosh, 2016 ). The consistent evaluation of SPECT images
is performed only through visual inspection by skilled nuclear medicine doctors. In certain cases,however, the diagnosis cannot be noticed, particularly in early PD patients. These challenges in theidentification of PD patients can be quickly eliminated by incorporating ML techniques. Recentstudies have shown that diagnostic methods relying on ML and DL approaches can help cliniciansin early diagnosis, clinical preparation, and PD progression monitoring (
Orru, Pettersson-Yeo,
Marquand, Sartori, & Mechelli, 2012; Prashanth, Roy, Mandal, & Ghosh, 2014 ). The latest research
on the early detection of PD has taken advantage of developments in ML techniques. Particularly,
by integrating the feature engineering stage into a learning phase, DL has proved to be an efficientinstrument in image recognition (
Schmidhuber, 2015 ). In fact, DL involves a set of data with less
preprocessing instead of extracting features in a handcrafted fashion, and then extract the discrimi-native features through self-learning (
Bengio, 2009; LeCun et al., 2015 ). DL relying on CNNs has
proved to be effective instruments for a wide variety of computer vision tasks in medical imaging(
Greenspan, Van Ginneken, & Summers, 2016 ). In order to differentiate PD patients from healthy
controls, Martinez-Murcia et al. (2017) proposed CNNs. For PD recognition, Choi, Ha, Im, Paek,
and Lee (2017) have introduced a DL based network named PDNet. A DL-based SPECT analysis
approach was utilized in their research to enhance the PD diagnosis. The model was trained on theSPECT images taken from PPMI. Their model reported a high performance similar to the evalua-tion results of the experts. Nevertheless, the DL networks introduced for PD identification are chal-lenging to develop. Since time is of great importance in diagnosing PD, the realistic application ofsuch network solutions is not practical. To fix this issue,
Mohammed, He, and Lin (2020) imple-
mented a smaller network and introduced CNNs to use SPECT images to differentiate PD patients197 8.1 Introductionfrom healthy controls. In the detection of PD, the experimental findings revealed high performance.
The proposed network architecture outperforms by a significant margin with enhancements in allassessment metrics compared to the previous studies (
Choi et al., 2017; Martinez-Murcia et al.,
2017 ). Using a comprehensive repository of SPECT images (2723 images), a CNN-based model
was introduced to diagnose PD specifically. With enhancements in precision, sensitivity, and speci-
ficity, the developed method surpasses the previous studies. The developed model analyzes whole
images and learns features from the images, contributing to maximum efficiency. For the develop-ment and implementations, the limited complexity of the network offers a comprehensive benefit.The CNN-based model could change the diagnosis of PD, considering its outstanding performanceand lower complexity (
Mohammed et al., 2020 ).
The stage of Alzheimer’s Disease (AD) patients was defined by Bringas, Salomo ´n, Duque,
Lage, and Montan ˜a (2020) utilizing the DL models and mobility data. This method makes it possi-
ble for the condition to be tracked and encourages steps to be taken to ensure ideal treatment and
prevent complications. They used data from 35 AD patients obtained from smartphones in a day-
care center for a week. Every patient’s data sequences recorded accelerometer variations as dailyactivities were carried out and labeled with the phase of the disorder. To recognize the patterns thatdefine each step, their methodology employs these time series and utilizes a CNN model. Over90% accuracy was achieved by the CNN-based method, significantly enhancing the traditionalfeature-based classifiers’ results. The results indicate that the data collected from smartphone canbe a useful source for the treatment of AD patients and the study of disease progression. Comparedto the typical supervised learning models, the proposed CNN-based model enhances the perfor-
mance of the detection of AD stages.
There has been a sudden increase in the patients affected by COVID-19 (
Li et al., 2020 ), which
has increased the load over healthcare systems across the world. But the available hospital bedsand personal protective equipment (PPE) (
Sawada, Kuklane, Wakatsuki, & Morikawa, 2017 ) and
ventilators are limited. Therefore it is of utmost importance to distinguish patients with severe acuterespiratory illness (SARI) (
Hatem et al., 2019 ) who might have COVID-19 infection in order to use
the limited resources better. When the number of sick patients is very high at the time of theCOVID-19 epidemic, and the disease is still circulating, many study groups use DL methods to
screen COVID-19 patients for temperature identification of fever, viral and COVID-19, pneumonia,
etc. Furthermore, DL may be employed for other practices, such as patient treatment, systemicbreach of social distance identification (
Sufian, Jat, & Banerjee, 2020 ).Wang S et al. (2020)
employed a CNN-based DL to scan COVID-19 patients utilizing computed tomography (CT)images with an accuracy, specificity and sensitivity of 89.5%, 88.0%, and 87.0%, respectively.Likewise,
Wang, Lin, and Wong (2020) utilized chest X-ray images to diagnose COVID-19 cases
with 83.5% accuracy ( Sufian, Ghosh et al., 2020 ).
In this chapter, diagnosis of COVID-19 using chest X-ray images is presented as disease diag-
noses example. Using numerous models, one can classify a given X-ray in one of the three classes:
normal, COVID-19, and pneumonia. X-ray is advantageous ( Hoheisel, Lawaczeck, Pietsch, &
Arkadiev, 2005 ) over most conventional diagnostic tests. Some advantages include cost-
effectiveness, easy transport from source to analysis points. Also, unlike CT scans, ( Kalender,
2006 ) portable X-ray machines reduce the utilization of extra personal protective equipment (PPE),
as the patient can be tested inside an isolation ward. Further, it reduces the risk of hospital-acquired198 Chapter 8 Advanced pattern recognition tools for disease diagnosisinfection for the patients. With the novel nature of the virus, many radiologists may not themselves
be familiar with all the nuances of the infection. They may lack the expertise required to make thediagnosis highly reliable. Therefore this digital method will act as an inspiration for those at theforefront of this research. Nevertheless, we note that both models reflect our current view of thiscontinually changing problem, which is still based on very limited data at the moment.
The rest of the chapter is organized as follows. In
Section 8.2 , a short introduction to the disease
diagnosis is presented. In Section 8.3 , pattern recognition tools for the disease diagnosis are given.
Section 8.4 presents a case study of COVID-19 detection using an AI approach. Discussion is pre-
sented in Section 8.5 . Finally, we conclude the chapter in Section 8.6 .
8.2 Disease diagnosis
The predictive models can achieve automated disease diagnosis. ML is part of AI, which enables pre-
dictive modeling. The idea of ML is to use a large dataset to build a model, predicting the desiredoutcome for new samples. The general framework for disease diagnosis using AI techniques is shownin
Fig. 8.1 . ML for disease diagnosis has been applied to medical images in various medical fields
including neurodegenerative diseases ( Myszczynska et al., 2020 ), cancer research ( Makaju, Prasad,
Labeled Data
Unlabeled DataLearned 
modelDisease 
DiagnosisTraining
PredictionAr/g415ﬁcalAr/g415ﬁcial 
Intelligence
Machine 
Learning
Deep 
Learning
FIGURE 8.1
General framework for disease diagnosis using artificial intelligencetechniques.199 8.2 Disease diagnosisAlsadoon, Singh, & Elchouemi, 2018; Yassin, Omran, El Houby, & Allam, 2018 ), cardiovascular dis-
eases ( Wong, Fortino, & Abbott, 2020 ), COVID-19 ( Elaziz et al., 2020 ), and malaria ( Poostchi,
Silamut, Maude, Jaeger, & Thoma, 2018 ). These applications cover various medical imaging techni-
ques such as magnetic resonance imaging ( Myszczynska et al., 2020; Wong et al., 2020; Yassin
et al., 2018 ), ultrasound ( Wong et al., 2020; Yassin et al., 2018 ), computed tomography ( Makaju
et al., 2018; Wong et al., 2020 ), X-ray ( Elaziz et al., 2020 ), and microscopic images ( Poostchi et al.,
2018; Yassin et al., 2018 ).
In ML, the data used for modeling is called training data. Typically, the training data is used
to build multiple models. In this case, a separate validation dataset is used for the evaluation ofthe models. Based on the evaluation, the final model is selected. The final model’s performance isestimated using a test dataset, which is again separate from the training and validation datasets.In the ideal case, the training, validation, and test datasets are distinct. In practice, it is not alwayspossible, and the datasets are separated from a larger dataset. Predictive models can be built using
supervised or unsupervised learning. Supervised learning means that the model is built using data
with known labels. For example, if we want to develop a predictive model for detecting COVID-19, then the data for model building should include a diagnosis for each sample as a label. In unsu-pervised learning, though, the labels are not used.
Disease diagnosis typically involves the interpretation of clinical tests supplemented with medi-
cal images. In the case of COVID-19, the most commonly used test is the reverse-transcriptasepolymerase chain reaction or, in short, RT-PCR. However, it is suggested that more sophisticatedCOVID-19 detection methods are needed (
Wu et al., 2020 ). While reading medical tests is a
straightforward task, reading medical images is a more complicated process. In image interpretation
software is used to display and analyze the images. In the simplest case, the software is used onlyto zoom and measure parts of the image. In more complicated cases, the software is used for imageanalysis, such as segmentation or even diagnosis via AI. In most cases, medical image analysis ismanual, or, at best, semiautomatic, and thus automated analysis tools would speed up the analysisand enable reproducible and objective results.
Physicians can use predictive models to decide the condition of a subject. This is called
computer-aided decision-making. Predictive models in clinical practice may be used to create, for
example, disease risk calculators (
Liang et al., 2020; Schalekamp et al., 2020 ) or automated seg-
mentation tools for images ( Shi et al., 2020 ). The idea of predictive models and computer-aided
decision-making is typically not to make the final clinical decision or disease diagnosis but to helpthe medical experts to do it. While there have been many predictive models for computer-aideddecision-making in scientific articles, it is unclear how many of these have been implemented inthe software used in everyday clinical work. Moreover, it is unknown how many of the implemen-ted methods have been adopted as a part of clinical practice.
Research on DL’s clinical employments has expanded significantly over the past few years,
with cancer being the foremost noticeable disease examined and medical images of the predomi-
nant data type (
Jiang et al., 2017 ). Two applications of DL for cancer diagnosis are automatic
investigation and knowledge discovery. Automatic investigation refers to the need for models forroutine clinical diagnostic activities, where success at the expert level was accomplished in manymedical fields (
Bejnordi et al., 2017; Esteva et al., 2017; Gulshan et al., 2016 ). In contrast, the
knowledge discovery seeks to reveal new patterns in data, which can guide diagnosis, prognosis,response to treatment, or genomic status (
Levine et al., 2019 ).200 Chapter 8 Advanced pattern recognition tools for disease diagnosisSegmentation of organs or lesions is often needed in order to promote further analysis and cer-
tain therapy methods by making it a key component of computerized systems. Numerous studieswere conducted across various organs and forms of pathology (
Litjens et al., 2017 ). Arterys, a
startup, recently received FDA approval, the first such approval, for a framework of DL-basedoncology image analysis software. Currently, the software concentrates on liver and lung analysis,
with permission to eventually expand to all consistent tumors, and can segment lesions, monitoring
them, and helping with conventional radiological scoring systems. DL has many other applicationareas, which can enhance the radiology workflow, in addition to benefits, which directly affectdiagnoses, including image quality improvement, content-based retrieval, alignment of multipleimages, and research database mining (
Liew, 2018; Litjens et al., 2017 ). A DL-based system treats
patients’ CT scans for cancer-specific diagnosis by reducing the time to review more urgent images(
Titano et al., 2018 ). In the past, methods focus on the utilization of a handcrafted features
described image-based diagnosis in several types of cancer, including breast cancer ( Mazurowski,
Zhang, Grimm, Yoon, & Silber, 2014 ), glioblastoma ( Gutman et al., 2013 ), renal cell carcinoma
(Karlo et al., 2014 ), and squamous cell carcinoma of the head and neck ( Leger et al., 2017 ). In
research areas such as the prediction of malignant potential in gastrointestinal stromal tumors ( Ning
et al., 2018 ) and breast cancer molecular subtype ( Zhu et al., 2019 ) based on imaging incorporating
DL techniques have achieved promising results ( Levine et al., 2019 ).
DL offers a substantially different approach for the analysis of histopathology images than hand-
crafted feature-based methods. DL architectures utilize the original feature extraction step as end-to-end systems. Instead, images are fed straightforwardly into the model after simple preprocessing,
including its automatic feature extraction into the network’s previous layers under a large parameter
space. An essential criterion with these techniques were only tested on metastatic cancer. Furtherstudies in breast cancer also include tumor regions segmentation in breast biopsy slides (
Cruz-Roa
et al., 2017 ), discrimination between numerous cancer types histotypes and benign breast changes
(Motlagh et al., 2018 ), and cancer identification based entirely on changes in the local stroma
(Bejnordi et al., 2018 ). The evaluation of prostate biopsies and laparoscopic samples is another task,
which is well tailored to computerization ( Fricker et al., 2018; Levine et al., 2019 ).
More recently, a Google team has released its large-scale analysis on prostate cancer scores in
prostatectomy samples ( Nagpal et al., 2019 ). The Cancer Genome Atlas (TCGA) digital slide
archive already proved to be a rich source of knowledge discovery for integrating histology withclinical and molecular evidence. In order to forecast glioma outcomes,
Mobadersany et al. (2018)
proposed a CNN model called a survival CNN. Their model could discern findings within molecu-lar subtypes of glioma based on histology alone, thus obtaining increased prognostic precision byintegrating histology with typical genomic markers. A CNN-based computational staining techniquewas employed by
Saltz et al. (2018) to trace tumor-infiltrating lymphocyte patterns in more than
5000 slides through 13 types of cancer and compare them with molecular subtypes and survival.
DL has been employed in prostate cancer to computerize the detection of the most pathological
areas on slides in predicting the status of speckle-type POZ protein (SPOP) ( Schaumberg, Rubin, &
Fuchs, 2018 ). Moreover, the authors utilized a groundbreaking approach to overcome the problem
of dataset imbalance of unusual mutations by developing an ensemble model trained on subsets ofthe data with selected number of negative and positive slides (
Levine et al., 2019 ).
Dopaminergic degeneration is a Parkinson’s disease (PD) pathological hallmark that can be
evaluated by the visualization of dopamine transporters such as FP-CIT SPECT. DL based FP-CIT201 8.2 Disease diagnosisSPECT analysis approach was implemented by Choi et al. (2017) to improve the prognosis of PD.
This scheme, trained by SPECT images of PD patients and normal controls, achieved a superiorclassification performance comparable to experts’ assessment. Moreover, they found that the pro-posed automated method could reclassify certain patients clinically identified as PD who had scanswithout evidence of dopaminergic deficit (SWEDD). The findings showed that the DL-based
approach could define FP-CIT SPECT correctly and eliminate human assessment inconsistency. It
may assist in imaging prognosis of patients with ambiguous Parkinson’s disease and, in more clini-cal trials, deliver objective patient group classification, especially for SWEDD.
Ear and mastoid disorders are common diseases that can safely be treated with early medical
treatment. Nonetheless, if one may not undergo prompt diagnosis and adequate care, sequelae, suchas hearing damage, can be left behind. Physical inspection using traditional otoscopy or otoendo-scopy and historical examination is the first step in assessing ear and mastoid disease in the clinic.Fortunately, diagnosis utilizing otoscopy or otoendoscopy by nonotolaryngologists is particularly
vulnerable to misdiagnosis (
Blomgren & Pitka ¨ranta, 2003 ). The shortage and relatively low diag-
nostic precision of specialists in the local clinic demands a new type of prognostic approach whereML can play a crucial role. DL were successfully applied to diverse areas of medicine. Most ofthese studies use a supervised DL process, the CNN. Building CNN from scratch, though, needs asignificant volume of dataset and computing capacity that is not feasible in many implementationareas. Instead, a particular application, called transfer learning, may be reused and fine-tuned withexisting CNN models pretrained for natural images. Many network layers in a public networkmodel are transferred to a different model during transfer learning, followed by a new fully-
connected layer, which classifies certain functions into a new class collection. Medical imaging
with transfer learning tests demonstrated a high degree of classification performance equal to, oreven higher than constructing a CNN from scratch (
Kermany et al., 2018; Shin et al., 2016 ).Cha,
Pae, Seong, Choi, and Park (2019) examined the efficiency of nine models to select the best mod-
els. Based on this appraisal, an ensemble classifier was suggested to integrate the classificationeffects of several models that is supposed to improve the overall classification efficiency relative toa single classifier. Although transfer learning is proven to be effective in a relatively limited data-set, there is still no example of the significance of the classification and model form on the scale of
the dataset. Therefore based on the data size, they checked the classifier performance.
COVID-19 is an infection, which affects respiratory symptoms and causes deaths across the
globe. In human life, the early prognosis of this infection is crucial. This phase is advancing rapidlywith diagnostic experiments relying on DL. A DL model, which can be utilized for early diagnosisof the COVID-19 disease, has been suggested by
Canayaz (2020) . A data set composed of three
classes of healthy, COVID-19, and pneumonia lung X-ray images, with each class comprising 364images, was employed for this approach. Preprocessing was done on the created data set utilizingthe image contrast enhancement algorithm, and a new data set was created. Feature extraction was
conducted from this data collection utilizing pretrained models such as VGG19, ResNet, AlexNet,
and GoogLeNet. Two metaheuristic algorithms of binary gray wolf optimization and binary particleswarm optimization were employed to choose the best possible features. They were categorizedusing SVM after combining the elements achieved after the feature selection of the optimized datacollection resulting in an average accuracy of 99.38%. The experimental results achieved by twoseparate metaheuristic algorithms revealed that the proposed methodology would support cliniciansduring the clinical diagnosis of COVID-19.202 Chapter 8 Advanced pattern recognition tools for disease diagnosisTo identify the patient diagnosed with COVID-19 from X-ray images, Altan and Karasu (2020)
introduced a hybrid model composed of two-dimensional (2D) curvelet transform, chaotic salp
swarm algorithm (CSSA), and DL approach. The 2D Curvelet transform is used with the patient’schest X-ray images in the developed framework, and a feature matrix is generated utilizing thecoefficients obtained. With the utilization of CSSA, the feature matrix’s coefficients are optimized,
and COVID-19 disease is identified by the EfficientNet-B0 DL model. Experimental studies indi-
cate that the suggested method can diagnose COVID-19 disease with high precision using chestX-ray images.
8.3 Pattern recognition tools for the disease diagnosis
The classification of medical images refers to the classification of different objects to detect dis-
eases in images such as MRI, X-ray, CT, positron emission tomography, etc. You have to define
and classify the numerous objects or regions in the medical image. The classification techniqueevaluates accuracy based on a single medical image or image set. If the medical image sets areemployed, multiple images of the same object with different views and under different circum-stances will be included in the set. Compared to classifying single medical images, it can be moreefficient in classification, as the algorithm can handle various factors, such as background varia-tions, lighting, or appearances. Image rotation and other transformations can also be invariant. Thenumerical pixels of the image are the input to the ML algorithm. A value or series of values repre-
senting the class would be the output. The ML algorithm is a mapping function that maps the
values of the pixels to the appropriate categories, namely, to the classes. It can be a single multiva-lued function output, or it can be a single-valued function multioutput to show the class.Classification techniques might be supervised or unsupervised for the diagnosis of disease. Thenumber of classes is known in the supervised classification, and a collection of training data withdetails about their class is given. It is like a teacher’s learning. The number of classes is not speci-fied in the unsupervised grouping, and the training data is not available. The interaction (or map-ping) must be learned between the data to be categorized and the different groups. Without a
teacher, it is like studying. If some knowledge about mapping data to groups is available, it is pos-
sible to merge supervised and unsupervised methods to become semisupervised. The most impor-tant parameters associated with input data, depending on which the information is categorized, arefeatures. In classification, identifying specific features of the object as characteristics plays a crucialrole. For classification, the selection of features from the objects in the medical image is often uti-lized (
Vasuki & Govindaraju, 2017 ).
Among biologically inspired AI techniques, CNNs are among the most frequently utilized DL
architectures, which achieve the best performance. By proposing numerous models to enhance
the efficiency of CNNs, several researchers have contributed. Deep CNN’s different classical and
modern architectures are usually employed as the building block of different architectures forclassification, diagnosis, and segmentation. The AlexNet architecture is the architecture that initi-ated the CNN architectures (
Krizhevsky et al., 2012 ). VGG architecture ( Simonyan & Zisserman,
2014 ) has, however, achieved tremendous popularity. The VGG16 and VGG19 models are the
most powerful of the VGG architecture. It is commonly used in CNN architectures for object iden-tification, diagnosis, and classification as a backbone. In addition, with 22 layers, GoogLeNet203 8.3 Pattern recognition tools for the disease diagnosis(Szegedy et al., 2015 ) was the competition winning model in 2014. Employing modules called
Inception, this architecture minimized the computational complexity and the risk of overfitting. In2015, the ResNet (
He et al., 2016a ) architecture, composed of 152 layers, won the ILSVRC compe-
tition. Eventually, several new architectures were introduced, such as MobileNets ( Howard et al.,
2017 ) and DenseNet ( Huang et al., 2017 ). ResNet, VGG, and Inception are the most widely utilized
architectures for image recognition and disease diagnosis. Also, while CNN architectures were ini-
tially employed for classification purposes, they were later employed broadly in object recognition,partitioning applications, and disease diagnosis because of their popularity (
Pacal et al., 2020 ).
8.3.1 Artificial neural networks
Artificial neural networks (ANN) is a group of connected input/output units that have a weight rele-
vant to each relation. During the learning process, the network will learn by changing the weightsso that the input tuples can predict the appropriate class mark. These need a variety of parameters,which are usually better empirically defined, such as the topology of the network or architecture.The benefits of neural networks involve their high tolerance to noisy data. They can be employedwhen the relationships between attributes and classes are little known to you. They are well-
matched for continuous-valued inputs and outputs when compared to most decision tree algorithms.
The most popular ANN architecture is the backpropagation that performs learning on a multilayerfeed-forward neural network. Such a network composed of an input layer, one or more hiddenlayers, and an output layer. Each layer consists of units. The network inputs refer to the attributesfor each training tuple being trained. The inputs are fed into the units, which make up the inputlayer at the same time. All inputs move through the input layer and are then instantaneouslyweighted and provided to the second layer of "hidden" units. The hidden layer unit outputs can beinput into another hidden layer, and so forth. There is no fixed rule to choose the hidden layers to
be used, but mostly a single hidden layer is used, and the number of layers is increased if neces-
sary. It is known as the feed-forward network since none of the weight’s cycles back to a previouslayer. In each layer, various activation functions improve the performance measures with mostly anonlinear activation function seemingly offers better results. Again, there is no such fixed algorithmto determine which activation function to be used at each layer, but it is determined by tuning eachlayer with different activation functions. By iteratively processing a data set of training tuples,backpropagation learns how to equate the network’s prediction for each tuple with the actual knowntarget value. The target value can be the training tuple’s established class mark (for classification
issues) or a continuous value (for numerical prediction). The weights are adjusted for each training
tuple to minimize the mean squared error between the network’s prediction and the actual targetvalue (
Han, Pei, & Kamber, 2011 ).
8.3.2 K-nearest neighbor
K-nearest neighbor (k-NN) finds in the training set a group of k objects closest to the test object
and bases the assignment of a label on the predominance of a particular class in the neighborhoods.This method has three main elements: a collection of labeled objects, a distance or similaritymetric for measuring distance between objects, and the value of k, the number of nearest neighbors.The distance of this object to the labeled objects is calculated to classify an unlabeled object.204 Chapter 8 Advanced pattern recognition tools for disease diagnosisThe unknown class label is assigned after the k-nearest neighbors are determined with the help of
these identified objects. One crucial aspect is the choice of the distance function. Although variousmethods can determine the distance between two points, the most appropriate distance measure is asmallest distance between two objects. For example, in identifying documents, using cosine mea-sure instead of the Euclidean distance can be considered better (
Wu et al., 2008 ).
8.3.3 Support vector machines
SVM are supervised learning techniques employed for the classification, regression, and outlier
detection. There are particular forms of SVMs that can be employed for specific ML problems,such as regression and classification. SVMs differ from other classification methods since theyselect a decision boundary, which maximizes the distance from the closest data points of all classes.The decision boundary provided by the SVMs can be referred to as the maximum margin hyper-plane or the maximum margin classifier. A basic linear SVM classifier operates by constructing astraight line or a hyperplane between two classes. This means that all data points on one side of aline or a hyperplane can indicate a class. The data points on the other side of the line/hyperplane
would be classified in a separate category, with the possibility of an unlimited number of lines/
hyperplanes to choose from. What makes a linear SVM learner stronger than any of the otherlearner, such as the k-nearest neighbors, decides the optimal line/hyperplane to categorize the datapoints. Choose a line/hyperplane that divides the data and is as far away as possible from the near-est data points (
Evgeniou & Pontil, 1999 ).
8.3.4 Random forests
Random forests (RF) can be constructed using tandem bagging with a random selection of the attri-
butes by growing tree votes and returning the most successful class at ranking. For the determina-
tion of the split, the selection of decision trees is based on some aspects. More precisely, for all
trees in the forest, each tree depends on the values of an individually and evenly distributed randomvector sampled. The trees are grown using the CART methodology. The trees are planted to a fullsize and are not pruned. For a forest, the generalization error converges as long as there are manytrees in the forest. So, overfitting is not a problem. Accuracy entirely depends on the performanceof individual classifier trees. Correlation between trees should be avoided to strengthen the individ-ual trees. RF are efficient so that they consider very random and few attributes that are necessarilyimportant for classification. They can be quicker than getting bagged or boosted trees (
Han et al.,
2011 ). External calculations measure error, power, and correlation are used to illustrate the answer
by increasing the number of features used in the split. Internal estimates are also used to gage vari-able significance (
Breiman, 2001 ).
8.3.5 Bagging
Different model decisions can be combined into one single prediction. Opinions are taken in classi-
fication and the average is calculated for numerical prediction. The models have equal weight inbagging, while weighting is used to impact the more efficient ones depending on the past. In bag-ging, numerous same-sized training datasets are randomly selected from the question to construct a205 8.3 Pattern recognition tools for the disease diagnosisdecision tree for each data set. Such trees are essentially similar to every new test instance, making
the same prediction that some trees offer correct results while others fail. Bagging attempts toremove the instability of ML approaches by utilizing random sampling. This sampling techniqueeventually eliminates some of the instances and duplicates the remainder. The resampling data setsare not independent since they are all created from one dataset. Yet bagging yields a combined
model, which achieves considerably better results than the single model generated from the original
training data. Bagging may be used for numerical prediction, in which the outcomes of the individ-ual predictions are summed instead of voting (
Hall, Witten, & Frank, 2011 ).
8.3.6 AdaBoost
AdaBoost or adaptive boosting is one of the ensemble boosting classifier that combines multiple
weak classifiers to improve the classifiers’ accuracy. It is an iterative ensemble method. It con-structs a robust learner by combining many weak learners to get a highly accurate and robustlearner. The basic idea behind AdaBoost is to adjust the weights of learners and training the datasample in every iteration in a way that it guarantees the precise predictions of unusual observations.
Any ML approach can be employed as a base classifier if it takes the weights on the training set.
AdaBoost must provide two conditions:
1.The learner must be trained interactively on numerous weighted training instances.
2.In each iteration, it tries to deliver an outstanding fit for these instances by minimizing training
error.
AdaBoost adds learners to the ensemble, iteratively making it better. This algorithm’s signifi-
cant disadvantage is that the model cannot be parallelized since each predictor can only be trained
after the previous one has been trained and assessed (
Chengsheng, Huacheng, & Bing, 2017 ).
8.3.7 XGBoost
XGBoost stands for Extreme Gradient Boosting, which applies a Gradient Boosting technique
based on decision trees. It constructs short, basic decision trees iteratively. Each tree is termed as a"weak learner" because of its high bias. XGBoost begins by building the first basic tree that has apoor performance. Then it builds another tree, trained to predict what the first tree, which is a weaklearner, cannot do. The technique sequentially produces weaker learners, each correcting the previ-ous tree before the stopping condition is met, such as the number of trees (estimators) to be created.
XGBoost has additional advantages: training is speedy and can be parallelized/distributed across
clusters (
Ramraj, Uzir, Sunil, & Banerjee, 2016 ).
8.3.8 Deep learning
ML includes the design and analysis of algorithms that allow a computer to extract (or learn) func-
tions from a dataset (sets of examples). It is necessary to consider three concepts to understandwhat ML implies: dataset, algorithm, and function. A dataset is a table in its simplest form, inwhich each row includes the definition of one example from a domain (
Kelleher, 2019 ). A mathe-
matical model that is (loosely) affected by the human brain is a DL network. Hence in order to206 Chapter 8 Advanced pattern recognition tools for disease diagnosisunderstand DL, it is needed to provide an abstract understanding of what a mathematical model is,
how model parameters can be tuned, how we can compose models, and how we can use geometryto explain, how a model interprets knowledge. The term DL defines a family of neural networkmodels, which includes multiple layers of basic information processing units, known as neurons, inthe network are defined (
Kelleher, 2019 ).
It has been shown that neural networks are ideally adapted for the application of ML algorithms.
There is one input layer, one output layer, and two or three hidden layers in conventional neuralnetworks. There is one input layer, one output layer, and several hidden layers for deep neural net-works. The more hidden layers there are, the stronger the network is. The layers are interconnected,with the new layer’s input being the output of the previous layer. The inputs/outputs are weighted,and the weights determine the network efficiency. The network’s training requires having theappropriate weights for the different layers. Deep networks need higher computational power, com-putation speed, a big dataset, and parallel processing of the appropriate software (
Vasuki &
Govindaraju, 2017 ).
DL is AI subfield, which focuses on developing massive neural network models capable of
making correct decisions driven by data. DL is specifically tailored to circumstances in which thedata is challenging and where big datasets are available. DL is utilized in the medical field to pro-cess diagnostic images (X-rays, CT, and MRI scans) and detect health problems. DL is now at theheart of self-driving vehicles. It is used for localization and navigation, motion planning and steer-ing, awareness of the environment, and driver status monitoring (
Kelleher, 2019 ).
8.3.9 Convolutional neural network
CNNs ( Albawi, Mohammed, & Al-Zawi, 2017 ) take, process, and classify an image input into
those categories. Computers interpret an input image as an array of pixels, which depends on the
image resolution. Based on image resolution it can be height, width, or depth. Regularized models
of CNNs are multilayer perceptrons. "Absolute reliability" of such networks makes them vulnerableto overfitting results. Typical methods of regularization involve adding the loss function to someform of weight calculation. CNNs use a particular approach to regularization: they take advantageof the hierarchical structure of data by using smaller and simplified patterns and generate morecomplicated patterns. Therefore CNNs are at the lower end on the connectivity scale and complex-ity. Convolution is the first layer for extracting features from an input file. Convolution maintainsthe relationship between pixels by employing small squares of input data to learn image properties.
It is a mathematical operation containing two inputs, such as an image vector and a filter or kernel.
Pooling layers will reduce number of parameters once the images become too large. Spatial pool-ing, also known as down-sampling or subsampling, reduces each map’s dimensionality but retainsessential information. Fully-connected layers in one layer bind each neuron to each neuron inanother layer. The flattened matrix moves through a totally connected row to define the images.
8.3.10 Transfer learning
One of the difficulties encountered in the image analysis is that labeled training data might not be
accessible for a specific use. Considering the condition where one has a collection of images, whichshould be utilized for image retrieval. Labels do not exist in retrieval applications, so the features207 8.3 Pattern recognition tools for the disease diagnosismust be semantically consistent. In certain other instances, one may be willing to perform classifi-
cation on a data set with a specific set of labels that may be limited in accessibility and differentfrom extensive resources like ImageNet. Since neural networks need a lot of training data to con-struct from scratch, these settings trigger complications. The categories selected and the wide rangeof images in the data set are so comprehensive and exhaustive that they can derive image features
for general-purpose settings. For instance, an entirely different image data set can be generated by
passing the features extracted from the ImageNet data through a pretrained CNN like AlexNet andextracting the multidimensional features from the fully-connected layers. For an entirely differentapplication, such as clustering or retrieval, this new representation may be utilized. This type ofmethodology is so popular that CNNs are barely trained from scratch. This kind of off-the-shelffeature extraction approach can be used as a kind of transfer learning since a shared resource suchas ImageNet can be utilized to extract features in the circumstances in which adequate training datais not accessible to solve various problems (
Aggarwal, 2018 ).
Suppose any extra training data is accessible and can only be utilized to fine-tune the deeper
layers. The weights of the early layers are fixed since the previous layers catch only simple featuressuch as edges. In contrast, the deeper layers capture more complex features. In other words, previ-ous layers tend to capture features that are highly generalizable, whereas later layers tend to capturefeatures unique to data (
Aggarwal, 2018 ). In addition, VGG ( Simonyan & Zisserman, 2014 )
highlighted the increasing trend in terms of expanded network depth. Although the best-performingmodels had 16 or more layers, the tested networks were built with different sizes varying from 11to 19 layers. VGG’s breakthrough is that it decreased filter sizes but boosted depth. It is essential
to realize that increased depth requires a reduced filter size. Owing to more rectified linear unit
(ReLU) layers, a deeper network would have more nonlinearity and more regularization as theincreased depth imposes a structure on the layers through repetitive convolution composition. TheVGG still uses 3 33 spatial footprint filters and 2 32 scale pooling. With step 1, the convolution
was performed, and a padding 1 was used. In phase 2, the pooling was utilized. Another curiousdesign decision for VGG was that after each max-pooling, the number of filters was always boostedby a factor of 2. Whenever the spatial footprint decreased by a factor of 2, the intention was still tomaximize the depth by a factor of 2. Note that after each max-pooling, the number of filters
increases by a factor of 2 (
Aggarwal, 2018 ).
ResNet (He et al., 2016a ) included 152 layers that was almost an order of magnitude greater
than most architectures had historically employed. In the training of such deep networks, the criti-cal problem is that the gradient flow between layers is hindered by the vast number of deep layeroperations, which can decrease or increase the gradient size. The basic unit is called a residualmodule, and by adding together all these basic units, the whole network is formed. An adequatelypadded filter with a step of 1 is utilized in most layers so that the spatial scale and depth of theinput do not change from layer to layer. Some layers, indeed, use stride convolutions to minimize
by a factor of 2 per spatial dimension. When employing a more significant number of filters, depth
is expanded by a factor of 2. In such a scenario, the identity function over the skipped connectionshould not be used (
Aggarwal, 2018 ).
Inception models were trained in a partitioned fashion, in which every replica was divided into
numerous subnetworks to match the whole model in memory. However, the Inception design isextremely tunable, which ensures several potential adjustments to the number of filters in the differ-ent layers that do not impact the fully-qualified network’s efficiency. Cheaper Inception blocks208 Chapter 8 Advanced pattern recognition tools for disease diagnosisthan the initial Inception are used for the residual iterations of the Inception networks. Each
Inception block is accompanied by a filter-expansion layer used before the inclusion to balance theinput’s depth to scale up the filter bank’s dimensionality. This is required to compensate for thereduction in dimensionality caused by the block of Inception. Another minor technological differ-ence between the residual and nonresidual variants of Inception is that batch-normalization is only
used on top of conventional layers in the case of Inception-ResNet, but not on top of summations
(
Szegedy, Ioffe, Vanhoucke, & Alemi, 2017 ).
A CNN architecture solely based on depthwise separable convolution layers was suggested by
Chollet (2017) . In reality, in the role maps of CNNs, the mapping of cross-channel correlations and
spatial correlations can be fully decoupled. This theory is a better variant of the Inception architec-ture hypothesis, and Xception, which stands for "Extreme Inception," is the proposed architecture.To map cross-channel correlations, an "extreme" variant of an Inception module will first utilizea131 convolution and then map each output channel’s spatial correlations individually.
This extreme type of an Inception module is almost similar to a depthwise separable convolution,
an operation employed in the neural network architecture. A depthwise separable convolution isgenerally referred to in DL systems as "separable convolution," composed of a depthwise convolu-tion. It is also possible to make other intermediate formulations of Inception modules, which lie inbetween regular Inception modules and depthwise separable convolutions. 36 convolutional layersin the Xception architecture form the network’s feature extraction framework. The experimentalassessment image description is exclusively explored, and a logistic regression layer would thenadopt the suggested convolutional base. Preferably, before the logistic regression layer, one might
introduce fully-connected layers. The 36 convolutional layers are organized into 14 modules, all
of which have linear residual relations across them except for the first and last modules (
Chollet,
2017 ).
A different connectivity pattern is suggested by Huang et al. (2017) to further optimize the
information flow between layers by adding direct links to all subsequent layers from each layer.This network architecture is called the Dense Convolutional Network (DenseNet) because of itsdense connectivity. Three sequential operations, batch normalization, followed by a ReLU, and a333 convolution are known as a composite function. When the size of the feature-maps increases,
the concatenation operation is not feasible. However, down-sampling layers that alter the size of
feature-maps are an integral component of convolutional networks. The network is divided intomany densely interconnected dense blocks to allow down-sampling in the proposed architecture.The layers within blocks are referred to as layers of transformation, convolution, and pooling.A batch normalization layer and a 1 31 convolutional layer followed by a 2 32 average pooling
layer are the transition layers employed in the experiments. DenseNet has very thin layers, which isthe difference from existing network architectures. It has been noted that before each 3 33 convo-
lution, a 1 31 convolution may be incorporated as a bottleneck layer to lessen the quantity of input
feature-maps and thereby increase classification performance. The number of feature-maps at tran-
sition layers could be minimized to maximize model robustness further. A global average poolingis applied at the end of the last dense block, and then a SoftMax layer is connected. In the threedense blocks, the feature-map dimensions are 32 332, 16316, and 8 38, respectively.
The MobileNet (
Howard et al., 2017 ) model is focused on depthwise separable convolutions
that are a type of factorized convolutions to divide a regular convolution into a depthwise convolu-tion. The depthwise convolution for MobileNets utilizes a single filter to each input channel.209 8.3 Pattern recognition tools for the disease diagnosisTo merge the outputs of the depthwise convolution, the pointwise convolution uses a 1 31 convo-
lution. In one step, a typical convolution filters and merges inputs into a new set of outputs. Thisis divided into two layers: a depthwise separable convolution, a separate layer for filtering, and aseparate layer for merging. This factorization has the potential to reduce computation and modelsize significantly. MobileNets employ Batchnorm and ReLU nonlinearities for both layers.
Compared with regular convolution, depthwise convolution is exceptionally effective. It filters just
input channels, though, and it does not merge them to create new features. In order to producethese new features, an extra layer, which calculates a linear combination of the output of depthwiseconvolution through 1 31 convolution, is therefore required. The fusion of convolution depthwise
and pointwise (1 3) convolution is referred to as depthwise separable convolution. MobileNet uses
333 depthwise separable convolution that utilizes 8 to 9 times less computing than regular convo-
lutions with just a slight decrease in precision.
8.4 Case study of COVID-19 detection
DL is a model employed to make exceptional advances in retrieving information from images by
computers. In certain areas of medicine, mainly radiology and pathology, these techniques havebeen introduced to tasks and, in some cases, achieved efficiency similar to that of human experts.It could also be employed to provide information on molecular state, prognosis, or sensitivity totreatment (
Levine et al., 2019 ). CNNs are DL architecture suggested in disease diagnosis to solve
image processing problems and adapt very well to images. Layers in separate blocks are connected
in a CNN architecture instead of being directly connected. The flow of knowledge between theseblocks is analogous to the visual cortex and reduces the difficulties of classical ML approaches. Itcan also automatically extract features from the raw data while eliminating the challenges of man-ual extraction. The network will allow faster learning in this way and eliminate the overfitting pro-blems. CNNs are called data-hungry DL architectures since it can train millions of parameters tohelp diagnose diseases by using medical images (
Pacal et al., 2020 ).
Transfer learning is a methodology that efficiently utilizes an already trained model to solve
another new problem with minimum re-training or fine-tuning requirements ( Pan & Yang, 2010;
Zamir et al., 2018 ). Compared to classical ML approaches, DL requires a significant volume of
training data. Therefore the need for a large number of labeled data is a major challenge in solvingsome critical domain-specific tasks, especially disease diagnosis applications (
Altman, 2017 ).
A DL-based transfer learning, DTL, drastically decreases the need for training time and trainingdata for a target domain-specific allocation by selecting a pretrained model for a defined featureextractor (
Koitka & Friedrich, 2016 ) or further fine-tuning ( Kumar, Kim, Lyndon, Fulham, & Feng,
2017 ). A DTL model can be trained for feature extraction utilizing a benchmark dataset. This pre-
trained model is then further employed to solve a new problem such as the role of COVID-19 by
modifying only a few last layers in the architecture head and needed fine-tuning. A VGG-basedtransfer learning model (
Simonyan & Zisserman, 2014 ) was proposed by Gao and Mosalam (2018) .
Classification of anomalies in MR images by DTL is proposed by Talo, Baloglu, Yıldırım, and
Rajendra Acharya (2019) . The developers of that study have used the pretrained fine-tuning
ResNet34 model. Their system used adaptation to condition distribution. Q-TRANSFER, which isanother DTL model, is suggested by
Phan, Sultana, Nguyen, and Bauschert (2020) . A DTL-based210 Chapter 8 Advanced pattern recognition tools for disease diagnosisreinforcement learning technique is employed to alleviate the dataset insufficiency issue in the
context of information networking. As the outbreak of COVID-19 disease is alarming worldwide,the detection, quarantine, and adequate care for COVID-19 patients have become a priority.However, with substantial false negative findings, the worldwide standard diagnostic pathogeniclaboratory procedure is massively time-consuming and more expensive (
Santosh, Parmar, Anand,
Srikanth, & Saritha, 2020 , p. 19). Around the same time, because of the inadequate facilities
and areas compared with many cases, studies are seldom to be carried out in the common healthcenters or hospitals. Researchers from this area are working hard to build potential DTL models toalleviate these problems to tackle this situation (
Apostolopoulos & Mpesiana, 2020; Loey,
Smarandache, & Khalifa, 2020 ). As in the case of Loey et al. (2020) , DTL is utilized along with
the GAN model to diagnose the COVID-19 disease-based on chest X-ray images. They have threestate-of-the-art pretrained models, namely AlexNet (
Krizhevsky et al., 2012 ), GoogLeNet ( Szegedy
et al., 2015 ), and ResNet18 ( He et al., 2016a ). In their experiments, the highest precision
is achieved by GoogLeNet among these three pretrained models ( Sufian, Ghosh et al., 2020 ). In
this chapter, a transfer learning-based feature extraction technique is utilized to diagnose theCOVID-19 using X-ray images. The general framework for COVID-19 detection using AI techni-ques is shown in
Fig. 8.2 .
Although there was no broad clinical adoption in these early forays into the computer-aided
diagnosis, it must be remembered that they employed technologies that followed the explosion of
FIGURE 8.2
General framework for COVID-19 detection using artificial intelligence techniques.211 8.4 Case study of COVID-19 detectionDL. Recent head-to-head comparisons shown superior DL performance over other systems ( Kooi
et al., 2017; Wang et al., 2017 ). Numerous recent research on big datasets have shown that the effi-
ciency of DL systems is comparable to that of experts conducting traditional diagnostic tasksthrough several modalities, including spine MRI (
Jamaludin et al., 2017 ), head CT ( Merkow et al.,
2017 ), mammography ( Kooi et al., 2017 ), chest X-rays ( Rajpurkar et al., 2017 ), and limb trauma
X-rays ( Olczak et al., 2017 ) With the growing evidence that CT chest screening can minimize lung
cancer mortality, substantial interest has been created by the automatic identification and assess-ment of lung nodules (
Levine et al., 2019; Setio et al., 2017 ).
Various DL-based approaches have been developed to identify different thoracic diseases,
including pneumonia. Most recently, a lot of research work has been done on the detection ofCOVID-19 (
Makris, Kontopoulos, & Tserpes, 2020; Narin, Kaya, & Pamuk, 2020 ). In particular,
the DL methods that have been employed on the radiography images have shown promising resultswith high precision. DeTraC (
Abbas, Abdelsamea, & Gaber, 2020 ), a relatively new CNN architec-
ture, is based on transfer learning and class decomposition to enhance the pretrained models’ per-
formance on X-ray image classification. It includes three phases. The first phase conducts the deepfeature extraction using a pretrained CNN by utilizing ImageNet (
Krizhevsky, Sutskever, & Hinton,
2017 ). The second phase of DeTraC employs the Stochastic gradient descent optimization method
(Bottou, 2010 ) for training the model. The final phase, also known as the Class-composition com-
ponent, is acquired for the final classification of the images. It uses error-correction measures at theSoftMax layer.
Zhang et al. (2020) developed a new deep anomaly detection model for fast and consistent
screening of COVID-19 cases based on chest X-ray images. The model designed in their approach
comprises of three components: (1) a backbone network, (2) a module for the detection of anoma-lies, and (3) a classification module. The backbone network extracts the chest X-ray image charac-teristics into a d-dimensional feature vector, fed into the classification module. The classificationmodule includes a new deep convolution neural network (DCNN) comprised of a hidden layer, the“sigmoid” activation function, and a single neuron output layer. The anomaly detection module hasa similar architecture as the classification module, except it produces the scalar anomaly scores toreveal the anomalous images. This particular model has proven to be a successful approach to
reduce the false-positive rate.
In another study focusing on detecting COVID-19 cases from chest X-ray images, namely the
COVID-Net (
Wang & Wong, 2020 ), a deep CNN incorporates the human-machine collaboration
and machine-driven design exploration stage, employs a lightweight residual projection-expansion-projection-extension (PEPX) design pattern. In addition to COVID-Net, the authors also establishedan explainability-driven audit approach for decision validation. A similar approach consists ofseven DL image classifiers proposed in COVIDX-Net (
Hemdan, Shouman, & Karar, 2020 )a sa
framework for integrating DL models altogether. Comparing the state-of-the-art DL modes, the
best performance was accomplished by the DenseNet201 and VGG19 classifiers with a classifica-
tion accuracy of 90%. A recent study, the Monte-Carlo Dropout method, and Bayesian CNN havebeen investigated to measure this uncertainty (
Seoh, 2020 ). Using COVID-19 X-ray images, the
Bayesian DL classifier was trained employing transfer learning on a pretrained ResNet50V2 ( He,
Zhang, Ren, & Sun, 2016b ) model utilizing COVID-19 X-ray images to evaluate the model uncer-
tainty. Their analysis has shown to quantify the uncertainty to enhance the efficiency of human-machine decisions.212 Chapter 8 Advanced pattern recognition tools for disease diagnosis8.4.1 Experimental data
We use the public datasets available on Kaggle.com, the Coronahack X-ray Dataset1, and the chest
X-ray pneumonia2. The datasets contain 6253 images, of which 1495 are normal lung images, 4371
are X-ray images infected with pneumonia, and 287 images are those infected with COVID-19. Wehave divided the data into three sets, namely, training, validation, and test sets. Initially, the originaldata is divided into a 95% set, further divided into 95% and 5% into training and validation sets.The division of the 5% original dataset was labeled as the test set.
8.4.2 Performance evaluation measures
The training set performance measures cannot be assumed similar to that of the test set. The train-
ing set may hold the bulk of examples, but the test set is identical to the real-world data. ForConvNets, having a low amount of data is a significant hindrance. The methods used to evaluatethe performance measures in such a case are still controversial. The majority of the support is given
to the performance measures obtained from the cross-validation sets. The classifier is tuned to pro-
vide the best performance measures in the case of the training set. The most important fact to havein mind while training is that none of the test set examples should be used while designing the clas-sifier. Hence the performance measures from the test set can be assumed to be a close comparisonto the performance measures on the real-world images. A learner classifies an image into differentcategories. If the category matches the given class, then it is assumed to be a success. If there isany mismatch, then it is assumed to be an error. The performance measures are mostly based onthe error rate of a classifier. The training set holds a bulk of examples. The validation dataset
should have a similar distribution to the test set to have the best performance measures. The valida-
tion set should be utilized for parameter tuning and the test set for finding the final values of per-formance measures (
Hall et al., 2011 ).
When vast volumes of data are available, there is no problem: We take a large sample, use it
for preparation, and presume that the remainder should be used for processing. In general, the effi-ciency of a classifier increases with an improvement in the size of the training range. Still, after acertain stage, the efficiency stays constant and does not change further (
Hall et al., 2011 ). In this
study accuracy, F1-measure and Kappa statistic is used as a performance evaluation measure.
8.4.3 Feature extraction using transfer learning
DL techniques require many examples or a large set of data. Yet COVID-19 current situation is rela-
tively new, and therefore the number of X-ray images for the classification function is fewer, so wecan go for transfer learning. Transfer learning (
Hussain, Bird, & Faria, 2018; Pan & Yang, 2010 )i s
about taking features learned on one domain’s problem and using them towards another, relatively
similar issue. Transfer learning is aimed at improving learning in the target task by incorporating
information from the source task. The key facts to test before using transfer learning are that thesource task should have been trained on a larger dataset than the target task, and the source and target
1https://www.kaggle.com/praveengovi/coronahack-chest-xraydataset?select 5Coronahack-Chest-XRay-Dataset
2https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia213 8.4 Case study of COVID-19 detectiontasks should be identical in nature. Three common measures could improve learning through transfer.
First is the initial achievable output in the target task utilizing only the transferred information, com-pared to an ignorant person’s initial output, before any more learning is performed. Second, theamount of time it takes to understand the target mission thoroughly, despite the information trans-ferred, relative to the amount of time it takes to understand it from scratch. Third, the final output
level can be reached in the target role close to the last level without transition.
Here we have used different pretrained models like VGG16, VGG19, Inception v3, MobileNet,
DenseNet169, DenseNet121, InceptionResNetV2, MobileNetV2, ResNet101. Simonyan andZisserman (
Simonyan & Zisserman, 2014 ) presented the VGG network architecture ( Jaderberg,
Simonyan, & Zisserman, 2015 ). This network is distinguished by its simplicity, using just three uni-
versal layers layered on top of each other in rising detail. Max-pooling works to high volumecapacity. In comparison to conventional sequential network architectures like AlexNet, OverFeat,and VGG, ResNet is instead a type of "exotic architecture" based on micro-architecture modules.
The term microrchitecture refers to the collection of "building blocks" that were used to create the
ResNet architecture, which was first presented by
He et al. (2016a) . They demonstrated incredibly
deep networks that can be equipped using normal Stochastic Gradient Decent (and a fair initializa-tion function) by using residual modules. The Inception module’s goal is to act as a "multilevel fea-ture extractor" by computing 1 bracket, 3 bracket, and 5 brackets within the same networkmodule—these filter outputs are then stacked along the channel dimension before being fed intothe next network layer. GoogleLeNet was the initial implementation of this architecture, but laterimplementations were simply named Iteration vN, where N corresponds to the version number
Google brought out. Xception was developed by Franc ¸ois Chollet, founder and chief maintainer of
the Keras library. Xception is an extension of the standard Inception architecture.
Transfer learning is one of the most effective approaches, particularly in image recognition and
classification for building accurate classifiers when there is a limited supply of target training data.To be more precise, in transfer learning, we use the knowledge gained from rich labeled data in asource domain to improve our predictive models’ accuracy. There are various reasons behind thesource of training data is being limited, such as data being inaccessible, expensive to harvest andcurate, or even rare to find. This approach has been used heavily in ML applications, including image
classification (
Duan, Xu, & Tsang, 2012; Kulis, Saenko, & Darrell, 2011; Zhu et al., 2011 ), human
motion classification ( Harel & Mannor, 2010 ) sentiment analysis ( Wang & Mahadevan, 2011 )e t c .
8.4.4 Experimental results
For training the algorithm with X-ray images, existing DL models pretrained with the ImageNet
database ( http://www.image-net.org ), capable of classifying 1000 natural objects, were employed.
VGG16 and VGG19, ResNet50, ResNet101, MobileNet-V2, MobileNet, Inception-V3, Inception-
ResNet-V2, DenseNet169, DenseNet121, and Xception have been used for deep feature extraction
among several publicly accessible deep feature extraction models, and many shallow ML techni-ques are compared because these deep feature extraction and ML models are considered to havehigher performance. Various ML algorithms were used on the extracted features obtained from dif-ferent pretrained models. The ML algorithms used are ANN, KNN, RF, SVM, Bagging, AdaBoost,and XGBoost. The entire dataset of X-ray images was divided into COVID-19, Normal and othertypes of pneumonia. Accuracy, F1-score, and Kappa were used as performance measures.214 Chapter 8 Advanced pattern recognition tools for disease diagnosisIf we use ML models on deep features, they give an outstanding result as they have already
acquired the necessary features. As we can see from the results below, nearly all the models showsignificant results. Note that ANN has been run for 100 epochs here.
Random forest and bagging have very high training accuracy as they beat all the models in
training accuracy. Still, at the same time, these classifiers present noticeable overfitting to a great
extent. ANN and XGBoost can be considered classifiers with less overfitting.
Here too, like in VGG 16, we see that random forest and bagging have a very high training
accuracy as they beat all the models in training accuracy, but, at the same time, these classifierspresent noticeable overfitting to a great extent, bagging much more than Random Forest. Here mostof the classifiers are giving good results with less overfitting.
Random forest and bagging beat all others like usual in the training accuracy, but they present
noticeable overfitting again. Moreover, XGBoost shows a very high training accuracy. Surprisingly,SVM shows very poor results in this case.
Random forest, bagging, and XGBoost. have a very high training accuracy as they beat all clas-
sifiers and they present obvious overfitting. ANN can be considered as a classifier with lessoverfitting.
Here again similar to the previous experiments, random forest and bagging present obvious
overfitting to a certain extent. Here SVM achieved the best performance and can be considered as aclassifier with the least overfitting.
With MobileNet, too, all the classifiers present noticeable overfitting to a certain extent. Also,
AdaBoost is seen performing very weakly here compared with the other pretrained models.
We can see that results of InceptionV3 stand out from the other pretrained models as nearly all
classifiers have significantly less overfitting and give very high accuracies. Surprisingly SVM per-forms very poorly with this pretrained model.
Random forest, bagging, and XGBoost have a very high training accuracy as they beat all clas-
sifiers, although they present obvious overfitting to a certain extent. Here AdaBoost is seen to showinferior results.
This model, just like the other, faces a problem of overfitting, and random forest and bagging
present noticeable overfitting and have the highest training accuracies. Here SVM achieved the best
performance and can be considered a model with the least overfitting.
This model shows much more promising results than DenseNet169. Each ML model gives
promising results with this pretrained model. Again, random forest and bagging present overfitting.Here XGBoost can be considered as a classifier with the least overfitting.
This model works great with nearly all the classifiers and is also seen to have no or significantly
less overfitting. Here too, AdaBoost gives very poor results, whereas XGBoost gives promisingresults without overfitting.
We would like to underline a few key concepts that make a significant impact in our analysis.
The ML models are designed to divide data into three sets, namely, the training, validation and test-
ing set, mainly used to overcome the problem of overfitting. Overfitting is a malfunction in themodel when a model perfectly understands (or fits) the training data. In contrast, it cannot makeaccurate predictions for the testing data that has not been seen before. This is why data is dividedinto two or more subsets. However, this still might not be enough as data with a lot of noise canlead the ML models to generate false positives, false negatives, and possibly both outcomes. Toassess the models’ accuracy better, most of the time, the F-measure (or also known as F1 score)215 8.4 Case study of COVID-19 detectionand the Cohen’s Kappa Statistics is employed. In our analysis, we provided not only the training,
validation, and test accuracies; we also included the F1 Scores along with Kappa values. Note thatif the model clears out all the weeds and produces perfect results, then the F1 score is consideredexcellent and gets closer to 1, 1 being the 100% achievement. In contrast, if the model is not suc-cessful, the F1 score leans towards the total failure score, which is zero.
Moreover, the accuracy, in all cases (training, validation, and test cases), is the percentage of
correctly classified instances out of all instances. Also, Kappa, or as known as Cohen’s KappaStatistics, is the performance measure with a normalization of the results with the baseline of a ran-dom selection of the data points picked from the data. Kappa results can be interpreted as if thevalues are less than or equal to zero (values #0), indicating no agreement. The values are less
than or equal to 1, meaning that there is almost perfect agreement.
Our results seen in
Tables 8.1 through 8.11 show some interesting results. Even though all the
models seem to have great accuracy ( .90%), the random forest and the bagging are the winners
in all deep feature extraction approaches. While random forest had 99.98% accuracy in training
accuracy, it falls short on test and validation accuracies. Hence, they present noticeable overfitting.However, considering the F1-measure (over 85%) and Kappa statistics (over 66%), we can assurethat, even though the outcome of random forest performs exceptional classification performancein all deep feature extraction techniques, namely, DenseNet121, DenseNet169, InceptionResNetV2,InceptionV3, MobileNet, MobileNetV2, Resnet50, ResNet101, VGG16, VGG19, Xception.Bagging has shown an outstanding performance overall in most deep feature extraction techniques,except, ResNet50 and MobileNet.
Even though the training and testing accuracies were not excellent compared to random forest
and XGBoost, Bagging has the highest F1 measure and kappa on both ResNet50 and MobileNetpretrained models. The interesting fact is that, while XGBoost has not performed good in most ofthe pretrained models, it produced an outstanding performance in MobileNet, InceptionResNetV2,ResNet50, and ResNet101 feature extraction methods.
These results imply that the accuracy combined with F1 measure and kappa gives the most pre-
cise picture in terms of the ML models’ performance. Depending on the classifier and the pretrainedmodels, the outcome can be significantly different.
Table 8.1 Performance of different classifiers with VGG16 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy
(%)F1-
Measure Kappa
Artificial neural networks 92.88 93.60 93.93 0.9386 0.8495
KNN 94.67 91.25 92.65 0.9215 0.8134
Support vector machines 90.27 90.57 92.33 0.9107 0.8020
Random forest 99.98 90.91 92.97 0.9209 0.8148
AdaBoost 86.76 88.89 86.58 0.8607 0.6737
Bagging 99.42 91.92 90.42 0.8984 0.7579
XGBoost 94.88 92.93 94.25 0.9395 0.8539216 Chapter 8 Advanced pattern recognition tools for disease diagnosisTable 8.2 Performance of different classifiers with VGG19 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy(%)F1-
Measure Kappa
Artificial neural networks 91.60 89.90 93.29 0.9325 0.8364
KNN 94.15 89.23 90.73 0.9065 0.7716
Support vector machines 89.10 89.56 91.69 0.9141 0.7904
Random forest 99.98 91.58 90.73 0.9051 0.7691
AdaBoost 82.42 81.82 84.46 0.8499 0.6486
Bagging 99.50 87.54 90.10 0.8992 0.7568
XGBoost 94.49 91.25 91.69 0.9161 0.7996
Table 8.3 Performance of different classifiers with ResNet50 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy(%)F1-
Measure Kappa
Artificial neural networks 86.30 87.54 84.66 0.8438 0.6209
KNN 92.66 87.54 85.62 0.8560 0.6469
Support vector machines 71.48 71.72 74.12 0.6451 0.0644
Random forest 99.98 90.24 89.54 0.8529 0.6389
AdaBoost 81.43 83.84 77.64 0.7795 0.4776
Bagging 99.42 91.92 90.42 0.8984 0.7579
XGBoost 99.98 90.24 86.90 0.8651 0.6663
Table 8.4 Performance of different classifiers with ResNet101 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy
(%)F1-
Measure Kappa
Artificial neural networks 88.43 88.89 87.54 83.760 0.6936
KNN 92.22 85.86 84.66 84.510 0.6290
Support vector machines 80.84 82.49 82.43 79.390 0.4916
Random forest 99.98 87.88 85.94 0.8569 0.6501
AdaBoost 82.12 84.51 80.19 0.8061 0.5477
Bagging 99.27 89.90 85.30 0.8491 0.6292
XGBoost 99.98 90.57 87.22 0.8689 0.6776217 8.4 Case study of COVID-19 detectionTable 8.5 Performance of different classifiers with MobileNetV2 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy(%)F1-
Measure Kappa
Artificial neural networks 95.52 89.23 92.33 0.9237 0.8169
KNN 93.97 88.89 90.42 0.9012 0.7618
Support vector machines 92.37 91.25 94.25 0.9407 0.8569
Random forest 99.98 87.88 93.29 0.9290 0.8291
AdaBoost 84.99 81.82 84.98 0.8555 0.6647
Bagging 99.31 88.89 90.73 0.9027 0.7666
XGBoost 94.51 89.23 93.61 0.9347 0.8436
Table 8.6 Performance of different classifiers with MobileNet deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy(%)F1-
Measure Kappa
Artificial neural networks 97.34 90.91 88.82 0.8871 0.7254
KNN 94.98 91.25 86.58 0.8665 0.6796
Support vector machines 93.71 93.27 88.50 0.8849 0.7224
Random forest 99.98 89.23 87.54 0.8715 0.6826
AdaBoost 81.11 81.82 75.08 0.7631 0.4792
Bagging 99.42 86.87 85.30 0.8511 0.6392
XGBoost 99.98 91.92 90.10 0.8997 0.7548
Table 8.7 Performance of different classifiers with InceptionV3 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy
(%)F1-
Measure Kappa
Artificial neural networks 95.45 94.28 95.53 0.9546 0.8909
KNN 96.51 94.28 95.21 0.9516 0.8834
Support vector machines 71.59 75.76 76.04 0.6795 0.1538
Random forest 99.98 94.61 96.49 0.9646 0.9147
AdaBoost 94.72 94.61 95.85 0.9595 0.9002
Bagging 99.63 94.95 95.53 0.9548 0.8916
XGBoost 96.62 95.62 94.89 0.9479 0.8752218 Chapter 8 Advanced pattern recognition tools for disease diagnosisTable 8.8 Performance of different classifiers with InceptionResNetV2 deep feature
extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy
(%)F1-
Measure Kappa
Artificial neural networks 96.12 95.96 92.01 0.9202 0.8049
KNN 96.77 95.62 90.73 0.9075 0.7770
Support vector machines 92.31 92.93 89.78 0.8734 0.7239
Random forest 99.98 96.30 91.37 0.9138 0.7915
AdaBoost 66.59 70.03 69.01 0.6416 0.1193
Bagging 99.56 95.96 90.10 0.9012 0.7616
XGBoost 99.98 95.62 91.37 0.9140 0.7926
Table 8.9 Performance of different classifiers with DenseNet169 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy(%)F1-
Measure Kappa
Artificial neural networks 96.92 94.28 92.97 0.9290 0.8293
KNN 94.67 93.60 94.25 0.9424 0.8603
Support vector machines 94.28 94.61 96.49 0.9641 0.9139
Random forest 99.98 92.26 92.01 0.9141 0.7892
AdaBoost 84.49 86.53 81.79 0.8213 0.5779
Bagging 99.31 90.24 91.69 0.9086 0.7833
XGBoost 95.52 93.94 93.29 0.9297 0.8279
Table 8.10 Performance of different classifiers with DenseNet121 deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy
(%)F1-
Measure Kappa
Artificial neural networks 96.35 96.97 96.81 0.9676 0.9216
KNN 96.56 95.96 96.17 0.9610 0.9065
Support vector machines 94.70 96.63 94.57 0.9446 0.8659
Random forest 99.98 96.30 95.53 0.9548 0.8919
AdaBoost 73.93 74.41 75.72 0.7639 0.4449
Bagging 99.68 96.63 95.85 0.9587 0.9018
XGBoost 97.43 97.31 96.49 0.9649 0.9156219 8.4 Case study of COVID-19 detection8.5 Discussion
In various sectors, AI’s rise has undoubtedly become a powerful player and is expected to create
much more disruption. Ultimately and naturally, this ability has led to opposing views regarding itspossible position and integration into society. AI is supposed to have a major effect on so-calledknowledge workers, unlike other previous technical developments that have primarily impactedmanual work. There is a wide debate among scientists about the expected future involvement ofhuman specialists and the capacity for human diagnostic capacities to be surpassed by AI (
Granter,
Beck, & Papke, 2017; Sharma & Carter, 2017 ). Nevertheless, it is necessary to note the
inevitable imprecision in technical prognosis and the role of viewpoints and biases in shaping indi-
vidual perceptions when considering these issues ( Granter, 2016; Levine et al., 2019 ).
This chapter addressed numerous challenges in typical medical applications and truly approxi-
mating a human physician’s cognitive processes. Most DL implementations were extremelytask-specific, while humans can build relationships, which can enhance performance through manysimilar tasks. In spite of the limitation of feature-engineered methods, the combination of semanticknowledge with the visual interpretation would likely benefit, particularly in the distinctionbetween uncommon diagnoses with minimal instances available. In addition, pathology and radiol-
ogy findings are mostly focused not only on a single specimen or scan but also on a connection
with the previous ones and other medical histories (
Levine et al., 2019; Sharma & Carter, 2017 ).
In general, DL is greedy for data, considerably more than previous feature-engineered methods,
which are less vulnerable to overfitting. The collection of relevant training data in almost alldomains is an ongoing problem. Though there are unsupervised and semisupervised learning meth-ods, data sets need manual annotation or curation for most medical tasks (
Hosny, Parmar,
Quackenbush, Schwartz, & Aerts, 2018 ). This could be sufficient for qualified research workers or
require medical professionals’ full input, depending on the assignment’s scope. Since the previous
layers in DNNs almost invariably learn quite common image characteristics, medical data can be
fine-tuned on networks that have been pretrained on broad general image sets, minimizing theamount of data needed total training time (
Tajbakhsh et al., 2016; Yosinski, Clune, Bengio, &
Lipson, 2014 ). DL has been criticized for being a ’black box’ in contrast to the feature-engineeredTable 8.11 Performance of different classifiers with Xception deep feature extraction.
Training
accuracy (%)Validation
accuracy (%)Test
accuracy(%)F1-
Measure Kappa
Artificial neural networks 96.35 96.97 96.81 0.9676 0.9216
KNN 96.56 95.96 96.17 0.9610 0.9065
Support vector machines 94.70 96.63 94.57 0.9446 0.8659
Random forest 99.98 96.30 95.53 0.9548 0.8919
AdaBoost 73.93 74.41 75.72 0.7639 0.4449
Bagging 99.68 96.63 95.85 0.9587 0.9018
XGBoost 97.43 97.31 96.49 0.9649 0.9156220 Chapter 8 Advanced pattern recognition tools for disease diagnosismethods, where it is not completely clear how the algorithm produces outputs from a given input.
Although this claim undoubtedly has some significance, strategies for visualizing a network’s acti-vation functions and the types of images that activate a given neuron have contributed to character-ize the inner workings of these techniques, which continues an active research area (
Zeiler &
Fergus, 2014; Zintgraf, Cohen, Adel, & Welling, 2017 ). However, considering the failure of current
DL algorithms to clarify their diagnostic mechanism, many challenges, including the degree of phy-
sician supervision needed and deciding who is responsible for system error, will need to be solvedbefore their introduction in clinical practice. Medical policymakers and clinicians must proceedcarefully and insist on new methods that can be tested extensively in practical circumstances priorto patient treatment, considering the speculation and high standards surrounding DL in medicine(
Levine et al., 2019; Topol, 2019 ).
AI is expected to stay in a diagnostic service position for the near future, where it will accu-
rately define disorders, automate manual operations, and enhance roadmap, but a human being
should bear responsibility of final decisions. Nevertheless, considering the current state of the field,
precise information remains highly speculative about applying this technology. Regardless of AI’seventual effect directly, as emerging innovations are implemented, the field of diagnostic medicinewill continue to evolve. If AI algorithms can generate widespread adoption by clinicians, theexpense of the computing technology required to implement AI algorithms is likely to be marginalin the light of total healthcare expenditure (
Liew, 2018 ). If AI can eventually automate a good por-
tion of image recognition, a radiologist or pathologist’s role can move to increasingly focusingother activities, such as medical record correlation, report formulation, clinician liaison, and
departmental quality control. To better explain the computer techniques employed, this technology
would also require a change in diagnostic physicians’ education, with the development of ancompletely new discipline or even a combination of pathology and radiology (
Jha & Topol, 2016;
Levine et al., 2019; Lundstro ¨m, Gilmore, & Ros, 2017 ).
Despite numerous attempts to increase medical accuracy, diseases depend on the procedure and
mostly rely on the physician’s expertise. X-ray imaging diagnosis needs experience in image diagno-sis; radiologists performed slightly more than general practitioners in a sample of video-presentedevaluation for diagnosis. In these cases, the new deep network paradigm could support physicians by
recommending potential X-ray image-based diagnosis. By integrating clinical knowledge with recom-
mendations, they could achieve greater diagnostic accuracy. COVID-19 was diagnosed with sufficientaccuracy by the existing image classification process, with transfer learning feature extraction,which is remarkable in accuracy and diagnostic diversity. It should be remembered that all the clini-cal X-rays were purposely used, without any selection bias for preparation. Any of the images wereused as long as a clinician could get an idea of the given image for diagnosis. We speculate that thisrealistic database of images makes the model’s output dependent on the scale of the database.COVID-19 disease is automatically diagnosed using deep feature extraction based on transfer learning
and chest X-ray images.
8.6 Conclusions
In the continued pursuit of computer-aided medical diagnostics, AI is a promising innovation.
Development over the past few years reveals its capacity to produce success at human experts’221 8.6 Conclusionslevel, but the technology continues to be far away from mainstream clinical application. The prac-
tice of diagnostic medicine is likely to be transformed by AI, and we are hopeful that it will eventu-ally contribute to increased patient safety and health care quality (
Levine et al., 2019 ).
In this chapter we presented how AI and ML techniques are utilized in disease diagnosis.
Besides an automated COVID-19 diagnosis approach based on deep feature extraction is also pre-
sented. Moreover, we have presented some results on detecting COVID-19 positive cases, a pan-
demic that infected many of the human population in 2020, from chest X-rays using different MLmodels. After extracting features using DTL, the X-ray images are fed into the shallow ML modelsto diagnose COVID-19 from X-ray images. Also, we compared different types of transfer learningmodels for feature extraction using traditional ML models over these pretrained models. Simpletransfer learning-based feature extraction gives promising results with models such as VGG16,VGG19, and MobileNet. Finally, the feature extraction method also shows promising results withnearly 90% accuracy in most models and the highest accuracy, reaching 96.49% with Random
Forest classifier using InceptionV3 feature extraction.
References
Abbas A., Abdelsamea M.M., & Gaber M.M. (2020). Classification of COVID-19 in chest X-ray images using
DeTraC deep convolutional neural network. arXiv200313815 Cs Eess Stat , May. ,http://arxiv.org/abs/
2003.13815 .. Accessed: 07.08.20.
Abd El-Salam, M., Reda, S., Lotfi, S., Refaat, T., & El-Abd, E. (2014). Imaging techniques in cancer diagno-
sis,” In .Cancer biomarkers: Minimal and noninvasive early diagnosis and prognosis (pp. 19 /C038). CRC
Press.
Aggarwal, C. C. (2018). Neural networks and deep learning . Springer.
Albawi S., Mohammed T.A., & Al-Zawi S. (2017). Understanding of a convolutional neural network ,
pp. 1 /C06.
Altan, A., & Karasu, S. (2020). Recognition of COVID-19 disease from X-ray images by hybrid model con-
sisting of 2D curvelet transform, chaotic salp swarm algorithm and deep learning technique. Chaos,
Solitons, and Fractals ,140, 110071.
Altman, R. (2017). Artificial intelligence (AI) systems for interpreting complex medical datasets. Clinical
Pharmacology and Therapeutics ,101(5), 585 /C0586.
Apostolopoulos, I. D., & Mpesiana, T. A. (2020). Covid-19: automatic detection from X-ray images utilizing
transfer learning with convolutional neural networks. Phys. Eng. Sci. Med. ,43(2), 635 /C0640. Available
from https://doi.org/10.1007/s13246-020-00865-4 .
Bejnordi, B. E., Veta, M., van Diest, P. J., van Ginneken, B., Karssemeijer, N., Litjens, N., ...Venaˆncio, R.
(2017). Diagnostic assessment of deep learning algorithms for detection of lymph node metastases inwomen with breast cancer. JAMA: The Journal of the American Medical Association ,318(22),
2199 /C02210.
Bejnordi, B. E., Mullooly, M., Pfeiffer, R. M., Fan, S., Vacek, P. M., Weaver, D. L., ...Sherman, M. E.
(2018). Using deep convolutional neural networks to identify and classify tumor-associated stroma in diag-nostic breast biopsies. Modern Pathology: An Official Journal of the United States and Canadian Academy
of Pathology, Inc ,31(10), 1502 /C01512.
Bengio, Y. (2009). Learning deep architectures for AI. Found. Trends sMach. Learn. ,2(1), Art. no. 1.
Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intelligence ,35(8), 1798 /C01828.222 Chapter 8 Advanced pattern recognition tools for disease diagnosisBlomgren, K., & Pitka ¨ranta, A. (2003). Is it possible to diagnose acute otitis media accurately in primary
health care? Family Practice ,20(5), 524 /C0527.
Booth, T. C., Nathan, M., Waldman, A. D., Quigley, A.-M., Schapira, A. H., & Buscombe, J. (2015). The role
of functional dopamine-transporter SPECT imaging in Parkinsonian syndromes, Part 1. American Journal
of Neuroradiology ,36(2), 229. Available from https://doi.org/10.3174/ajnr.A3970 .
Bottou L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010 , Y. Lechevallier and G. Saporta, (Eds.) Heidelberg: Physica-Verlag HD, pp. 177 /C0186.
Breiman, L. (2001). Random forests. Machine Learning ,45(1), 5 /C032.
Bringas, S., Salomo ´n, S., Duque, R., Lage, C., & Montan ˜a, J. L. (2020). Alzheimer’s Disease stage identifica-
tion using deep learning models. Journal of Biomedical Informatics ,109, 103514.
Canayaz, M. (2020). “MH-COVIDNet: Diagnosis of COVID-19 using deep neural networks and meta -heuris-
tic-based feature selection on X-ray images. Biomedical Signal Processing and Control ,64, 102257.
Cha, D., Pae, C., Seong, S.-B., Choi, J. Y., & Park, H.-J. (2019). Automated diagnosis of ear disease using
ensemble deep learning with a big otoendoscopy image database. EBioMedicine ,45, 606 /C0614.
Chengsheng T., Huacheng L., & Bing X. (2017). AdaBoost typical algorithm and its application research ,
vol. 139, p. 00222.
Choi, H., Ha, S., Im, H. J., Paek, S. H., & Lee, D. S. (2017). Refining diagnosis of Parkinson’s disease with
deep learning-based interpretation of dopamine transporter imaging. NeuroImage: Clinical ,16, 586 /C0594.
Available from https://doi.org/10.1016/j.nicl.2017.09.010 , Jan.
Chollet F. (2017). Xception: Deep learning with depthwise separable convolutions , pp. 1251 /C01258.
Cruz-Roa, A., Gilmore, H., Basavanhally, A., Feldman, M., Ganesan, S., Shih, N. N. C., ...Madabhushi, A.
(2017). Accurate and reproducible invasive breast cancer detection in whole-slide images: A deep learningapproach for quantifying tumor extent. Science Report ,7(1), 1 /C014.
Cummings, J. L., Henchcliffe, C., Schaier, S., Simuni, T., Waxman, A., & Kemp, P. (2011). The role of dopa-
minergic imaging in patients with symptoms of dopaminergic system neurodegeneration. Brain , awr177.
Duan L., Xu D., & Tsang I. (2012). Learning with augmented features for heterogeneous domain adaptation.
arXiv12064660 .
Durga, P., Jebakumari, V. S., & Shanthi, D. (2016). Diagnosis and classification of parkinsons disease using
data mining techniques. International Journal of Advanced Research Trends in Engineering and
Technology ,3,8 6/C090.
Elaziz, M. A., Hosny, K. M., Salah, A., Darwish, M. M., Lu, S., & Sahlol, A. T. (2020). New machine learning
method for image-based diagnosis of COVID-19. PLoS One ,15(6), e0235187. Available from https://doi.
org/10.1371/journal.pone.0235187 , Jun.
Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-
level classification of skin cancer with deep neural networks. Nature ,542(7639), 115 /C0118.
Evgeniou T. & Pontil M. (1999). Support vector machines: Theory and applications , pp. 249 /C0257.
Fricker, K. S., Moret, M., Rupp, N., Hermanns, T., Fankhauser, C., Wey, N., ...Claassen, M. (2018). Automated
Gleason grading of prostate cancer tissue microarrays via deep learning. Science Report ,8(1), 1 /C011.
Gao, Y., & Mosalam, K. M. (2018). Deep transfer learning for image-based structural damage recognition.
Computer-Aided Civil and Infrastructure Engineering ,33(9), 748 /C0768. Available from https://doi.org/
10.1111/mice.12363 .
Ghosh, A., Sufian, A., Sultana, F., Chakrabarti, A., & De, D. (2020). Fundamental concepts of convolutional
neural network .Recent Trends and Advances in Artificial Intelligence and Internet of Things
(pp. 519 /C0567). Springer.
Giger, M. L., Doi, K., & MacMahon, H. (1988). Image feature analysis and computer-aided diagnosis in digital
radiography. 3. Automated detection of nodules in peripheral lung fields. Medical Physics ,15(2), 158 /C0166.
Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). no. 2 Deep learning (1). Cambridge: MIT
press.223 ReferencesGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., & Bengio, Y. (2014).
Generative adversarial nets. Proceedings of the twenty-seventh international conference on neural informa-
tion processing systems ,2, 2672 /C02680.
Granter, S. R. (2016). Reports of the death of the microscope have been greatly exaggerated. Archives of
Pathology & Laboratory Medicine ,140(8), 744 /C0745. Available from https://doi.org/10.5858/arpa.2016-
0046-(ED.) .
Granter, S. R., Beck, A. H., & Papke, D. J., Jr (2017). AlphaGo, deep learning, and the future of the human
microscopist. Archives of Pathology & Laboratory Medicine ,141(5), 619 /C0621. Available from https://doi.
org/10.5858/arpa.2016-0471-(ED.) .
Greenspan, H., Van Ginneken, B., & Summers, R. M. (2016). Guest editorial deep learning in medical imag-
ing: Overview and future promise of an exciting new technique. IEEE Transactions on Medical Imaging ,
35(5), 1153 /C01159.
Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., ...Webster, D. R. (2016).
Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal
fundus photographs. JAMA: The Journal of the American Medical Association ,316(22), 2402 /C02410.
Gutman, D. A., Cooper, L. A. D., Hwang, S. N., Holder, C. A., Gao, J., Aurora, T. D., ...Brat, D. J. (2013).
MR imaging predictors of molecular profile and survival: Multi-institutional study of the TCGA glioblas-
toma data set. Radiology ,267(2), 560 /C0569.
Hall M., Witten I., & Frank E. (2011). Data mining: Practical machine learning tools and techniques .
Kaufmann Burlingt.
Han, J., Pei, J., & Kamber, M. (2011). Data mining: Concepts and techniques . Elsevier.
Harel, M. & Mannor, S. (2010). Learning from multiple outlooks. arXiv10050027 .
Hatem, A., Mohamed, S., Elhassan, U. E. A., Ismael, E. A. M., Rizk, M. S., El-Kholy, A., & El-Harras, M.
(2019). Clinical characteristics and outcomes of patients with severe acute respiratory infections (SARI):Results from the Egyptian surveillance study 2010 /C02014. Multidisciplinary Respiratory Medicine ,14(1),
11. Available from
https://doi.org/10.1186/s40248-019-0174-7 .
He, K., Zhang, X., Ren, S., & Sun, J. (2016a). Deep residual learning for image recognition , pp. 770 /C0778.
He, K., Zhang, X., Ren, S., & Sun, J. (2016b). Identity mappings in deep residual networks. arXiv160305027
Cs.,http://arxiv.org/abs/1603.05027 .. Accessed 07.08.20.
Hemdan, E. E.-D., Shouman, M. A., & Karar, M. E. (2020). COVIDX-Net: A framework of deep learning
classifiers to diagnose COVID-19 in X-ray images. arXiv200311055 Cs Eess.,http://arxiv.org/abs/
2003.11055 .. Accessed 07.08.20.
Hoheisel, M., Lawaczeck, R., Pietsch, H., & Arkadiev, V. (2005). Advantages of monochromatic X-rays for
imaging. Proceedings of SPIE - The International Society for Optical Engineering ,5745 . Available from
https://doi.org/10.1117/12.593398 , Apr.
Hosny, A., Parmar, C., Quackenbush, J., Schwartz, L. H., & Aerts, H. J. W. L. (2018). Artificial intelligence
in radiology. Nature Reviews. Cancer ,18(8), 500 /C0510. Available from https://doi.org/10.1038/s41568-
018-0016-5 .
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., ..., Adam, H. (2017).
Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv170404861 .
Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional net-
works , pp. 4700 /C04708.
Hussain, M., Bird, J., & Faria, D. (2018). A study on CNN transfer learning for image classification .
Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. Advances in Neural
Information Processing Systems ,28, 2017 /C02025.
Jamaludin, A., Lootus, M., Kadir, T., Zisserman, A., Urban, J., Batti ´e, M. C. ...The Genodisc Consortium.
(2017). Automation of reading of radiological features from magnetic resonance images (MRIs) of thelumbar spine without human intervention is comparable with an expert radiologist. European Spine224 Chapter 8 Advanced pattern recognition tools for disease diagnosisJournal: Official Publication of the European Spine Society, the European Spinal Deformity Society, and
the European Section of the Cervical Spine Research Society ,26(5), 1374 /C01383.
Jha, S., & Topol, E. J. (2016). Adapting to artificial intelligence: Radiologists and pathologists as information
specialists. JAMA: The Journal of the American Medical Association ,316(22), 2353 /C02354. Available
from https://doi.org/10.1001/jama.2016.17438 .
Jiang, F., Jiang, Y., Zhi, H., Dong, Y., Li, H., Ma, S., ...Wang, Y. (2017). Artificial intelligence in healthcare:
Past, present and future. Stroke and Vascular Neurology ,2(4).
Kalender, W. A. (2006). X-ray computed tomography. Physics in Medicine and Biology ,51(13), R29 /C0R43.
Available from https://doi.org/10.1088/0031-9155/51/13/R03 , Jul.
Karlo, C. A., Paolo, P. L. D., Chaim, J., Hakimi, A. A., Ostrovnaya, I., Russo, P., ...Akin, O. (2014).
Radiogenomics of clear cell renal cell carcinoma: associations between CT imaging features and muta-
tions. Radiology ,270(2), 464 /C0471.
Kawata Y., Niki, N., Ohmatsu, H., Kusumoto, M., Kakinuma, R., Mori, K., . . . Moriyama, N. (1999).
Computer aided differential diagnosis of pulm onary nodules using curvature based analysis ,
pp. 470 /C0475.
Kelleher, J. D. (2019). Deep learning . Mit Press.
Kermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C. S., Liang, H., Baxter, S. L., ...Zhang, K. (2018).
Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell,172(5),
1122 /C01131. Available from https://doi.org/10.1016/j.cell.2018.02.010 , e9, Feb.
Koitka, S. & Friedrich, C. M. (2016). Traditional feature engineering and deep learning approaches at medi-
cal classification task of ImageCLEF 2016 . pp. 304 /C0317.
Kooi, T., Litjens, G., van Ginneken, B., Gubern-M ´erida, A., S ´anchez, C. I., Mann, R., ...Karssemeijer, N.
(2017). Large scale deep learning for computer aided detection of mammographic lesions. Medical Image
Analysis ,35, 303 /C0312. Available from https://doi.org/10.1016/j.media.2016.07.007 , Jan.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural
networks. Advances in Neural Information Processing Systems ,25, 1097 /C01105.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2017). ImageNet classification with deep convolutional neural
networks. Communications of the ACM ,60(6), 84 /C090. Available from https://doi.org/10.1145/3065386 .
Kulis, B., Saenko, K., & Darrell, T. (2011). What you saw is not what you get: Domain adaptation using asym-
metric kernel transforms . pp. 1785 /C01792.
Kumar, A., Kim, J., Lyndon, D., Fulham, M., & Feng, D. (2017). An ensemble of fine-tuned convolutional
neural networks for medical image classification. IEEE Journal of Biomedical and Health Informatics ,
21(1), 31 /C040. Available from https://doi.org/10.1109/JBHI.2016.2635663 , Jan.
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature ,521(7553), 436 /C0444.
Leger, S., Zwanenburg, A., Pilz, K., Lohaus, F., Linge, A., Zo ¨phel, K., ...Richter, C. (2017). A comparative
study of machine learning methods for time-to-event survival data for radiomics risk modelling. Science
Report ,7(1), 1 /C011.
Levine, A. B., Schlosser, C., Grewal, J., Coope, R., Jones, S. J., & Yip, S. (2019). “Rise of the machines:
Advances in deep learning for cancer diagnosis. Trends Cancer ,5(3), 157 /C0169.
Li, R., Pei, S., Chen, B., Song, Y., Zhang, T, Yang, W., ...Shaman, J. (2020). Substantial undocumented
infection facilitates the rapid dissemination of novel coronavirus (COVID-19). Infectious Diseases (except
HIV/AIDS), preprint . Available from https://doi.org/10.1101/2020.02.14.20023127 , Feb.
Liang, W., Liang, H., Ou, L., Chen, B., Chen, A., & Li, C.. . . for the China Medical Treatment Expert Group
for COVID-19. (2020). Development and validation of a clinical risk score to predict the occurrence ofcritical illness in hospitalized patients with COVID-19. JAMA Internal Medicine ,180(8), 1081 /C01089.
Available from
https://doi.org/10.1001/jamainternmed.2020.2033 .
Liew, C. (2018). The future of radiology augmented with artificial intelligence: A strategy for success.
European Journal of Radiology ,102, 152 /C0156.225 ReferencesLitjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A., Ciompi, F., Ghafoorian, M., ...S´anchez, C. I. (2017).
A survey on deep learning in medical image analysis. Medical Image Analysis ,42,6 0/C088.
Loey, M., Smarandache, F., & Khalifa, N.E.M. (2020). Within the lack of COVID-19 benchmark dataset: A
novel gan with deep transfer learning for corona-virus detection in chest x-ray images . No April.
Lundstro ¨m, C. F., Gilmore, H. L., & Ros, P. R. (2017). Integrated diagnostics: The computational revolution
catalyzing cross-disciplinary practices in radiology, pathology, and genomics. Radiology ,285(1), 12 /C015.
Available from https://doi.org/10.1148/radiol.2017170062 .
Makaju, S., Prasad, P. W. C., Alsadoon, A., Singh, A. K., & Elchouemi, A. (2018). Lung cancer detection
using CT scan images. Procedia Computer Science ,125, 107 /C0114. Available from https://doi.org/10.1016/
j.procs.2017.12.016 , Jan.
Makris, A., Kontopoulos, I., & Tserpes, K. (2020). COVID-19 detection from chest X-ray images using deep
learning and convolutional neural networks. Radiology and Imaging . Available from https://doi.org/
10.1101/2020.05.22.20110817 , May.
Mandic, D., & Chambers, J. (2001). Recurrent neural networks for prediction: Learning algorithms, architec-
tures and stability . Wiley.
Martinez-Murcia F.J., Ortiz, A., Gorriz, J.M., Ramı ´rez, J., Segovia, F., Salas-Gonzalez, D., ... Illan, I.A. (2017).
A 3D convolutional neural network approach for the diagnosis of Parkinson’s disease , pp. 324 /C0333.
Mazurowski, M. A., Zhang, J., Grimm, L. J., Yoon, S. C., & Silber, J. I. (2014). Radiogenomic analysis of
breast cancer: Luminal B molecular subtype is associated with enhancement dynamics at MR imaging.Radiology ,273(2), 365 /C0372.
Merkow, J., Lufkin, R., Nguyen, K., Soatto, S., Tu, Z., & Vedaldi, A. (2017). DeepRadiologyNet: Radiologist
level pathology detection in CT head images. arXiv171109313 .
Mobadersany, P., Yousefi, S., Amgad, M., Gutman, D. A., Barnholtz-Sloan, J. S., Vega, J. E. V., ...Cooper,
L. A. D. (2018). Predicting cancer outcomes from histology and genomics using convolutional networks.
Proceedings of the National Academy of Sciences of the United States of America ,115(13),
E2970 /C0E2979.
Mohammed, F., He, X., & Lin, Y. (2020). An easy-to-use deep-learning model for highly accurate diagnosis
of Parkinson’s disease using SPECT images. Computerized Medical Imaging and Graphics: The Official
Journal of the Computerized Medical Imaging Society ,87, 101810.
Motlagh, M. H., Jannesari, M., Aboulkheyr, H., Khosravi, P., Elemento, O., Totonchi, M., & Hajirasouliha, I.
(2018). Breast cancer histopathological image classification: A deep learning approach. BioRxiv , 242818.
Myszczynska, M. A., Ojamies, P. N., Lacoste, A. M. B., Neil, D., Saffari, A., Mead, R., ...Ferraiuolo, L.
(2020). Applications of machine learning to diagnosis and treatment of neurodegenerative diseases. Nature
Reviews Neurology ,16(8). Available from https://doi.org/10.1038/s41582-020-0377-8 , Art. no. 8.
Nagpal, K., Foote, D., Liu, Y., Chen, P.-H. C., Wulczyn, E., Tan, F., ...Stumpe, M. C. (2019). Development
and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. NPJ Digital
Medicine ,2(1), 1 /C010.
Nanni, L., Ghidoni, S., & Brahnam, S. (2017). Handcrafted vs. non-handcrafted features for computer vision
classification. Pattern Recognition ,71, 158 /C0172.
Narin, A., Kaya, C., & Pamuk, Z. (2020). Automatic detection of coronavirus disease (COVID-19) using X-
ray images and deep convolutional neural networks. arXiv200310849 Cs Eess .,http://arxiv.org/abs/
20030.10849 .. Accessed: 08.08.20.
Ning, Z., Luo, J., Li, Y., Han, S., Feng, Q., Xu, Y., ...Zhang, Y. (2018). Pattern classification for gastrointes-
tinal stromal tumors by integration of radiomics and deep convolutional features. IEEE Journal of
Biomedical and Health Informatics ,23(3), 1181 /C01191.
Olczak, J., Fahlberg, N., Maki, A., Razavian, A. S., Jilert, A., Stark, A., ...Gordon, M. (2017). Artificial intel-
ligence for analyzing orthopedic trauma radiographs. Acta Orthopaedica ,88(6), 581 /C0586. Available from
https://doi.org/10.1080/17453674.2017.1344459 .226 Chapter 8 Advanced pattern recognition tools for disease diagnosisOliveira, F. P., Faria, D. B., Costa, D. C., Castelo-Branco, M., & Tavares, J. M. R. (2018). Extraction, selec-
tion and comparison of features for an effective automated computer-aided diagnosis of Parkinson’s dis-
ease based on [123 I] FP-CIT SPECT images. European Journal of Nuclear Medicine and Molecular
Imaging ,45(6), 1052 /C01062.
Orru, G., Pettersson-Yeo, W., Marquand, A. F., Sartori, G., & Mechelli, A. (2012). Using support vector
machine to identify imaging biomarkers of neurological and psychiatric disease: a critical review.Neuroscience and Biobehavioral Reviews ,36(4), 1140 /C01152.
Pacal, I., Karaboga, D., Basturk, A., Akay, B., & Nalbantoglu, U. (2020). A comprehensive review of deep
learning in colon cancer. Computers in Biology and Medicine , 104003.
Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data
Engineering ,22(10), 1345 /C01359. Available from https://doi.org/10.1109/TKDE.2009.191 .
Phan, T. V., Sultana, S., Nguyen, T. G., & Bauschert, T. (2020). Q-TRANSFER: A novel framework for effi-
cient deep transfer learning in networking . pp. 146 /C0151.
Poostchi, M., Silamut, K., Maude, R. J., Jaeger, S., & Thoma, G. (2018). Image analysis and machine learning
for detecting malaria. Translational Research: the Journal of Laboratory and Clinical Medicine ,194,
36/C055. Available from https://doi.org/10.1016/j.trsl.2017.12.004 , Apr.
Prashanth, R., Roy, S. D., Mandal, P. K., & Ghosh, S. (2014). Automatic classification and prediction models
for early Parkinson’s disease diagnosis from SPECT imaging. Expert Systems with Applications ,41(7),
Art. no. 7.
Prashanth, R., Roy, S. D., Mandal, P. K., & Ghosh, S. (2016). High-accuracy classification of parkinson’s dis-
ease through shape analysis and surface fitting in 123I-Ioflupane SPECT imaging. IEEE Journal of
Biomedical and Health Informatics ,21(3), 794 /C0802.
Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., ..., Ng, A.Y. (2017). Chexnet: Radiologist-
level pneumonia detection on chest x-rays with deep learning. arXiv171105225 .
Ramraj, S., Uzir, N., Sunil, R., & Banerjee, S. (2016). Experimenting XGBoost algorithm for prediction and
classification of different datasets. International Journal of Control Theory and Applications ,9, 651 /C0662.
Sak, H., Senior, A. W., & Beaufays, F. (2014). Long short-term memory recurrent neural network architec-
tures for large scale acoustic modeling .
Saltz, J., Gupta, R., Hou, L., Kurc, T., Singh, P., Nguyen, V., ...Thorsson, V. (2018). Spatial organization
and molecular correlation of tumor-infiltrating lymphocytes using deep learning on pathology images. Cell
Reports ,23(1), 181 /C0193.
Santosh, T. S., Parmar, R., Anand, H., Srikanth, K., & Saritha, M. (2020). A review of salivary diagnostics
and its potential implication in detection of Covid-19. Cureus ,12(4).
Sawada, S., Kuklane, K., Wakatsuki, K., & Morikawa, H. (2017). New development of research on personal
protective equipment (PPE) for occupational safety and health. Industrial Health ,55(6), 471 /C0472.
Available from https://doi.org/10.2486/indhealth.55-471 .
Schalekamp, S., Huisman, M., van Dijk, R. A., Boomsma, M. F., Jorge, P. J. F., de Boer, W. S., ...Schaefer-
Prokop, C. M. (2020). Model-based prediction of critical illness in hospitalized patients with COVID-19.Radiology ,298(1), E46 /C0E54. Available from
https://doi.org/10.1148/radiol.2020202723 .
Schaumberg, A. J., Rubin, M. A., & Fuchs, T. J. (2018). H&E-stained whole slide image deep learning pre-
dicts SPOP mutation state in prostate cancer. BioRxiv , 064279.
Schiffman, J. D., Fisher, P. G., & Gibbs, P. (2015). Early detection of cancer: Past, present, and future.
American Society of Clinical Oncology Educational Book ,35(1), 57 /C065.
Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks: The Official
Journal of the International Neural Network Society ,61,8 5/C0117. Available from https://doi.org/10.1016/j.
neunet.2014.09.003 , Jan.
Seoh R. (2020). Qualitative analysis of Monte Carlo dropout. arXiv200701720 Cs Stat .,http://arxiv.org/abs/
2007.01720 .. Accessed: 07.08.20.227 ReferencesSetio, A. A. A., Traverso, A., De Bei, T., Berens, M. S. N., van den Bogaard, C., Cerello, P., ...Jacob, C.
(2017). Validation, comparison, and combination of algorithms for automatic detection of pulmonary
nodules in computed tomography images: The LUNA16 challenge. Medical Image Analysis ,42,1/C013.
Available from https://doi.org/10.1016/j.media.2017.06.015 , Dec.
Sharma, G., & Carter, A. (2017). Artificial intelligence and the pathologist: Future Frenemies? Archives
of Pathology & Laboratory Medicine ,141(5), 622 /C0623. Available from https://doi.org/10.5858/arpa.2016-
0593-(ED.) .
Shi, F., Wang, J., Shi, J., Wu, Z., Wang, Q., Tang, Q., ...Shen, D. (2020). Review of artificial intelligence
techniques in imaging data acquisition, segmentation and diagnosis for COVID-19. IEEE Reviews in
Biomedical Engineering , 1. Available from https://doi.org/10.1109/RBME.2020.2987975 ,1 .
Shin, H., Roth, H. R, Gao, M., Lu, L., Xu, Z., Nogues, I., ...Summers, R. M. (2016). Deep convolutional neu-
ral networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning.
IEEE Transactions on Medical Imaging ,35(5), 1285 /C01298. Available from https://doi.org/10.1109/
TMI.2016.2528162 .
Simonyan, K. & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition.
arXiv14091556 .
Sufian, A., Ghosh, A., Sadiq, A. S., & Smarandache, F. (2020). A survey on deep transfer learning to edge
computing for mitigating the covid-19 pandemic. Journal of Systems Architecture ,108, 101830.
Sufian, A., Jat, D. S., & Banerjee, A. (2020). Insights of artificial intelligence to stop spread of covid-19 .Big
Data Analytics and Artificial Intelligence Against COVID-19: Innovation Vision and Approach
(pp. 177 /C0190). Springer.
Sultana, F., Sufian, A., & Dutta, P. (2018). Advancements in image classification using convolutional neural
network , pp. 122 /C0129.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., & Rabinovich, A. (2015). Going deeper
with convolutions (pp. 1 /C09).
Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. (2017). Inception-v4, inception-resnet and the impact of
residual connections on learning . 31, 1.
Tajbakhsh, N., Shin, J. Y., Gurudu, S. R., Hurst, T., Kendall, C. B., Gotway, M. B., & Liang, J. (2016).
Convolutional neural networks for medical image analysis: Full training or fine tuning? IEEE Transactions
on Medical Imaging ,35(5), 1299 /C01312.
Talo, M., Baloglu, U. B., Yıldırım, O ¨., & Rajendra Acharya, U. (2019). Application of deep transfer learning
for automated brain abnormality classification using MR images. Cognitive Systems Research ,54,
176/C0188. Available from https://doi.org/10.1016/j.cogsys.2018.12.007 , May.
Titano, J. J., Badgeley, M., Schefflein, J., Pain, M., Su, A., Cai, M., ...Oermann, E. K. (2018). “Automated
deep-neural-network surveillance of cranial images for acute neurologic events. Nature Medicine ,24(9),
1337 /C01341.
Topol, E. J. (2019). High-performance medicine: the convergence of human and artificial intelligence. Nature
Medicine ,25(1), 44 /C056. Available from https://doi.org/10.1038/s41591-018-0300-7 .
Vasuki, A., & Govindaraju, S. (2017). Deep neural networks for image classification ,.Deep Learning for
Image Processing Applications (vol. 31, p. 27). IOS Press.
Waite, S., Scott, J. M., Legasto, A., Kolla, S., Gale, B., & Krupinski, E. A. (2017). Systemic error in radiology.
American Journal of Roentgenology ,209(3), 629 /C0639.
Wang, C. & Mahadevan, S. (2011). Heterogeneous domain adaptation using manifold alignment . 22, 1, p. 1541.
Wang, L. & Wong, A. (2020). COVID-Net: A tailored deep convolutional neural network design for detection
of COVID-19 cases from chest X-ray images. arXiv200309871 Cs Eess .,http://arxiv.org/abs/
2003.09871 .. Accessed 07.08.20.
Wang, L., Lin, Z. Q., & Wong, A. (2020). Covid-net: A tailored deep convolutional neural network design for
detection of covid-19 cases from chest x-ray images. Science Reports ,10(1), 1 /C012.228 Chapter 8 Advanced pattern recognition tools for disease diagnosisWang, S., Kang, B., Ma, J., Zeng, X., Xiao, M., Guo, J., ...Xu, B. (2020). A deep learning algorithm using
CT images to screen for Corona Virus disease (COVID-19). MedRxiv .
Wang, X., Yang, W., Weinreb, J., Han, J., Li, Q., Kong, X., ...Wang, L. (2017). Searching for prostate cancer
by fully automated magnetic resonance imaging classification: deep learning vs non-deep learning. Science
Reports ,7(1), 15415. Available from https://doi.org/10.1038/s41598-017-15720-y .
Wong, K. K. L., Fortino, G., & Abbott, D. (2020). Deep learning-based cardiovascular image diagnosis: A
promising challenge. Future Generation Computer Systems ,110, 802 /C0811. Available from https://doi.org/
10.1016/j.future.2019.09.047 , Sep.
Wu, S. Y., Yau, H. S., Yu, M. Y., Tsang, H. F., Chan, L. W. C., Cho, W. C. S., ...Wong, S. C. C. (2020).
The diagnostic methods in the COVID-19 pandemic, today and in the future. Expert Review of Molecular
Diagnostics ,20(9), 985 /C0993. Available from https://doi.org/10.1080/14737159.2020.1816171 , Sep.
Wu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., Motoda, H., ...Steinberg, D. (2008). Top 10 algo-
rithms in data mining. Knowledge and Information Systems ,14(1), 1 /C037.
Yassin, N. I. R., Omran, S., El Houby, E. M. F., & Allam, H. (2018). Machine learning techniques for breast can-
cer computer aided diagnosis using differe nt image modalities: A systematic review. Computer Methods and
Programs in Biomedicine ,156,2 5/C045. Available from https://doi.org/10.10 16/j.cmpb.2017.12.012 ,M a r .
Yosinski, J., Clune, J., Bengio, Y., & Lipson, H. (2014). How transferable are features in deep neural net-
works? arXiv14111792 .
Zamir, A. R., Sax, A., Shen, W., Guibas, L., Malik, J., & Savarese, S. (2018). Taskonomy: Disentangling task
transfer learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recog nition,
June, pp. 3712 /C03722, doi: 10.1109/CVPR.2018.00391.
Zeiler, M. D. & Fergus, R. (2014). Visualizing and understanding convolutional networks , pp. 818 /C0833.
Zhang, J., Xie, Y., Pang, G., Liao, Z., Verjans, J., Li, W., . . . Xia, Y. (2020). Viral pneumonia screening on
chest x-ray images using confidence-aw are anomaly detection. arXiv200312338 , vol. 3.
Zhu, Y., Chen, Y., Lu, Z., Pan, S. J., Xue, G.-R., Yu, Y., & Yang, Q. (2011). Heterogeneous transfer learning
for image classification ,25(1).
Zhu, Z., Albadawy, E., Saha, A., Zhang, J., Harowicz, M. R., & Mazurowski, M. A. (2019). Deep learning for
identifying radiogenomic associations in breast cancer. Computers in Biology and Medicine ,109,8 5/C090.
Zintgraf, L. M., Cohen, T. S., Adel, T., & Welling, M. (2017). Visualizing deep neural network decisions:
Prediction difference analysis. arXiv170204595 .229 ReferencesThis page intentionally left blankCHAPTER
9Brain-computer interface in Internet
of Things environment
Vijay Jeyakumar1, Palani Thanaraj Krishnan2, Prema Sundaram3and Alex Noel Joseph Raj4
1Department of Biomedical Engineering, Sri Sivasubramaniya Nadar College of Engineering, Chennai, Tamil Nadu,
India2Department of Electronics and Instrumentation Engineering, St. Joseph’s College of Engineering, Chennai,
Tamil Nadu, India3Department of Biomedical Engineering, RVS Educational Trust’s Group of Institutions, Dindigul,
Tamil Nadu, India4Key Laboratory of Digital Signal and Image Processing of Guangdong Province, Department of
Electronic Engineering, College of Engineering, Shantou University, Shantou, P.R. China
9.1 Introduction
Brain-computer interface (BCI) popularly called brain-machine interface (BMI) is a communication
system involving hardware and software components. It acquires brain signals generated by thecentral nervous system, interprets, translates them into commands, converts them and relays theminto an artificial output device to carry out the desired action, thereby changing the interaction ofthe user with their external and internal environment.
BCI provides direct communication between a wired brain of a person and a device attached exter-
nally to translate the neuronal activity of the brain into signals to assist, augment or repair vision, hear-
ing, mobility, communication, human cognitive or senso ry-motor functions without involving peripheral
nerves and muscles to achieve a therapeutic effect. BCI records and decodes brain signals and develops
a new nonmuscular channel to communicate the intention of a person to systems, speech producers,assistive appliances and neuroprostheses which is d ue to the neuroplasticity of the brain. The user and
the interface work together in which the trained u ser generates signals from prostheses which can be
handled by the user’s brain, similar to natural sensor channels by encoding intention, whereas the BCI
decodes signals and converts them into commands to an external device by the user intention.
The advancement of technology has allowed humans to employ the electrical signals produced
from the brain to interrelate with, control or modify one’s environment. The upcoming era of BCItechnology will soon improve the quality of lives of many disabled individuals who are not able tospeak, utilize their limbs to operate assistive devices and reduce the cost of their medical expenses.
BCI technology is an upcoming research and development area which is of great interest to
scientists, clinicians and engineers. Research on BCI is a young multidisciplinary field and hailsresearchers from different areas such as neuroscience, engineering, physiology, psychology, com-
puter science, mathematics and rehabilitation.
Hans Berger detected neural activity and recorded electroencephalography (EEG) in 1924. The
term BCI was first introduced and coined by Dr. J. Vidal in the early 1970s and the field has beenevolving at a fast pace and reached many milestones for creating a pathway for severely disabled
2315G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00012-7
©2022 Elsevier Inc. All rights reserved.patients and in further helping humans for control of external devices. The principal aim of BCI is
to rehabilitate the normal physiology of patients affected by neuromuscular disorders like cerebralpalsy, stroke, sclerosis or injury in the spine. Research on BCI is growing extremely at a rapid rateover the past decade and enables patients to check a simulated e-mail, control television and toperform activities with the robotic arm. In principle, BCI uses brain signals (electroencephalo-
gram—EEG) to control and to gather data on user intention based on polarity changes generated by
neuronal postsynaptic membrane due to activation of voltage or ion gated channels. In BCI, electro-des placed on the scalp, on the surface or in the cortex are used to measure electrical signals pro-duced from brain activity and are the widely studied signals in BCI. Thus, brain activity ismeasured and is translated into tractable electrical signals using a BCI (
Table 9.1 ).
Usually, BCI monitors two types of brain activities:
1.Electrophysiological activity
It is produced by neurons exchanging electrochemical neurotransmitters generating ionic
currents within and across neurons traveling through the dendritic trunk called primary currents.This primary current is enclosed by secondary currents and this activity can be measured usingtechniques such as EEG, electrocorticography (ECoG), magnetoencephalography (MEG) andsignal acquisition in individual neurons.
2.Hemodynamic response
It involves the release of glucose by blood at a greater rate at the active neurons than in the
inactive ones leading to a change in the proportion of oxyhemoglobin to de-oxyhemoglobinlevels in the veins of the active area, which can be estimated by indirect methods such asfunctional magnetic resonance (fMRI) and near-infrared spectroscopy (NIRS).
9.1.1 Components of BCI
The working component of a BCI consists of:
1.Sensors to measure brain signals (usually electrodes)
2.Amplifier to strengthen signals and
3.A computer system to transmit signals into commands to control external devices which can be
portable or wearable.
Table 9.1 Electroencephalography frequency bands with properties.
BandFrequency
(Hz)Amplitude
(µV) Location Activity
Delta 0.5 /C04 100 /C0200 Frontal Deep sleep
Theta 4 /C085 /C010 Various Drowsiness, light
sleep
Alpha 8 /C013 20 /C080 Posterior region of the head Relaxed
Beta 13 /C030 1 /C05 Left and right side, symmetrical distribution, most
evident frontallyActive thinking,
alert
Gamma .30 0.5 /C02 Somatosensory cortex Hyperactivity232 Chapter 9 Brain-computer interface in Internet of Things environment9.1.2 Types of BCI
Based on the nature of the input signals, BCI can be categorized as
1.Exogenous BCI
This involves the use of an external stimulus to e licit neuron activity with in the brain and does not
require extensive training due to ea sy and quick setup, which shows a fa st information transfer rate of
up to 60 bits/min and could be acquired with only one EEG channel. This mode of BCI requirespermanent attention to the applied stimulus and sometimes causes tiredness in users.
2.Endogenous BCI
Involves self-regulation of brain potential without the use of an external stimulus. Through
training, the users whose sensory organs are affected, learn to produce brain patterns decoded
by BCI that can act as a two-dimensional space, suitable for cursor control applications.
Multichannel EEG recordings are required for best performance at a lower bit rate of20/C030 bits/min and are a tiresome process where all users are not able to gain control.
Based on the input data processing module, BCI systems can be categorized as
1.Synchronous (or cue paced)
In this type of BCI, brain signals in a predefined time window are analyzed whereas those
outside the window are rejected. Hence the user is permitted to send commands within the
stipulated period represented by the BCI system. Thus the time of mental activity is known
beforehand and is linked with a specific cue. Also, the patient can blink and do all other eyemovements but may not show a usual mode of interaction.
2.Asynchronous (or self-paced)
In asynchronous mode, brain signals are analyzed continually without considering the user’s
actions. This is the natural way of BCI involving complex computation and difficult evaluation.
9.1.3 How does BCI work?
A BCI documents and elucidates brain signals. The electrical signals are transmitted by the neurons
communicating with each other which could be acquired through advanced electrical sensors.
A precise communication pathway between the brain and the muscle is needed for a normal per-
son to control body movements. Medical conditions such as stroke or neuromuscular disorders canalter or cease the communication between brain and body and can cause paralysis or cerebral palsy.However, the brain is still able to bring about activity for desired movements and thus, BCI canuse those brain signals to control assistive devices (
Christian, 2018 ).
Various techniques are made available to compute brain signals. The most commonly used
among them is Electroencephalography which implies electrodes to be placed on the scale. Certain
other techniques employ electrodes to be placed directly under the scalp or in the brain tissue itselfinvolving surgical intervention without damaging the brain. The quality of the electrical signalsobtained directly from the brain is better than signals recorded from the scalp and thusimplantable BCIs are preferred and developed for paralyzed people.
Other procedures followed for measuring brain activity includes functional MRI (fMRI), using
MRI scanner and MEG with the MEG-scanner not suitable for home use, large and expensive233 9.1 Introductioninfrastructure. An advanced technique of NIRS is used to measure brain activity through the skull
employing near-infrared light and does not require surgery.
Various centers in the brain can control different parts of the body accountable for a variety of
movements. The different techniques employed to measure brain activity can analyze when the dif-ferent control centers in the brain are active, thereby allowing BCIs to recognize body movements
from brain activity. A unique characteristic of the brain is that these control centers are active
when thinking about moving without actually doing it. The distinct control centers in the brain canbe purposefully turned on or off by doing mental or physical tasks even in paralyzed patients, thusmaking BCI a realistic and promising technology.
By incorporating electrodes on specific control centers in the brain, the obtained signals are
detected and could be converted to a command to perform a specific task or function on a device.An example is to make a computer mouse “click” by paralyzed patients when they lift their eye-brows. Thus BCI can be used as a “button” to control devices.
9.1.4 Key features of BCI
The essential key features of BCI to be
 Effectiveness—The BCI system should be really helpful to users.
 Robustness—The system must be stable during regular use and robust concerning anomalies. Quick operation—Task execution time should be as low as possible.
 More functionality—The system must allow the user to do as many tasks as feasible to increase
the autonomy of the user.
 Safety—The system must pose no danger to user-health and comfort. The EEG cap should be
comfortable enough to be worn for several hours.
 Mobility—To ensure mobility, the BCI system should be wireless, lightweight, and compact. User-friendliness—The system should be simple to operate and need no expert help for daily use.
 Cost-effectiveness—The price should be affordable to all kinds of users.
9.1.5 Applications
BCI exerts its applications in different fields of:
 Medicine
 Neuroergonomics and smart environment Neuromarketing and advertisement Educational and self-regulation Games and entertainment
 Security and authentication
Being a dynamic and growing field, BCI finds many useful applications in medicine for-
1.Disease prevention
a.Smoking
b.Alcoholism
c.Motion sickness234 Chapter 9 Brain-computer interface in Internet of Things environment2.Detection and diagnosis of diseases
a.Tumors
b.Brain disorders
c.Sleep disorders
3.Rehabilitation and restoration
a.Human-computer interaction using EEG data for classifying various mental states such as
Relaxed, neutral or concentrating and mental-emotional states namely negative, neutral orpositive, provides assistive technology for disabled individuals having neuromuscularproblems such as schizophrenia or depression.
b.In locked-in state patients, to allow commu nication within them and to restore normal
function and who have no control ove r motor functions to improve sensory
processing.
c.To restore social interaction for patients by bringing out cursor control, robotic arms,
prosthetic devices and wheelchairs.
d.To improve rehabilitation of patients after stroke, head trauma and other disorders.
e.Augments natural motor outputs for the improvement in the achievement of skilled pilots,
surgeons and other professionals.
9.2 Brain-computer interface classification
Generally, BCI can be classified based on depe ndability, method of recording and operating
mode as shown in Fig. 9.1 . Regarding dependability, it can be dependent or independent. In
the case of dependent BCI, the motor control b y the user is required and vice versa for the
independent BCI. MI-based BCI is the best exa mple of independent BCI and are exclusively
useful for stroke and impaired patients. Whereas in the case of the method of recording,BCI can be classified as invasive and noninvasi ve. Usually, a signal to acquire elements
like sensors, electrodes are used primaril y to record EEG. If any needle or implant type
(usually microelectrode arrays are often used) sensing elements are used to acquire EEG,then it is an invasive BCI and surface electrodes are used in most of the noninvasive BCIapplications.
9.2.1 Noninvasive BCI
In noninvasive types of BCI, the sensors are located to quantify brain activity by measuring electri-
cal potentials on the scalp using EEG or the magnetic field using MEG. Various advanced techni-
ques such as fMRI, positron emission tomography (PET), NIRS are also used in noninvasive BCIto measure brain activity which depends on changes in the rate of flow of blood called a hemody-namic response.
In severely and partially paralyzed patients, noninvasive BCI seems to be successful
to reacquire communication and to control p rosthetic devices, but motor recovery is of
limited use.235 9.2 Brain-computer interface classificationFIGURE 9.1
Classification of BCI systems based on dependability, method of recording and operating mode.
Courtesy Rashid, M., Sulaiman, N., Abdul Majeed, A. P. P., Mosa, R. M., Nasir, A. F. A., Bari, B. S., & Khatun, S. (2020). Current
status, challenges, and possible solutions of EEG-based brain-computer interface: A comprehensive review. Frontiers in
Neurorobotics, 14 .https://doi.org/10.3389/fnbot.2020.00025 .236 Chapter 9 Brain-computer interface in Internet of Things environment9.2.2 Semiinvasive or partially invasive BCI
In this approach, BCI devices are implanted on the outer surface of the brain, inside the skull, but
not within the gray matter to acquire brain electric potentials from the cerebral cortex using ECoG.This method requires a craniotomy to implant the electrodes and is used only when surgery is man-datory especially in epileptic patients.
Being a very promising modality, semiinvasive BCI can provide greater resolution signals than
the noninvasive type and shows a reduced level of formation of scar tissue in the brain than in inva-
sive BCI. ECoG based BCI recordings and performance proves to be stable over several months
and is best suited for long-term use.
9.2.3 Invasive BCI
In this technique, microelectrodes are directly implanted into the intraparenchymal cortex, and the
signals obtained from a single neuron are measured. Invasive BCI enables straight communication
between the brain and a device but involves placing electrodes directly into the gray matter of the
brain through neurosurgery. They can be a single unit BCI to find a signal from a single region ofbrain cells or a multiunit BCI to detect from multiple regions. These devices provide more accurateand best-quality signals rather than are prone to the formation of scar tissue making brain signalsweaker as the body responds to a foreign object placed in the brain. Because of the expensivenessand high risk, the usual main targets of direct brain implants are noncongenital blind (acquired) andparalyzed patients.
Thus, shortly, direct brain implants used for neuronal recordings in humans may permit valuable
insights into the live human brain to improve the quality of life and to regain autonomy.
9.3 Key elements of BCI
The brain is constantly producing electrical sign als that can be recorded by keeping electrodes
on the scalp or skull which act as good electrical insulators. In humans, every nerve is linked
to nearly 10,000 nerves through dendrites. So, when the neurons make contact, currents occurand an electrical signal is passed along an axon or dendrite by the release of neurotransmitters,
which transmit through the synapse to the dendrite of another neuron and are reconverted toelectrical signals.
When the electric current leaves the neuron, it creates positive polarity and vice versa. These
primary currents in the brain tissue reach the skull and the scalp and these voltage differences gen-erated or trapped by EEG electrodes. Millions of parallelly oriented adjacent dendrites have to be
active synchronously to have a quantifiable signal.
BCI aims to recognize and measure brain activity to denote the user objective and to interpret
the features into device commands to fulfill the user intention in real-time (
Fig. 9.2 ).
To achieve this, BCI being an artificial intelligence system is designed to detect a set of patterns
in brain activity in six sequential stages:237 9.3 Key elements of BCI9.3.1 Signal acquisition
Using any one of the particular sensors, for example, scalp or intracranial electrodes, brain signals
are measured. The signals obtained are amplified and are made suitable for electronic processing.
Numerous wired and wireless devices are available in the market to acquire EEG signals withoutthe use of conductive gels or pastes. A list of devices with their specifications are available in
Table 9.2 .
9.3.2 Preprocessing or signal enhancement
This is a multistep process involving the removal of noise and artifacts from the data. High and
low pass filters are applied to get rid of undesired power line interference. Other methods employedare used to cut away artifacts such as eyeball movements or eye blinking. After which the recordingof signals, the significant features are generated for further processing and analysis and also forcomputer transmission and communication.
9.3.3 Feature extraction
Being a very challenging task, it analyzes brain signals and extracts information from them. EEG is
very complex and needs processing algorithms to differentiate important signal characteristics from
extra content and to map them onto vectors in a ready form suitable for interpretation into output
commands. The extracted features must have strong relevance to the user intention as the moststrongly correlated brain activity can be transient or oscillatory. In current BCI systems, the mostprominently extracted features are response amplitudes and frequency ranges in EEG or ECoG andfiring rates of individual cortical neurons. To guarantee precise computation of brain signals, envi-ronmental and physiologic artifacts are avoided or eliminated without relevant information loss.Many methods are available for feature extraction and include band powers and cross-correlation.
9.3.4 Classification stage
This step classifies the signals considering feature vectors with the help of classification algorithms.
Using different techniques, a classifier is trained to recognize different features into different
FIGURE 9.2
The general architecture of brain-computer interface.238 Chapter 9 Brain-computer interface in Internet of Things environmentclasses mathematically to achieve effective pattern recognition and to decrypt the user’s intention
in performing mental tasks.
9.3.5 Feature translation or control interface stage
The signals classified are fed to the feature translation algorithm that translates the features into
respective meaningful commands to the external device such as a wheelchair or a computer toaccomplish the user’s intent. The algorithm used must be dynamic enough to assist or adapt to thenatural or acquired alterations in the signal features and must contribute to a complete range ofdevice control.
9.3.6 Device output or feedback stage
The translated commands from the algorithms could be able to operate external devices and
provide operations such as letter selection, a rob otic arm to allow movements, cursor control in
a computer and so on. The operating device will se nd feedback to the user, thus terminating the
process.Table 9.2 Electroencephalography devices.
Device name No. of channels Sampling frequency Communication
NeuroScan SynAmps: 64
Grael: 32NuAmps: 40
Siesta: 32SynAmps: 20 kHz
Grael: 4096 HzNuAmps: 1000 Hz
Siesta: 1024 HzWired
Brain Products LiveAmp: 8/16/32 Between 250, 500, and 1000 Hz Wireless
BioSemi 16, 32, or 64 2/4/8/16 kHz WiredEmotiv INSIGHT: 5
EPOC1:1 4
EPOC FLEX: 32128 Hz Wireless
NeuroSky 1 512 Hz Wireless
Advanced brain monitoring ABM B-Alert X24: 24 256 Hz WirelessG.tec nautilus 64 500 Hz WirelessAntNeuro EEG 64 2048 Hz WirelessNeuroelectrics Enobio 32 32 500 Hz WirelessMuse 4 256 Hz WirelessOpenBCI Up to 16 256 Hz WirelessCognionics Mobile 72 500 /C01000 Hz Wireless
mBrainTrain 24 250 /C0500 Hz Wireless
MyndBand EEG headset 3 512 Hz WirelessEnobio 8, 20, or 32 500 Hz Wireless239 9.3 Key elements of BCI9.4 Modalities of BCI
With the advancement of functional neuroimaging technology, various potential novel modalities
are employed based on the following factors.
9.4.1 Electrical and magnetic signals
Recording of electrical signals and generation of magnetic signals to control a BCI involves the
use of:
9.4.1.1 Intracortical electrode array
Intracortical neuron recording technology is the most invasive method because it implants a singleor array of electrodes into the cortex of the brain to detect the action signals out of individual neu-
rons. The electrodes need to be implanted near to the signal source to ensure their stability over
long periods but may face long-term signal variability due to neuron death or high tissue resistanceand significant noise effect.
This technology is used for some severely disabled people suffering from a neurodegenerative
disease whose neurons in the brain and spinal cord are affected, for example, amyotrophic lateralsclerosis (ALS). These direct brain implant electrodes are implanted into the visual cortex of blindpeople to restore sight and to synthesize phosphenes, the sensation of seeing light. Also, motor neu-roprosthetics are developed to restore movements in paralytic patients to use computer cursors,
robotic prosthetic arms, lights and television.
9.4.1.2 Electrocorticography
ECoG is measured by placing electrodes on the cortex of the brain, outside the dura mater called
epidural ECoG or below the dura mater termed subdural ECoG, requiring craniotomy to implantelectrodes onto the cortex involving 4 /C0256 electrodes to perform a diverse range of studies. It is a
less invasive modality, however, preserves the benefits of the invasive approach. It provides highspatial resolution and signals fidelity with the reduction in noise and artifacts, lesser clinical riskand better long-term stability with minimal training, rapid control, wider frequency and low techni-cal difficulties. Thus making ECoG a good candidate for seizure localization in epileptic patientsfor carrying out motor tasks such as the finger, hand and arm flexion to control cursor movements
or prosthetic hands in paralyzed and nonparalyzed epileptic patients and to provide speech and lan-
guage processing for speech synthesis.
9.4.1.3 Electroencephalography
It provides a recording of the electrical activity of the tuft of neurons along the surface of the scalpby detecting voltage changes accompanying neurotransmission with the neurons. Multiple electro-des of up to 500 can be mounted on cap-like devices allowing data collection from the same scalpregion taking thousands of snapshots of brain activity from different sensors per second. EEG datacontains rhythmic activity reflecting neural oscillations explained by frequency, phase and power toshow an association between rhythms and different states of brain activity. EEG provides uniqueusability benefits and makes them suitable for commercial use. It is the most widely preferred240 Chapter 9 Brain-computer interface in Internet of Things environmentapproach due to minimal risk, portability, inexpensiveness, ease of convenience in wearing without
the need for surgery, relative ease of implementation and performance. Brain activity acquiredthrough BCI can control a cursor, spelling device, assistive devices and prosthetic devices at a fas-ter response time but show poor spatial resolution, signal to noise ratio and cannot be effectivelyused for high-frequency signals as the skull dampens the signal produced by the neurons and also
requires training for each session. Spatial resolution can be enhanced by employing suitable filters
or by integrating EEG with other modalities.
9.4.1.4 Magnetoencephalography
Using highly sensitive magnetometers, magnetic fields created by electric currents occurring in thebrain are utilized for mapping brain activity. Superconducting quantum interference devices(SQUID) is used for acquiring magnetic signals outside the head. This recording method involveslab configuration with shields and special equipment so that MEG signals did not interfere with theearth’s magnetic field. The received signals are less disturbed by the skull and cerebrospinal fluid(CSF). It offers better spatial resolution and is highly sensitive to tangential sources. They are better
in detecting high-frequency signals as the magnetic field could easily penetrate through the skull
and scalp, however, show portability and cost issues.
9.4.2 Metabolic signals
Acquiring metabolic signals based on oxygen level-dependent changes from the brain to control a
BCI involves the use of:
9.4.2.1 Positron emission tomography
It is the nuclear imaging technique used to analyze blood flow, metabolism and neurotransmitteractivity within brain cells. A specific amount of a radioactive tracer is injected into the bloodstreamto reach brain cells. Radiotracer attaches to the glucose molecule and forms fluorodeoxyglucose.
The principal fuel of the brain is glucose. So, the regions of the brain which are active will use glu-
cose at a higher rate than inactive regions. By performing a PET scan, it allows scientists to findthe working pattern of the brain and to diagnose diseases like dementia, Alzheimer’s, Parkinson’sand epilepsy by color codings showing highly active regions in warmer colors of yellow and red.
9.4.2.2 Functional magnetic resonance imaging
fMRI underlays MRI Technology that analyzes blood flow changes associated with brain activity.fMRI works on the principle that cerebral blood flow is closely coupled to neuronal activation andthus helps in mapping activity to the respective used area of the brain. It relies on the fact thatwhen an area of the brain is in the active state, it requires increased blood flow which is termed
blood-oxygen-level-dependent contrast, dependent on hemodynamic response.
Hemoglobin, the oxygen-carrying protein present in the red blood cells that distribute oxygen to
the brain tissue exists in two forms-oxyhemoglobin and deoxyhemoglobin and the magnetic proper-ties of the two states of hemoglobin change based on their oxygen levels. A more active state ofthe brain increases the demand for more oxygen leading to an increased blood flow to that part ofbrain tissue thereby changing the magnetic characteristics which would detect the more active areasof the brain at a specified time. Though it provides low temporal resolution, fMRI proves to be a241 9.4 Modalities of BCIsafe and convenient technique with high spatial resolution as it could acquire details from deep
parts of the brain than any other technique.
9.4.2.3 Functional near-infrared spectroscopy
NIRS is employed for functional neuroimaging to evaluate brain activity through blood dynamicsassociated with neuron activity using a near-infrared range of light. fNIRS could collect only 10samples per second with low spatial resolution and cannot detect electrical signals past the cortex.
However, this technology uses devices that are portable, more accessible, less expensive and less
sensible to artifacts.
The upcoming field of BCI relies on the progress in three critical areas of research- to develop
a stable acquisition system to function in all e nvironments, to validate and disseminate in the
long-term and to improve moment-to-moment relia bility of BCI performanc e for different user
populations.
9.5 Computational intelligence methods in BCI/BMI
The main objective of this chapter is to audit a wide choice of EEG signal handling procedures uti-
lized in BCI-based EEG frameworks, with a specific spotlight on the cutting edge with respect tofeature extraction and selection. It also examines the difficulties and impediments faced during theplan and execution of signal processing procedures. In the vast majority of the EEG-BCI frame-works these days, essential EEG information is pre-prepared to dispose of noise, however not allframeworks pre-process information. Features are then extracted from the EEG information and themost notable highlights for classification might be chosen. Each part of this outline will be exam-ined in more prominent detail in this section.
9.5.1 State of the prior art
9.5.1.1 Preprocessing
EEG signals are ordinarily utilized for BCIs because of their high resolution in time and the cost-
adequacy of brain signal acquisition when contrasted with different strategies like fMRI and MEG.Notwithstanding, EEG signals present processing challenges; since they are nonfixed, they canexperience the ill effects of external noise and are inclined to signal distortion.
Jafarifarmand et al. (2017) suggested an automatic artifact removal in EEG recording is possible
by independent component analysis (ICA) involving five methods for identifying trials consistingof artifacts. They divided trials into six epochs and applied ICA for them. The illegal epochs were
marked and if the percentage of unmarked epochs is below 30% in each trial, then they were
deleted from the training and testing dataset. ICA can distinguish between physiological noise frommultichannel EEG signal which enables restoration to a noise-free physiological signal. The inde-pendent components are determined by their spectral density.
Eltaf et al. (2018) utilized a temporal
filter for preprocessing which is a bandpass filter to remove artifacts and has a range of 0.5 /C030 Hz.
A filter bank approach can be used for extracting alpha and beta activity since these waves are
related to mental images by Serdar Bascil et al. (2015) .Khan and Hong (2017) demonstrated that242 Chapter 9 Brain-computer interface in Internet of Things environmentmany studies utilized channel averaging, value-based channel selection and vector phase analysis
as preprocessing tools to remove artifacts. In the channel averaging technique all channels relatedto brain activity are averaged. The channel selection technique allows the removal of a redundantchannel.
9.5.1.2 Feature extraction
The extracted features should seize salient signal characteristics which may be used as a foundationfor the differentiation between specific brain states. For feature extraction, many different methodsare available in time Domain and Frequency Domain. Time-frequency evaluation is strong because
it allows spectral data about an EEG signal to be mapped to the temporal domain, which is benefi-
cial for BCI technology given that spectral brain interest varies at some point of the duration of useof the device as different tasks are executed.
Jafarifarmand et al. (2017) used wavelet transform
(WT) as a feature extraction technique because they are powerful decomposition techniques toderive dynamic features from the nonstationary EEG signals. Wavelet packet decomposition(WPD) is utilized to split the EEG signals into various frequency bands. Haar mother wavelet isused for decomposition and the extracted signal features are namely power spectrum, variance andmean.
Eltaf et al. (2018) utilized intrinsic time scale decomposition (ITD) which is an updated version
of empirical mode decomposition (EMD) for analyzing the EEG signal into various proper rotationcomponents (PRC). ITD is advantageous since it can provide exact time, frequency and energylocalization. Serdar et al. proposed an approach combining the average signal power method forfeature extraction and the principal component analysis (PCA) algorithm for the selection of fea-tures. PCA enables a reduction in redundancy in terms of Complexity. The main purpose of PCA isfor dimension reduction. EEG and fNIRS are the two primary BCI modalities requiring mobilityand trends have demonstrated that these two frameworks can be coordinated to improve the BCI
execution.
Hong et al. (2017) have briefly discussed the various feature extraction schemes such as
power spectrum density based on Welch’s method.
Logarithmic band power is another approach in which the logarithmic power of different fre-
quency bands is calculated. For EEG feature extraction, common spatial pattern (CSP) is used tomap multichannel EEG signals into a less dimensional subspace.
Chakladar and Chakraborty
(2018) documented that the fast Fourier transform (FFT) is implemented to obtain a frequency
spectrum for extracting features from the EEG signal. Since EEGs are nonstationary signals, short-time Fourier transform (SSFT) is applied to get needed features. An extensive feature extraction
category has also been explained. PCA is used for reducing the dimension by removing artifacts.
Autoregressive components (AR) is a time-domain feature extraction technique. It uses a paramet-ric approach. AR is not suitable with nonstationary EEG signals hence to overcome this disadvan-tage multivariate adaptive AR (MVAAR) is used.
9.5.1.3 Feature classification
This subsection intends to offer a short precis of the various classification techniques used. Supportvector machines (SVMs) and linear discriminant analysis (LDA) have been determined to be thebroadly used linear classifiers and artificial neural network (ANN), K /C0nearest neighbor (KNN) is
mostly used as a nonlinear classifier.
Jafarifarmand et al. (2017) employed SVM and artificial neu-
ral types [multilayer perceptron, probabilistic neural network (PNN)] as classifiers. Among which243 9.5 Computational intelligence methods in BCI/BMISVM uses optimum hyperplane to distinguish between two classes and gives better results. ANN
approaches such as feed-forward back propagation neural networks (FFBPNN) were designed,trained and investigated with different activation functions by
Eltaf et al. (2018) . The mean classifi-
cation accuracy of 92.20% was obtained.
Serdar Bascil et al. (2015) implemented three classifiers: learning vector quantization (LVQ),
multilayer neural network (MLNN) and PNN. LVQ is a supervised competitive learning algorithm
used for classifying patterns. They have used the Levenberg /C0Marquardt algorithm for training the
MLNN which gives a faster convergence. PNN is used which is formulated on a probability densityfunction for pattern recognition. The most common classifier as per
Khan and Hong (2017) used
for hybrid EEG-fNIRS systems is LDA, SVM, extreme machine learning (ELM). ELM is a form ofa feed-forward neural network, with high generalization performance at a fast learning speed.
It utilizes Moore-Penrose generalized inverse to set weights. ELMs aren’t as correct as conven-
tional neural networks, but they may be used while managing issues that require retraining of the
network in real-time.
Convolutional neural networks (CNN) are the other schemes of ANN-based on the visual cortex.
It can understand features from the processed input data by forward and backward propagation foroptimization of weight parameters to minimize the classification error. One benefit of using a deeplearning (DL) method is that it calls for minimum preprocessing considering that most desirable set-tings are discovered automatically. Feature extraction and classification are integrated into a singleentity and optimized automatically by CNN. In
Chakladar and Chakraborty (2018) ,s e v e r a l
approaches for classification have been discussed. The Bayesian statistical classifier uses prior proba-
bilities and calculates the posterior probability for classification. Enhanced versions of LDA namely
Fisher LDA (FLDA) and Bayesian LDA (BLDA) provide better classification accuracy in disabledsubjects. Quadratic discriminant analysis (QDA) gives better classification accuracy compared toLDA and KNN. Hidden Markov Model is a classification technique for classifying features of EEGsignals responsible for finger movements. Ensemble classifiers deal with training multiple classifiersto solve the same problems. Ensemble methods usually employ three techniques: Bagging, boostingand random forest. Bagging is used to decrease variance by making use of additional data from thetraining dataset. Boosting is an iterative method wherein a strong classifier is built from several weak
classifiers. Random forest is based on decision trees. The oddity of the technique employed by Ewan
et al. is that it requires no predecided features to perform grouping. All things considered; the neuraltime series is allowed straightforwardly into an ANN that has a structure that is obliged by an arbi-trary search (genetic algorithm). In this way, a broadly useful BCI arrangement calculation is madethat mitigates dependence on a priori assumptions about the information.
9.5.1.4 Performance evaluation of BCI systems
The BCI systems efficiency relies on the performance of a classifier. Hence, the sensitivity-specificity pair, and precision are the commonly estimating parameters. The most important perfor-mance measure is accuracy which is usually calculated from the confusion matrix. Receiver operat-ing characteristic curve, and area under the curve (AUC) are very often utilized when the
classification variables are continuous. Few researchers reported information rate (ITR), the practi-
cal bit rate, task completion duration, and the count of successful trials in their results. All theseparameters do not emphasize much about the overall performance of BCI systems.244 Chapter 9 Brain-computer interface in Internet of Things environment9.6 Online and offline BCI applications
It is at most important to categorize BCI and its applications. The buzz word “BCI” means a
system that can record, perform analysis, and transforms the inputs (i.e., Brain signal) intodevice instructions. Based on the commands gen erated by BCI algorithms, the applications of
BCI can be categorized as clinical or nonclinical. In recent days, the BCI has focused on non-
clinical applications such as automotive vehic le control, ambient control, communication, gam-
ing, home control, and many more. A list of BCI applications with their associated steps is
given in
Table 9.3 .
9.7 BCI for the Internet of Things
In a few applications, the BCI and Internet of Things (IoT) systems are combined to bring a new
technology called brain /C0to/C0thing (communication) system, particularly used for healthcare appli-
cations. The main objective is to connect small objects like wheelchairs with the BCI environmentto assist the end-users who are affected by paraplegics, ALS, and brain stroke. BTC offers featureslike smart environments, mobility, transparency for the system, user-aware interfaces and confirma-tions. The simple architecture of BTC is shown in
Fig. 9.3 .
From the Fig. 9.3 , the signals evolving from the brain ca nb eu s e dt oc o n t r o lt h et a r g e ti t e m s
like home appliances through the IoT features. I f the commands and the data being used for the
control mechanism are simple, then the traditiona l machine learning algorithms including neural
networks, pattern recognition, and SVM like class ifiers are sufficed. The application of BCI can
not be limited to control only a few objects. Since the IoT has proliferated rapidly, it is expectedto extend its impact in other environments includi ng industry, manufacturi ng, and transportation.
Though several algorithms are pr oposed for cognitive recognition systems, the accuracy attained
is between 70% and 80% which is not sufficient for r eal-time applications. Also, they consume
much time when the data increases.
To make a reliable BCI system with the IoT, a DL framework can be proposed to deal with
huge dimensional data. Such frameworks creat e a great impact on BCI users to connect multiple
devices rather than a single task. The availab ility of EEGs large datasets resulted in the use of
DL architecture to reveal the pote ntial information which was no t recognized by the traditional
approaches (
Jeyakumar, Nirmala, & Sarate, 2022 ). BCI systems have been explored by various
DL models like CNN, Boltzmann machine, recu rrent neural network (RNN), long short term
memory (LSTM), spatial-temporal neural netw ork (STNN), multilayer L STM-RNN, and genera-
tive adversarial network (GAN). An autoencod er (AE) being a DL method focused on unsuper-
vised learning, consisting of phases of encoding and decoding. In the encoding phase, the inputsignals are mapped with a constructive feature (l ow dimensional feature space). In the decoding
phase, the actual features are revived from the lower features (
Kurian & Jeyakumar, 2020 ).
There are many architectures, that is, stacked en coders, variational autoe ncoder, event-related
potential (ERP) encoder network, stacked spar se encoders and subject -specific multivariate
empirical mode decomposition, which can be us ed for various applications like control of a245 9.7 BCI for the Internet of ThingsTable 9.3 Brain-computer interface applications.
BCI wheelchair
ReferencesNo. of
subjectsEEG control
signal EEG featuresClassification
algorithmPerformance
evaluation
Cao et al.(2014) 3M I 1SSVEP CSP for MI;
CCA for SSVEPRBF SVM ITR: 295.20;
Time required: 370
Mara et al.(2013) 9 SSVEP PSD Decision tree Success rate: 83% 6
15%,
ITR: 70.3 628.8 bits/min
Li et al.
(2013)3 MI CSP SVM Success rate: 82.56%
BCI cursor control studies
Serdar Bascil
et al. (2015)2 MI BP PNN CA: PNN: 93.05%
Bascil et al.
(2016)5 MI PSD SVM CA: 81.22%
Chakladar andChakraborty
(2018)1 MI PSD DBSCAN Execution time:
4.663 min,
Success rate: 70.36%
Gaming and VR studies
Djamal et al.
(2017)10 MI FFT LVQ Success rate: 70%
Kreilinger
et al. (2016)10 MI BP LDA “Upper 10%” MI
detection rates: .70%
Robotic arm studies
Yang et al.
(2018)2 SSVEP FFT CCA Five tasks performed
Bhattacharyyaet al. (2015) 11 MI MFDFA ANFIS Success rate: .60%
Boussetta
et al. (2018)4 MI FFT RBF SVM Success rate: 85.45%
BCI speller studies
References No. of
subjectsEEG control
signalMethod Classification
algorithmTyping
speedSuccess
rate
Cao et al.(2014) 3 MI Oct-O-Spell SVM 67.33
bits/min98.23%
Ansari and
Singla (2016)20 SSVEP Multiphase spellers SVM 13
chars/
min96.04%
Chang et al.
(2016)10 SSVEP 1P300 Hybrid speller CCA,
SWLDA31.8
bits/min93%
(Continued)246 Chapter 9 Brain-computer interface in Internet of Things environmentrobot, environment control, figuring out the workl oad, driver fatigue monitoring, virtual reality,
biometric systems, emotion recognition and gami ng. An IoT connected with different scenarios
through a DL framework is shown in Fig. 9.4 .BCI biometrics research
References No. of
subjectsEEG features Classification
algorithmPerformance
evaluationApplications
Hu (2018) 28 Fuzzy Entropy Vote classifier Accuracy—
99.8%Gender recognition
Nguyen et al.
(2017)125 PSD Enroll and keygen 99 Cryptographic
key Generation
Bashar et al.
(2016)9 MSD, WPS,
WPESSVM 94.44 Human
identification
Blondet et al.
(2016)50 Average ERP SVM 100 Biometric
identification
Environmental control
Aydin et al.
(2018)10 P300 LDA 93.71 Environmental control
system
Zhang et al.(2017) 3 ERP BLDA 89.2% Environmental control
system
Shyu et al.(2013) 15 SSVEP FPGA Accuracy:
92.5%Hospital bed nursing
system
Emotion recognition studies
References No. of
subjectsStimulation Emotion
typesEEG
featuresClassification
algorithmPerformance
evaluation
Wei et al.(2017) 12 Pictures Positive and
negativePSD, CP,
CSPLDA 86.83%
Liu et al.
(2017)30 Movie clips Joy, anger,
fear, sadness,
disgust, andneutralityPSD LIBSVM 89.45%
Kaur et al.
(2018)10 Video clips Calm, angry,
and happyFD RBF SVM CA: 60%
Murugappan
(2011)20 Video clips Disgust,
happiness,fear, surprise,and neutralEntropy K-NN CA: 82.87%Table 9.3 Brain-computer interface applications. Continued247 9.7 BCI for the Internet of ThingsFIGURE 9.3
Elements of BTC communication.
FIGURE 9.4IoT with deep learning framework.9.8 Secure brain-brain communication
The brain-brain communication termed the “Internet of Brains” is a technology-mediated communi-
cation system connecting two brains without the concern of the peripheral nervous system. It con-sists of two components:
1.BCI finds neural signals from one brain and converts them to computer commands, and
2.computer-brain interface carries computer commands to another brain.
The networking of human brains results in a networked biological supercomputer to func-
tion across language barriers.
Hu (2018) explained their experiment as the “first organic
computer” with the connected brains together . These systems were experimented with for
their capability to differentiate two types of el ectrical stimuli patterns and were better than
individual animals. After so many trials, r ecently they achieved 80% accuracy by imple-
menting “brain Net” to decode an activity similar to a Tetris game. EEG signals from two
senders were obtained, recorded, decoded and translated and sent to a receiver person in the
network ( Jiang et al., 2019 ). The decision of the sender on whether to move or not to move a
block in the Tetris-like game was conveyed as si gnals of transcranial magnetic stimulation
(TMS) to the occipital cortex of a receiver. If the response is “yes,” then a flash of light is
recognized by the receiver, that is, a phosphene. Based on this experience, using an EEGinterface, a decision was made by the receiver whether or not to move the block. A feedbackcontrol was included in the experimental study to allow senders to give feedback on whetherthey accepted the decision taken by the receiver. Based on brain-to-brain communication,
the senders’ information reliability is varied to validate who is more reliable by the receiver
(
Fig. 9.5 ).
Department of Defense Advanced Research Projects Agency (DARPA), USA has recently sanc-
tioned $8 billion to Prof. Jacob Rabinson, Rice University for his Proof of principle toward a wire-less brain link in 2018.
FIGURE 9.5
Brain /C0to/C0brain architecture.249 9.8 Secure brain-brain communication9.8.1 Edge computing for brain /C0to/C0things
Edge computing is becoming more popular rather than cloud computing due to its processing of
time-sensitive data while cloud computing is used to process the data which is not. Also, it is pre-ferred in remote locations where connectivity is limited or not available. The devices connected inan edge computing environment are designed to gather and process onsite data very quickly insteadof revolving around large, centralized servers stored in data centers. It does not focus on storingdata. There are various limitations such as security threats, issues in performance, and higher cost
of operations in traditional cloud computing. Since the information stored in the cloud is not fre-
quently utilized, it leads to wastage of resources and storage space. Thus, edge computing comes into ease and localize data processing and to reduce cloud dependence.
Edge computing has many benefits namely,
 Enhancing data security and privacy
 Betterment of application performance in a more responsive and robust manner Lowering the cost of operation Ameliorating efficiency and reliability Boundless scalability Preserving network and computing resources Minimizing latency
Rajesh et al. (2020) proposed a stable brain-to-brain interface with edge computing for help-
ing poststroke paralyzed patients. A paralyzed person’s thought based message transmission to
the caretaker has been developed using a light weight tiny symmetric algorithm (NTSA). IF a
person is connected to multiple ob jects to share his thoughts to activate the object or give alerts
to the end-user. During the brain-to-devices o r brain-to-brain communication, the device
authentication, secure channel establishment, device management, data privacy and analysismust be ensured by the proposed methods and algorithms. As given in
Fig. 9.6 , The framework
is divided into four parts, including the consortium blockchain network, Internet of MedicalThings environment, trusted edge computing lay er and cloud service layer. As the foundation
of other parts of the framework , the consortium blockchain network is composed of multiple
medical and health institutions.
The Certificate Authority (CA) in the blockchain n etwork is responsible for the authentication
of externally connecting BCI interfacing devices, and the smart contract in the blockchain man-
ages the data generated by the BCI interfacing devices, including metadata and access policies.The BTC environment plays the role of the data source in the framework . Once the data gener-
ated by the user is over, the data is encrypted by the device itself or the gateway and pushed to
the edge computing node for further processing. The edge computing layer connects the BTCenvironment and cloud services to provide data preprocessing and storage services. In particular,in this framework, edge computing integrates Sof tware Guard Extensions-based trusted comput-
ing services to ensure the integrity and confiden tiality of BTC data and protect the privacy of
data owners. The final cloud service layer is re sponsible for further processing of the BTC data
preprocessed by the edge computing layer, such as b ig data analysis, machine learning prediction,
etc. (
Abdulkader et al., 2015 ). Finally, the command will be issued to the respective devices to
be activated.250 Chapter 9 Brain-computer interface in Internet of Things environment9.9 Summary and conclusion
There are many limitations in the current BCI applications. For example, the application using
P300 has poor ITR. If the system expects external visual stimuli, it is another drawback of the sys-tem when the person is suffering from sore eyes due to continuous exposure. Though there aremany advantages of BCI controlled wheelchairs, the unexpected impediments, collision detection,unflatten areas, staircase climbing, pathfinding and planning along with GPS navigation systemsmust be addressed. In most cursor control and emotion recognition applications, the accuracy rate
is less than 90% which is not sufficient for real-time scenarios. Though many data preprocessing,
feature extraction and classification models are proposed for signal acquisition, deriving an opti-mum method is the need of the hour. A good deal between the BCI hardware and software musthappen to have a significant impact on the improvement of BCI system performance. Brain activityrecognition in the usage of the Biometric system is very crucial due to its sensitivity to externalfactors like emotions. As the recording of EEG signals may vary from 1 to 5 weeks, the stability ofEEG is questionable. In most of the researcher’s findings, the stability issues were not addressed.
9.10 Future research directions and challenges
The conventional kind of feature extraction and selection from the EEG signal does not provide a
common solution for various BCI applications. This is the right time to utilize the advantage ofDeep learning models while dealing with real-time EEG signals in huge dimensions. As the BCI
FIGURE 9.6
Brain /C0to/C0things (BTC) communication with Edge computing with the secure Blockchain mechanism.251 9.10 Future research directions and challengesmarket is expected to be a trillion-dollar business in 2025, BCI interfacing devices in the Internet
of Things would be a promising platform for any disabled person. The researchers working inmachine vision need to derive a common approach from the varied evaluation metrics thereby anefficient and standard method will be followed. The data privacy and authentication must beensured while more number of objects are connected in a BTC environment. If the above limita-
tions and scope for improvements are considered by the researchers working in BCI,
human /C0machine interaction will emerge to new great heights soon.
Abbreviations
ALS amyotrophic lateral sclerosis
ANFIS adaptive neuro fuzzy inference system
ANN artificial neural network
AR auto regression
BCI brain-computer interface
BLDA Bayesian LDA
BMI brain-machine interface
BP band power
BTC brain /C0to/C0thing communication system
CCA Canonical correlation analysis
CNN convolutional neural networks
CSF cerebrospinal fluid
CSP common spatial pattern
DARPA Defense Advanced Research Projects Agency
DBSCAN density-based spatial clustering of applications with noise
DL deep learning
ECoG electrocorticography
EEG electroencephalography
EMD empirical mode decomposition
ERP event-related potential fMRI
fMRI functional magnetic resonance imaging
fNIRS functional near-infrared spectroscopy
FFBPNN feed-forward back propagation neural networks
FFT fast Fourier transform
FPGA field programmable gate array
GAN generative adversarial network
GPS global positioning system
Hz hertz
ICA independent component analysis
IoT Internet of Things
ITD intrinsic time scale decomposition
ITR information rate
kHz kilohertz
KNN K/C0nearest neighbor
LDA linear discriminant analysis252 Chapter 9 Brain-computer interface in Internet of Things environmentLSTM long short term memory
LVQ learning vector quantization
MEG magnetoencephalography
MI Mental Imagery
MLNN multilayer neural network
MRI magnetic resonance imaging
MVAAR multivariate adaptive AR
NIRS near-infrared spectroscopy
NTSA Novel Tiny Symmetric Encryption Algorithm
PCA principal component analysis
PET positron emission tomography
PNN probabilistic neural network
PSD power spectral density
PRC proper rotation components
RBF radial basis function
RNN recurrent neural network
SQUID superconducting quantum interference devices
SSFT short-time Fourier transform
SSVEP steady state visually evoked potential
STNN spatial-temporal neural network
SVM support vector machines
VR virtual reality
WPD wavelet packet decomposition
WT wavelet transform
References
Abdulkader, S. N., Atia, A., & Mostafa, M.-S. M. (2015). Brain computer interfacing: Applications and chal-
lenges. Egyptian Informatics Journal ,16(2), 213 /C0230, ISSN 1110-8665. Available from https://doi.org/
10.1016/j.eij.2015.06.002 .
Ansari, I. A., & Singla, R. (2016). BCI : An optimised speller using SSVEP. International Journal of
Biomedical Engineering and Technology ,22, 31. Available from https://doi.org/10.1504/
IJBET.2016.078988 .
Aydin, E. A., Bay, O. F., & Guler, I. (2018). P300-based asynchronous brain-computer interface for environ-
mental control system. IEEE Journal of Biomedical and Health Informatics ,22, 653 /C0663. Available from
https://doi.org/10.1109/JBHI.2017.2690801 .
Bascil, M. S., Tesneli, A. Y., & Temurtas, F. (2016). Spectral feature extraction of EEG signals and pattern
recognition during mental tasks of 2-D cursor movements for BCI using SVM and ANN. Australasian
Physical & Engineering Sciences in Medicine/Supported by the Australasian College of Physical Scientists
in Medicine and the Australasian Association of Physical Sciences in Medicine ,39, 665 /C0676. Available
from https://doi.org/10.1007/s13246-016-0462-x .
Bashar, M. K., Chiaki, I., & Yoshida, H. (2016). Human identification from brain EEG signals using advanced
machine learning method EEG-based biometrics .2016 IEEE EMBS conference on biomedical engineering
and sciences (IECBES) (pp. 475 /C0479). Kuala Lumpur: IEEE. Available from http://doi.org/10.1109/
IECBES.2016.7843496 .253 ReferencesBhattacharyya, S., Basu, D., Konar, A., & Tibarewala, D. N. (2015). Interval type-2 fuzzy logic based multi-
class ANFIS algorithm for real-time EEG based movement control of a robot arm. Robotics and
Autonomous Systems ,68, 104 /C0115. Available from https://doi.org/10.1016/J.ROBOT.2015.01.007 .
Blondet, M., Jin, Z., & Laszlo, S. (2016). CEREBRE: A novel method for very high accuracy event-related
potential biometric identification. IEEE Transactions on Information Forensics and Security ,11,1 .
Available from https://doi.org/10.1109/TIFS.2016.2543524 .
Boussetta, R., Elouai kouak, I., Gharbi, M., & Regragui, F. (2018). EEG based brain computer interface for
controlling a robot arm movement through thought. IRBM ,39, 129 /C0135. Available from https://doi.org/
10.1016/J.IRBM.2018.02.001 .
Cao, L., Li, J., Ji, H., & Jiang, C. (2014). A hybrid brain computer interface system based on the neurophysio-
logical protocol and brain-actuated switch for wheelchair control. Journal of Neuroscience Methods ,229,
33/C043. Available from https://doi.org/10.1016/j.jneumeth.2014.03.011 .
Chakladar, D. D., & Chakraborty, S. (2018). Multi-target way of cursor movement in brain computer interface
using unsupervised learning. Biologically Inspired Cognitive Architectures ,25,8 8/C0100. Available from
https://doi.org/10.1016/J.BICA.2018.06.001 .
Chang, M. H., Lee, J. S., Heo, J., & Park, K. S. (2016). Eliciting dual-frequency SSVEP using a hybrid
SSVEP-P300 BCI. Journal of Neuroscience Methods ,258, 104 /C0113. Available from https://doi.org/
10.1016/j.jneumeth.2015.11.001 .
Christian, K. (2018). Invasive brain-computer interfaces and neural recordings from humans .Handbook of
behavioral neuroscience (28, pp. 527 /C0539). Elsevier ISSN 1569-7339, ISBN 9780128120286. Available
from https://doi.org/10.1016/B978-0-12-812028-6.00028-8 .
Djamal, E. C., Abdullah, M. Y., & Renaldi, F. (2017). Brain computer interface game controlling using fast
fourier transform and learning vector quantization. Journal of Telecommunication, Electronic and
Computer Engineering ,9,7 1/C074.
Eltaf, A., Mohd Zuki, Y., Ibrahim, A., Abdalla, H., Ali, E., Fares, A.-S., & Muhammad, M. (2018). Enhancing EEG
signals in brain computer interface usin g intrinsic time-scale decomposition. Journal of Physics: Conference
Series ,1123 , 012004. Available from https://doi.org/10.1088 /1742-6596/1123/1/012004 .
Hong, X., Lu, Z. K., Teh, I., Nasrallah, F. A., Teo, W. P., Ang, K. K., Phua, K. S., Guan, C., Chew, E., &
Chuang, K.-H. (2017). Brain plasticity following MI-BCI training combined with tDCS in a randomizedtrial in chronic subcortical stroke subjects: A preliminary study. Scientific Reports ,7, 9222. Available from
https://doi.org/10.1038/s41598-017-08928-5 .
Hu, J. (2018). An approach to EEG-based gender recognition using entropy measurement methods.
Knowledge-Based Systems ,140, 134 /C0141. Available from https://doi.org/10.1016/J.KNOSYS.2017.10.032 .
Jafarifarmand, A., Badamchizadeh, M. A., Khanmohammadi, S., Nazari, M. A., & Tazehkand, B. M. (2017).
A new self-regulated neuro-fuzzy frame-work for classification of EEG signals in motor imagery BCI.
IEEE Transactions on Fuzzy Systems ,26(3), 1485 /C01497.
Jeyakumar, V., Nirmala, K., & Sarate, G. S. (2022). Non-contact measurement system for COVID-19 vital
signs to aid mass screening—An alternate approach. In Ramesh Chandra Poonia, et al. (Eds.), Cyber-
Physical Systems: AI and COVID-19 (pp. 75 /C092). Academic Press.
Jiang, L., Stocco, A., Losey, D. M., Abernethy, J. A., Prat, C. S., & Rao, R. P. N. (2019). BrainNet: A multi-
person brain-to-brain interface for direct collaboration between brains. Scientific Reports ,9, 6115.
Available from https://doi.org/10.1038/s41598-019-41895-7 .
Kaur, B., Singh, D., & Roy, P. P. (2018). EEG based emotion classification mechanism in BCI. Procedia
Computer Science ,132, 752 /C0758. Available from https://doi.org/10.1016/J.PROCS.2018.05.087 .
Khan, M. J., & Hong, K.-S. (2017). Hybrid EEG /C0fNIRS-based eight-command decoding for BCI: Application
to quadcopter control. Frontiers in Neurorobotics ,11,1/C06. Available from https://doi.org/10.3389/
fnbot.2017.00006 .254 Chapter 9 Brain-computer interface in Internet of Things environmentKreilinger, A., Hiebel, H., & Muller-Putz, G. R. (2016). Single vs multiple events error potential detection in a
BCI-controlled car game with continuous and discrete feedback. IEEE Transactions on Bio-medical
Engineering ,63, 519 /C0529. Available from https://doi.org/10.1109/TBME.2015.2465866 .
Kurian, P., & Jeyakumar, V. (2020). Multimodality medical image retrieval using convolutional neural net-
work. In Deep Learning Techniques for Biomedical and Health Informatics , (pp. 53 /C095). Academic Press.
Li, J., Liang, J., Zhao, Q., Li, J., Hong, K., & Zhang, L. (2013). Design of an assistive wheelchair system
directly steered by human thoughts. International Journal of Neural Systems ,23, 1350013. Available from
https://doi.org/10.1142/S0129065713500135 .
Liu, Y., Yu, M., Zhao, G., Song, J., Ge, Y., & Shi, Y. (2017). Real-time movie-induced discrete emotion rec-
ognition from EEG signals. IEEE Transactions on Affective Computing ,9, 2660485. Available from
https://doi.org/10.1109/TAFFC.2017.2660485 .
Mara, S., Mu ¨ller, T., Freire, T., M ´ario, B., & Filho, S. (2013). Proposal of a SSVEP BCI to command a robotic
wheelchair. Journal of Control, Automation and Electrical Systems ,24,9 7/C0105. Available from https://
doi.org/10.1007/s40313-013-0002-9 .
Murugappan, M. (2011). Human emotion classification using wavelet transform and KNN .Proceedings of the
2011 international conference pattern analysis. intelligence robot ICPAIR 2011 (Vol. 1, pp. 148 /C0153).
Putrajaya: ICPAIR. Available from http://doi.org/10.1109/ICPAIR.2011.5976886 .
Nguyen, D., Tran, D., Sharma, D., & Ma, W. (2017). On the study of EEG-based cryptographic key genera-
tion. Procedia Computer Science ,112, 936 /C0945. Available from https://doi.org/10.1016/
JPROCS.2017.08.126 .
Rajesh, S., Paul, V., Menon, V. G., Jacob, S., & Vinod, P. (2020). Secure brain-to-brain communication with
edge computing for assisting post-stroke paralyzed patients. IEEE Internet of Things Journal ,7(4),
2531 /C02538. Available from https://doi.org/10.1109/JIOT.2019.2951405 .
Rashid, M., Sulaiman, N., Abdul Majeed, A. P. P., Mosa, R. M., Nasir, A. F. A., Bari, B. S., & Khatun,
S. (2020). Current status, challenge s, and possible solutions of EEG- based brain-computer interface:
A comprehensive review. Frontiers in Neurorobotics , 14. Available from https://doi.org/10.3389/
fnbot.2020.00025 .
Serdar Bascil, M., Tesneli, A. Y., & Temurtas, F. (2015). Multi-channel EEG signal feature extraction and pat-
tern recognition on horizontal mental imagination task of 1-D cursor movement for brain computer inter-face. Australasian Physical & Engineering Sciences in Medicine/Supported by the Australasian College of
Physical Scientists in Medicine and the Australasian Association of Physical Sciences in Medicine ,38,
229/C0239. Available from
https://doi.org/10.1007/s13246-015-0345-6 .
Shyu, K.-K., Chiu, Y.-J., Lee, P.-L., Lee, M.-H., Sie, J.-J., Wu, C.-H., et al. (2013). Total design of an FPGA-
based brain /C0computer interface control hospital bed nursing system. IEEE Transactions on Industrial
Electronics ,60, 2731 /C02739. Available from https://doi.org/10.1109/TIE.2012.2196897 .
Wei, Y., Wu, Y., & Tudor, J. (2017). A real-time wearable emotion detection headband based on EEG mea-
surement. Sensors and Actuators A: Physical ,263, 614 /C0621. Available from https://doi.org/10.1016/J.
SNA.2017.07.012 .
Yang, C., Wu, H., Li, Z., He, W., Wang, N., & Su, C.-Y. (2018). Mind control of a robotic arm with visual
fusion technology. IEEE Transactions on Industrial Informatics ,14, 3822 /C03830. Available from https://
doi.org/10.1109/TII.2017.2785415 .
Zhang, R., Wang, Q., Li, K., He, S., Qin, S., Feng, Z., et al. (2017). A BCI-based environmental control sys-
tem for patients with severe spinal cord injuries. IEEE Transactions on Bio-medical Engineering ,64,
1959 /C01971. Available from https://doi.org/10.1109/TBME.2016.2628861 .255 ReferencesThis page intentionally left blankCHAPTER
10Early detection of COVID-19
pneumonia based on ground-glassopacity (GGO) features ofcomputerized tomography (CT)
angiography
H.M.K.K.M.B. Herath, G.M.K.B. Karunasena and B.G.D.A. Madhusanka
Faculty of Engineering Technology, The Open University of Sri Lanka, Nugegoda, Sri Lanka
10.1 Introduction
Novel COVID-19 has caused a coronavirus strain termed Severe Acute Respiratory Syndrome
Coronavirus 2 or SARS-CoV-2 and the Middle East Respiratory Syndrome Coronavirus (MERS-CoV) (
Herath et al., 2020 ). It has also been described as an acute respiratory infection caused by a
novel coronavirus (CoV-2), later referred to as COVID-19 by the World Health Organization ( Zhu
et al., 2020 ). As of December 2020, Fig. 10.1 indicates COVID-19 recovered, and death cases
worldwide. In December 2019, a pneumonia case of uncertain cause has been identified in Wuhan
(Eurosurveillance Editorial Team, 2020 ). Since then, COVID-19 has spread rapidly in China and
other countries, rendering it a significant global public health threat ( Jin et al., 2020 ). The disease
spread quickly across the globe after it was first identified and has become an international concern.The symptoms of infection with COVID-19 are similar to previously reported infections withSARS (Severe Acute Respiratory Syndrome), MERS (Middle East Respiratory Syndrome) (
Herath,
2021; Yan, Chang, & Wang, 2020; Herath, Karunasena, & Herath, 2021 ), influenza ( Potter, 2001 )
with fever, sore throat, nasal inflammation, pain in the chest, and dry cough.
A valuable noninvasive technique for identifying and diagnosing a range of diseases is medical
image processing. Deep learning algorithms are now gaining a lot of interest for solving numerousmedical imaging challenges. Latest advancements in medical image processing have focused on thedetection of brain tumors (
Kadkhodaei et al., 2016 ), skin lesions ( Jafari et al., 2016 ), heart ventricle
monitoring ( Nasr-Esfahani et al., 2018 ), and liver diagnosis ( Rafiei et al., 2018 ). Traditional medi-
cal imaging modalities ( Llovet, Schwartz, & Mazzaferro, 2005 ) used in modern healthcare indus-
tries include X-rays (X-radiation), Ultrasound, Computed Tomography (CT), and MagneticResonance Imaging (MRI) (
Katti, Ara, & Shireen, 2011 ).
Lung-related diseases have emerged worldwi de as one of the most common medical disorders
in humans. Early detection for ti mely treatment of patients with COVID-19 pneumonia is crucial
2575G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00013-9
©2022 Elsevier Inc. All rights reserved.for preventing spread, particularly in epidemic regions. Real-time Reverse Transcription
Polymerase Chain Reaction or RT-PCR ( Gibson, Heid, & Williams, 1996 ) is the most recent
technique used to make a definitive diagnosis of SARS-CoV-2 infection ( Jin et al., 2020 ).
Computerized tomography, or CT scan, is a type of computer-assisted imaging constructed frommultiple X-rays. Doctors and researchers can acquire precise, highly accurate, 3D photographs ofa body with solid mass depth through CT scans. This facilitates visual diagnosis of the interior of
patients without actual intrusion into the body. In diagnosing and evaluating internal fractures,
diseases, cancers, blood clots, or excess fluid a nd conditions in the heart, kidneys, lungs, bones,
and joints, CT scans are used. X-ray and CT pho tographs of a Chinese citizen infected by
COVID-19 revealed the harm done to the human lungs, according to the evidence shared by the
RSNA (Radiological Society of North America).
In COVID-19 patients with severe symptoms, including pneumonia, GGOs were observed
in both lungs. It is important to remember that GGOs may suggest the emergence of lungfibrosis and maybe an indicative characteris tic of lung cancer growth in high-risk individuals
(
Sadhukhan, Ugurlu, & Hoque, 2020 ). Galiatsatos, a lung disease specialist at Johns Hopkins
Bayview Medical Center, treats COVID-19 patients and has identified some of the short-termand long-term lung complications caused by t he novel coronavirus. COVID-19, a disease
caused by the new coronavirus, may cause lung complications such as Acute Respiratory
Distress Syndrome or ARDS (
Umbrello, Formenti, Bolgiaghi, & Chiumello, 2017 ), pneumo-
nia, and in the most severe cases. pneumonia allows the lungs to swell with fluid and become
FIGURE 10.1
COVID-19 global recovered and death cases as of 31st December 2020 ( Zhu et al., 2020 ).258 Chapter 10 Early detection of COVID-19 pneumoniainflamed, resulting in respiratory complications. Individual patients’ respiratory conditions
can become serious enough to necessitate hospitalization for oxygen or ventilator help.COVID-19 induced pneumonia begins to take place in both lungs. In the lungs, air sacs fillwith fluid, restricting their capacity to draw in oxygen and causing symptoms such as short-ness of breath, coughing, and fatigue. While most people recover from pneumonia without
permanent lung injury, COVID-19-associated pneumonia can be serious. Lung damage can
lead to respiratory problems long after the illness has passed, which may take months torecover. Considering the lung damages caused by COVID-19 pneumonia, there is a highrequirement for early detection of patients w ith COVID-19, particularly the elderly. This
study aims to develop a system that uses mac hine intelligence (MI) with CT angiography to
diagnose COVID-19 pneumonia patients.
Rest of this chapter is composed of four sections. The theoretical background and previous
research related to medical imaging and deep learning techniques are listed in
Section 10.2 .T h e
materials and methods of the proposed schemes are discussed in Section 10.3 . The results and
analysis of the experiments are listed in Section 10.4 .F i n a l l y , Section 10.5 concludes the pro-
posed system.
10.2 Background
Recently, the use of image recognition in machine learning (ML) has been growing increasingly.
Medical image recognition ( Jiang, Trundle, & Ren, 2010 ), image segmentation ( Haralick &
Shapiro, 1985 ), computer-aided diagnostics ( Duggirala, Paine, Comaniciu, Krishnan, & Zhou,
2007 ), image transformation ( Acharya & Ray, 2005 ), image fusion ( Sahu & Parsai, 2012 ) associ-
ated with artificial intelligence (AI) plays a vital role in the field of healthcare. This section dis-cussed the theoretical background related to deep learning techniques and characteristics ofCOVID-19 pneumonia by leveraging medical imaging data.
10.2.1 Ground-glass opacity (GGO)
COVID-19 can affect different organ systems , with symptoms most frequently seen in the
respiratory tract. In Wuhan, a prospective stu dy reported bilateral lung opacities in 40 of 41
(98%) chest CTs in COVID-19 patients and identif ied their most typical findings as lobular
and subsegmental consolidation areas ( Huang et al., 2020 ). Ground-glass opacity (GGO)
(Collins & Stern, 1997 ) describes a finding on high-reso lution CT (HRCT) of the lungs in
which is seen hazy increased attenuation of the lung. Previous studies have shown that the
existence of multifocal GGOs is the most commo n CT characteristic of COVID-19 pneumo-
nia. GGOs are commonly detected and found to be present in 77% /C0100% of confirmed
COVID-19 cases ( Basler et al., 2020; Caruso et al., 2020; Song et al., 2020 ). Pure GGO
lesions could be seen in the early stages of COVID-19 pneumonia ( Zu et al., 2020 ), and were
confirmed to be the critical finding after symptom onset ( Wang et al., 2020 ). Schmitta and
Marchiorib ( Schmitt & Marchiori, 2020 ) have identified two characteristics of GGOs that
may suggest the diagnosis of COVID-19 in the sense of the current pandemic. Fig. 10.2259 10.2 Backgrounddepicts GGO regions on a COVID-19 patient’s CT scan image. The arrowheads show the dis-
cernible hazy region on the lungs’ outer edges.
10.2.2 Support vector machine (SVM)
Support vector machine (SVM) is a supervised algorithm for machine learning that can be used
for classification or regression problems. In class ification problems, however, it is mostly used.
The SVM is originally a binary classification method developed by Vladimir N. Vapnik andAlexey Ya at Bell laboratories (
Burges, 1998; Vapnik, 2013 ), with further algorithm improve-
m e n t sb yo t h e r s( Joachims, 1998 ). For a binary problem, training data points are described, as
shown in Eq. (10.1) :
xi;yi/C8/C9
;i51; :::;l;yiA21;1 fg ;xiARd(10.1)
SVMs are based on statistical learning systems or VC theory suggested by Vapnik and
Chervonenkis (1974) andVapnik (1982, 1995) , which are one of the most robust prediction methods.
In the SVM algorithm, each data object is interpreted as a point in n-dimensional space (where n is
the number of characteristics). The value of each characteristic is the value of a certain coordinate. In
this study, we have used the SVM algorithm to detect GGO regions of the lungs.
10.2.3 Histogram of oriented gradients (HOG) algorithm
The histogram of oriented gradients (HOG) algorithm is a feature descriptor used for object detec-
tion purposes in computer vision and digital image processing. The HOG algorithm’s basic princi-
ple is to use each pixel’s gradient information to extract discriminating object detection features. In1986, Robert K. McConnell of Wayland Research Inc. defined the ideas underlying HOG withoutusing the term HOG in a patent application (
McConnell, 1986 ). Mitsubishi Electric Research
FIGURE 10.2
CT scan image of a patient with severe COVID-19 ( Schmitt & Marchiori, 2020 ).260 Chapter 10 Early detection of COVID-19 pneumoniaLaboratories used the concepts in 1994 ( Freeman & Roth, 1995 ). Navneet Dalal and Bill Triggs,
researchers for the French National Institute for Research in Computer Science and Automation(INRIA), presented their additional work on HOG descriptors at the Conference on ComputerVision and Pattern Recognition (CVPR) in 2005.
HOG characteristics in the im age are typically derived from different window sizes. The
image window in the original HOG algorithm is split into several blocks, and each block is
divided into several cells. The extraction algo rithm for the HOG function is defined based on
the following parameters:
1.Size of the ROI image
2.Type of the gradient ("signed" or "unsigned")
3.Cell size
4.Number of bins of cell histogram
5.Size of the block
6.The overlapping ratio of the block
We decided to use the HOG feature as a descriptive element for the GGOs, introduced by Dalal
and Triggs (
Dalal & Triggs, 2005 ) in 2005. The features were inspired by the previous work of
Lowe et al. Lowe (1999) , who in the late nineties, invented the SIFT (Scale-invariant feature trans-
form) key point method. Binning the magnitude of the gradients into a histogram according to theirorientation is the core concept of this function.
10.2.4 Convolutional neural network (CNN)
The convolutional neural network (CNN) has recently been highlighted as part of deep learning in
machine vision for supervised and unsupervised learning tasks ( Krizhevsky, Sutskever, & Hinton,
2017 ). Over the past decade, the convolutional neural network (CNN) has seen groundbreaking
results in various fields related to pattern recognition, from image processing to voice recognition.
The compositions of CNNs are convolutional, pooling, and fully connected layers. The primary
roles of the convolutional layer are the identification of patterns, lines, and edges. Each CNN hid-den layer consists of convolution layers that combine the input array of convolution kernels param-eterized by weight. The multiple kernels produce multiple feature images and have succeeded innumerous vision tasks, such as segmentation and classification. Feature maps are locally progres-sive and spatially pooled pooling layers between the convolutional layers. The pooling layer trans-fers the maximum or average value, and therefore the size of feature maps is reduced. Thisapproach captures an image’s features with a strong position and shape.
10.2.4.1 Rectified Linear Unit (ReLU) activation function
One of the most prominent nonsaturated activation functions is the Rectified Linear Unit (ReLU).
Eq. (10.2) defines the ReLU activation function, where Zi,j,kis the input of the activation function
at the location ( i,j) on the kth channel.
ai;j;k5max ðZi;j;k;0Þ (10.2)261 10.2 BackgroundReLU is a piecewise linear function that prunes the negative portion to zero and preserves the
positive portion. ReLU’s simple max ( /C1)operation helps it to compute much faster than the activa-
tion functions of “ Sigmoid ”o r“ Tanh ,” and it also induces sparsity in the hidden units and makes it
possible for the network to obtain sparse representations.
10.2.5 Literature
To prevent the COVID-19 pandemi c, science, technical knowle dge, and resources were widely
used. There is a daily increase in the number of s tudies related to the novel COVID-19. To detect
infection with COVID-19, researchers have rece ntly used imaging patterns on chest CT. In recent
years, deep learning has led to excellent success i n many research fields, including visual recog-
nition, speech recognition, and natural language processing. A broad survey of the recent devel-
opments in convolutional neural networks ( CNN) has been provided by Jiuxiang et al. ( Gu et al.,
2018) . They have also addressed the enhancements to CNNs in terms of layer architecture, acti-
vation function, loss function, regularization, optimization, and fast computing. Brosch et al.
(Brosch et al., 2015 ;Brosch et al., 2016) have suggested and applied a novel segmentation
method focused on deep convolutional encoder netwo rks to the segmentation of multiple sclero-
sis lesions in magnetic resonance photographs. Lee et al. (2017) have reviewed perspectives on
deep learning technology’s history, development, an d implementations, especially concerning its
medical imaging applications. Mishra, Tripathy, and Acharya (2020) have discussed the different
levels of deep learning design, image classificat ions, and medical image segmentation optimiza-
tion. They have also presented a thorough revi ew of the deep learning algorithms used in the
clinical image concerning recent works and their potential approaches. Das, Kumar, Kaur,
Kumar, and Singh (2020) have developed an artificial deep tra nsfer learning-based approach for
d e t e c t i n gC O V I D - 1 9i n f e c t i o ni nc h e s tX - r a y s using the extreme variant of the inception
(Xception) model.
The aim of this study is to develop a COVID-19 pneumonia early detection system focused on
CT angiography by referencing theoretical background and previous research work discussed here.
The material and process used for constructing the proposed system are described in this chapter’s
next section.
10.3 Materials and methods
This section presents the materials and methodology used for the development of the COVID-19
pneumonia early detection system. Section 10.3.1 lists the datasets used for the study. In
Section 10.3.2 , the methodology of the proposed algorithm is present.
10.3.1 Dataset description
The CT scan image dataset from Kaggle ( Ning et al., 2020 ) was used in this study and is freely
accessible to researchers. Three forms of CT images collected from Union Hospital (HUST-UH),and Liyuan Hospital (HUST-LH) was included i n the dataset. The CT scan image dataset is262 Chapter 10 Early detection of COVID-19 pneumoniacomposed of noninformative CT (NiCT) images, positive CT (pCT) images, and negative CT
(nCT) images. 27% of pCT and 11% of nCT data were extracted randomly for this study. In thetesting process, 120 of the nCT and pCT dat a were used to validate the algorithm.
Table 10.1
depicts an overview of the datasets used in the s tudy. Initially, the images in the datasets were
scaled at 512 3512 pixels.
10.3.2 Methodology
In the past few decades, computer vision technology has learned, changed, and evolved dramati-
cally by being developed on the roots of AI and deep learning techniques ( Mittal et al., 2019 ). The
precise identification of natural characteristics from medical photographs greatly influences accu-rate inference (
Pandey, Pallavi, & Pandey, 2020 ). Recent efforts have been made to improve the
performance of artificial intelligence algorithms. The introduction of hybridized AI techniques is aprominent and essential feature in this area (
Pham, Xu, & Prince, 2000 ). This section describes the
methodology of the proposed system. The architecture of the proposed machine learning algorithm
is represented in Fig. 10.3 .
All the tests and evaluations were carried out on a personal computer (PC) with an Intel(R)
Core (TM) i7 /C03.07 GHz CPU, 4GB RAM, running a Windows 7 Professional 64-bit operating sys-
tem (OS).
10.3.2.1 Data preprocessing
As the first stage of this study, chest CT scan images of COVID-19 subjects and normal healthysubjects were taken and saved on a computer. We performed multiple image preprocessing(
Bhattacharyya, 2011 ) techniques, such as image cropping and image resizing, before using the
dataset to extract usable pulmonary regions. MATLAB image processing tools were used to cropand resize the images. The preprocessed images were manually verified for the training process.Images that were not adequately preprocessed were withdrawn from the training set but notreplaced.
10.3.2.2 The development of convolution neural network (CNN) model
A well-known deep learning architecture influenced by living organisms’ natural visual perceptionmechanism is the convolution neural network (CNN) (
O’Shea & Nash, 2015 ). CNN is a scalable
technology that is widely used to classify images. CNN is a complex image classification modelTable 10.1 Description of the COVID-19 positive and negative CT scan image dataset.
CT scan image
type Data used Description
Positive CT (pCT)
images27% of data from the
original datasetImaging characteristics are related to COVID-19
pneumonia
Negative CT (nCT)
images11% of data from the
original datasetImaging characteristics are irrelevant to COVID-19
pneumonia in both lungs.263 10.3 Materials and methodsdue to its hierarchical structure and efficient image extraction features. The proposed CNN archi-
tecture comprises two stages: a feature learning stage and a classification stage, as shown in
Fig. 10.4 .
The developed feature learning stage consists of two convolutional layers and two pooling
layers. A 3 33 convolutional filter for initial feature extraction was used in the first convolution
layer. The resulting characteristics were then transferred to the first pooling layer composed of232 max-pooling filters. Then, from the first convolution and pooling, the extracted features trans-
ferred to the second convolution and pooling layers.
The second convolution layer consists of 3 33 convolutional filters, a nd the pooling layer
consists of 2 32 max-pooling filters. The feature sco re matrix passed into a fully connected
layer in the classificati on stage, consisting of three neural layers fully connected. Each layer
includes 500, 100, and 2 artificia l neurons. Finally, by obtaini ng probability and classify input
samples, the softmax layer was used to determ ine whether the test subjects were positive or
negative for COVID-19 infection. We also used an input layer of 200 3200 sizes in this model.
The Model Hyperparameters (
Ali, Caruana, & Kapoor, 2014 ) are properties that govern the
entire process of training. This includes the var iables that decide the network structure and
the variables that determine how to train the network. As the training network solver, the
FIGURE 10.3
Architecture of the proposed machine learning algorithm.264 Chapter 10 Early detection of COVID-19 pneumoniaStochastic Gradient Descent with Momentum (SGDM) ( Dogo, Afolabi, Nwulu, Twala, &
Aigbavboa, 2018 ) optimizer was used. For a particular ta sk, a proper activation function signif-
icantly enhances the efficiency of CNN. To activate the nodes, the ReLU ( Schmidt-Hieber,
2020 ) activation function was used.
10.3.2.3 The development of cascade classifier model
This section aims to develop a cascade classifier model to detect GGO regions on the lungs. The pro-
posed cascade classifier model consists of the HOG feature extraction process and feature classifica-
tion process. For the training process of the cascade classifier, we used 200 positive sample CT scanimages and 500 negative sample CT scan images. The trained model was then used to distinguishinfected regions by classifying extracted HOG features from the model. By splitting the picture intotiny cells, the HOG descriptor technique was carried out. Each cell evaluated a gradient direction his-togram for the pixel inside the cell. For the object extraction, the HOG features extraction was com-pleted in four phases. An overview of the proposed HOG/SVM architecture is shown in
Fig. 10.5 .
Gradient values are calculated in the first step by using the horizontal and vertical directions of
the derivative mask using Eqs. (10.3) and (10.4) .
Dx52 101/C2/C3
(10.3)
Dy51
0
212
43
5 (10.4)
The convolution procedure was used to obtain the x and y derivatives of object Image I, as
defined by Eq. (10.5) .
Ix5Ix3Dx;Iy5Iy3Dy (10.5)
FIGURE 10.4
Proposed CNN architecture of the COVID-19 detection algorithm.265 10.3 Materials and methodsThe gradient magnitude was estimated using Eq. (10.6) ,
Gjj5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
I2
x3I2
yq
(10.6)
The gradient orientation was estimated using Eq. (10.7) ,
θ5arctanIx
Iy(10.7)
The second stage was the spatial orientation binning method. This stage aims to give the polling
method for the product of a cell histogram. Based on the direction, each pixel of the CT scan imageinside the cell was divided. The nearest bin in the range of 0 to 180 degree was allocated by thepixels. In the next phase, the cell and histogram normalized to be a vector shape using the HOGdescriptor. In the final phase, the block’s standardization was achieved using the L2 norm (
Luo,
Chang, & Ban, 2016 ) as described in Eq. (10.8) .
b5bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:b:21ε2q (10.8)
All of the block descriptors were translated into vector form during the HOG normalization pro-
cedure. For the identification process, the descriptor was used a detection window, which includesa5 2332 pixel window. This detection window was divided into 16 316 pixel blocks. Each block
contains four cells, each cell contains an 8 38 pixel size with a 9-bin histogram, and each block
includes 36 values. Finally, all the vector results of the classification process have been stored inthe XML database.
10.4 Results and analysis
The main objective of this study is to develop a n algorithm to early detection of COVID-19
pneumonia using CT scan images. In this sect ion, results and analysis are presented.
FIGURE 10.5
Overview of the proposed HOG/SVM architecture.266 Chapter 10 Early detection of COVID-19 pneumoniaSection 10.4.1 describes the results of the COVID- 19 pneumonia detection system. In
Section 10.4.2 ,A n a l y s i si sp r e s e n t e d .
10.4.1 Test results of the COVID-19 pneumonia detection system
A classification model was developed for the training process. A total of 20 epochs and 2000 itera-
tions were performed during the data training phase to achieve maximal model parameters. Theaccuracy curve of the training phase is shown in
Fig. 10.6 .
Based on the training data, each mini-batch classification’s average accuracy was 94.25%, and
the accuracy of the classification for each mini-batch has reached the maximum at epoch 8. Themini-batch loss for multiclass classifications decreased from 0.695 to 0.0977 at the end of the 20epochs, according to the mini-batch loss curve shown in
Fig. 10.7 .Fig. 10.8 depicts the sample of
healthy test subjects used in this study.
According to the images, lungs are observed to be clear and detect fewer gray spots. Detection
of fewer gray spots suggested that the test subject is negative from COVID-19. Clinical trials havedetermined that these healthy test subjects are COVID-19 negative.
The CNN test results of 30 test subjects are shown in
Table 10.2 . (67% of COVID-19 pos-
itive subjects and 33% of negative subjects). E ach test results describe the COVID-19 detec-
tion through the number of GGO regions of the patients. The test findings showed thatpositive subjects for COVID-19 were graded with a range of 0.69 /C00.99 probabilities, while
stable subjects (COVID-19 negative subjects) ranged from 0.05 /C00.48 probabilities.
Fig. 10.9
FIGURE 10.6
Mini-batch accuracy curve of the model training for 20 epochs.267 10.4 Results and analysisillustrates the experiment results for 10 tes t subjects of COVID-19 positive cases in this
study.
Fig. 10.10 displays the test outcomes for 200 test subjects. The positive test subjects for
COVID-19 describe the range between 0.69 and 0.99. A probability ranging from 0.05 to 0.48 isdefined for healthy test subjects.
FIGURE 10.8
CT chest scan images for healthy test subjects (COVID-19 negative test subjects).
FIGURE 10.7Mini-batch loss curve of the model training for 20 epochs.268 Chapter 10 Early detection of COVID-19 pneumoniaFor ten COVID-19 test subjects, Fig. 10.11 demonstrates the detection of GGO regions.
Fig. 10.12 indicates the test results shown in the GUI for the COVID-19 negative test subject. As
shown in the figure, GGO regions were not found in the subject’s lungs. Therefore, detected GGOs
were counted as zero. The possibility of having COVID-19 of this test subject was 6.6%. The inter-face shown in
Fig. 10.13 indicates the test results for positive test subjects with COVID-19. As
shown in the CT image, five GGO regions in the lungs were identified by the system. Test findingsTable 10.2 Sample test results of the COVID-19 detection system.
Test noProbability COVID-19 detection
GGO Count Positive ( 1) Negative ( 2) Real Predicted
1 0.8213 0.1787 Positive 82.1% Positive 3
2 0.8014 0.1986 Positive 80.1% Positive 1
3 0.9116 0.0884 Positive 91.1% Positive 44 0.8323 0.1677 Positive 83.2% Positive 35 0.2453 0.7547 Negative 75.4% Negative 06 0.1569 0.8431 Negative 84.3% Negative 07 0.9214 0.0786 Positive 92.1% Positive 58 0.0187 0.9813 Negative 98.1% Negative 09 0.8473 0.1527 Positive 84.7% Positive 4
10 0.9256 0.0744 Positive 92.56% Positive 5
11 0.3048 0.6952 Negative 69.5% Negative 112 0.8823 0.1177 Positive 88.2% Positive 413 0.8015 0.1985 Positive 80.1% Positive 214 0.2541 0.7459 Negative 74.5% Negative 015 0.8341 0.1659 Positive 83.4% Positive 316 0.9084 0.0916 Positive 90.8% Positive 217 0.2209 0.7791 Negative 77.9% Negative 0
18 0.7913 0.2087 Positive 79.1% Positive 2
19 0.7215 0.2785 Positive 72.1% Positive 120 0.1057 0.8943 Negative 89.4% Negative 021 0.9403 0.0597 Positive 94.0% Positive 422 0.3301 0.6699 Negative 66.9% Negative 023 0.7012 0.2988 Positive 70.1% Positive 124 0.7756 0.2244 Positive 77.5% Positive 225 0.2185 0.7815 Negative 78.1% Negative 0
26 0.8702 0.1298 Positive 87.0% Positive 3
27 0.3047 0.6953 Negative 69.5% Negative 028 0.9503 0.0497 Positive 95.0% Positive 529 0.8981 0.1019 Positive 89.8% Positive 430 0.8213 0.1787 Positive 82.1% Positive 3269 10.4 Results and analysisFIGURE 10.9
CT chest scan images for COVID-19 positive test subjects.
FIGURE 10.10
Model validation results of the COVID-19 positive and healthy test subjects.270 Chapter 10 Early detection of COVID-19 pneumoniahave identified the patient as positive for 98.32% COVID-19 contamination. Besides, the cascade
classifier algorithm identified the gray areas (GGO regions) of the lungs.
10.4.2 Analysis of the test results
A confusion matrix represents the output of an algorithm, also known as an error matrix. In the
field of machine learning, usually supervised learning, the confusion matrix is widely used. Using
the following assumption ( Table 10.3 ), the entries in the confusion matrix were determined from
the coincidence matrix.
The number of points at which the expected label is equal to the true label is defined by
the diagonal elements, while off-diagonal elemen ts are mislabeled by the classifier. The high-
er the diagonal values of the confusion matrix, the better the proposed algorithm’s predictionresults.
Fig. 10.14 illustrates the confusion matrix of the proposed COVID-19 pneumonia
detection system. According to the illustrati on, diagonal values are higher than off-diagonal
values. As a result, the proposed algorithm produced higher accurate predictions. The
FIGURE 10.11
Ground-glass opacity (GGO) feature identification for ten COVID-19 test subjects.271 10.4 Results and analysisaccuracy and precision of the test results based on the confusion matrix data are shown in
Eqs. (10.9) and (10.10) .
Accuracy 5PðTP1TNÞPðTP1FP1FN1TNÞ(10.9)
Where,
 True Positives (TP): 93
 True Negatives (TN): 91 False Positives (FP): 07
 False Negatives (FN): 09
Precision 5PðTPÞPðTP1FPÞ(10.10)
MAE5PN
t51Yt2Ft jj
N(10.11)
FIGURE 10.12
GUI test results of the COVID-19 detection system for negative test subjects.272 Chapter 10 Early detection of COVID-19 pneumoniaMean Absolute Error (MAE) ( Chai & Draxler, 2014 ) calculates errors which describing the
same phenomenon between paired measurements. Eq. (10.11) represents the relationship between
actual data and expected data. Less than 0.200 is known to be the best MAE of a system, and MAEof the proposed system was 0.095. Therefore, using chest CT scan images, lower MAE validatesthe proposed model’s accuracy for the COVID-19 pneumonia detection. Root Mean Square ErrorTable 10.3 Hypothesis of the coincidence matrix.
Notation Case Description
TN True Negative The number of correct predictions of a negative case
TP True Positive The number of correct predictions of a positive caseFP False Positive The number of incorrect predictions of a positive caseFN False Negative The number of incorrect predictions of a negative case
FIGURE 10.13
GUI test results of the COVID-19 detection system for positive test subjects.273 10.4 Results and analysis(RMSE) ( Chai & Draxler, 2014 ) of the system was 0.149 calculated. Lower RMSE suggested the
higher accuracy of the proposed algorithm ( Table 10.4 ).
10.5 Conclusion
The novel coronavirus (COVID-19) has caused world alarm since its emergence in Wuhan, China,in late 2019. Infection can lead to severe pneumonia with clusters of diseases. It is essential to clar-
ify the clinical features of other pneumonia due to the impact on public health. Real-time Reverse
Transcription Polymerase Chain Reaction (RT-PCR) is the most commonly used novel coronavirus(COVID-19) detection technique. However, to confirm COVID-19 infection in the patient, RT-PCR kits are expensive and require 6 /C09 h to get results. It provides higher false negative outcomes
due to less RT-PCR sensitivity.
This chapter suggested a possible way of using medical imaging (CT scan) to early detection of
COVID-19 pneumonia. A GUI was developed using the MATLAB development environment forthe identification of GGO regions of the lungs. GGOs were detected using the cascade classifier
model, while CNN detected the probability of infection with COVID-19. The convolutional neural
network and cascade classifier algorithm for COVID-19 pneumonia detection achieved 92.8% accu-racy at 0.931 precision. Moreover, as positive cases of COVID-19, patients with lung diseases werereported. As a response, in the other lung condition with hazy regions of the lungs, an error couldbe developed. The probability of COVID-19 recognition indicates the stage of COVID-19 infectionin the patient. A higher probability implies the existence of larger hazy spots in the lungs, showingthat the individual is in a higher level of the COVID-19 virus.
FIGURE 10.14
Confusion matrix of the proposed COVID-19 detection system.
Table 10.4 Accuracy, precision, MAE, and F1-score of the proposed system.
Measure Observation
Accuracy 92.8%
Precision 0.931F1 Score 0.921MAE 0.095274 Chapter 10 Early detection of COVID-19 pneumoniaIn the future, we plan to use transfer learning to expand the algorithm to quantify the severity
of other pneumonia or bacterial infections and to verify the findings using bio-imaging dataobtained from various geographical regions.
References
Acharya, T., & Ray, A. K. (2005). Image processing: Principles and applications . John Wiley & Sons.
Ali, A., Caruana, R., & Kapoor, A. (2014, June). Active learning with model selection. In Proceedings of the
AAAI conference on artificial intelligence (Vol. 28, No. 1).
Basler, L., Gabry ´s, H. S., Hogan, S. A., Pavic, M., Bogowicz, M., Vuong, D., & Levesque, M. P. (2020).
Radiomics, tumor volume, and blood biomarkers for early prediction of pseudoprogression in patients withmetastatic melanoma treated with immune checkpoint inhibition. Clinical Cancer Research ,26(16),
4414 /C04425.
Bhattacharyya, S. (2011). A brief survey of color image preprocessing and segmentation techniques. Journal
of Pattern Recognition Research ,1(1), 120 /C0129.
Brosch, T., Tang, L. Y., Yoo, Y., Li, D. K., Traboulsee, A., & Tam, R. (2016). Deep 3D convolutional encoder
networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation.
IEEE Transactions on Medical Imaging ,35(5), 1229 /C01239.
Brosch, T., Yoo, Y., Tang, L.Y., Li, D.K., Traboulsee, A., & Tam, R. (2015, October). Deep convolutional
encoder networks for multiple sclerosis lesion segmentation. In I nternational conference on medical image
computing and computer-assisted intervention (pp. 3 /C011). Springer, Cham.
Burges, C. J. (1998). A tutorial on support vector machines for pattern recognition. Data Mining and
Knowledge Discovery ,2(2), 121 /C0167.
Caruso, D., Zerunian, M., Polici, M., Pucciarelli, F., Polidori, T., Rucci, C., & Laghi, A. (2020). Chest CT fea-
tures of COVID-19 in Rome, Italy. Radiology ,296(2), E79 /C0E85.
Chai, T., & Draxler, R. R. (2014). Root mean square error (RMSE) or mean absolute error (MAE)? /C0
Arguments against avoiding RMSE in the literature. Geoscientific Model Development ,7(3), 1247 /C01250.
Collins, J., & Stern, E. J. (1997). Ground-glass opacity at CT: The ABCs. American Journal of Roentgenology ,
169(2), 355 /C0367.
Dalal, N., & Triggs, B. (2005, June). Histograms o f oriented gradients for human detection. In 2005 IEEE computer
society conference on computer visi on and pattern recognition (CVPR’05) (Vol. 1, pp. 886 /C0893). IEEE.
Das, N.N., Kumar, N., Kaur, M., Kumar, V., & Singh, D. (2020). https://www.sciencedirect.com/science/arti-
cle/pii/S1959031820301172 .
Dogo, E.M., Afolabi, O.J., Nwulu, N.I., Twala, B., & Aigbavboa, C.O. (2018, December). A comparative anal-
ysis of gradient descent-based optimization algorithms on convolutional neural networks. In 2018
International conference on computational techniques, electronics and mechanical systems (CTEMS)
(pp. 92 /C099). IEEE.
Duggirala, B., Paine, D.S., Comaniciu, D., Krishnan, A., & Zhou, X. (2007). U.S. Patent No. 7,244,230.
Washington, DC: U.S. Patent and Trademark Office.
Eurosurveillance Editorial Team. (2020). Note from the editors: World Health Organization declares novel
Coronavirus (2019-nCoV) sixth public health emergency of international concern. Eurosurveillance ,25(5),
200131e.
Freeman, W.T., & Roth, M. (1995, June). Orientation histograms for hand gesture recognition. In International
workshop on automatic face and gesture recognition (Vol. 12, pp. 296 /C0301).
Gibson, U. E., Heid, C. A., & Williams, P. M. (1996). A novel method for real time quantitative RT-PCR.
Genome Research ,6(10), 995 /C01001.275 ReferencesGu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., & Chen, T. (2018). Recent Advances in
Convolutional Neural Networks. Pattern Recognition ,77, 354 /C0377.
Haralick, R. M., & Shapiro, L. G. (1985). Image segmentation techniques. Computer Vision, Graphics, and
Image Processing ,29(1), 100 /C0132.
Herath, H.M.K.K.M.B. (2021). Internet of Things (IoT) enable designs for identify and control the COVID-19
pandemic. In D. Oliva, S. A. Hassan, & A. Mohamed (Eds.), Artificial intelligence for COVID-19 (Studies
in systems, decision and cntrol (ed.), Vol. 358, pp. 1 /C014). https://doi.org/10.1007/978-3-030-69744-0_24 .
Herath, H.M.K.K.M.B., Karunasena, G.M.K.B., & Herath, H.M.W.T. (2021). Development of an IoT based
systems to mitigate the impact of COVID-19 pandemic in smart cities. In U. Ghosh, Y. Maleh, M. Alazab,& A.-S. Khan Pathan (Eds.), Machine intelligence and data analytics for sustainable future smart cities
(Vol. 971, pp. 1 /C023).
https://doi.org/10.1007/978-3-030-72065-0_16 .
Herath, H.M.K.K.M.B., Karunasena, G.M.K.B., Ariyathunge, S.V.A.S.H., Priyankara, H.D.N.S., Madhusanka,
B.G.D.A., Herath, H.M.W.T., & Nimanthi, U.D.C. (2020, December). Deep learning approach to recogni-tion of novel COVID-19 using CT scans and digital image processing. In 4th SLAAI - international confer-
ence on artificial intelligence (pp. 1 /C012). SLAAI.
Huang, C., Wang, Y., Li, X., Ren, L., Zhao, J., Hu, Y., & Cao, B. (2020). Clinical features of patients infected
with 2019 novel coronavirus in Wuhan, China. The lancet ,395(10223), 497 /C0506.
Jafari, M.H., Karimi, N., Nasr-Esfahani, E., Samavi, S., Soroushmehr, S.M.R., Ward, K., & Najarian, K.
(2016, December). Skin lesion segmentation in clinical images using deep learning. In 2016 23rd
International conference on pattern recognition (ICPR) (pp. 337 /C0342). IEEE.
Jiang, J., Trundle, P., & Ren, J. (2010). Medical image analysis with artificial neural networks. Computerized
Medical Imaging and Graphics ,34(8), 617 /C0631.
Jin, Y. H., Cai, L., Cheng, Z. S., Cheng, H., Deng, T., Fan, Y. P., & Wang, X. H. (2020). A Rapid Advice
Guideline for the diagnosis and treatment of 2019 Novel Coronavirus (2019-nCoV) infected pneumonia
(standard version). Military Medical Research ,7(1), 1 /C023.
Joachims, T. (1998). Making large-scale SVM learning practical (no. 1998, 28). Technical report .
Kadkhodaei, M., Samavi, S., Karimi, N., Mohaghegh, H., Soroushmehr, S.M.R., Ward, K., & Najarian, K.
(2016, August). Automatic segmentation of multimodal brain tumor images based on classification of
super-voxels. In 2016 38th Annual international conference of the IEEE Engineering in Medicine and
Biology Society (EMBC) (pp. 5945 /C05948). IEEE.
Katti, G., Ara, S. A., & Shireen, A. (2011). Magnetic Resonance Imaging (MRI)-A review. International
Journal of Dental Clinics ,3(1), 65 /C070.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2017). ImageNet classification with deep convolutional neural
networks. Communications of the ACM ,60(6), 84 /C090.
Lee, J. G., Jun, S., Cho, Y. W., Lee, H., Kim, G. B., Seo, J. B., & Kim, N. (2017). Deep learning in medical
imaging: General overview. Korean Journal of Radiology ,18(4), 570.
Llovet, J.M., Schwartz, M., & Mazzaferro, V. (2005, May). Resection and liver transplantation for hepatocellu-
lar carcinoma. In Seminars in liver disease (Vol. 25, No. 02, pp. 181 /C0200). New York, NY 10001, USA:
Thieme Medical Publishers, Inc., 333 Seventh Avenue.
Lowe, D.G. (1999, September). Object recognition from local scale-invariant features. In Proceedings of the
seventh IEEE international conference on computer vision (Vol. 2, pp. 1150 /C01157). IEEE.
Luo, X., Chang, X., & Ban, X. (2016). Regression and Classification Using Extreme Learning Machine Based
on L1-norm and L2-norm. Neurocomputing ,174, 179 /C0186.
McConnell, R.K. (1986). U.S. Patent No. 4,567,610. Washington, DC: U.S. Patent and Trademark Office.
Mishra, S., Tripathy, H. K., & Acharya, B. (2020). A precise analysis of deep learning for medical image pro-
cessing .Bio-inspired neurocomputing (pp. 25 /C041). Singapore: Springer.276 Chapter 10 Early detection of COVID-19 pneumoniaMittal, M., Verma, A., Kaur, I., Kaur, B., Sharma, M., Goyal, L. M., & Kim, T. H. (2019). An efficient edge
detection approach to provide better edge connectivity for image analysis. IEEE Access ,7, 33240 /C033255.
Nasr-Esfahani, M., Mohrekesh, M., Akbari, M., Soroushmehr, S.R., Nasr-Esfahani, E., Karimi, N., & Najarian,
K. (2018, July). Left ventricle segmentation in cardiac MR images using fully convolutional network. In
2018 40th Annual international conference of the IEEE Engineering in Medicine and Biology Society
(EMBC) (pp. 1275 /C01278). IEEE.
Ning, W., Lei, S., Yang, J., Cao, Y., Jiang, P., Yang, Q., & Wang, Z. (2020). iCTCF: An integrative resource
of chest computed tomography images and clinical features of patients with COVID-19 pneumonia.
O’Shea, K., & Nash, R. (2015). An introduction to convolutional neural networks. Available online: https://
white.stanford.edu/teach/index.php/An_Introduction_to_Convolutional_Neural_Networks (accessed on 28
December 2021).
Pandey, P., Pallavi, S., & Pandey, S. C. (2020). Pragmatic medical image analysis and deep learning: An
emerging trend .Advancement of machine intelligence in interactive medical image analysis (pp. 1 /C018).
Singapore: Springer.
Pham, D. L., Xu, C., & Prince, J. L. (2000). Current methods in medical image segmentation. Annual Review
of Biomedical Engineering ,2(1), 315 /C0337.
Potter, C. W. (2001). A history of influenza. Journal of Applied Microbiology ,91(4), 572 /C0579.
Rafiei, S., Nasr-Esfahani, E., Najarian, K., Karimi, N., Samavi, S., & Soroushmehr, S.R. (2018, October).
Liver Segmentation in CT Images using Three Dimensional to Two Dimensional Fully ConvolutionalNetwork. In 2018 25th IEEE international conference on image processing (ICIP) (pp. 2067 /C02071).
IEEE.
Sadhukhan, P., Ugurlu, M. T., & Hoque, M. O. (2020). Effect of COVID-19 on lungs: Focusing on prospective
malignant phenotypes. Cancers ,12(12), 3822.
Sahu, D. K., & Parsai, M. P. (2012). Different image fusion techniques - A critical review. International
Journal of Modern Engineering Research (IJMER) ,2(5), 4298 /C04301.
Schmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with ReLU activation func-
tion. Annals of Statistics ,48(4), 1875 /C01897.
Schmitt, W., & Marchiori, E. (2020). Covid-19: Round and oval areas of ground-glassopacity. Pulmonology ,
26(4), 246.
Song, F., Shi, N., Shan, F., Zhang, Z., Shen, J., Lu, H., & Shi, Y. (2020). Emerging 2019 novel Coronavirus
(2019-nCoV) Pneumonia. Radiology ,295(1), 210 /C0217.
Umbrello, M., Formenti, P., Bolgiaghi, L., & Chiumello, D. (2017). Current concepts of ARDS: A narrative
review. International Journal of Molecular Sciences ,18(1), 64.
Vapnik, V. (2013). The nature of Statistical Learning Theory . Springer Science & Business Media.
Vapnik, V. N. (1982). Estimation of dependencies based on empirical data . New York: Springer-Verlag.
Vapnik, V. N. (1995). The nature of Statistical Learning Theory . New York: Springer-Verlag.
Vapnik, V. N., & Chervonenkis, A. Ya. (1974). Theory of Pattern Recognition . Moscow: Nauka.
Wang, Y., Dong, C., Hu, Y., Li, C., Ren, Q., Zhang, X., & Zhou, M. (2020). Temporal changes of CT findings
in 90 patients with COVID-19 pneumonia: A longitudinal study. Radiology ,296(2), E55 /C0E64.
Yan, Y., Chang, L., & Wang, L. (2020). Laboratory testing of SARS-CoV, MERS-CoV, and SARS-CoV-2
(2019-nCoV): Current status, challenges, and countermeasures. Reviews in Medical Virology ,30(3), e2106.
Zhu, N., Zhang, D., Wang, W., Li, X., Yang, B., Song, J., & Tan, W. (2020). A Novel Coronavirus from
patients with pneumonia in China, 2019. New England Journal of Medicine ,382(8), 727 /C0733. Available
from https://doi.org/10.1056/nejmoa2001017 .
Zu, Z. Y., Jiang, M. D., Xu, P. P., Chen, W., Ni, Q. Q., Lu, G. M., & Zhang, L. J. (2020). Coronavirus
Disease 2019 (COVID-19): A perspective from China. Radiology ,296(2), E15 /C0E25.277 ReferencesThis page intentionally left blankCHAPTER
11Applications of wearable
technologies in healthcare: ananalytical study
Hiren Kumar Thakkar1, Shamit Roy Chowdhury2, Akash Kumar Bhoi3and Paolo Barsocchi4
1Department of Computer Engineering, Marwadi University, Rajkot, Gujarat, India2School of Computer
Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha, India3KIET Group of Institutions, Delhi-NCR,
Ghaziabad, India4Institute of Information Science and Technologies, National Research Council, Pisa, Italy
11.1 Introduction
The first wearable technology was found when eyeglasses were invented, which may be trace-
able since the 13th century. Simple, robust clo cks were developed in the 16th century, named
Nuremberg eggs ( Evenson, Goto, & Furberg, 2015 ). They were designed so that they had to be
worn around the neck. Before the pocket watches and wristwatches arrived, it became afamous status symbol in Europe. During the 17 th century in China, the abacus ring could be
referred to as another early example of wearable t echnology. During the early 1960s, statistics
academician Edward Thorp built the first co mputer using wearable technology. Thorp
explained in his book, which was named “Beat the Dealer,” that he was successful in design-ing a tiny computer.
According to Thorp, that computer could fit in a shoe that would help to cheat at roulette.
Thorp and co-developer Claude Shannon got 44 probability edge as an outcome because of the tim-ing device. That same timing device helped to predict the landing of the ball on a roulette table.The next couple of decades marked the modernization of wearable technologies. In 1975, the firstcalculator wristwatch was released. After four years, the Sony Walkman came into the market. The
1980s saw the first release of digital hearing aids. In
Fig. 11.1 , the chronological order of the wear-
ables has been shown.
The wearable units based on bio-metrics integrated within mobile units were used to collect spe-
cific information ( Klonoff, 2013; Lee, Kim, & Welk, 2014 ). It allowed the accurate and precise
management of patient’s medical status from the healthcare unit’s outskirts. The diagnosis and pre-diction of the patient’s outcome could be made easy from the massive amount of information gath-ered from the wearable units. It also might play a significant role for healthcare professionals tochoose the optimal diagnosis for their patients. Recently, there has been a lot of “hype” for artificial
intelligence (AI)-based tools. The wearables help the user gain a better understanding of one’s
body.
2795G IoT and Edge Computing for Smart Healthcare. DOI: https://doi.org/10.1016/B978-0-323-90548-0.00001-2
©2022 Elsevier Inc. All rights reserved.There are different types of wearables in the market ( Wu, Li, Cheng, & Lin, 2016 ). Some of
them are mentioned below:
 Smart Watches: It helps keep track of time and helps the user by notifying calls, messages, emails, etc.
 Fitness Tracker: Keeps check on the health of the user. The device notes down the number of
steps walked by the user.
 Head-Mounted Display: Takes the user to a different virtual reality world and provides virtual
information directly to the eyes.
 Sports Watches: These wearable watches are specifically for fitness freaks and sportspersons
like swimmers, gymnastics, cyclists, etc.
A GPS tracker is connected to that, which helps record information of the user’s pace, heart
rate, calories burned, etc. ( Bhoi, Sherpa, & Khandelwal, 2018 ).
 Smart Jewelry: Smartwatches are designed as jewelry, especially for women.
 Smart clothing: The wearable devices are planted in the clothing to give a more fashionable look.
 Implantable: These devices are implanted under the skin of the patient. They help in tracking
contraception, insulin level, etc.
FIGURE 11.1
Types of wearable devices ( Sahoo, Thakkar, & Lee, 2017 ).280 Chapter 11 Applications of wearable technologies in healthcareIn addition to the sensors used in wearable devices to monitor vital health care data, sensor technology
advancements greatly influence the diversified domains such as Wireless Sensors Networks ( Sahoo &
Thakkar, 2019; Sahoo, Thakkar, & Hwang, 2017 ). Usually, the sensor acquired data are meaningless
unless processed and analyzed using intelligent techni ques. Several machine learning (ML)-assisted techni-
ques can effectively help analyze healthcare and data acquired for diversified domains. For instance, in
Rai, Thakkar, and Rajput (2020) , the Seismocardiogram data are analy zed for automatic annotation using
ML assisted binary classification. Similarly, robust AI techniques such as Reinforcement Learning canhelp analyzed health care data in a c loud of useful resource utilization (
Thakkar, Dehury, & Sahoo, 2020 ).
Moreover, ML tools can also be used for predicting clin ically significant motor function improvements
(Thakkar, Liao, Wu, Hsieh, & Lee, 2020 ). Wearables influence health an d medicine, fitness, disability,
education, transportation, enterp r i s e ,f i n a n c e ,g a m i n g ,m u s i c ,e t c .( Mishra, Dash, & Mishra, 2020 ).
11.2 Application of wearable devices
Wearable technology is an emerging field of innovation that includes designing a portable device worn by
people. Wearable devices essen tially help monitor vital inform ation related to human health ( Bhoi, Sherpa,
& Khandelwal, 2015; Mishra, Patel, Panda, & Mishra, 2019 ). Wearable devices are more or less designed
to capture the image and signal data and transfer it to cloud storage via smartphone-enabled wireless tech-nology to monitor vital health parameters. From healt hcare to finance wearables, devices have stepped
their foot in every domain. Due to the small size of a wearable device, it has become very comfortable for
people to use them anywhere, let it be for jogging or while working. By using fitness trackers, people cancount the exact number of calories they burned (
Mishra, Mishra, Tripathy, & Dutta, 2020 ), the distance
they have walked, and can analyze the sleep trend. Some wearable devices also give an estimation of your
heart rate and blood pressure. Wearables help one to live a healthy and balanced life ( Mishra, Tripathy,
Mishra, & Mohapatra, 2018 ). The use of wearables can also be seen in the finance sector. Every financial
institution wants to create a better experience for its customers ( Bhoi, 2017 ). With the help of fast-
growing internet services and India’s smartphone market, many private and public sector banks have
moved to internet banking, mobile banking, etc. Payments from mobiles have become very common now.
Wearables are a revolution in the finance sector. Wearables can also be used for payments and to extendthe current mobile banking functions. Many wearables like wristbands, smartwatches (Apple Watch,Samsung Gear s2) jewelry (NFC ring) allows users to c onnect it with their mobile phones or bank account
to make payment for goods and services. Apart from payment, it also helps its customers check the moneybalance, control money spending, and track cash flow. By the use of wearables, we can ensure the safetyof women and elderly people. For example, ATHEN A is a wearable technology by the ROAR company
that aims to protect women against violence and assa ults. It is designed especially for women. It can be
easily worn as a pendant or clipped to their bag or dress. It can produce an alarm of 85db and also sends
notifications to family members. In
Fig. 11.2 , the use of wearables in various domains has been described.
The most common wearable technologies include smart jewelry, such as rings, watches, wristbands,
and pins ( Mishra, Tripathy, Mallick, Bhoi, & Barsocchi, 2020 ). Specific smaller devices for display and
interaction go best with a smartphone app. Smartwat ches and fitness trackers have become very common
and recognizable as examples of wearable techno logy. Examples: Garmin, Fitbit, Apple Watch, and
Samsung Galaxy Gear. In Fig. 11.3 , the different examples of wearables have been shown.281 11.2 Application of wearable devicesFIGURE 11.2
Number of connected wearable devices worldwide from 2016 to 2020.
Courtesy: https://www.electronic.se/ .
FIGURE 11.3Examples of wearables.282 Chapter 11 Applications of wearable technologies in healthcare11.3 The importance of wearable technology in healthcare
11.3.1 Personalization
InFig. 11.4 , few more examples of wearables have been shown. Inter-sharing norms to monitor fit-
ness and clinical data using wearables and smartphones are to be adopted rather than conventionalguidelines were declared in late 2018 by John Hancock, an eminent foreign-based insurer. Thecompany would determine the risks included in every person’s lifestyle habits by keeping track ofevery client’s health status. The clients who have the habits included with less time span (e.g., lessphysical labor, inconsistent diet) must deal with high premiums. This insurance industry took agreat step, making their clients adopt healthier lifestyles eventually, which will help reduce the
pressure on healthcare.
11.3.2 Remote patient monitoring
The main advantage of a small smartwatch is that it can be worn regularly. A smartwatch can pro-
vide various events along with measuring the pulse all day long ( Mishra, Mishra, & Tripathy,
2020 ). If these kinds of devices gathered more data, medicare’s feasibility to develop a prevention-
based prototype would increase, which can interpret the patient’s data and alert patients for timely
diagnoses.
11.3.3 Early diagnosis
Patterns can be established using AI by gathering heaps of information through wearables to track
patients’ health status. The system will determine the potential problems of the individual much
before the problem arrives by using the information aggregated from every person. It will help to
adopt much cost-effective and better curative decisions, which is opposite to diagnosing the risks atits full swing.
FIGURE 11.4
Use of wearables in healthcare.283 11.3 The importance of wearable technology in healthcare11.3.4 Medication adherence
On many occasions, we find patients often forget to take their medications on time. Wearables play
a very important role in this aspect. It helps remind the doctors of the patients’ medications andremind the doctors whether their patients are taking medications as prescribed.
11.3.5 Complete information
Generally, very limited information is there with the doctors about their cases. The patient’s medi-
cal analysis regarding risk factors and the beginning of the health discrepancies and the patient’s
clinical data based on past interactions is everything on which the doctors must rely upon. These
kinds of data are often incomplete or imprecise Balas (2021) . A wearable device’s task is to collect
all a patient’s data in real-time, mistakes are not made, or an important symptom report never for-gets to note. Therefore, the doctor can collect a detailed and precise report about the patient’s con-dition. Hence, it will aid the doctor to establish a more accurate analysis leading to correctdecision-making.
11.3.6 Cost savings
Investing in wearables might look like a costly investment, but it will be useful in saving unneces-
sary costs overall. These units often permit medical staff to keep track of patient’s health status.
This eliminates the requirement to shift the patient to a medical department. Moreover, with wear-
able devices, the symptoms could be detected at an early phase, which will allow less expensivetreatments (
Mishra, Mallick, Jena, & Chae, 2020 ).
The wearables in health care are still in a developing phase, but in the coming years, it is
expected that this market will grow at a rapid pace ( Bhoi & Sherpa, 2014; Mallick, Mishra, &
Chae, 2020 ). In Fig. 11.4 , some uses of wearables in healthcare have been shown.
Wearables have drastically increased; there are many reasons for the increase in the wearables
market. Some of the key points responsible for these change is as follow ( Mishra, Sahoo, Mishra,
& Satapathy, 2016 ).
 Rising demand for round the clock inspection of the patients
 The growing culture of health and fitness Rising of per capita income
 Growth of technology.
11.4 Current scenario of wearable computing
A flexible distributed mobile platform was developed by the Media Laboratory of MIT,
Cambridge. It was named LiveNet. It is primarily aimed at specific health monitoring long-termapplications along with data processing in real-time. It also has context classification and streaming(
Sung, Marci, & Pentland, 2005 ).284 Chapter 11 Applications of wearable technologies in healthcareThe EU FP5 IST program financed AMON’s project (advanced care and alert portable telemedical
monitor) ( Anliker et al., 2004 ). The project’s outcome was a wrist-worn device designed to acquire the
data related to blood pressure. Moreover, the same w rist-worn device can acquire the blood oxygen sat-
uration along with the skin temperature usin g the one lead Electrocardiography (ECG). In Lin, Lin,
Chou, Chong, and Chen (2006) , a real-time wireless physiologic al monitoring system (RTWPMS) is
described. Like AMON, the RTWPMS is primarily de signed using the low-power digital 2G cordless
phone to acquire data related to the body temperature, pulse rate, and blood pressure. The RTWPMS isa highly custom-made healthcare in formation retrieval module. In
Mundt et al. (2005) , a system called
LifeGuard is designed on the top of the multiparamet er wearables with an application of space and ter-
restrial. The LifeGuard module comprises a crew phy siologic observation device that helps measure the
diversified vital health parameters such as oxygen sat uration, body movement, respiration rate, heart
rate, and blood pressure. In Ren, Chien, and Tai (2006) , a prototype of a portable healthcare system is
designed. The proposed portable healthcare system is targeted to acquire vital cardiac health-related
data such as ECG, body temperature, and phonocardiography.
In addition to the cardiac health-related data, brain injury detection is an important aspect. In
Tura, Badanai, Longo, and Guarani (2003) , a portable wearable device is designed to detect brain
injuries in children. The proposed portable wearable device uses different sensors to acquire healthinformation, such as a pulse oximeter to acquire heart rate and blood oxygen saturation. In additionto that piezoelectric sensor-based belt is designed to acquire the respiration rate from the chestarea. Furthermore, the dual-axis thermal accelerometer is used to acquire body movement.Cardiovascular diseases (CVD) are on the rise, and to contain them, the European Commission sup-
ported a project named MyHeart. This MyHeart project has engaged nearly 33 partners across ten
major developed and developing countries. To make the MyHeart project successful and commer-cially viable, the European Commission has included several major industry collaborators such asPhilips, Nokia, Vodafone, and Medtronic. The primary object of MyHeart is to fight against CVDby early diagnosis and prevention (
Habetha, 2006 ). Under the MyHeart project, smart clothing is
designed using the sensing module in garment-integrated or cloth-embedded wearable ( Lubrano,
Sola, Dasen, Koller, & Chetelat, 2006; Pacelli, Loriga, Tacchini, & Paradiso, 2006 ). Heart belts
were developed by MyHeart, which could be worn across the chest. It could be attached to a stan-
dard bra or the waistband of standard underwear ( Uhlsteff et al., 2004 ).
One of the Projects, called Wearable Health Care System (WEALTHY), has designed a wearable
garment that covers the upper half of the body, and the garment is designed to be comfortable by wear-ing it as such the normal clothing. The wearabl e garment of the WEALTHY project is targeted to
acquire physiological signals and biomechanical variables (
Paradiso, Loriga, & Tacchini, 2005 ). A
three-lead ECG can be designed using the sensor y units combined with conducting items over some
textile template ( Lymperis & Paradiso, 20 08; Scilingo et al., 2005 ). A washable sensorized vest was
developed by researchers in Milan, Italy, named MagIC ( Di Rienzo et al., 2005 ). It involved completely
covered textile sensory units for breathing rate and hea rt rate tracking. It has a robust electronic module
for evaluating the movement of the wearer. It performs signal processing and data sharing tasks byBluetooth to a local system. A very simple and cost-e ffective stretchable sensory garment was devel-
oped in 2006 by the Medical Remote Monitoring of clothes (MERMOTH) project as a part of theEuropean project (
Lubrano, 2006; Weber & Porotte, 2006 ). A physiological tacking module consisting
of a vest using several sensors was described by Pandian et al. (2008) . It integrates with a fabric of gar-
ments that reliably collect ed various bio-signals.285 11.4 Current scenario of wearable computingA wearable body area network was developed in ( Milenkovic, Otto, & Jovanov, 2006 ) consist-
ing of a general wireless interface with transceivers and less powered control units. HarvardUniversity proposed a healthcare sensory model for multipotent tracking developed Code-Blue(
Shnayder, Chen, Lorincz, Fulford-Jones, & Welsh, 2005 ) based on Zigbee protocol. Customized
sensory boards were available in the design to monitor pulse rate and heart irregularities. In
Monton et al. (2008) , a body area network based on a star topology and IEEE 802.15.4 protocol
was developed. It constituted two primary units which include the communication unit and the pro-cessing unit. Communication unit interfaced with digital and analog sensors while processing unitcoordinated with BAN and controlled all SCMs and sharing with external modules.
A customized e-medicare model is presented by Chung et al.
Chung, Lee, and Toh (2008) .I t
constitutes several customized interfacing nodes fitted with heart rate and blood pressure sensors. Italso consists of an essential mobile phone unit used to display information and extraction featuresin signals. The ubiquitous sensor nodes hardware is described in detail in
Chung, Lee, and Jung
(2008 ) by Chung et al. A wireless body sensor network hardware developed recently is presented
byYuce, Ng, Myo, Khan, and Liu (2007) . It makes use of an assigned clinical sharing band. An
analytical study termed Human 11 proposed a body area network that consists of sensory modules
and a sink node ( Gyselinckx, Penders, & Vullers, 2007 ). A heart-related application is presented by
Leijdekkers and Gay ( Leijdekkers & Gay, 2008 ), which mainly focuses on a personal healthcare
status monitoring model. It involves a traditional cell phone service along with a Bluetooth-basedECG sensor. A wearable device for continual monitoring of ECG is analyzed in work in
Fensli,
Gunnarson, and Gundersen (2005) ,Sahoo et al. (2017) ,Sahoo, Thakkar, Lin, Chang, and Lee
(2018) ,Thakkar and Sahoo (2019 ). Here the ECG sensor transmits the original and the amplified
ECG signal regularly. This band is held device (HHD), which is a general PDA.
11.5 The wearable working procedure
In brief, it lets us know the working of a wearable explicitly. There are video cameras attached to
head-mounted displays and glasses that track and record particular objects. Sensors attached cantrack motion, brain activity, the muscles’ activity, and heart rate. They are usually found in health-oriented devices. Wearable devices also contain miniature computers or processors inside them.
This helps in interacting with other objects. The data is collected from the different sensors present
in the device. After that, the data is passed through algorithms, which helps to make sense of thedata. The data collected is stored in the databases and, when required, is displayed in mobile appli-cations and the device’s web portals. In
Fig. 11.5 , the workflow of wearables has been shown.
11.6 Wearables in healthcare
11.6.1 Weight loss
Lose It! It is a kind of application-based service that helps people lose weight and undergo a
healthy lifestyle. There are a lot of digital mediums through which Lose It offers several services.It includes the ability to log meals, count the maintenance calories, provide a variety of fitness286 Chapter 11 Applications of wearable technologies in healthcarechallenges. It also offers to provide reminders and reports on daily weight loss goals. It also pro-
vides a precise method that analyzes the daily calorie and nutrient intake by the wearable applica-tion. Lose it platform works in association with several popular health brands such as Google Fit,Fitbit, Nike, Misfit, MapMyFitness, etc.
11.6.2 Medication tracking
Medisafe is a mobile platform that helps patients to stick to their daily medical regime strictly
through wearable technology. There are four key features of medisafe,
 Patients can record their medications on the app by a virtual pillbox.
 Alerts and notifications are there to make the patients aware (remind) about their medications
according to the doctor’s prescription.
 Medfriends, or third parties can monitor the user and receive medication reminders, which will
help track the patient’s progress.
 Location-specific coupons are sent to the patients by a partnership that has been formed
recently. These medications are imported directly from the hospital records.
Medication notifications and reminders are the focus of the Medisafe wearable application. Its
availability can be found on Android and Apple. The wearable interactions include quick finger
taps, which indicate whether the medication has been taken.
11.6.3 Virtual doctor consultations
HealthTap, a health management program, provides comprehensive virtual care. Three core compo-
nents are included in the digital platform.
FIGURE 11.5
Workflow of wearables.287 11.6 Wearables in healthcare An extensive network of doctors creates contents that are present in the information module.
Doctors can give real-time answers to the patients according to their questions asked.
 There are doctor consultations 24 37 through voice, video, or text by services like There
HealthTap Prime and Concierge.
 HealthTap encourages users to remain healthy by providing knowledge and information related
to a healthy lifestyle.
Certain health search capabilities are controlled by voice.
11.6.4 Geiger counter for illnesses
Sickweather is a mobile platform. It acts as a Doppler radar for illness. It made its place in the
Apple Watch Release in June 2014. From social media and crowdsourcing, the data is mainly col-lected by Sickweather’s mobile platform. Hence the data gets mapped into a real-time map of ill-ness. The advantage of the new platform was taken by Graham Dodge, president, and CEO ofSickweather, to create an innovative, new feature, Sick Score, described by him as a “Geigercounter for contagious illness.” There is a feature which contains the top three contagious illness.
There is also a handwashing timer. The handwashing timer makes the people alert whenever they
are in an area with a high sick score.
11.6.5 Hydration tool
App design and development company Stanfy designed an Apple Watch application named Water
balance. It helps users in tracking their water intake, which will improve their hydration habits. The
eatable application helps in analyzing the amount of water an individual needs. Height, weight,
age, and physical activity level determine the amount of water intake. The Apple Watch wasdescribed as a convenient device by Andrew Garkavyi, CEO of Stanfy, which tracks hydration as itis located on the wrist.
11.6.6 Pregnancy and fertility tracking
An independent pregnancy and fertility named BabyMed worked with application design and devel-
opment company Blue Label Labs to put its mobile application to the Apple Watch. A fertility cal-endar and a pregnancy calendar feature in the mobile application will help users track, prepare, andcalculate due dates or ovulation dates. It allows a user to track these dates in many ways, but thewearable mainly focuses on only two features.
Fig. 11.6 shows an example pregnancy and fertility
tracking mobile application.
 The user can check the status of her fertility using animations using the fertility tracker.
 The user’s present milestone is identified, and the length of time until he/ she reaches the next
landmark, is shown by the pregnancy tracker. This, in turn, helps the doctor to monitor thepatient easily.288 Chapter 11 Applications of wearable technologies in healthcare11.7 State-of-the-art implementation of wearables
We will be discussing three case studies below. Wearables have played a significant role in keeping
track of the fitness information and detecting older people’s stress levels. It will be more apparentnow when we will discuss three case studies now.
11.7.1 Detection of soft fall in disabled or elderly people
According to a World Health Organization report, between 28% and 35% of older people aged
above 65 years old fall at least once a year, and these accidents lead to significant injuries. There
are already many approaches to solving this particular problem, but most are expensive and time-consuming, leading to delayed results. With the rise of wearables in healthcare, a collection of real-life data samples is possible using wearables. By using the proper ML algorithm, the accuracy ofpredicting a fall increase. Presently, in wearable devices used to detect falls, the input data is col-lected from sensors such as accelerometers, altimeters, or gyroscopes. These sensors provide databased on the sudden change in the movement or position of the user. In
Fig. 11.7 , a flow chart anal-
ysis of a specific wearable to detect the soft fall is presented.
 Accelerometer—Detect rapid change in movements.
 Gyroscopes—It is a device used for measuring or maintaining orientation and angular velocity. Altimeter—measures the altitude.
FIGURE 11.6
Pregnancy and fertility tracking functionality ( https, mhealthspot ).289 11.7 State-of-the-art implementation of wearablesThe experiments recorded based on these sensors could majorly detect hard falls but not
soft falls. Soft falls are referred to as falls originating from another initial axis rather than thestanding coordinates, for example-chair sliding. The present work focuses on soft fall identifi-cation. At the end of this work, a prototype will be presented, which will be based on ML. Itwill be executed on a real-time model using a wearable device running on Android OS. Thereare several objectives to be achieved to impleme nt a case study in a real-life system. Firstly,
choose already available and affordable hardware. Secondly, it should be convenient for an
elderly user. Thirdly, provide the user with accu rate predictions. Android platform is chosen to
implement the model because of its large market size. Using Predictive Model Mark-upLanguage (PMML), an XML-based language, the predictive model’s shareable form isexported. The PMML is generated from the Knime platform, a graphical user interface thatintegrates ML and data mining through its modular data pipelining concept. Then the con-sumption of the model is done. The precision of the system is guaranteed using the MLapproach as described below.
In the first step, the real-life data collected is divided into two data sets: the training set and
the test set. The model is developed with the help of a training set in the training phase and the
test phase; with test data, the accuracy of predi ction is tested. The data set is divided as 50%
randomly allocated training set data and the ot her half to the test set data; this process is
repeated at least 20 times with random allocatio n of data in the training set and the test set to
increase the statistical relevance. As the work is based on soft fall detection, the hard fall datais removed from the dataset, thus leaving only s oft fall and other activities in the dataset. The
Area Under Curve of a Receiver Operating Chara cteristic is used to measure the separation of
two classes.
FIGURE 11.7
A flow chart analysis of a specific wearable.290 Chapter 11 Applications of wearable technologies in healthcareThe data is collected from two wearable devices (LG-G and Moto 360). Table 11.1 presents the
characteristics of the collected data. The data is collected and processed with the help of an android
application. Twenty persons perform different activities, which include motion and soft fall. Only
the data related to the linear accelerometer are kept as the data is sufficient to detect a soft fall andreduces the time required to detect the fall. For stability purpose, the values of three linear accelera-tor sensors are modified as
VniðÞ5OðaxiðÞðÞ21ðay iðÞðÞ21ðaziðÞðÞ2(11.1)
After a first inspection of the pattern of the soft fall, the inputs of the ML algorithm are needed
with records of 200 vectors Vn; the time window is 20/C320054000 Ms.
Different algorithms have been used to detect a soft fall accurately. To counterattack the con-
vergence problem when the ratio of samples between classes is not equilibrated, the simple boot-strap sampling method balances the training data. The platform Knime is used to implement MLmodels. The algorithms used in the experiment are:
 Decision tree: A decision tree is used to build a classification or regression prototype in the
tree-like structure. It splits a dataset into subsets with an increase in the level of the tree.
 DT ensemble: Maximum of 300 trees are used in this algorithm; this algorithm will learn an
ensemble of decision trees.
 KNN-50 max neighbors are configuring.
The results observed from the experiment are shown in
Table 11.2 . Among all other algorithms,
the DT ensemble algorithm gives the best accuracy of detecting a soft fall, and the leave-one-outscoring does not show any changes in the result, which shows the robustness of the dataset. Withthis approach, the disabled and elderly people can be easily monitored by doctors and their familymembers, and immediate help can be provided when required. This method is very beneficial for
someone who lives away from their family. With little modification, the patients’ different body
conditions, like heart rate, blood pressure, and temperature, can also be shown and immediatelyalerted if any abnormal conditions occur.Table 11.1 Some records.
Labels Records Samples
Soft fall 475 146,743
Others 2651 12,789,643
Table 11.2 Performance of different ML algorithms.
ML algorithms AUC/Random AUC/Loo
Decision tree 0.65 60.03 0.69 60.03
Random forest 0.92 60.02 0.92 60.02
KNN 0.85 60.02 0.86 60.02291 11.7 State-of-the-art implementation of wearables11.7.2 The third case study is based on the detection of stress using a smart
wearable band
Mental stress is one of the most common mental phenomena that most people suffer from; stress is
not a serious health problem. But if a person goes through stress and anxiety, it can often be the
foundation for a more serious health problem. Stress detection may help prevent stress-related med-ical issues. With the help of a smart band based on skin conductance, stress detection can be done.
In this approach, a smart wearable band is designed to detect stress based on the different con-
ductance levels of the skin. But some activities like running can also produce the same conductancelevel, so distinguishing between stress and other activities ML algorithm is used. Sleep trends andvarious activities can also be tracked, and the data collected can thus be transmitted to the user’ssmartphone via Bluetooth or can be uploaded in a web portal. Apart from many parameters like
heart rate, HRV, and blood pressure, this device uses skin conductance to detect stress. The sweat
secreted in stress conditions decreases the resistance of the skin and increases skin conductance.This fluctuation in the parameter allows it to become reliable for stress detection. If the biologicalaspects are seen, the conductance level of skin is denoted to be a marker to detect the degree ofstress where the excess level of sweat may be regulated specifically by the sympathetic neuronmodel can be determined. If an individual is feeling stressed, in that scenario, the neuron model isset into play. The sympathetic nervous system activates the sweat glands. Thus, sweat secretiontakes place. The sweat decreases the resistive functionality of the skin and increases the conduc-
tance of the skin.
The band’s physical structure contains a micro-controller, Bluetooth, skin conductance sensor,
and 3-axis accelerometer. The conductance sensor is present at the band’s underside. The conduc-tance is measured through a static voltage level provided to silver-coated electrodes that are embed-ded into the skin layer. To maximize the stress detection efficiency, an accelerometer is alsopresent in the device.
The data was collected by retrieving electric signals among participants with a 23 /C056 age
group. The sampling frequency was taken to be 4 Hz. The data was collected from participants after
identifying the psychological state of patients. The information was divided into two groups,
namely with efforts and without efforts.
Table 11.3 shows the samples of the collected data.
The ML algorithm used for analyzing data is logistic regression. Logistic regression gives the
output as 1 or 0. In this case, if the output is 1, the stress is detected; if 0, the individual is notstressed.
The electrodes located at the internal end of the band continuously monitors the skin conduc-
tance of the user. The electrodes release a minor voltage to the skin layer while the response isextracted. The data is recorded, and then, using an algorithm, the user’s stress level is detected. The
band is associated through Bluetooth to the mobile, from where the user can see the results.
The algorithm is found to be 91.66% accurate. The smart band was able to distinguish between
a stressed and unstressed individual accurately. It also helps the doctor and family member monitorthe user’s mental health remotely and take proper preventive measures when required. The designand development of wearables in the medical field hold great attention in the people. The devicesprovide low-cost, unobtrusive solutions for continuous monitoring of the mental health of thepatients. The doctors on the data collected can provide improved treatments. It also helps the userto understand the stress patterns better and know their body better.292 Chapter 11 Applications of wearable technologies in healthcare11.7.3 Use of wearables to reduce cardiovascular risk
The third case study is based on the determining sedentary nature by computational intelligence
study of wearables on a day-to-day activities to reduce cardiovascular risk. With continuous devel-opments in wearable research, energy-constrained methodologies, and the data analysis approach, itis important to contain better, effective methods of providing healthcare services. Based on severalreports on heart-related diseases, treating and curing such diseases is not an easy task due to theincreasing population, inefficient healthcare services, and rising medical expenses. The primary ele-ments for heart-related risk factors include lack of physical activity, improper food habits, regular
smoking nature, and high alcohol intake.
The elimination of disease metrics will prevent around 80% of high risks as disclosed during
research.
 The evidence in what way to sort the concern of an inactive way of leading life is negatively
conceived and act as an issue for further analysis based on the National Institute for Health andCare Excellence guidelines.
 Use of hips, wrists, or even accelerometers mounted on thighs using belts helps analyze static
lifestyle study.
 This study persuades elderly people to remain physically active. The target of this study is to
prepare a model that automatically recognizes sedentary behavior.
 This depends on the smart shirt equipped with a processor, wearable sensors, power supply, and
telemedicine interface.
 There are two phases to this method. Firstly, the usage of sensors comprising two distinct
modes is interlinked with a variable vector which is further subjected to detection methods todecode this sedentary nature.
Secondly, A dummy model of a smart shirt is associated with the telemedicine unit responsible
for measurement detection and computing relevant data. This way is very helpful for elderlyTable 11.3 Records of collected data samples.
Name of users Without efforts With efforts Stressed
U1 1.739 1.611 Yes
U2 1.537 1.630 YesU3 1.615 1.615 YesU4 1.383 1.671 YesU5 1.126 1.143 YesU6 1.138 1.018 Yes
U7 1.001 1.094 No
U8 0.823 0.958 NoU9 1.101 1.123 NoU10 1.060 1.097 NoU11 0.709 0.793 NoU12 1.052 1.081 No293 11.7 State-of-the-art implementation of wearablespatients who need constant monitoring in the ICUs. The signals that are collected by smart shirts
consist of information that recognizes sedentary behavior during day-to-day activities and stored inthe telemedical service provider for further analysis.
The experiment was performed with the help of five individuals (three females and two males).
The volunteers are required to wear a prototype design of a smart shirt that was embedded with
accelerometers, pulse sensors, ambient sensors. The volunteers were asked to perform a set of dif-
ferent activities through which data was generated, and this data was used to test the accuracy ofthe inactivity classification algorithm. The tasks were conducted in a home environment. Theexperiments consisted of tasks such as:
 Ambulatory events: lying unmoved, idle sitting, and walking.
 Day-to-day events such as constantly working for hours over a system. The battery test functionalities like stand & sit, feet stand, standing at tandem, etc.
The duration of each task was the 60 s, which was followed by a break of 60 s. Over 28,800
samples were collected and lasted 23 min. The shirt prototype is designed so that it is simple to uti-lize and operates primarily in elderly and physically challenged individuals. Three types of modulespresent interlink in these three non-disjointed groups.
 Personal Area Network (PAN) is dynamically developed near users.
 Local Area Network (LAN) and the internet.
The shirt is integrated with a body control unit, which creates a PAN network and waits for incom-
ing connections. The network is established when the network coordinator is connected to the BCU.
The data is collected, filtered, and encapsulated, and features are extracted with the help of the BCU.
The NC and the results obtained to analyze the data packets from BCU are sent to the cloud provider(CP) via LAN. In
Fig. 11.8 , the design implementation of a sam ple shirt prototype is presented.
CP does data aggregation, data storage, data analysis for authorized users. The system architec-
ture is shown in Fig. 11.8 .
Supervised learning was used to develop the model. States of activity are determined in a regu-
lar 1 minute time span. Enough text-based data and coordination for the working of the SPPB pro-
tocol were obtained. The sampling rate was predefined at 40 Hz. This approached attribute
reduction approach was applied with every time span. Heart rate is estimated with a peak rate ofsignals beyond a maximum limit of 540( B1.74 V) with at least 19 samples. The time-domain fea-
ture of accelerometer and intensity computation includes addition, average, standard deviation, vari-ance, and the root mean square is investigated during the experiment. A vector for the optimumintensity for every axis was chosen from the frequency-domain feature of the accelerometer. Thestandard deviation of bit-to-bit time duration and the square root of the mean of consecutive differ-ences were computed.
In the second step, the 2D feature vector obtained is transferred by the categorization system
that differentiates between suitable labels. The first-class events for sedentary nature are lying, sit-ting, working on the system. The second set of activities is the remaining activities. The data islabeled into the appropriate class and stored in the database. The various ML algorithms used were:Binary decision trees, the Discriminant Analysis model, Naive Bayes, K-nearest neighbors, SVM,and ANN. The validation of the classification model was done by using a 10-fold-cross-validation.The entire process is presented in
Fig. 11.9 .294 Chapter 11 Applications of wearable technologies in healthcareThe results are shown in Table 11.4 , which have concluded the raw hypothesis regarding the
method’s feasibility to get and discuss multi-sensors signals for the experimental protocol. The two
classifiers: SVM (Support Vector Machines) and Artificial Neural Network (ANN), provide themaximal efficiency with respect to accuracy. The Naive Bayes model delivered a relatively poorperformance. With the use of this process, the sedentary behavior in individuals, especially older
FIGURE 11.8
A look of the shirt prototype.
FIGURE 11.9The functionality of the shirt prototype.295 11.7 State-of-the-art implementation of wearablespeople, can be monitored. Using machine learning, high accuracy can be obtained to estimate sed-
entary behavior. The prototype can be personalized to a shirt-based health technology telemonitor-ing system, which can provide significant data about the doctors’ behavior. This data can be veryuseful in early diagnosis and detection to the prevention and treatment of cardiovascular disease. It
also motivates the user to keep track of their health.
11.8 Future scope and conclusion
The true potential of wearables is not only to wear the screens on the wrist but also about the new,
amazing, and interesting sensors incorporated in it. In other words, New sensors signify new data,
new data leads to new insights, and new insights take to new applicability of the insights acrossmarkets. The beginning was all about accelerometers, gyroscopes, and magnetometers of threeaxes. Skin Galvanic, glucose levels, and electromyography, and many others can begin to beviewed with the expansion in the scope of better developed wearable devices. It is absolutely,unimaginably fascinating to know the amount of health data that can be gathered with various sen-sors. This can make humanity much healthier with the possibility of constantly analyzing one’shealth. Humanity can now know the response of an activity biologically on the human body. These
devices’ benefits could be as simple as tracking a pushup to early detection of cardiac arrest to var-
ious others. The wearable health industry trend is not slowing down any soon as healthcare profes-sionals are excited to invade more possibilities for helping patients with chronic illness and helpingpeople in remote areas. Effective collection of individual health information, more actionable out-puts, and clinical validation are the need of the hour. Demand also is increasing for the ability tointegrate data from wearables into electronic health records. In conclusion, technology throughwearables boosts the healthcare industry by making health analysis of people a much easier task.
References
Anliker, U., Ward, J. A., Lukowicz, P., Tr Oster, G., Dolveck, F., Baer, M., ...Vuckovic, M. (2004). AMON:
A wearable multiparameter medical monitoring and alert system. IEEE Transactions on Information
Technology in Biomedicine ,8(4), 415 /C0427.Table 11.4 Few details of algorithms.
Methods Accuracy Sensitivity Specificity Precision F1
Linear discriminative analysis 95.0 86.6 97.7 92.8 89.6
Support vector machine 96.6 93.3 97.7 93.3 93.3KNN 93.3 80.0 97.7 92.3 85.7Binary decision tree 96.6 93.3 97.7 93.3 93.3Naive Bayes 91.6 100.0 88.8 75.0 85.7ANN 96.6 93.3 97.7 93.3 93.3
Average 95.0 91.1 96.2 90.0 90.1296 Chapter 11 Applications of wearable technologies in healthcareBhoi, A. K. (2017). Classification and Clustering of Parkinson’s and healthy control gait dynamics using LDA
and K-means. International Journal Bioautomation ,21(1).
Bhoi, A. K., Mallick, P. K., Liu, C. M., & Balas, V. E. (Eds.), (2021). Bio-inspired neurocomputing . Springer.
Bhoi, A. K., Sherpa, K. S., & Khandelwal, B. (2018). Ischemia and arrhythmia classification using time-
frequency domain features of QRS complex. Procedia Computer Science ,132, 606 /C0613.
Bhoi, A. K., Sherpa, K. S., & Khandelwal, B. (2015). Multidimensional analytical study of heart sounds: A
review. International Journal Bioautomation ,19(3), 351 /C0376.
Bhoi, A. K., & Sherpa, K. S. (2014). QRS complex detection and analysis of cardiovascular abnormalities: A
review. International Journal Bioautomation ,18(3), 181 /C0194.
Chung, W. Y., Lee, Y. D., & Jung, S. J. (2008). A wireless sensor network compatible wearable u-healthcare
monitoring system using integrated ECG, accelerometer and SpO2. In Proc. 30th ann. int. IEEE EMBS
conf. (pp. 1529 /C01532).
Chung, W. Y., Lee, S. C., & Toh, S. H. (2008). WSN based mobile u-healthcare system with ECG, blood pres-
sure measurement function. In Proc. 30th ann. int. IEEE EMBS conf . (pp: 1533 /C01536).
Di Rienzo, M., Rizzo, F., Parati, G., Brambilla, G., Ferrarini, M., & Castiglioni, P. (2005). MagIC system: A
new textile-based wearable device for biological signal monitoring applicability in daily life and clinicalsetting. In Proc. 27th ann. int. IEEE EMBS Conf . (pp. 7167 /C07169).
Evenson, K. R., Goto, M. M., & Furberg, R. D. (2015). Systematic review of the validity and reliability of
consumer-wearable activity trackers. International Journal of Behavioral Nutrition and Physical Activity ,
12(1), 159.
Fensli, R., Gunnarson, E., & Gundersen, T. (2005). A wearable ECG-recording system for continuous arrhyth-
mia monitoring in a wireless tele-home-care situation. In Proc. 18th IEEE symp. comput.-based med. syst .
(pp. 407 /C0412).
Gyselinckx, B., Penders, J., & Vullers, R. (2007). Potential and challenges of body area networks for cardiac
monitoring. Journal of Electrocardiology ,40, S165 /C0S168.
Habetha, J. (2006). The MyHeart project—Fighting cardiovascular diseases prevention and early diagnosis. In
Proc. 28th ann. int. IEEE EMBS con f. (pp. 6746 /C06749).
https://mhealthspot.com .
Klonoff, D. C. (2013). Twelve modern digital technologies that are transforming decision making for diabetes
and all areas of health care. Journal of Diabetes Science and Technology ,7(2), 291 /C0295.
Lee, J. M., Kim, Y., & Welk, G. J. (2014). Validity of consumer-based physical activity monitors. Medicine &
Science in Sports & Exercise ,46(9), 1840 /C01848.
Leijdekkers, P. & Gay, V. (2008). A self-test to detect a heart attack using a mobile phone and wearable sen-
sors. In Proc. 21st IEEE CBMS int. symp . (pp. 93 /C098).
Lin, B. S., Lin, B. S., Chou, N. K., Chong, F. C., & Chen, S . J. (2006). RTWPMS: A real-tim e wireless physiologi-
cal monitoring system. IEEE Transactions on Information Technology in Biomedicine ,10(4), 647 /C0656.
Lubrano, J., Sola, J., Dasen, S., Koller, J. M., & Chetelat, O. (2006). Combination of body sensor networks
and on-body signal processing algorithms: The practical case of MyHeart project. In Proc. int. workshop
wearable implantable body sens. netw . (pp. 76 /C079).
Lubrano, J. (2006). European projects on smart fabrics, interactive textiles: Sharing opportunities and chal-
lenges. Presented at the workshop wearable technol. intel. textiles . Helsinki, Finland.
Lymperis, A. & Paradiso, R. (2008). Smart and interactive textile enabling wear-able personal applications:
R&D state of the art and future challenges. In Proc. 30th ann. Int. IEEE EMBS Conf . (pp. 5270 /C05273).
Mallick, P. K., Mishra, S., & Chae, G. S. (2020). Digital media news categorization using Bernoulli document
model for web content convergence. Personal and Ubiquitous Computing . Available from https://doi.org/
10.1007/s00779-020-01461-9 .
Milenkovic, A., Otto, C., & Jovanov, E. (2006). Wireless sensor networks for personal health monitoring:
Issues and an implementation. Computer Communications ,29, 2521 /C02533.297 ReferencesMishra, S., Dash, A., & Mishra, B. K. (2020). An insight of Internet of Things app lications in pharmaceutical domain .
Emergence of pharmaceutical industr y growth with industrial IoT approach (pp. 245 /C0273). Academic Press.
Mishra, S., Mallick, P. K., Jena, L., & Chae, G. S. (2020). Opt imization of skewed data usi ng sampling-based prepro-
cessing approach. Frontiers in Public Health ,8, 274. Available from https://doi.org/10.3389/fpubh.2020.00274 .
Mishra, S., Mishra, B. K., Tripathy, H. K., & Dutta, A. (2020). Analysis of the role and scope of big data ana-
lytics with IoT in health care domain .Handbook of data science approaches for biomedical engineering
(pp. 1 /C023). Academic Press.
Mishra, S., Mishra, B. K., & Tripathy, H. K. (2020). Significance of biologically inspired optimization techni-
ques in real-time applications .Robotic systems: Concepts, methodologies, tools, and applications
(pp. 224 /C0248). IGI Global.
Mishra, S., Patel, S., Panda, A. R., & Mishra, B. K. (2019). Exploring IoT-enabled smart transportation system.
In D. Goyal, S. Balamurugan, S. Peng, & D. S. Jat (Eds.), The IoT and the next revolutions automating the
world (pp. 186 /C0202). IGI Global. Available from http://doi:10.4018/978-1-5225-9246-4.ch012 .
Mishra, S., Sahoo, S., Mishra, B. K., & Satapathy, S. (2016, October). A quality-based automated admission
system for the educational domain. In 2016 International conference on signal processing, communication,
power and embedded system (SCOPES) (pp. 221 /C0223). IEEE.
Mishra, S., Tripathy, H. K., Mallick, P. K., Bhoi, A. K., & Barsocchi, P. (2020). EAGA-MLP—An enhanced
and adaptive hybrid classification model for diabetes diagnosis. Sensors ,20(14), 4036.
Mishra, S., Tripathy, H. K., Mishra, B. K., & Mohapatra, S. K. (2018). A succinct analysis of applications and
services provided by IoT .Big data management and the Internet of Things for improved health systems
(pp. 142 /C0162). IGI Global.
Monton, E., Hernandez, J. F., Blasco, J. M., Herve, T., Micallef, J., Grech, I., ...Traver, V. (2008). Body area
network for wireless patient monitoring. IET Communications ,2, 215 /C0222. Available from https://digital-
library.theiet.org/content/journals/10.1049/iet-com_20070046 .https://doi.org/10.1049/iet-com:20070046IET
DigitalLibrary .
Mundt, C. W., Montgomery, K. N., Udoh, U. E., Barker, V. N., Thornier, G. C., Tellier, A. M., ...Kovacs,
G. T. A. (2005). A multiparameter wearable physiological monitoring system for space and terrestrial
applications. IEEE Transactions on Information Technology in Biomedicine: A Publication of the IEEE
Engineering in Medicine and Biology Society ,9(3), 382 /C0391.
Pacelli, M., Loriga, G., Tacchini, N., & Paradiso, R. (2006). Sensing fabrics for monitoring physiological and
biomechanical variables: E-textile solutions. In Proc. 3rd IEEE-EMBS int. summer school symp. med. dev.
biosens . (pp. 1 /C04).
Pandian, P. S., Mohanavelu, K., Safeer, K. P., Kotresh, T. M., Shakunthala, D. T., Gopal, P., & Padaki, V. C.
(2008). Smart vest: Wearable multiparameter remote physiological monitoring system. Medical
Engineering & Physics ,30, 466 /C0477.
Paradiso, R., Loriga, G., & Tacchini, N. (2005). A wearable health care system based on knitted integral sen-
sors. IEEE Transactions on Information Technology in Biomedicine: A Publication of the IEEE
Engineering in Medicine and Biology Society ,9(3), 337 /C0344.
Rai, D., Thakkar, H. K., & Rajput, S. S. (2020, October). Performance characterization of binary classifiers for
automatic annotation of aortic valve opening in Seismocardiogram signals. In 2020 9th International con-
ference on bioinformatics and biomedical science (pp. 77 /C082).
Ren, J., Chien, C., & Tai, C. C. (2006, Dec). A new wireless-type physiological signal measuring system using
a PDA and the Bluetooth technology. In Proc. IEEE int. conf. ind. technol . (pp. 3026 /C03031).
Sahoo, P. K., & Thakkar, H. K. (2019). TLS: Traffic load based scheduling protocol for wireless sensor net-
works. International Journal of Ad Hoc and Ubiquitous Computing ,30(3), 150 /C0160.
Sahoo, P. K., Thakkar, H. K., & Hwang, I. (2017). Pre-scheduled and self organized sleep-scheduling algo-
rithms for efficient K-coverage in wireless sensor networks. Sensors ,17(12), 2945.298 Chapter 11 Applications of wearable technologies in healthcareSahoo, P. K., Thakkar, H. K., & Lee, M. Y. (2017). A cardiac early warning system with multi-channel SCG
and ECG monitoring for mobile health. Sensors ,17(4), 711.
Sahoo, P. K., Thakkar, H. K., Lin, W. Y., Chang, P. C., & Lee, M. Y. (2018). On the design of an efficient
cardiac health monitoring system through combined analysis of ECG and SCG signals. Sensors ,18(2),
379.
Scilingo, E. P., Gemignani, A., Paradiso, R., Tacchini, N., Ghelarducci, B., & De Rossi, D. (2005).
Performance evaluation of sensing fabrics for monitoring physiological and biomechanical variables. IEEE
Transactions on Information Technology in Biomedicine ,9(3), 345 /C0352.
Shnayder, V., Chen, B. R., Lorincz, K, Fulford-Jones, T. R. F., & Welsh, M. (2005). Sensor networks for med-
ical care . Cambridge, MA: Division Eng. Appl. Sci., Harvard Univ. Tech. Rep. TR-08-05.
Sung, M., Marci, C., & Pentland, A. (2005). Wearable feedback systems for rehabilitation. Journal of
Neuroengineering and Rehabilitation ,2, 17.
Thakkar, H. K., Dehury, C. K., & Sahoo, P. K. (2020). MUVINE: Multi-stage virtual network embedding in
cloud data centers using reinforcement learning-based predictions. IEEE Journal on Selected Areas in
Communications ,38(6), 1058 /C01074.
Thakkar, H. K., Liao, W. W., Wu, C. Y., Hsieh, Y. W., & Lee, T. H. (2020). Predicting clinically significant
motor function improvement after contemporary task-oriented interventions using machine learning
approaches. Journal of Neuroengineering and Rehabilitation ,17(1), 1 /C010.
Thakkar, H. K., & Sahoo, P. K. (2019). Towards automatic and fast annotation of seismocardiogram signals
using machine learning. IEEE Sensors Journal ,20(5), 2578 /C02589.
Tura, A., Badanai, M., Longo, D., & Guarani, L. (2003). A medical wearable device with wireless Bluetooth-
based data transmission. Measurement Science Review ,3,1/C04.
Uhlsteff, J. M., Such, O., Schmidt, R., Perkuhn, M., Reiter, H., Lauter, J., ...Harris, M. (2004). Wearable
approach for continuous ECG and activity patient-monitoring. In Proc. 26th ann. int. IEEE EMBS Conf .
(pp. 2184 /C02187).
Weber, J. L. & Porotte, F. (2006). Medical remote monitoring with clothes. Presented at the int. workshop on
PHealth . Luzern, Switzerland.
Wu, J., Li, H., Cheng, S., & Lin, Z. (2016). The promising future of healthcare services: When big data analyt-
ics meets wearable technology. Information & Management ,53(8), 1020 /C01033.
Yuce, M. R., Ng, S. W. P., Myo, N. L., Khan, J. Y., & Liu, W. (2007). Wireless body sensor network using
medical implant band. Journal of Medical Systems ,31, 467 /C0474.299 ReferencesThis page intentionally left blankIndex
Note : Page numbers followed by “ f” and “ t” refer to figures and tables, respectively.
A
Accelerometer, 289
Access network technologies (ANT), 45 /C046
Acute Respiratory Distress Syndrome (ARDS), 258 /C0259
AdaBoost, 197 /C0198, 206
Additive White Gaussian Noise (AWGN), 35
Advanced Encryption Standard (AES), 140AlexNet, 196, 210 /C0211
architecture, 203 /C0204
Algorithmic optimization, 99 /C0100
Altimeter, 289
Alzheimer’s Disease (AD), 198
Apple Watch, 288 /C0289
Application Programming Interface (API), 47 /C048, 112,
119/C0120
Architecture of IoT/IoMT system, 29 /C033
gateway layer, 31 /C032
network layer, 32 /C033
sensor layer, 31visualization layer, 33
Architecture of smart healthcare system, 141 f
Artificial intelligence (AI), 12, 89 /C090, 114, 163, 165, 195
Artificial-IoT (AIoT) systems, 146
Artificial Neural Network (ANN), 163 /C0164, 171, 204,
243/C0244, 295 /C0296
Asymmetric encryption, 140
Asymmetric Key Encryption (AKE), 149
Asymptotic outage probability of x
1,3 7
Asymptotic outage probability of x 2,3 7/C038
Atrial fibrillation (AFib), 166, 173 /C0174
CardioSignal smartphone detection of, 175 f
Augmented Reality (AR), 48
Autoencoder (AE), 245 /C0247
Autonomous System (AS), 45 /C046
Autoregressive components (AR), 243
B
Bagging, 205 /C0206, 216, 244
Band’s physical structure, 292
Base-band Units (BBUs), 50 /C051
Base station (BS), 34
Bayes Classifier, Support Vector Machine (SVM), 195 /C0196
Bayesian LDA (BLDA), 244BCI systems, performance evaluation of, 244
Big data analytics, 20
Biomedical signal processing, 176 /C0177
Biometric data, 31
Blockchain technology, 32 /C033, 33 fBody Area Networks (BAN), 101
Body Sensor Network (BSN), 7
Boltzmann machine, 245 /C0247
Boosting, 244Brain-computer interface (BCI), 231 /C0235
applications, 234 /C0235, 246 t
classification, 235 /C0237
invasive BCI, 237
non-invasive BCI, 235 /C0236
semiinvasive or partially invasive BCI, 237
components of, 232
computational intelligence methods in BCI/BMI, 242 /C0244
state of prior art, 242 /C0244
future research directions and challenges, 251 /C0252
general architecture of, 238 f
for Internet of Things (IoT), 245 /C0248
key elements of, 237 /C0239
classification stage, 238 /C0239
device output or feedback stage, 239feature extraction, 238
feature translation or control interface stage, 239
preprocessing or signal enhancement, 238signal acquisition, 238
key features of, 234
modalities of, 240 /C0242
electrical and magnetic signals, 240 /C0241
metabolic signals, 241 /C0242
online and offline BCI applications, 245secure brain-brain communication, 249 /C0250
edge computing for brain /C0to/C0things, 250
types of, 233working, 233 /C0234
Brain-machine interface (BMI), 231
Brain /C0to/C0brain architecture, 249 f
BTC communication, elements of, 248 f
C
Cardiac arrhythmia, 166
CardioSignal, 174, 184
Cardiovascular disease (CVD), 166, 285Cardiovascular disease diagnosis using smartphone
case study of smartphone-based Atrial Fibrillation
Detection, 173 /C0184
biomedical signal processing, 176 /C0177
experimental data, 181 /C0182
experimental results, 183 /C0184
performance evaluation measures, 182 /C0183
prediction and classification, 177 /C0181
301Cardiovascular disease diagnosis using smartphone
(Continued )
smartphone data acquisition, 175 /C0176
deep learning approaches for disease diagnosis and
treatment, 170 /C0173
artificial neural networks, 171
Convolutional Neural Networks (CNNs), 172 /C0173
deep learning, 171 /C0172
disease diagnosis and treatment, 167 /C0170
Cascade classifier model, development of, 265 /C0266
Cellular users (CUs), 60Central processing unit (CPU) architecture, 96 /C098, 97 f
Certificate Authority (CA), 250
Channel state information (CSI), 34
Chaotic salp swarm algorithm (CSSA), 203
Chronic kidney disease (CKD), 169Class-composition component, 212
Classification of interoperability, in Internet of Things, 72 t
Cloud-based and edge-based computational time, comparison
of, 157 t
Cloud based Radio Access Network (C-RAN), 45 /C046
Cloud Computing as a Service (CCaaS), 101Cloud computing bandwidth, 2
Cloud computing framework, 119 /C0120
Cloud computing infrastructure, 12Cloud computing technology, 13 /C014
Cloudlets, 93
Cloud of Things (CoT), 91 /C092
Cloud provider (CP), 294
Cloud radio access network (C-RAN), 125
Cloud-RAN (C-RAN), 50 /C051
Cloud servers, 8
CNN architecture, 209
CNN-LSTM, 180Cognitive Radio technology driven Smart Objects (CRSOs),
61
Coincidence matrix, 271
hypothesis of, 273 t
Common spatial pattern (CSP), 243
Communication layer, 141Compressive sensing, 100
Computational complexity, 75 /C076
Computer-aided decision-making, 200Computer-aided diagnosis (CAD), 195
Computer-aided scanning, 169
Computerized tomography (CT), 257 /C0258
Computing Service Providers (CSP), 101
Conditional Random Fields (CRF), 167
Confusion matrix, 271 /C0272, 274 f
Constraint Application Protocol (CoAP), 47 /C048
Conventional AI-based systems, 12 /C013
Conventional cryptography algorithms, 140Convergent network architecture, 112 /C0113, 122 /C0123, 124 f
Convolutional layers, 180Convolutional neural network (CNN), 163 /C0164, 172 /C0173,
207, 244, 261 /C0262
Convolution neural network model, development of, 263
/C0265
Convolution process, 172
Co-operative MultiPoint (CoMP) communication, 57Coordinated Multipoint (CoMP) transmission, 50 /C051
COVID-19, 202, 257
COVID-19 detection, case study of, 210 /C0219
experimental data, 213
experimental results, 214 /C0219
feature extraction using transfer learning, 213 /C0214
performance evaluation measures, 213
COVID-19 detection algorithm, 265 f
COVID-19 pneumonia detection based on ground-glass
opacity features of CT angiography
background, 259 /C0262
convolutional neural network (CNN), 261 /C0262
ground-glass opacity, 259 /C0260
histogram of oriented gradients (HOG) algorithm,
260/C0261
literature, 262
support vector machine (SVM), 260
materials and methods, 262 /C0266
dataset description, 262 /C0263
methodology, 263 /C0266
results and methodology, 266 /C0274
analysis of test results, 271 /C0274
test results of COVID-19 pneumonia detection system,
267/C0271
COVID-Net, 212
COVIDX-Net, 212Customized e-medicare model, 286
D
D2D Users (DUs), 60
Data augmentation, 181 /C0182
Data preprocessing, 263Data processing, 89 /C090, 96, 99 /C0100
Datasets, 213
Decentralized Multiple Gateway Assignment Protocol
(DGAP), 68 /C069
Deep convolutional-recurrent neural networks, 179
Deep convolution neural network (DCNN), 212Deep feature extraction techniques, 216
Deep learning (DL), 7, 11, 31 /C033, 163 /C0164, 171 /C0172,
206/C0207
Deep learning (DL) algorithms, 195
Deep learning (DL) method, 244
Dense Convolutional Network (DenseNet), 209DenseNet, 196
DenseNet121, 216, 219 t
DenseNet169, 216, 219 t
Department of Defense Advanced Research Projects Agency
(DARPA), 249302 IndexDeTraC, 212
Device interoperability, 72Device-to-device (D2D) communications, 34
Diabetes mellitus (DM), 15
Diabetic retinopathy (Dr), 168Disease diagnosis, 199 /C0203
pattern recognition tools for, 203 /C0210
AdaBoost, 206artificial neural networks, 204
bagging, 205 /C0206
convolutional neural network, 207deep learning, 206 /C0207
K-nearest neighbor, 204 /C0205
random forests, 205
support vector machine, 205
transfer learning, 207 /C0210
XGBoost, 206
Distributed Denial of Service Attacks Protection (DDSAP),
149
DL-based Hierarchical Convolutional Neural Network
(HCNN), 169
Domain Name System (DNS), 47 /C048
Dopaminergic degeneration, 201 /C0202
DTL-based reinforcement learning technique, 210 /C0211
Dual-axis thermal accelerometer, 285Dynamic power, 97
Dynamic voltage and frequency scaling (DVFS), 97
E
Edge based healthcare systems
advantages of, 103 f
challenges of, 104 f
Edge based Internet of Things, 92 f
Edge cloud infrastructure and resource management, 133Edge-Cognitive-Computing-based (ECC-based) smart
healthcare system, 95
Edge computing, 140, 250Edge Computing as a service (ECaaS), 101
Edge computing devices
basic concepts of cloud, fog and edge computing
infrastructure, 91 /C095
edge intelligence and 5G in Internet of Things based
smart healthcare system, 94 /C095
role of edge computing in Internet of Things,
93/C094
examples of, 100impact of edge computing, Internet of Things and 5G on
smart healthcare systems, 106
for intelligent healthcare applications, 101 /C0106
advantages of edge computing for healthcare
applications, 102 /C0103
applications of edge computing based healthcare system,
104/C0105
edge computing for healthcare applications, 101 /C0102implementation challenges of edge computing in
healthcare systems, 104
patient data security in edge computing, 105 /C0106
low power hardware architecture for, 95 /C0100
central processing unit architecture, 96 /C098
data processing and algorithmic optimization, 99 /C0100
input /C0output architecture, 98 /C099
objectives of hardware development in edge computing,
95/C096
power consumption, 99
system architecture, 96
Edge computing-enabled health monitoring systems, 10
Edge computing framework, 119
Edge computing technology, 1 /C02, 146
Edge-intelligent algorithms, 12 /C013
Edge-IoMT-based enabled architecture
application of edge computing, 7 /C011
applications of IoMT-based system, 3 /C06
case study for application of, for diagnosis of diabetes
mellitus, 15 /C018
challenges of using, 11 /C013
framework for, 13 /C015
future prospects, 18 /C021
Edge Learning as a Service (EdgeLaaS) framework, 95
Electrocardiograms (ECGs), 170Electrocardiography (ECG), 166, 285
Electrocorticography (EcoG), 240
Electroencephalography (EEG), 239 t, 240 /C0241
Electroencephalography frequency bands with properties, 232
t
Electronic Medical Record (EMR), 142
Empirical mode decomposition (EMD), 243Endogenous BCI, 233
Energy harvesting (EH), 125
Enhanced common API framework, 112Enhanced Intel SpeedStep Technology, 100
Enhanced mobile broadband (eMBB), 48, 112
EPICs (Edge Programmable Industrial Controllers), 89 /C090
European Telecommunications Standards Institute (ETSI),
18/C020, 112, 121
Exogenous BCI, 233Exotic architecture, 214
Extensible edge server architecture, 93, 93 f
Extraction algorithm, 261
F
Fast Fourier transform (FFT), 177, 243Feature classification, 243 /C0244
Feature extraction, 243
Feed-forward back propagation neural networks (FFBPNN),
243/C0244
Fifth-generation (5G) mobile communication, 112
File Transfer Protocol (FTP), 47 /C048
Fisher LDA (FLDA), 244
Fitness trackers, 281303 Index5G5G communication system, 76
5G5G master core, 77 /C078
5G5G system architecture, 77
5G Communication, 125
5G communication systems, 345G-EmPower, 55
5G HetNets. SeeHeterogenous networks (HetNets) in the
context of 5G
5G-HetNets architecture, 58
5G IoT applications, 111 /C0112
5G M2M Heterogonous Network framework, 48, 49 f
5G-MEC framework, 116, 117 f
5G-NR, 48
5G technology, 2 /C03
5th generation mobile communication standard New-Radio
(5G-NR), 45 /C046
Fog computing (FC), 91 /C092, 112
Fog computing nodes (FCNs), 121
4th generation (4G) LTE networks, 1124th generation (4G) Mobile communication standards, 45 /C046
Fully Convolutional Neural Networks (FCNN), 167
Functional magnetic resonance imaging (fMRI), 168, 232,
241/C0242
Functional near-infrared spectroscopy, 242
Fuzzy Logic System (FLS) based algorithm, 69 /C070
G
Gateway devices (GW), 45 /C046, 50 /C051
Gateway layer, 31 /C032
Gateway-to-Sink (GW2S) traffic, 67 /C068
Gaussian Chebyshev quadrature, 38General Data Protection Regulations (GDPR), 185
Generative adversarial network (GAN), 196, 245 /C0247
GoogLeNet, 196, 210 /C0211
GPS tracker, 280 /C0281
Graphics processing units (GPUs), 114, 168
Ground-glass opacity (GGO), 259 /C0260
Gyrocardiography, 166
Gyroscopes, 289
H
HAEC (Highly Adaptive Energy-efficient Computing),
131/C0132
Hamming window, 177
Healthcare industry, 164
Healthcare Service Consumers (HSC), 101Healthcare system, survey on types of devices in, 81 t
Health Insurance Portability and Accountability Act (HIPAA),
98
HealthTap, 287 /C0288
Hemoglobin, 241 /C0242
Heterogeneous Cloud Radio Access Network (H-CRAN)
framework, 50 /C051Heterogeneous Cloud-RAN (H-CRAN)-based 5G Network
Architecture, 50 /C051, 51 f
Heterogeneous Modified Artificial Neural Network
(HMANN), 169
Heterogeneous Networks (HetNets), 20, 125Heterogenous networks (HetNets) in the context of 5G,
48/C058
5G-HetNet H-CRAN fronthaul and TWDM-PON backhaul,
57
5G mobile communication standards and enhanced features,
48/C049
algorithm for clusterization, aggregation, and prioritization
of M2M devices in, 69 /C070
architecture, 50 /C054
device interoperability, 72
Device-to-Device communication in, 58 /C061
diverse service provisioning in 5G and beyond, 58
intelligent software network framework of, 54
Internet of Things toward 5G and heterogenous wireless
networks, 55 /C056
M2M traffic in, 66 /C068
Machine-to-Machine communication in, 61 /C070
recent advancement in Internet of Things related
standard, 62 /C066
state of art architecture, recent advances and challenges,
61/C062
Next-Gen 5G wireless network, 54 /C055
research issues and challenges, 72 /C076
computational complexity and multiaccess edge
computing, 75 /C076
current research in HetNet based on various technologies,
76
interference management, 74
power allocation, 74 /C075
resource allocation, 73 /C074
user association, 75
smart healthcare using 5G5G Inter of Things,
77/C081
healthcare system architecture using wireless sensor
network and mobile cellular network, 78 /C081
mobile cellular network architecture, 77 /C078
ZigBee IP, 78
spectrum allocation and user association in 5G HetNet mm
Wave communication, 58
user interoperability, 70 /C072
Hidden Markov Model, 244High-Power Nodes (HPNs), 50 /C051
Histogram of oriented gradients (HOG) algorithm, 260 /C0261
Human machine interface (HMI) products, 100Hybrid cloud computing, 10
Hybrid deep networks, 163 /C0164
Hyperparameters, 180 /C0181
Hypertext Transfer Protocol (HTTP), 47 /C048304 IndexI
IEEE 802.11 (Wi-Fi), 48
IEEE 802.15.1 (BLE), 48, 112 /C0113
IEEE 802.15.4 (Zigbee), 48
Image recognition, 195 /C0196
Implanted medical things (IMTs), 29Inception models, 208 /C0209
InceptionResNetV2, 216, 219 t
InceptionV3, 216, 218 t
Independent component analysis (ICA), 242
Industrial Automation and Control (IAC), 48
Industrial IoT application, in 5G, 133 /C0134
Industry Specification Group (ISG), 121
Information and communication technology (ICT), 3, 139
Infrastructure-as-a-Service (IaaS), 117
Instruction Set Architecture (ISA), 98
Intel Hyper-Threading Technology (Intel HT Technology),
100
Intel Virtualization Technology (VT-x), 100
Interconnected Content Delivery networks (CDNs), 20Internet of Things (IoT)-based edge computing systems, 2 /C03
Internet of Healthcare Things (IoHT), 46 /C047, 113 /C0114
Internet of Medical Things (IoMT), 1, 3 /C04, 29, 169
IoMT-based cloud systems, 5 /C06
IoMT-based devices, 2
IoMT-based medical system, 5IoMT-based system, 2
Internet of Things (IoT), 139, 174
Internet of Vehicles (IoV), 54 /C055
Inter-RAN Virtualization, 54
Intracortical electrode array, 240
Intracortical neuron recording technology, 240Intra-RAN Radio Resource Virtualization, 54
Intrinsic time scale decomposition (ITD), 243
IoT-based Smart Healthcare system, 157 /C0158
IoT-based system, 157 /C0158
IoT with deep learning framework, 248 f
K
K nearest neighbor (k-NN), 16, 17 t,1 9t, 163 /C0164, 204 /C0205,
243/C0244
L
Laboratory Information Management System (LIMS), 142
Large scale healthcare and big data management, 134
Learning vector quantization (LVQ), 244
Lightweight cryptography (LWC), 149Lightweight encryption, 140, 151
Lightweight speck technique for edge-IoT-based smart
healthcare systems
application of edge computing, 146 /C0148
application of encryptions algorithm, 148 /C0154
speck encryption, 150 /C0154Internet of Things in smart healthcare system, 141 /C0146
management of diseases, 143 /C0144
risk monitoring and prevention of disease, 144
smart healthcare hospitals support, 145 /C0146
support for diagnosis treatment, 142 /C0143
virtual support, 144 /C0145
Linear discriminant analysis (LDA), 243 /C0244
Local Area Network (LAN), 89 /C090, 294
Long Range Wide-Area Network (LoRaWAN), 112 /C0113
Long Short Term Memory (LSTM), 15, 18, 163 /C0164, 196,
245/C0247
Long-Term Evolution (LTE), 45 /C046
Low Overhead DGAP (Lo-DGAP), 68 /C069
Low-power system-on-chip (SoC) systems, 8
Low Power-Wireless Personal Area Network (LP-WPAN), 56
LTE, 48Lung-related diseases, 257 /C0258
M
Machine-based computing algorithms, 142 /C0143
Machine learning (ML), 31 /C033, 114, 163, 195
algorithms, 7ML-assisted techniques, 281
Machine-to-Gateway (M2GW) traffic, 67 /C068
Machine-to-machine (M2M) architecture, 30 /C031
Machine-to-Machine (M2M) communication, 29 /C030, 62
in 5G HetNets, 61 /C070
cognitive 5G5G networks
distributed gateway selection for, 68 /C069
M2M communication domain, 62
M2M device domain, 62M2M server/application domain, 62
Magnetoencephalography, 241
Massive Machine Type Communication (mMTC) framework,
48
Master core technology, 77 /C078
Maximum Likelihood Classifier, 195 /C0196
Mean Absolute Error (MAE), 273 /C0274
Mechanocardiography (MCG), 166, 174 /C0175
Media Access Control (MAC), 45 /C046
Medical Internet of Things (MIoT), 102 /C0103
Medical Remote Monitoring of clothes (MERMOTH), 285
Mental stress, 292Message Queue Telemetry Transport (MQTT), 47 /C048
Microcontroller unit (MCU), 96
Micro Data Centers (MDCs), 18 /C020
Microelectromechanical systems (MEMS) communication,
29/C030
Microprocessor Units (MPUs), 114Middle-ware layer, 141
Minimum Distance Classifier, 195 /C0196
MIPI (mobile industry processor interface), 98 /C099
Mobile Cellular Network (MCN), 45 /C046, 77
Mobile cellular network architecture, 77 /C078305 IndexMobile cellular network architecture ( Continued )
5G5G system architecture, 77master core technology, 77 /C078
Mobile cloud computing (MCC), 121
Mobile data offloading, 133Mobile edge computing, 18 /C020, 91
Mobile healthcare, 101
Mobile/Infrastructure Edge Node (MEN), 147 /C0148
MobileNet, 196, 209 /C0210, 215 /C0216, 218 t
Modern-day computer vision technology, 263
Modern day digital networks, 45 /C046
Multiaccess edge computing (MEC), 75 /C076, 147 /C0148
challenges and issues in implementation of, 129 /C0134
application perspective, 133 /C0134
communication and computation perspective, 131 /C0133
convergent network architecture for 5G with, 122 /C0125
current research in 5G with, 125 /C0129
MEC device access and management layer (MDAML), 119
MEC enabled healthcare application, integration of AI and
5G for, 134
security and privacy, 132
service continuity and mobility and service enhancements,
131/C0132
service monetization, 132
service orchestration and programmability, 131technical overview on 5G network with, 114 /C0122
application splitting in MEC, 117 /C0119
layered service oriented architecture for 5G MEC,
119/C0122
Multicore processors, 96 /C097
Multilayer LSTM-RNN, 245 /C0247
Multilayer neural network (MLNN), 244
Multilayer perceptron, 243 /C0244
Multiple-input-multiple-output (MIMO), 33 /C034, 112 /C0113
Multivariate adaptive AR (MVAAR), 243
MyHeart project, 285
N
Naive Bayes model, 295 /C0296
Naı¨ve Bayes (NB), 16, 17 t,1 9t
Network Functions Virtualization (NFV), 45 /C046, 50 /C051,
75/C076, 112
Network layer, 32 /C033
Neural networks (NN), 163
Neurons, 206 /C0207
Next-Generation Wireless (NG-W) networks, 112 /C0113
Node-C, 53 /C054
Noise removal technique, 177
Nonorthogonal multiple access (NOMA), 33 /C034, 125
O
OneM2M, advantages of, 65
OneM2M communication framework, 67 f
OneM2M protocols, 65 /C066OneM2M standard platform, 66
Outage probability of x 1,3 6/C037
Outage probability of x 2,3 7
P
PACs (Programmable Automation Controllers),
89/C090
Parallel computing, 99 /C0100
Parkinson’s disease (PD), 169, 197 /C0198
Particle Swam Optimization (PSO)-based QoS mapping
algorithm, 57
PDNet, 197 /C0198
Permutation method, 181 /C0182
Personal Area Network (PAN), 294
Photoplethysmography (PPG), 166
Physical layer architecture of 5G enabled IoT/IoMT system,
29/C033
gateway layer, 31 /C032
network layer, 32 /C033
sensor layer, 31
uplink healthcare IoT system on NOMA, 33 /C041
ergodic capacity of UL NOMA, 38numerical results and discussions, 38 /C041
outage probability for UL NOMA, 35 /C037
system model, 34 /C035
visualization layer, 33
Physical layer Convergence Protocol (PLCP), 45 /C046
Physical Media Dependent Protocol (PMDP), 45 /C046
Picture Archiving and Communication Systems (PACS), 142
Platform-as-a-Service (PaaS), 50 /C051, 117
PLCs (Programmable Logic Controllers), 89 /C090
Positron emission tomography (PET), 241
Precision medicine, 167
Predictive Model Mark-up Language (PMML), 290Predictive models, 200
Pregnancy and fertility tracking functionality, 289 f
Preprocessing, 242 /C0243
Principal component analysis (PCA), 195 /C0196, 243
Probabilistic neural network (PNN), 243 /C0244
Probability density function, 35 /C036
Process Scheduler /C0Virtualization Layer (PSVL), 119
Proper rotation components (PRC), 243
Proposed HOG/SVM architecture, 266 f
Proposed machine learning algorithm, architecture of, 263,
264f
Protocol Data Unit (PDU), 45 /C046
PVT (process, voltage, and temperature) compensation, 99
Q
Quadratic discriminant analysis (QDA), 244
Quality of Experience (QoE), 73 /C074, 146
Quality of Life (QoL), 146Quality of Service (QoS), 73 /C074
Quantum-Computing, 114306 IndexR
Radio Access Network (RAN), 7, 48, 91, 112
Radio Frequency Identification (RFID), 141 /C0142
Random Coefficient Selection and Mean Modification
Approach (RC-SMMA), 148 /C0149
Random Forest (RF), 16, 17 t,1 9t, 205, 244
Real time analytics, 103
Real-time Reverse Transcription Polymerase Chain Reaction
(RT-PCR), 257 /C0258
Real-time wireless physiological monitoring system
(RTWPMS), 285
Reception mechanism, 50 /C051
Rectified Linear Unit (ReLU), 261 /C0262
Rectified Linear Unit (ReLU) activation function, 261 /C0262
Recurrent neural networks (RNN), 163 /C0164, 196, 245 /C0247
Reduced functional devices (RFDs), 46 /C047
Remote Radio Heads (RRHs), 50 /C051
Residual neural network (ResNet), 179 /C0180, 196, 208
ResNet18, 210 /C0211
ResNet50, 216, 217 t
ResNet101, 216, 217 t
Resource slicing, 54
Reverse-transcriptase polymerase chain reaction (RT-PCR),
200
Rivest /C0Shamir /C0Adleman (RSA), 140
Robotic assistance, 144 /C0145
Root Mean Square Error (RMSE), 273 /C0274
S
SATA (Serial Advanced Technology Attachment), 100
Scans without evidence of dopaminergic deficit (SWEDD),
201/C0202
Segmentation, 177
Seismocardiography, 166
Self-Learning Networks (SLN), 58Semantic interoperability, 71 /C072
Sensor layer, 31
Serial peripheral interface (SPI), 98Severe acute respiratory illness (SARI), 198
Short-time Fourier transform (SSFT), 243
Sickweather, 288Signal to interference and noise ratio (SINR), 35
SimulinkR, 157
Single core and multicore architecture, 98, 98 f
Single-photon emission computed tomography (SPECT),
197/C0198
Sinus rhythm (SR), 166Smart band, 292
Smart healthcare components, 145
Smart Healthcare Platforms, 164Smart jewelry, 281
Smartphone data acquisition, 175 /C0176
Smartphone-derived MCG signals, 178 fSmartphone’s IMU sensors, 176
Smart watches, 281Software-as-a-Service (SaaS), 50 /C051, 117
Software defined network (SDN), 45 /C046, 50 /C051, 112
Software Defined RAN (SD-RAN), 50 /C051
Spatial-temporal neural network (STNN), 245 /C0247
Speck2n, 152
Speck encryption flow, 154 f
Speckle-type POZ protein (SPOP), 201
Speck parameters, 151 t
Speck rounds, 152, 153 f, 153 t
Standardization of protocols, 132
Static power, 97
Substitution-Permutation Networks (SPNs), 151
Successive Interference Cancellation (SIC) procedure, 33 /C034
Superconducting quantum interference devices (SQUID), 241Supervised learning, 294
Support vector machine (SVM), 16, 17 t,1 9t, 163 /C0164, 205,
243/C0244, 260, 295 /C0296
Symmetric encryption, 140
System architecture, 96, 96 f
System model, 34 /C035
System protocol, 79
T
Tele-health tools, 105
Telemedicine, 3
Telnet, 47 /C048
The Cancer Genome Atlas (TCGA), 201
3rd generation (3G) mobile communication standards, 45 /C046
3rd generation partnership project (3GPP), 1123-axis accelerometer, 176 /C0177
3GPP-inspired HetNet model, 55
3GPP release 14 (R14), 48Time-sensitive data, 10
Time Wavelength Division Multiplexing- Passive Optical
Network (TWDM-PON), 57
Traffic Engineering (TE) model, 66 /C067
Training set, 213
Transcranial magnetic stimulation (TMS), 249Transfer learning, 202, 207 /C0211, 213 /C0214
Transmission Control Protocol (TCP), 47 /C048
Two-dimensional (2D) curvelet transform, 203Two-Fish (TF) method, 149
Two-phase traffic control (2PTC) architecture, 66 /C067
for M2M communication, 68 f
U
UL NOMA
ergodic capacity of, 38
outage probability for, 35 /C037
Ultra-reliable low latency communications (URLLC), 2 /C03,
48, 112
Universal asynchronous receiver-transmitter (UART), 98307 IndexUnmanned aerial vehicle (UAV), 125
User Datagram Protocol (UDP), 47 /C048
User interoperability, 70 /C072
identification and classification, 71
syntactic and semantic interoperability for interconnecting
devices, 71 /C072
V
Validation set, 213
VGG16, 216, 216 t, 217 t
VGG19, 216VGGNet, 196
Virtual infrastructure, 54
Virtual machines (VMs), 54
Virtual Reality (VR), 48
Virtual Service Gateway (V-SGW), 66 /C067
Visualization layer, 33
Voice over 5G (Vo5G), 112 /C0113
Voice over internet protocol (VoIP), 112 /C0113
Voice over LTE (VoLTE), 112 /C0113
Voice over new radio (VoNR), 112 /C0113
W
Wavelet packet decomposition (WPD), 243
Wavelet transform (WT), 243Wearable body area network, 286
Wearable devices, types of, 280 f
Wearable Health Care System (WEALTHY), 285Wearable Internet-of-Things (WIoT), 29 /C030
Wearable sensor, 10
Wearable technologies, in healthcare
application of wearable devices, 281 /C0282
current scenario of wearable computing, 284 /C0286
Geiger counter for illnesses, 288hydration tool, 288
importance of, 283 /C0284
complete information, 284cost savings, 284
early diagnosis, 283medication adherence, 284
personalization, 283
remote patient monitoring, 283
medication tracking, 287
pregnancy and fertility tracking, 288
state-of-the-art implementation of wearables, 289 /C0296
detection of soft fall in disabled/elderly people, 289 /C0291
detection of stress using smart wearable band, case study
based on, 292
use of wearables to reduce cardiovascular risk, 293 /C0296
virtual doctor consultations, 287 /C0288
wearable working procedure, 286
weight loss, 286 /C0287
Wide Area Network (WAN) cloud computing, 92Wi-Fi (IEEE 802.11), 47 /C048
Wi-MAX, 48
Wireless Body Area Network (WBAN), 2 /C03, 46 /C047, 101
Wireless Local Area Network (WLAN), 45 /C046
Wireless Personal Area Network (WPAN), 45 /C047
Wireless power transfer (WPT), 125Wireless Sensor Networks (WSN), 112 /C0113, 140
Workflow of wearables, 287 f
World Combination Service Mode (WCSM), 77 /C078
WSN/Zigbee (IEEE 802.15.4), 47 /C048
X
Xception, 216, 220 t
XGBoost, 206, 216
Z
ZFNet, 196
Zigbee (IEEE 802.15.4), 112 /C0113
ZigBee IP, 78
data transmission by 5G terminal in, 79 /C080308 IndexVOLUME ONE HUNDRED AND TWENTY SEVEN
ADVANCES IN
COMPUTERS
Edge/Fog Computing Paradigm:
The Concept, Platforms and
ApplicationsThis page intentionally left blankVOLUME ONE HUNDRED AND TWENTY SEVEN
ADVANCES IN
COMPUTERS
Edge/Fog Computing Paradigm:
The Concept, Platforms and
Applications
Edited by
PETHURU RAJ
Edge AI Division, Reliance Jio Platforms Ltd,
Bangalore, India
KAVITA SAINI
School of Computing Science and Engineering (SCSE),Galgotias University, Delhi, Uttar Pradesh, India
CHELLAMMAL SURIANARAYANAN
Government Arts and Science College, Srirangam(Affiliated to Bharathidasan University), Tiruchirappalli,Tamilnadu, IndiaAcademic Press is an imprint of Elsevier
50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States525 B Street, Suite 1650, San Diego, CA 92101, United StatesThe Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, United Kingdom
125 London Wall, London, EC2Y 5AS, United Kingdom
First edition 2022Copyright © 2022 Elsevier Inc. All rights reserved.No part of this publication may be reproduced or transmitted in any form or by any means, electronic
or mechanical, including photocopying, recording, or any information storage and retrieval system,
without permission in writing from the publisher. Details on how to seek permission, furtherinformation about the Publisher ’s permissions policies and our arrangements with organizations such
as the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website:
www.elsevier.com/permissions .
This book and the individual contributions contained in it are protected under copyright by the
Publisher
(other than as may be noted herein).
NoticesKnowledge and best practice in this field are constantly changing. As new research and experiencebroaden our understanding, changes in research methods, professional practices, or medicaltreatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating
and using any information, methods, compounds, or experiments described herein. In using suchinformation or methods they should be mindful of their own safety and the safety of others, including
parties for whom they have a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume
any liability for any injury and/or damage to persons or property as a matter of products liability,negligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas
contained in the material herein.
ISBN: 978-0-12-824506-4
ISSN: 0065-2458
For information on all Academic Press publicationsvisit our website at https:/ /www.elsevier.com/books-and-journals
Publisher: Zoe
 Kruze
Developmental Editor:
Cindy Angelita Pe Benito-Gardose
Production Project Manager: James Selvam
Cover Designer: Greg Harris
Typeset by STRAIVE, IndiaContents
Contributors xiii
Preface xvii
1. Exploring the edge AI space: Industry use cases 1
Pethuru Raj and Jenn-Wei Lin
1.The proliferation of IoT devices and sensors 2
2.Activating on-device intelligence 4
3.The artificial intelligence (AI) processing at the edge 8
4.Machine learning (ML) at the edge 10
5.Deep learning at the edge 15
6.Digging into the paradigm of edge AI 18
7.Edge AI for next-generation retail experiences 23
8.Edge AI for smarter cities 25
9.Edge AI for telecommunication 29
10.Conclusion 33
Further reading 33
About the authors 34
2. Edge computing: Types and attributes 35
Sunku Ranganath
1.Introduction 36
2.Internet of Things (IoT) edge 37
3.On-premises edge 41
4.Wireless Access Edge 46
5.Network edge 48
6.Challenges in edge computing 51
7.Multi-Access Edge Computing 55
References 61
About the author 62
3. Industry initiatives across edge computing 63
Sunku Ranganath
1.Linux Foundation Edge 64
2.Linux Foundation for Networking 76
3.O-RAN alliance 81
v4.Open Network Foundation 86
5.3GPP 91
6.Small Cell Forum 92
7.Broadband Forum 93
8.5G Alliance for connected industry and automation (5G-ACIA) 96
9.5G Automotive Association (5GAA) 97
10.Automotive Edge Computing Consortium (AECC) 98
11.Telecom Infra Project 101
12.IEEE International Network Generations Roadmap Edge Services
Platform (ESP) 104
13.KubeEdge 105
14.StarlingX 106
15.Open Edge Computing Initiative 108
16.Smart Edge Open 110
17.Edge Multi Cluster Orchestrator (EMCO) 110
18.Global Systems for Mobile Association (GSMA) 113
References 114
About the author 115
4. IoT-edge analytics for BACON-assisted multivariate health data
anomalies 117
Partha Pratim Ray
1.Introduction 118
2.Related works 119
3.System design 120
4.Results 128
5.Conclusion 134
References 134
About the author 137
5. The edge AI paradigm: Technologies, platforms and use cases 139
Pethuru Raj, J. Akilandeswari, and M. Marimuthu
1.Introduction 140
2.Delineating the two paradigms 140
3.Tending toward the digital era 143
4.The key connectivity technologies 148
5.The 5G use cases and benefits 149
6.About edge computing 151
7.Edge computing architecture 153vi Contents8.Edge cloud infrastructures 155
9.Edge analytics 157
10.The key benefits of edge computing 159
11.Tending toward edge AI 162
12.Artificial intelligence (AI) chips for edge devices 164
13.The noteworthy trends toward edge AI 165
14.Why edge processing? 167
15.Edge-based AI solutions: The advantages 168
16.Applications that can be performed on edge devices 170
17.Edge AI use cases 174
18.Conclusion 179
About the author 180
6. Microservices architecture for edge computing environments 183
Chellammal Surianarayanan
1.Introduction 184
2.Need for edge and fog computing 186
3.Nature and requirements in edge and fog computing environment 188
4.Why microservices architecture for edge/fog computing applications? 189
5.How the unique features of MSA fits as a natural choice for edge and fog
layers? 190
6.Overview about elements of microservices 192
7.MSA for edge/fog computing 204
8.Challenges 206
9.Conclusion 207
References 207
About the author 207
7. Edge data analytics technologies and tools 209
N. Jayashree and B. Sathish Babu
1.Introduction to edge data analytics and benefits 210
2.Edge data analytics versus server-based data analytics 212
3.Architecture and methodology of edge data analytics 213
4.Edge data analytics technologies and solutions 215
5.Working principles and feature comparisons 232
6.Some of the other use cases of edge analytics 233
References 234
About the authors 235vii Contents8. Edge platforms, frameworks and applications 237
Kavita Saini and Pethuru Raj
1.Introduction to cloud computing 238
2.Cloud computing to edge computing 238
3.Edge computing: A brief overview 239
4.Essential of edge computing 240
5.Advantages of edge computing 240
6.Significance of cloudlets 243
7.Conclusion 255
References 255
About the authors 257
9. Edge computing challenges and concerns 259
Kavita Saini, Uttama Pandey, and Pethuru Raj
1.Introduction 260
2.Cloud, fog and edge computing 261
3.Implications and challenges in adopting edge computing 263
4.Concerns with edge computing 266
5.Security and privacy attacks on edge computing enabled devices 268
6.Countermeasures to security and privacy attacks in edge infrastructure 270
7.Future of edge computing 273
8.Conclusion 275
References 275
Further reading 276About the authors 277
10. A smart framework through the Internet of Things and machine
learning for precision agriculture 279
Veeramuthu Venkatesh, Pethuru Raj, and R. Anushia Devi
1.Introduction 281
2.Existing infrastructure in agriculture 282
3.IoT ecosystem —A complete view 285
4.Agricultural monitoring system based on sensors 287
5.Difficulties in sensor-based agribusiness observing frameworks 290
6.Factors affecting climatic changes in savvy agribusiness 291
7.AI in agriculture —An introduction 295
8.Machine learning techniques for smart agriculture 295
9.Artificial neural network (ANN) 296
10.Automation and wireless system networks in agriculture 296viii Contents11.Hardware components in the smart agriculture system 299
12.Use cases 301
13.Conclusion 303
References 303
About the authors 304
11. 5G Communication for edge computing 307
D. Sumathi, S. Karthikeyan, P. Sivaprakash, and Prabha Selvaraj
1.Introduction 308
2.Architectures of edge computing 309
3.5G and edge computing 313
4.5G and edge computing use cases 321
5.Challenges during the deployment of edge computing in 5G 325
6.Conclusion 327
References 327About the authors 329
12. The future of edge computing 333
Swaroop S. Sonone, Kavita Saini, Swapnali Jadhav,
Mahipal Singh Sankhla, and Varad Nagar
1.Introduction 334
2.Emergence of edge computing 335
3.Drawbacks of out-of-date cloud computing 335
4.Significance of edge computing 338
5.Edge computing technologies 339
6.Possible advancements in digitization using edge computing 342
7.Opportunities for edge in future 346
8.Conclusion 352
References 353
About the authors 355
13. Edge computing security: Layered classification of attacks and
possible countermeasures 359
G. Nagarajan, Serin V. Simpson, and R.I. Minu
1.Introduction 360
2.Four layer architecture of edge computing 361
3.Security attacks in edge computing: Layered classification and analysis 363
4.Edge based existing solutions for the security issues present in real world
IoT applications 368ix Contents5.Discussion 371
6.Conclusion and future works 374
References 375
About the authors 376
14. Blockchain technology for IoT edge devices and data security 379
M.P. Anuradha and K. Lino Fathima Chinna Rani
1.Introduction 381
2.IoT layered architecture 382
3.IoT security threats and attacks 384
4.IoT—Edge computing 391
5.Requirements for integration of blockchain and edge computing 395
6.Integration of blockchain and edge computing 396
7.IoT framework: Secure edge computing with blockchain technology 400
8.Factors to be addressed in secure edge computing 403
9.Advantages —Integration of blockchain and edge computing 404
10.Use cases —Blockchain with edge computing 405
11.Further challenges and recommendations 408
12.Conclusion 409
References 410
About the authors 411
15. EDGE/FOG computing paradigm: Concept, platforms and
toolchains 413
N. Krishnaraj, A. Daniel, Kavita Saini, and Kiranmai Bellam
1.Introduction 414
2.Machine learning (ML) in FC 416
3.Classes of service for fog applications 416
4.Clusters for lightweight edge clouds 418
5.IoT Application with fog real time application 425
6.Safeguarding data consistency at the edge 426
7.Cloud-fog-edge-IoT collaborative framework 428
8.Edge computing with machine learning 429
9.Security challenges in fog computing 432
10.Conclusion 433
Reference 433
About the authors 435x Contents16. Artificial intelligence in edge devices 437
Anubhav Singh, Kavita Saini, Varad Nagar, Vinay Aseri,
Mahipal Singh Sankhla, Pritam P. Pandit, and Rushikesh L. Chopade
1.Introduction 438
2.Primer on artificial intelligence 440
3.Edge intelligence 444
4.Edge intelligence model training 449
5.Edge intelligence model interface 458
6.Future research directions 469
7.Conclusions 474
References 474Further reading 480
About the authors 481
17. 5G—Communication in HealthCare applications 485
R. Satheeshkumar, Kavita Saini, A. Daniel, and Manju Khari
1.Introduction 486
2.5G—IOT for E-healthcare 486
3.5G—Industrial Internet of Thongs (IIoT) 492
4.5G—Network requirements for healthcare 494
5.5G—Virtual HealthCare 496
6.TeleHealth vs. virtual health 497
7.5G—Remote HealthCare monitoring 498
8.5G—Remote surgery 498
9.5G—Futures and robotics in healthcare 499
10.5G—Impact on HealthCare 500
11.Conclusion 501
References 502
About the authors 503
18. The integration of blockchain and IoT edge devices for smart
agriculture: Challenges and use cases 507
Swati Nigam, Urvashi Sugandh, and Manju Khari
1.Introduction 508
2.Blockchain technology: An overview 512
3.Working of blockchain 514xi Contents4.IoT: An overview 515
5.Working of IoT 515
6.Edge computing: An overview 517
7.A proposed model for smart agriculture using blockchain and IoT 518
8.Advantages of blockchain, edge computing and IoT based agriculture 523
9.Summary of the research for applying blockchain and IoT in agriculture 524
10.Challenges and open issues 529
11.Conclusion 530
References 531About the authors 535xii ContentsContributors
J. Akilandeswari
Department of Information Technology, Sona College of Technology, Salem, TN, India
M.P. Anuradha
Department of Computer Science, Bishop Heber College, Affiliated to Bharathidasan
University, Tiruchirappalli, Tamil Nadu, India
R. Anushia Devi
School of Computing, SASTRA Deemed University, Thanjavur, India
Vinay Aseri
Department of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
B. Sathish Babu
Department of Computer Science and Engineering, R V College of Engineering, Bengaluru,
Karnataka, India
Kiranmai Bellam
Department of Computer Science, A & M University, Prairie View, TX, United States
Rushikesh L. Chopade
Department of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
A. Daniel
School of Computing Science and Engineering (SCSE), Galgotias University, Delhi,
Uttar Pradesh, India
Swapnali Jadhav
Government Institute of Forensic Science, Aurangabad, Maharashtra, India
N. Jayashree
Department of Computer Science and Engineering, C Byregowda Institute of Technology,
Kolar, Karnataka, India
S. Karthikeyan
VIT-AP University, Amaravati, Andhra Pradesh, India
Manju Khari
School of Computer and System Sciences, Jawaharlal Nehru University, New Delhi, India
N. Krishnaraj
Department of Networking and Communications, School of Computing, SRM Institute of
Science and Technology, Kattankulathur, Tamil Nadu, India
Jenn-Wei Lin
Department of Computer Science and Information Engineering, Fu Jen Catholic University,Taipei, Taiwan
xiiiK. Lino Fathima Chinna Rani
Department of Computer Applications, Bishop Heber College, Affiliated to Bharathidasan
University, Tiruchirappalli, Tamil Nadu, India
M. Marimuthu
Research Scholar, Sona College of Technology, Salem, TN, India
R.I. Minu
Department of Computer Science and Engineering, School of Computing, SRM Institute ofScience and Technology, Kattankulathur, Tamil Nadu, India
Varad Nagar
Department of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
G. Nagarajan
Department of Computer Science and Engineering, Sathyabama Institute of Science andTechnology, Chennai, Tamil Nadu, India
Swati Nigam
Department of Computer Science, Faculty of Mathematics and Computing, Banasthali
Vidyapith, Banasthali, India
Uttama Pandey
School of Computing Science and Engineering (SCSE), Galgotias University, Delhi,Uttar Pradesh, India
Pritam P. Pandit
Department of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
Pethuru Raj
Site Reliability Engineering (SRE) Division, Reliance Jio Platforms Ltd. (JPL); Reliance JioCloud Services (JCS), Bangalore, India
Sunku Ranganath
Intel Corporation, Hillsboro, OR, United States
Partha Pratim Ray
Department of Computer Applications, Sikkim University, Gangtok, India
Kavita Saini
School of Computing Science and Engineering (SCSE), Galgotias University, Delhi, UttarPradesh, India
Mahipal Singh Sankhla
Department of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
R. Satheeshkumar
Department of Electronics and Communication Engineering, Galgotias College ofEngineering and Technology, Noida, India
Prabha Selvaraj
VIT-AP University, Amaravati, Andhra Pradesh, Indiaxiv ContributorsSerin V. Simpson
Department of Computer Science and Engineering, Thejus Engineering College, Thrissur,
Kerala, India
Anubhav Singh
School of Forensic Science and Risk Management, Rashtriya Raksha University, Lavad,India
P. Sivaprakash
VIT-AP University, Amaravati, Andhra Pradesh; PPGIT, Coimbatore, India
Swaroop S. Sonone
Department of Forensic Science, Dr. Babasaheb Ambedkar Marathwada University,
Aurangabad, Maharashtra, India
Urvashi Sugandh
Department of Computer Science, Faculty of Mathematics and Computing, Banasthali
Vidyapith, Banasthali, India
D. Sumathi
VIT-AP University, Amaravati, Andhra Pradesh, India
Chellammal Surianarayanan
Government Arts and Science College, Srirangam (Affiliated to Bharathidasan University),Tiruchirappalli, Tamilnadu, India
Veeramuthu Venkatesh
School of Computing, SASTRA Deemed University, Thanjavur, Indiaxv ContributorsThis page intentionally left blankPreface
The faster adoption of several transformative and trend-setting technologies
such as the Internet of Things (IoT), artificial intelligence (AI), edgecomputing, cloud-native and serverless computing models, 5G communi-cation, etc. has set the ball rolling for the edge AI era. Centralized and
expensive computing has become decentralized, distributed, and affordable
as edge computing has surged in popularity. Market watchers estimate thatbillions of IoT edge devices and sensors are currently in use. The deviceecosystem is growing fast with the plentiful production of sleek, fashionable,
and useful devices. When these devices connect, communicate, collaborate,
correlate, and corroborate directly or indirectly, the quantity of IoT databeing generated and collected is massive, and IoT device data come inmultistructured forms.
At present, IoT edge devices, through a plethora of technological
innovations and disruptions, are being filled with more memory, storage,and networking capacities, and processing capability. In this way, edgedevices are becoming part of mainstream computing. That is, centralizedand consolidated computing is being gradually replaced by edge devices,
which are decentralized and disaggregated. AI libraries are being deployed
in resources-intensive IoT edge devices to carry out proximate and real-timedata processing. This is described as on-device AI processing to create intel-ligent devices. Such a strategically sound shift is intended to produce a
dazzling array of advancements and automations not only for businesses
but also for individuals in their everyday activities. This book aims to artic-ulate and accentuate how edge computing, analytics, and AI concepts arecontributing immensely to visualizing, producing, and delivering a wide
range of services that are state of the art, context-aware, people-centric,
service-oriented, event-driven, mission-critical, and knowledge-filled.
Chapter 1, “Exploring the edge AI space: Industry use cases,” explains
the latest trends and transitions that are occurring in the edge AI space.Pioneering technologies and tools are emerging and quickly evolving to
establish and sustain next-generation edge-native applications. This chapter
illustrates the recent developments in edge computing and on-device AIprocessing domains.
Chapter 2, “Edge computing types and attributes,” provides an intro-
duction to various types of edge computing by broadly classifying it into four
xviitypes, based on round trip latency requirements: IoT edge, wireless access
edge, on premise edge, and network edge. The requirements and attributes
of each of these edge computing types are discussed. The chapter then detailsthe practical challenges across these edge deployments and explores howETSI (European Telecommunications Standards Institute) Multi-accessEdge Computing specifications help to address these challenges.
Chapter 3, “Industry initiatives across edge computing,” focuses on
various industry initiatives in the edge computing paradigm. To acceleratethe evolution and adoption of edge computing, various standard bodies,open-source projects, and industry consortia have come together in recent
times to revolutionize edge computing. This chapter discusses various ini-
tiatives around the world that have major traction in terms of collaboration,collateral produced, and industry impact.
Chapter 4, “IoT-edge analytics for BACON-assisted multivariate health
data anomalies,” explains anomaly detection, which in IoT-enabled systemscan significantly improve the quality of the deployed systems. Althoughexisting techniques can detect anomalies from a dataset, more efficientalgorithms can be used to minimize the burden of excessive computational
overheads on the resource-constrained IoT-edge device pool. In this chap-
ter, the blocked adaptive computationally efficient outlier nominators(BACON) algorithm is implemented and illustrated along with theestimated-expectation/maximization method to improve the anomalynomination for IoT-based health datasets. The weighted variant of the
BACON algorithm package—“wbacon”—from the R repository is
deployed to validate the utilization of anomaly nomination for anIoT-edge enabled health dataset.
Chapter 5, “The edge AI paradigm: Technologies, platforms and use
cases,” explains the various implementation technologies of the edge AIparadigm. Prominent industrial use cases are also discussed in this chapter.
Chapter 6, “Microservices architecture for edge computing environ-
ments,” focuses on the importance of microservices architecture and
event-driven architecture styles, as they help to visualize and implement
edge-native applications that can be deployed and run on IoT devices.
Chapter 7, “Edge data analytics technologies and tools,” explains the
concept of edge analytics, the technologies and tools for enabling edgeanalytics, and provides various delectable use cases.
Chapter 8, “Edge platforms, frameworks and applications,” observes
how edge computing and analytics domains are receiving a lot of attentionthese days. Industries are exploring a variety of use cases that leveragexviii Prefaceintegrated edge platforms and enable frameworks. These advancements
enable enterprises to build a wide range of applications.
Chapter 9, “Edge computing challenges and concerns,” discusses the
various challenges and concerns of edge computing. There are several
advantages associated with edge computing. However, there are also afew lacunae, and experts are working together to develop appropriate
solutions and approaches that overcome the identified limitations of edge
computing.
Chapter 10, “A smart framework through the Internet of Things and
machine learning for precision agriculture,” focuses on advanced techniques
used in smart agriculture system based on IoT and machine learning algo-
rithms. Several studies have been carried out on this system to offer smartservices for real-time monitoring of any agricultural environment.IoT-based smart agriculture systems are an ideal approach to enhance the
productivity of food items with reduced power and water consumption.
Chapter 11, “5G Communication for edge computing,” highlights the
concepts of edge computing and the working methodology. It also discusses
in detail the importance and taxonomy of edge computing in 5G, and the
functional components of edge computing. The evolution of 5G is summa-
rized, and a brief explanation of the architecture of edge computing and 5Gis also provided. Finally, the chapter explores recent advancements in edgecomputing for 5G.
Chapter 12 envisages “The future of edge computing.” This chapter
identifies the various technologies that are likely to be integrated with edgecomputing in order to realize next-generation edge applications andservices.
Chapter 13, “Edge computing security: Layered classification of attacks
and possible countermeasures,” is about edge security. This chapter focuseson presenting layered classification of security attacks and identifies howthese attacks can be overcome.
Chapter 14, “Blockchain technology for IoT edge devices and data
security,” explains the need for blockchain technology to ensure the tightestsecurity for IoT edge devices and data.
Chapter 15, “EDGE/FOG computing paradigm: Concept, platforms
and toolchains,” focuses on edge concepts. The chapter also considers theemergence of various platforms and toolchains in facilitating the goals of
edge computing.
Chapter 16, “Artificial intelligence in edge devices,” discusses the
implications of running AI algorithms and frameworks on edge devices.xix PrefaceThe chapter also reviews the unique advantages and use cases when AI
capabilities are embedded in IoT edge devices.
Chapter 17, “5G—Communication in healthcare applications,” investi-
gates the unique capabilities of 5G communication networks and how this
next-generation cellular communication is useful in fulfilling a next-generation healthcare application.
Chapter 18, “The integration of blockchain and IoT edge devices for
smart agriculture: Challenges and use cases,” illustrates the architecture,challenges, and benefits of using edge computing to enable the industry4.0 vision. This final chapter explains how the integration of blockchain
technology and IoT edge devices is beneficial in visualizing and realizing
smart agriculture applications.
The book aims to articulate and accentuate the significant contributions
of edge computing and analytics technologies for the ensuing digital era. The
book illustrates how the real digital transformation can be accomplished
through the smart leverage of various innovations and disruptions in theedge AI space.
U
RVASHI SUGANDH
Research Scholar, Department of Computer Science,
Faculty of Mathematics and Computing, Banasthali Vidyapith,
Banasthali, India
DR.M ANJU KHARI
Associate Professor, School of Computer and System Sciences,
Jawaharlal Nehru University,
New Delhi, India
DR.SWATI NIGAM
Assistant Professor, Department of Computer Science,
Faculty of Mathematics and Computing, Banasthali Vidyapith,
Banasthali, Indiaxx PrefaceCHAPTER ONE
Exploring the edge AI space:
Industry use cases
Pethuru Rajaand Jenn-Wei Linb
aSite Reliability Engineering (SRE) Division, Reliance Jio Platforms Ltd. (JPL), Bangalore, India
bDepartment of Computer Science and Information Engineering, Fu Jen Catholic University, Taipei, Taiwan
Contents
1.The proliferation of IoT devices and sensors 2
2.Activating on-device intelligence 4
3.The artificial intelligence (AI) processing at the edge 8
4.Machine learning (ML) at the edge 10
5.Deep learning at the edge 15
6.Digging into the paradigm of edge AI 18
6.1 The blend of edge AI and 5G rekindles state-of-the-art applications 20
6.2 Person re-identification (Re-ID) ( https:/ /towardsdatascience.com/why-we-
need-person-re-identification-3a45d170098b ) 22
7.Edge
AI for next-generation retail experiences 23
8.Edge AI for smarter cities 25
9.Edge AI for telecommunication 29
10. Conclusion 33
Further reading 33
About the authors 34
Abstract
Now, edge devices, through a plethora of technological innovations and disruptions,
are being stuffed with more memory, storage and networking capacities andprocessing capability. Thereby, edge devices are joining in mainstream computing.That is, the centralized and consolidated computing moves over to edge devices tobe decentralized and disaggregated. AI libraries are being deployed in IoT edge devicesto do proximate and real-time data processing. This is termed as on-device intelligence.Such a strategically sound shift is to bring forth a dazzling array of advancements andautomations not only for business houses but also for common people in their everydayassignments. This chapter is to throw some light on the theoretical and the practical
aspect of the edge AI paradigm.
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0011The faster adoption of a few transformative and trend-setting technologies
such as the Internet of Things (IoT), artificial intelligence (AI), edge com-
puting, 5G communication, etc., has set the ball rolling for the edge AI era.The centralized and expensive computing now becomes decentralized, dis-tributed, and affordable with the surging popularity of edge computing.Market watchers forecast that there are billions of IoT edge devices and -
sensors. The device ecosystem is growing fast with the plentiful production
of slim and sleek, trendy and handy devices. When these devices connect,communicate, collaborate, correlate, and corroborate directly or indirectly,the size of the IoT data getting generated and collection is literally massive.
Also, the IoT data comes in a multi-structured form is exponentially grow-
ing. With cloud computing, the IT becomes highly optimized and orga-nized. Further on, the longstanding goal of IT industrialization andconsumerization are seeing the reality with the steady growth of cloud appli-
cations, platforms and infrastructures. Now, with the growing solidity and
sagacity of AI algorithms and frameworks being widely deployed on cloudenvironments, transitioning the fast-growing IoT data heaps into actionableinsights in time is gaining the much-needed speed. Resultantly, there are
insights-driven business workloads and IT services.
Now, edge devices, through a plethora of technological innovations and
disruptions, are being stuffed with more memory, storage and networkingcapacities and processing capability. Thereby, edge devices are joining inmainstream computing. That is, the centralized and consolidated computing
moves over to edge devices to be decentralized and disaggregated. AI librar-
ies are being deployed in IoT edge devices to do proximate and real-timedata processing. This is termed as on-device intelligence. Such a strategicallysound shift is to bring forth a dazzling array of advancements and automa-
tions not only for business houses but also for common people in their every-
day assignments. This chapter is to throw some light on the theoretical andthe practical aspect of the edge AI paradigm.
1. The proliferation of IoT devices and sensors
There is a plethora of noteworthy breakthroughs in the field of arti-
ficial intelligence (AI). AI is succulently enabled through a host of machineand deep learning (ML/DL) algorithms. The prominent possibilities arecomputer vision (CV) and natural language processing (NLP) applications.
Precisely speaking, the mesmerizing AI phenomenon is producing a daz-
zling array of sophisticated digital life applications. Not only business housesbut also common people also started to experience the beauty and power of2 Pethuru Raj and Jenn-Wei LinAI technologies and tools. In this chapter, we are to focus on convolutional
neural networks (CNNs) and recurrent neural networks (RNNs) and their
unique contributions in visualizing and realizing game-changing use cases.These deep neural networks (DNNs), a critical part of artificial neural net-works (ANNs), are guaranteeing in releasing and running vision-enabledand human-intractable business workloads and IT services.
Further on, the Internet of things (IoT) is being touted as the next-
generation Internet comprising not only server machines, storage appli-ances, and networking solutions but also all kinds of digitized entities (Allsorts of physical, mechanical, and electrical systems in our everyday environ-
ments will become digitized and integrated with the Internet). The size of
the future Internet is bound to see an exponential growth due to the enormousand elegant participation of digital devices. Going forward, most of theenterprise-scale, service-oriented, process-aware, knowledge-filled, and
event-driven business workloads and IT services are being modernized,
migrated and executed in heterogeneous cloud servers (public, private, edgeand hybrid). With the concept of cloud-native computing flourishing, ITinfrastructures are being organized well and optimized deeply to run
microservices-centric applications efficiently to achieve the much-needed
portability, interoperability, findability, accessibility, availability, maneuver-ability, scalability, extensibility, and reliability.
There is another fledgling concept of cyber physical systems (CPS)
gaining momentum in the recent past. Mission-critical physical assets at
the ground level are being digitized and synchronized with cloud-hosted
software applications and databases in order to be cognitive in their everydayoperations. The concept of digital twins is also fast emerging and evolvingfor empowering ground-level artifacts. Especially sophisticated systems such
as medical instruments, defense equipment, automobiles, avionics, space
electronics, manufacturing assembly lines, etc. are empowered through theircloud-hosted digital twins, which continuously collect the data from theirphysical twins, leverage AI-driven data analytics in real time, and articulate
their findings with the concerned in order to intrinsically automate a range
of manual and error-prone tasks. Technically speaking, the IoT technologiesand tools are for digitization, connectivity and integration of everythingwith everything else in the vicinity/neighborhood and with remotely heldentities. Such a brewing trend is seen as a breakthrough transition for the
future of the world.
In short, we are seeing an unprecedented growth of the IoT devices in
our everyday environments. By fusing the distinct AI capabilities with thesehandy and trendy, slim and sleek handhelds, wearables, portables, and3 Exploring the edge AI spaceimplantable, the world is all set to be adequately and artistically enabled
through self, surrounding and situation-aware devices, systems, and environ-
ments. In other words, digitized and connected assets are to be empoweredwith vision, perception, decision-making and actuation capabilities. Also,these empowered devices are to interact with human beings through naturalinterfaces. This chapter is incorporated in this book in order to tell all about
how technologies gel well to envisage advanced and automated capabilities.
2. Activating on-device intelligence
There are billions of IoT devices and sensors generating a staggering
amount of multi-structured data at the network edge. That is, the IoT edge
devices deployed across the globe could easily generate exabytes of data. To
make sense out of exponentially growing edge device data, the concepts ofedge computing and analytics are being seen as the trend-setting phenom-enon. AI processing is being done directly on edge devices. Streaming
data analytics is being performed on edge devices. Therefore, the next-
generation capability of on-device intelligence is gaining prominence asedge devices are being instrumented with more processing capability,network, memory and storage capacities. Such a well-defined and strategi-
cally sound instrumentation facilitates interconnectivity and acquiring
intelligence in an automated manner. Another noteworthy point is themuch-discussed knowledge discovery and dissemination happen in realtime. Thereby real-time intelligent applications are bound to flourish inthe days to come and the world is to see real-time enterprises sooner
than later.
Precisely speaking, the hugely popular IoT paradigm is for digitization.
That is, all the ordinary things become extraordinary in their actions andreactions with the digitization-enablement process. The common and cheap
things in our midst are designated to be digitized to join in the mainstream
computing. Digital assets and assistants, when interacting with one anotheror when collaborating for accomplishing complex business processes, or cor-relating with one another for uncovering hidden patterns, are bound to emit
out a tremendous amount of poly-structured data. The IoT devices and sen-
sors generate a lot of data through interaction and collaboration. Now thereis a widespread acceptance that data is a strategic asset for any organization tobe competent and clever in their offerings. When the generated data gets
collected, cleansed and crunched, the world is ready to receive and leverage
actionable insights out of voluminous data. In other words, data gets4 Pethuru Raj and Jenn-Wei Lintransitioned into information and into knowledge. There are path-breaking
technologies and tools to automate the arduous task of converting data into
knowledge. That is, knowledge discovery and dissemination have becomethe talk of the town with the ready availability of big data. There are a num-ber of digital technologies such as software-defined cloud centers, big, fastand stream data analytics, blockchain, artificial intelligence, microservices
architecture, cybersecurity, etc., the IoT-generated data gets processed
and analyzed to produce useful insights in time. The resulting insights arethen looped back to business workloads, IT services, and IoT devices toempower them to be cognitive in their service deliveries.
Device to Cloud (D2C) Integration —The figure below vividly illus-
trates how cloud and enterprise applications get empowered through theseamless and spontaneous integration with ground-level devices and sensors.There are wider options for data transmission protocols, data representation,
exchange and persistence formats, and network topologies.
Real-time Data Analytics at the Edge —With the accumulation of
IoT edge devices, there is a massive amount of poly-structured data. Thereal-time analytics capability is gaining wider acceptance at the edge. In this
big data era, IT experts understand that AI turns out to be a great
game-changer especially in arriving at accurate inferences out of voluminousdata. Also, there is a paradigm shift in proximate processing. Today IoT datagets collected, transmitted and stocked in hyperscale cloud centers to docasual and comprehensive data analytics. But due to latency, bandwidth,
and security issues, increasingly data gets analyzed at the source itself.
That is, one or more edge devices are being clubbed together to embarkon real-time data analytics in an affordable, amenable, and artistic manner.The solidity and significance of edge analytics are being understood by busi-
ness executives and technology professionals. There are technologies and
tools galore for setting up and sustaining ad hoc, dynamic, purpose-specificand temporary clouds being formed out of edge devices in order to facilitatereal-time edge data analytics at the source itself. With the general availability
of lightweight artificial intelligence (AI) libraries, models, frameworks, and
engines, the aspect of AI processing at the edge brightens and blossoms.
The formation of Edge Clouds for enabling Edge Analytics —The
data processing slowly and steadily moves over from large-scale, centralized,clustered, consolidated, automated, and shared cloud environments to edge
clouds. On one side, edge clouds are being formed out of a few server
machines for enabling proximate data processing. On the other side, hetero-geneous edge devices in a particular environment such as homes, hotels,5 Exploring the edge AI spacehospitals, etc. can form ad hoc, small, and purpose-specific device clusters/
clouds quickly. There are enabling technologies and tools for automatic set
up and sustenance of edge device clouds. Leading market watchers and ana-lysts have predicted that there will be billions of connected devices in theyears to come. Increasingly edge devices are being stuffed with more com-puting, storage and networking capacities and capabilities and hence they are
intrinsically capable of finding, binding and collaborating with one another
to solve bigger business problems. Thus, the edge power is growing rapidlywith the proliferation of edge devices. The amount of data getting generatedby machines is far higher than men-generated data. It is estimated that more
than 850 zettabytes of data will be generated every year hereafter. Besides
faraway cloud storage appliances, resource-intensive edge devices offer abetter platform for stocking and processing edge data.
The Promise of Edge AI —The edge computing model is the recent
phenomenon gaining a lot of interest among business executives and ITexperts. As indicated above, sending all the IoT device data to faraway cloudenvironments to be stored and subjected to a variety of investigations iswasting network a lot of bandwidth and also increasing the network-
induced latency. For low-latency and real-world applications, the concepts
of edge computing, storage, and analytics are gaining momentum. In short,the local computing through edge devices and AI-inspired deep analytics ofedge data generated by scores of IoT devices and sensors are gaining a lot ofattention. Since the edge devices and services are closer to users than the
cloud, the idea of edge computing is expected to flourish and fulfil hitherto
unheard real-world and real-time requirements. The figure below vividlyillustrates the macro-level edge computing architecture ( Fig. 1 ).
The
technologies and tools empowering edge computing and the faster
maturity and stability of AI algorithms have concertedly resulted in a series ofdigital disruptions and innovations. The fusion of AI and edge computing isincreasingly natural and beneficial. AI model creation, evaluation, optimi-zation and deployment have been accomplished in cloud environments.
Now with the general availability of highly miniaturized, multifaceted yet
powerful processor architectures empowering edge devices and the surgeof lightweight AI frameworks and libraries, the new era of edge AI hasdawned. That is, running AI models directly in edge devices is gainingmomentum for practical reasons. There is a bright and better scope for such
an intersection. The point is that the distinct AI powers are being now real-
ized through our everyday devices. In other words, the aspect of AI democ-ratization is surging ahead.6 Pethuru Raj and Jenn-Wei LinThrough these transitions, there will be a plethora of people-centric
applications. The enterprise IT has moved to cloud IT and now it is heading
toward edge IT. In other words, not only business behemoths but also com-
moners are literally to enjoy a dazzling array of durable and distinct benefitsout of the booming AI domain. The movement toward people IT is pro-gressing well. There is a greater affinity between AI and edge computingparadigms. There are explanatory and exploratory articles articulating and
accentuating long-term benefits of this disruptive and deft convergence.
AI is all set to become penetrative, participative and pervasive. AI has turnedout to be a key driving force behind emerging frontiers such as self-drivingcars, smart diagnosis and treatment, intelligent transportation, cognitive
applications, smart homes, hotels, hospitals, etc.
As indicated above, there will be billions of IoT edge devices including
smartphones, consumer electronics, manufacturing machineries, robots,cameras, wearables, handhelds, portables, and fixed devices. The sky-
rocketing number of heterogeneous IoT devices lead to the generation of
massive amounts of multi-modal data (still and dynamic images, audio files,etc.) of the edge devices’ physical surroundings that are minutely monitoredand sensed. There are big, fast and streaming analytics platforms for
extracting actionable insights out of edge device and environment data.
The data analytics platforms are being shrunken and deployed in edgedevices to facilitate real-time data capture and crunching. In the recent past,AI models are being developed and deposited in model repository to solve aEdge Controller
Edge Cluster
Device Device Device Device Device DeviceEdge Application Edge Application Edge ApplicationEdge Cluster Edge Cluster
Fig. 1 Edge device to edge controller integration.7 Exploring the edge AI spacevariety of problems. With the faster adoption of AI model optimization
techniques, AI models are pruned to be run on edge devices. Thus, the dis-
tant and distinct goal of edge AI is seeing the grandiose reality with a series ofnoteworthy technology and tool developments.
The world is elegantly experiencing the power of AI algorithms. There
are a litany of innovations and disruptions in the AI space. There came
pioneering machine and deep learning (ML/DL) algorithms in order to
enable everyday machines to automatically learn from data directly. Suchknowledge-filled machines are capable of exhibiting real-time and real-world intelligence in their actions and reactions. Further on, the domains
of computer vision (CV) and natural language processing (NLP) are seeing
a lot of advancements with the steady stability of powerful ML and DL algo-rithms. However, due to the network latency issue, increasingly ML and DLframeworks are being installed in edge devices in order to democratize and
demonstrate AI capabilities for every person and every group of people at
any place. Edge-based AI processing feature is being keenly used for a widerrange of applications such as smart factories and cities, face recognition,machine translation, human machine interfaces (HMIs), medical imaging,
etc. Several industry verticals and business functions are exploring the pos-
sibility of leveraging the distinct advancements in the AI space to conceiveand provide premium services to their consumers, employees, partners andother constituents.
3. The artificial intelligence (AI) processing at the edge
The faster maturity and stability of machine and deep learning (ML/
DL) algorithms are setting a stimulating foundation for the huge success ofthe AI paradigm. Now there are miniaturized AI libraries to be deployed tofacilitate running AI applications on IoT edge devices. Such a combination
brings forth a number of personal, social and business use cases. For an exam-
ple, it brings the ability to identify usable patterns and detect anomalies in thedata points sensed and captured by the edge device, Sensors attached withedge devices are capable of monitoring population distribution, traffic flow,
humidity, temperature, pressure, and air quality continuously and if there is
any perceptible deviation, then the edge device will raise an alarm or alert tothe concerned to ponder about the best course of remedial actions in time.Further on, the insights extracted from the sensed data are fed to automated
systems for taking decisions and then for plunging into appropriate actions.
There are automated systems such as traffic congestion avoidance systems,8 Pethuru Raj and Jenn-Wei Linpublic transportation planning, driving and parking assistance systems, etc.
The automation capabilities being introduced through AI processing at
the edge fulfils the long-pending goal of operational efficiency, propertyand people safety, etc.
The aspect of edge AI is gaining prominence these days as there are
requirements for producing and running real-time applications, which are
prepared to succulently automate and orchestrate multiple manual tasks
for industrial environments as well as peoples’ every day environments.With the general availability of powerful processors, AI processing at theedge is seeing a good progress. The device ecosystem is on the growth path.
There are multifaceted yet disappearing sensors and actuators being pro-
duced in large quantities. The emergence of data transmission protocols, datarepresentation, exchange and persistence formats, the steady adoption ofevent processing middleware solutions and streaming databases, etc. are seen
as a positive factor toward the ensuing era of edge AI. The direct or indirect
integration of edge devices with faraway cloud platforms through interme-diaries is also making it easier and popular for the edge AI phenomenon togain the much-needed boost. Historical and comprehensive data analytics in
consonance with cloud-based data lakes in hyperscale cloud environments is
being touted as a key differentiator.
Digital twins are being constructed and made to run on cloud environ-
ments for all kinds of mission-critical and complicated appliances, machin-eries, rockets and their launchers, defense equipment, medical instruments,
robots, drones, consumer electronics, gadgets and gizmos, etc. That is, these
ground-level devices are having their virtual/cyber/software/logical ver-sions running in clouds. Through the seamless and spontaneous connectivityand integration between physical entities at the ground and their equivalent
constructs at virtualized and containerized cloud environments significantly,
it is possible to visualize and realize additional competencies for edgedevices. Digital twins also incorporate data analytics and AI capabilities inorder to substantially empower edge devices. The performance and
throughput levels, health condition, and operational quality of distributed
and disparate edge devices can be minutely monitored and digital twinscan integrate both current and historical data of edge devices to embarkon comprehensive analytics to unearth actionable insights. Thus, theon-device intelligence capability coupled with cloud-based analytics facility
is being pronounced as the way forward for the ensuing era of knowledge.
The traditional cloud architecture is primarily challenged on several
fronts. Latency, centralization, cost, reliability, and security are seen as the9 Exploring the edge AI spacegrave barriers for the thriving of the conventional cloud computing para-
digm. With edge devices natively supporting the aspects of distribution
and decentralization, the widely articulated concerns of public cloud envi-ronments are being surmounted through edge cloud computing. Edge dataare environment-specific and people-centric. The self-, surroundings- andsituation-aware details are carefully collected, cleansed and crunched
quickly and efficiently to arrive at and articulate actionable insights in time.
Scores of real-time and context-aware services can be accurately decided anddelivered in time. Thus, the phenomena of data-driven insights and insights-driven applications are set to be the new normal. There are two aspects:
Machine learning and Deep learning at the Edge.
4. Machine learning (ML) at the edge
Machine learning libraries are being taken to edge devices in order to
sufficiently enable them to do self-learning without any involvement,
instruction and interpretation. Primarily, blogs and articles illustrate two
main use cases for ML in the industrial IoT landscape.1.
Anomaly detection
2.Extract higher-valued features such as remaining uptime for industrial
machineries
For getting these use cases, experts turn toward executing ML frameworks
(there are several lightweight versions) in edge devices, which are typicallysituated near the data source. Where the action is, there edge devices are.Even people carry and wear edge devices. The process of digitization is as
follows. All kinds of physical, mechanical and electrical systems in our per-
sonal, social and professional environments are enabled by externally and/orinternally embedding a variety of sensors and other enablers on them. Thereare several powerful digitization and edge technologies fast emerging and
evolving to set up and sustain the digital world. Through this act of digiti-
zation, ordinary items become extraordinary. Casual things become smartobjects by applying noteworthy technological advancements. Dumb objectsbecome animated things and sentient objects. Common things become cog-
nitive entities. In short, everything is destined to be digitized. Digitized
objects can be further enabled to be sensitive, perceptive, computational,vision-enabled, communicative, responsive, decision-making, and active.Natural interfaces can be attached with digitized elements to seamlessly
and spontaneously interact to complete everyday tasks with all the confi-
dence and clarity.10 Pethuru Raj and Jenn-Wei LinPrecisely speaking, digital and edge technologies and tools, on meticu-
lous planning and execution, result in scores of connected and cognitive
systems in and around us in large numbers. The sensors attached in thesesystems minutely monitor different parameters (physical, operational, andtransactional details, health condition, performance level, and log data), cap-ture and transmit them the ML frameworks and models, which, then, infer
something unique and useful immediately. The ML frameworks and models
can run in those devices or in nearby edge devices. The brewing idea is to doproximate data processing and local analytics, which emits out a lot of advan-tageous information and insights. Such an extracted knowledge can be dis-
seminated to the concerned systems, devices, services and people in time.
The workflow for machine learning consists of two main steps: firstly, it
is all about training and developing a ML model. Once the model reaches astate of stability and accuracy, then the model is destined to production envi-
ronment. The first step is typically an off-line operation where stored data is
used to train and tune a model. Then the trained, tested, and optimizedmodel is ready to make inferences on real-time data. Increasingly IoT edgedevices are used for executing matured ML models. However, ML model
generation is an iterative process. That is, the model output is being checked
and a proper feedback is created and shared to the original model architectureto bring in appropriate changes. Hence it is not a single and straightforwardactivity. Instead, the ML model is bound to go through multiple iterations toattain a reasonably good model. In this refinement activity, there can be a
number of traversals between edge devices and the central cloud.
Machine Learning for Streaming Data —When preparing and pro-
ducing ML models, data has to go through a variety of pre-processing stepssuch as data cleaning, deletion, augmentation, and addition, In other words,
data has to be presented in such a way that the target environment can unam-
biguously understand and consume the data without any hitch or hurdle.
Generally, when training a ML model, data is stored in files or a database
with all time-stamped sensor values. This is because the model gets the same
set of data each time. In a streaming environment, the scenario is quite dif-
ferent. That is, the sensor data is getting received serially, with each sensorsending data at repetitive intervals but independent of all other sensors.Before we can deliver streaming data to a ML model, we must align the dataon regular time boundaries. Also, sometimes it is necessary to adjust the
time windows to get data from sensors that deliver data with a lower rate.
Video cameras presented as multifaceted sensors are being stuffed withnew-generation capabilities in order to enable them to be intelligent in their11 Exploring the edge AI spaceoperations, offerings and outputs. Newer possibilities are being unearthed
and fresh use cases are being articulated and accentuated. That is, video
cameras are being deployed in industrial areas, advanced homes and officebuildings, entertainment plazas, stadiums, eating joints, bus stations, nuclearinstallations, hospitals, and other important junctions. Experts have visual-ized the following use cases.

Vision inspection for yield optimization —Intelligent video cameras
can perfectly identify scrap material and faulty products as early as pos-sible in order to ensure the highest quality.
Count and measure —Empowered video cameras can easily count
products, objects, people and measure position, alignment, color and
other attributes.
Intrusion detection and people safety —Enabled video cameras can
quickly trigger an alarm or stop a machine when people or objects get
too close or enter where they shouldn’t be.
With intelligent cameras abounding in our everyday environments, a variety
of physical and behavioral aspects are being monitored ceaselessly. Not onlycapturing the happenings in those places precisely, but also taking appropri-
ate counter measures in time based on the gathered and gained understand-
ing are being widely appreciated. By applying ML capabilities on sensor datalocally, it is possible to transition ordinary devices into cognitive devices.Real-time actions can be initiated and implemented through streaming dataanalytics on edge devices. Simple as well as complex activities such as
real-time classification, clustering, association, regression, vision, recogni-
tion, detection, and translation can be accomplished by running advanceddata analytics tasks on edge platforms and infrastructures. With the readyavailability of container orchestration platform solutions such as K3s,
KubeEdge, and other lightweight Kubernetes versions, edge devices are
clubbed together or clustered to form dynamic, purpose-specific, and adhoc edge device clouds. Thus, with the capability of setting up and sustain-ing edge infrastructures quickly and easily is fast maturing and stabilizing,
edge analytics is all set to become the new normal. Other prominent use
cases are being explained below.
Edge devices in healthcare —The need for on-device intelligence is
gaining ground as decisions and actions have to be taken quickly in orderto save lives. Value-adding body parameters have to be collected and
crunched locally to take correct and real-time decisions. Collecting and car-
rying them to faraway cloud environments for data storage and analytics isnot a good sign especially for the healthcare sector. Even performing12 Pethuru Raj and Jenn-Wei Linstreaming data analytics using cloud-based streaming databases and analytics
platforms is being seen as a risky thing. The network latency comes into the
picture. If there is a slowdown in network speed or if there is a networkbreakdown, then the consequences could be unthinkable and irreparable.Thus edge-based real-time machine learning is emerging as the way forwardto monitor, measure, and manage ICU patients safe.
We have features-rich sensors and they can form ad hoc smart networks
to accomplish bigger and better things. The connectivity solutions are per-vasive with 5G emerging as the boon for reliable indoor networking. Thereare lightweight libraries and frameworks to run AI models on edge devices.
In the recent past, there are concerted research works and contributions to
bring in highly optimized AI models. Thus, the technological power is facil-itating real-time healthcare services. Neurological activity and cardiacrhythms can be monitored continuously and analyzed in real time to identify
if there is any deviation. There is another term “Ambient intelligence
(AmI)” coined and popularized some years back to fulfil the vision of intel-ligence everywhere. Now with the ambient communication, ubiquitouscomputing, pervasive sensing and perception, edge-based AI processing,
etc., the goal of AmI is all set to see the grand reality soon. Every sneeze
and activity of people under observation can be meticulously monitoredand cared in case of any abnormality.
The device ecosystem grows fast. We have a variety of purpose-specific
and agnostic devices. We have toasters, robots, smartphones, physical activ-
ity trackers, smart watches cameras, drones, and wearable gyroscopes or
accelerometers. With the ready availability of a dazzling array of instruments,electronics, equipment, wares, gadgets, gizmos, appliances, etc. the era ofedge devices joining in the mainstream computing is brighter than ever.
Fresh use cases are being achieved with the surge of multifaceted devices.
By applying the distinct ML capabilities, on-device intelligence is beingenabled. That is, the realization of intelligent devices and services is beingsimplified and speeded up.
Mining, oil, and gas and industrial automation —The business
value of edge-based ML becomes is gaining momentum in the oil, gas, ormining industry. Employees have to work in isolated, risky and farawayplaces. Therein, the connectivity is irregular. Therefore, the much-neededcomputation has to be taken to the devices and machines there. Sensors
attached on edge devices and robots can capture large amounts of data
and accurately predict things like as pressure across pumps. If there is anydeviation in one or more operating parameters, then appropriate alerts13 Exploring the edge AI spaceget articulated in time. Preventive and predictive maintenance of industrial
assets and artifacts are being facilitated through real-time prediction of
ML-enabled devices.
Further on, embedded sensors of all machineries inside a factory or ware-
house can capture and store all kinds of data (images, videos and audios)locally. By applying machine and deep learning algorithms on the data, it
is automatically decided by the machines to have some self-rest. If there
is any repair required, that details can be shared across. With the fast-growingAI power, machines can self-diagnose, heal, configure, defend, manage, etc.Machines become fault-tolerance and performant.
Streaming high-frequency data —In the increasingly streaming
world, data is pouring in continuously in high speed. It is therefore impor-tant to extract and report knowledge continuously matching up with thedata generation and ingestion speeds. As accentuated above, AI models play
a vital role here in knowledge extraction. But because of high-frequency
data, there is a need to accommodate a proven messaging system beforesending the streaming data to a polished AI model. The figure below vividlyillustrates the data flow and knowledge discovery and dissemination ( Fig. 2 ).
With
the confluence of the IoT, emerging computing paradigms such as
cloud-native, serverless and edge, artificial intelligence, the goal of digitallytransformed homes, cities, offices, retail stores, manufacturing floors, etc. isto be fulfilled quickly. AI-enabled data analytics go a long way hand in handin setting up and sustaining digitally transformed systems, solutions, services
and environments. Easily usable and manageable intelligence will become
penetrative, pervasive and persuasive.
The adoption of ML capabilities is gaining momentum these days. There
are several everyday applications getting immense support from ML algo-
rithms and models. The majority of these ML models are process and
data-intensive. Generally, ML models need cloud-like infrastructure to berun comfortably. For real-time computation and analytics, edge-centriccomputing is being presented as the way forward. Also, IoT device and
Continuous Data
Sensor DataDashboard
Alerts
Stora geMessaging Service Make Predictions
f(x)Update Model
!
Fig. 2 The data flow toward knowledge discovery and dissemination.14 Pethuru Raj and Jenn-Wei Linsensor data are mainly streaming. In order to ensure real-time data analytics,
ML models are being taken to edge devices. To bring ML libraries, frame-
works and accelerators to the edge, a variety of innovations and transforma-tions are being introduced and implemented. Processor architectures aregoing through a host of distinct disruptions. Highly optimized communica-tion protocols are emerging and evolving. Data security at transit and per-
sistence is being ensured through a host of powerful technologies and
methodologies. Energy efficiency at the edge is being given extra thrustthese days. Mitigating time and space complexities and achieving energycomplexity are pronounced as the key barriers. Researchers are also equally
working in a concerted fashion to surmount these constraints. Creating ML
models with smaller memory footprint is the need of the hour.
There are an arsenal of tricks and techniques for ML model compression.
The prominent ones among them are pruning, quantization, sparse model-
ling, knowledge distillation, transfer and federated learning, etc. Such break-
through models can be easily run on a cluster of edge devices to performedge-centric and ML-based inferences.
5. Deep learning at the edge
There arise new DL models to do image segmentation at the edge
(https:/ /bdtechtalks.com/2021/05/ 07/at tendseg-deep-learning-edge-
semantic-segmentation/ ).
There are a number of neural network (NN) architectures for fulfilling a
variety
of problems such as computer vision, speech recognition, etc. A new
neural network architecture has made it possible to perform image segmen-tation on resource-constrained IoT edge devices. As we all know, imagesegmentation is the process of determining the boundaries and areas ofobjects in images. Image segmentation is a vital requirement for mobile
robots, self-driving cars, and other vision-based systems that interact and
navigate the real world. However, segmentation requires a heavy load ofcomputing power besides huge network and storage capacities. That is,developing such deep learning models mandate cloud servers and storages.
The scientists at DarwinAI and the University of Waterloo have created a
neural network architecture that provides near-optimal segmentation.Also, it (is named as AttendSeg) is small enough to run on resource-constrained IoT devices.
Computer vision is one of the key applications of deep learning. The
domain of computer vision includes image classification, object detection,15 Exploring the edge AI spaceand segmentation. Segmentation is a complex classification task. As indi-
cated above, CNNs are primarily used for computer vision tasks. The
CNN complexity is being measured by the number of parameters. Moreparameters mean more memory and computation are needed. For an exam-ple, RefineNet, a popular semantic segmentation neural network, containsmore than 85 million parameters. At 4 bytes per parameter, any application
using RefineNet requires at least 340 megabytes of memory just to run the
neural network.
Due to the huge hardware requirements of neural networks, applications
of image segmentation mandate for cloud infrastructures. That is, deep
learning models are being run on cloud platforms for their versatility and
resource needs. However, this poses a series of challenges. The channelbetween the image segmentation application running on robots, cameras,drones and smartphones and the faraway cloud server has to be wider and
secure. The network bandwidth has to be on the higher side for transmitting
images between ground-level devices with cloud-based image segmentationapplications and services. Also, the problematic network latency comes intopicture here. There are other limitations to be looked into. The strategic
requirement is to empower IoT edge devices with much-needed vision
capability to collect image and video data streams quickly and process theminstantaneously to emit out actionable insights in time. Real-time image seg-mentation is needed in sensitive environments wherein AI agents are beingdeployed. Running AI models locally is definitely taxing energy-starved IoT
devices. Thus, the way forward is as follows. Building and sustaining highly
optimized AI models is insisted for the ensuing edge era. Edge-native appli-cations ought to be empowered through sophisticated AI models, whichconsume less energy with small memory footprint. That is the main reason
for the domains of edge AI and TinyML to flourish lately. As articulated
above, highly miniaturized, generalizable, and transferable models have tobe designed, developed and deposited in order to publicly be found andbound. There are public and private model repositories to kickstart and
rekindle the long-pending AI era.
In short, the aspect of image segmentation goes to the edge to serve bet-
ter. The industry 4.0 vision involves a plethora of industrial applications withsegmentation capability. The authors of this paper claim that the AttendSegdeep learning model performs semantic segmentation at an accuracy that is
almost on-par with RefineNet while cutting down the number of param-
eters to 1.19 million. The memory footprint of AttendSeg gets reduced toone fourth. The model requires a little above one megabyte of memory.16 Pethuru Raj and Jenn-Wei LinThis is quite easy to manage with most of the IoT edge devices. Experiments
clearly show that AttendSeg provides optimal semantic segmentation by
sharply cutting down the number of parameters and by reducing the mem-ory footprint.
Model Accuracy
(%)MACs
(G)Parameters
(M)Weight memory
(Mb)
RefineNet 90.0 202.47 85.69 343
EdgeSegNet 89.15 77.89 7.09 28.3
AttendSeg 89.89 7.45 1.19 1.19
AttendSeg leverages “attention condensers” to reduce model size without
compromising performance. Self-attention mechanisms improve the effi-ciency of neural networks by focusing on information that matters. Self-attention techniques have been widely used in producing natural language
processing (NLP) applications. As found out, recurrent neural networks
(RNNs) had a limited capacity on long sequences of data. Transformers, akey deep learning (DL) architecture, use self-attention mechanisms to over-come the RNN limitations to expand their range. That is, deep learning
models such as GPT-3 leverage Transformers and self-attention to churn
out long strings of text.
AI researchers started to leverage the proven attention mechanisms to
improve the CNN performance substantially. Attention condensersimprove the performance of CNNs in a memory-efficient way. One of
the key challenges of designing TinyML neural networks is finding the best
performing architecture while reducing the computational requirements.To address this, the researchers have used the progressive concept of“generative synthesis”. This is a potential ML technique to create neural net-
work architectures based on specified goals and constraints. The idea is that
instead of manually fiddling with all kinds of configurations and architec-tures, the researchers can bring forth a problem space to empower theML model to zero down the best combination.
The application areas for the AttendSeg neural network are growing
steadily. It is gaining a lot of attention as it is capable of getting deployedand executed on edge devices. It is impacting decisively on manufacturingapplications such as parts inspection, quality assessment, etc. Fixed as well as
mobile robots gain a lot. Medical applications such as cell analysis, tumor
segmentation, etc. are also benefiting immensely out of this shrunken17 Exploring the edge AI spaceneural network. Remote sensing applications such as land cover segmenta-
tion are also hugely simplified through the smart application of this neural
network solution.
Thus, computer vision and speech recognition capabilities are being
embedded in IoT edge devices with the help of deep learning (DL) models.Edge devices, through the on-device intelligence feature, are all set to be
intuitive, informative, and intelligent in their operations, outputs, and offer-
ings. Performing deep learning tasks typically requires a lot of computationalpower and a massive amount of data. With big data, the decision accuracyof deep learning algorithms/models is higher. Today as per the estimates
provided by various market watchers and analysts, there are billions of
connected devices, which, when interacting purposefully, can generate amassive amount of multi-structured data. Thus, the data availability inhumongous quantity has speeded up the adoption of deep learning methods.
Now with cloud computing facilitating on-demand, online, on-premise/
off-premise availability of enormous amount of compute, network and stor-age resources, problem resolution through the unique power of deep learn-ing processes gains the much-needed momentum. IoT devices and sensors
generate a lot of data, which gets collected, cleansed and crunched through
cloud-based AI platforms to squeeze out actionable insights in time. Now,considering the network latency issue, AI processing is being taken to edgedevices to guarantee real-time data processing. With edge devices joining inthe mainstream computing, we will be bombarded with edge-native cogni-
tive applications.
6. Digging into the paradigm of edge AI
The diagram below vividly illustrates how edge devices (generally
resource-constrained and primarily used for data collection and transmis-
sion) are empowered through edge servers (resource-intensive and used
for edge device data storage and processing). Edge servers are embeddedwith communication modules (wired as well as wireless) to take edge devicedata to faraway cloud environments for comprehensive and cognitive ana-
lytics. The Internet is the prime communication infrastructure. There are
other dedicated and high-end communication infrastructure to guaranteehigh-quality communication.
With all-round advancements, the domain of edge AI is gaining
momentum. A number of industry verticals are keenly exploring and
experimenting with edge AI pilots and proof-of-concepts (PoCs) to gain18 Pethuru Raj and Jenn-Wei Lina deeper and broader understanding about the power of edge AI. The IT
teams and organizations besides cloud and communication services pro-
viders are readily equipping themselves with the necessary know-hows tojump into the edge AI bandwagon. This section is to dig deeper to discussthe advantages of edge AI.
The distinct Advantages of Edge AI —Edge computing enables
bringing AI processing tasks from the cloud to near the end devices in order
to overcome the intrinsic problems of the traditional cloud, such as highlatency and the lack of security. As experts indicated, moving AI computa-tions to the network edge has several advantages.

Lower data transfer volume —Data is processed by the edge device
and only a significantly lower amount of processed data is sent to thecloud for long-term storage and comprehensive data analytics. Bysharply reducing data traffic between edge devices at the ground with
cloud-based applications and data sources, the unnecessary wastage of
precious network capacity can be reduced.
Real-time computing —Enterprises are all set to become real-time in
their offerings and operations. Real-time processing has become a new
normal in the digital era. Real-time applications are becoming para-
mount and prevalent. Real-time data capture, storage, processing, ana-lytics, knowledge discovery and dissemination, decision-making andaction are vital for real-time enterprises. In short, edge computing isthe way forward. Time-sensitive or low-latency applications are being
mandated across industry verticals.
The physical proximity of edge devices to the data sources makes it possible
to considerably reduce network latency. This improves real-time processingof edge device data. It supports delay-sensitive applications and services
such as remote surgery, self-driving vehicles, park assistance systems, etc.
With more resources being stuffed in edge devices, real-time analytics ofedge data gets facilitated through the setting up and sustenance of edgedevice clouds.

Privacy and security - Keeping data at the edge strengthens its
privacy. Edge data, most probably, does not travel in the porous, public,open, affordable, and worldwide communication infrastructure (theInternet). Therefore, data is safe and secure and is not vulnerable forany kind of theft and distortion. Edge computing ensures data stays in
its device(s) itself. If there is a need to send edge data to off-premise cloud
platforms, confidential and critical information can be still kept locally toensure its privacy and security.19 Exploring the edge AI spaceHigh availability —Edge AI intrinsically prescribes decentralization.
Edge devices individually and collectively guarantee high availability.
If there is a slowdown even breakdown between edge devices and thecloud environments, edge devices can function offline and deliver theirdesignated tasks without any failure. The widely articulated offline capa-bilities make Edge AI more robust. Edge devices, thus, can provide tran-
sient services during a network failure or when the network is under
cyberattacks. Therefore, enabling edge devices to perform AI processingensures high availability for mission-critical AI applications.
6.1 The blend of edge AI and 5G rekindles state-of-the-art
applications
High-throughput and highly reliable communication is the need of the
hour. The faster adoption of the 5G communication standard across theworld has stimulated many industries to visualize and realize a plethora of
next-generation applications. There are certain domains such as self-driving
cars, real-time virtual reality (VR) experiences, remote surgery, etc. yearn-ing for 5G communication and proximate data processing capability.Edge-native applications are the most sought-after ones in the digital world.
Thus, the combination of 5G and edge computing spells a bright future for
the digital era. 5G cellular communication is now extensively used in indus-trial environments. Thus, factory automation and smart manufacturing forcreating and sustaining the industry 4.0 vision are being enabled through
the faster maturity and stability of 5G and edge analytics. Especially with
the deployment and usage of machine and deep learning frameworks,libraries and accelerators at edge devices, producing people-centric,context-aware, real-time and intelligent applications becomes a simplerand simplified process.
Edge AI Applications— With Edge AI capability, it becomes possible
to power up real-time, edge-native and intelligent applications.
Computer and Machine Vision —With the flourish of highly opti-
mized convolutional neural network (CNN) models, computer vision
especially machine vision capability is becoming the new normal.
Vision-enabled devices and machineries are being greatly used in differ-ent mission-critical environments. Video and audio data analytics tasksare technologically simplified and strengthened to produce and deploy
path-breaking AI applications. Computer and machine vision applica-
tions will be thriving. Vision will become the core and central aspect20 Pethuru Raj and Jenn-Wei Linfor next-generation applications not only at the centralized cloud envi-
ronments but also decentralized edge devices.
Smart energy applications will be prevalent and paramount. Especially
connected wind farms will thrive. Smart microgrids will become ubiqui-tous. The hybrid model of cloud and edge computing paradigms willbecome the new normal in building and running smart energy systems.
The wind farm uses a variety of multifaceted sensors and actuators for
video cameras, access control systems, etc. Also, wind turbines will beinternally as well as externally fitted with features-rich sensors to minutelymonitor and manage them. All kinds of right and relevant data get metic-
ulously collected and processed to emit out actionable insights in time.
Smart healthcare applications such as remote surgery and diagnostics
will see the grand reality. Various body parameters are proactively andpre-emptively being captured and subjected to a variety of investigations
in order to extract intelligence to act upon with all the care, clarity and
confidence. Virtual diagnoses followed by correct medication can berealized with the blend of 5G and edge AI competencies.
Entertainment applications—With the general availability of 5G com-
munication, fresh gaming and entertainment applications such as virtual
reality, augmented reality, and mixed reality are flourishing these days.Increasingly streaming video contents are sent to virtual reality glasses.The form factor of such glasses can be reduced sufficiently by offloadingcomputation from the glasses to nearby edge servers.
Smart manufacturing and factory automation capabilities are
being realized through the combination of cloud environments, 5G,and edge device clouds. People and property safety and security require-ments are fully met through the advancements happening in the technol-
ogy space. Machineries are empowered to be vision, perception,
knowledge discovery, decision-making and actuation capabilities.Localized AI processing is being facilitated by taking computation toedge devices and servers.
Intelligent transportation systems —These systems are becoming
important for next-generation travel, trip and transport. Edge devicesand clouds are playing an extremely vital role here in shaping up thefuture transport. Driver assistance systems, traffic congestion avoidancesystems, accident-avoidance systems, etc. are being given thrust these
days with the faster maturity of edge AI technologies and tools. In addi-
tion, unmanned vehicles can sense their surroundings and move safely inan autonomous and artistic manner.21 Exploring the edge AI space6.2 Person re-identification (Re-ID) ( https:/ /
towardsdatas
cience.com/why-we-need-person-re-
identification-3a45d170098b )
This is a new initiative getting a lot of attention these days. The gist of this
project is to correctly pinpoint a person of interest across multiple and
non-overlapping cameras. With the advancement of deep neural networks
(DNNs) and the increasing demand for intelligent video surveillance, thisproblem has gained significant interest in the computer vision community.The goal of Re-ID is to determine whether a person-of-interest has
appeared in another place at a distinct time captured by a different camera,
or even the same camera at a different time instant. While tracking allows usto receive all the trajectories of movement of anyone in the scene andidentify one person from another, the problem surfaces if there are multiplecameras. If the same person moves across a shopping mall and, for example,
takes off his jacket in-between cameras, he will not be recognized. Different
poses, outfits, backpacks, and other details can mess up our AI model andrecognize the same person as two different ones. Generally, building a per-son re-identification system requires five main steps (video data collection,
bounding box generation, training data annotation, model training and
evaluation, and pedestrian retrieval).
Re-ID with Deep Learning Methods —Many of the recent models
use deep learning (DL) models to extract features and achieve good perfor-
mance. Convolutional neural networks (CNNs) due to their automatic fea-
ture extraction and engineering ability are being proposed by AI researchers.
Thus, with a series of noteworthy advancements in the DL space, person
re-identification has achieved amazing performance. Especially this is to
solve the problem of pedestrian retrieval across multiple surveillance cam-
eras. In recent years, video-based re-identification has made great advances,because video sequences provide visual and temporal information that canbe obtained using tracking algorithms. However, correctly annotating largenumber of visual data is a time-consuming activity. This is the reason why
unsupervised video re-identification is being insisted. As we all know, in the
case of supervised learning, each data point has to be labelled (each datapointgets annotated). An intuitive idea for unsupervised learning is to estimatere-identification labels as accurately as possible. The estimated labels are
subsequently used for feature learning to train robust re-ID models.
Re-identification(reID) is the process of associating images or videos of the
same person taken from different angles and cameras. The key to the issue is
to find features that represent a person. There are research works and22 Pethuru Raj and Jenn-Wei Linpractical implementations ( https:/ /arxiv.org/pdf/1807.11042.pdf ) to build
accurate CNN models. Here are a set of best practices to build and release
highly optimized and generalizable re-identification model.
Using batch normalization after the global pooling layer —It is
important to avoid overfitting during training. Therefore, the recom-mendation is to go for batch normalization. Herein, the task is to nor-
malize the output of each neuron using some mean and variance. Due to
this, certain features are getting generalized during training. Such gen-eralization allows us to apply the same model on different datasets.
Use one fully-connected layer for identity categorization —In
 a
CNN, there generally are two fully-connected layers. The first one playsthe role of a “bottleneck” to reduce feature dimensions. The secondlayer performs identity categorization. The authors suggest removingthe “bottleneck” and use the second layer directly.
Use Adam for CCN optimization —The Adam
 optimization
algorithm is an extension to stochastic gradient descent (SGD) that
has recently seen broader adoption for deep learning applications incomputer vision and natural language processing. Compared to the most
popular used SGD, Adam works on lower-order moments, which allows
us to smooth the variation between gradients.
By using re-identification and tracking models, it is possible to follow thepath
that a person is taking and make sure nothing illegal or inappropriate
is done. Additionally, vehicles and other objects could be tracked. In this
way, the road situation can be analyzed and further improved. If the power
of edge devices is used wisely and in a timely manner, crimes and other illegalactions can be prevented and the offenders could be easily tracked. Edge AIwill be a game-changing technology as the application space for such track-
ing feature is expanding fast.
7. Edge AI for next-generation retail experiences
Brick-and-mortar retailing is facing tough competition from B2C
e-commerce due to the pandemic situation prevailing across the globe.As the world is through the third wave, online e-commerce activity is
gaining momentum. How to empower physical stores to compete with vir-tual stores is the challenge. Smart Retail technologies ( https:/ /hailo.ai/
industry/smart-retail/ )
is helping physical stores to compete against online
ones through a series of technological innovations and disruptions.Digitalizing everything, establishing deeper and extreme connectivity,23 Exploring the edge AI spaceand deploying AI for intelligent automation empowers customers to get
extraordinary experience, which is easily comparable with digital stores.
A number of herewith unheard facilities and features are being added tophysical stores to attract shoppers and to explore fresh avenues to bring inadditional revenue.
The shoppers can enjoy an improved in-store experience. Mobile robots
assist customers in different ways and means. Automated checkout and per-
sonalized offers are being realized. All the features of online stores are beingprovided to entice buyers. Plus, the shoppers can touch and feel the productsin person. For the retailers, intelligent automation and in-store real-time
analytics could translate into operational optimization and cost savings. AI
is extensively used to increase customer traffic. Inventory and replenishmentmanagement get automated. Actionable insights for succulently increasingprofits, and scores of means for optimizing store layout, shelves and displays,
etc. Further on, embarking on personalized promotions and nurturing per-
sonal and professional relationship with customers, etc. gain momentum.
Computer and machine vision functionalities come handy here. IoT
devices, sensors and systems are enabled to have the much-needed vision
power. Such vision-enabled systems and environments are capable of
bringing forth a number of breakthrough facilities. The clear business ben-efits of AI-based data analytics have made retailers to keenly adopt thepath-breaking AI technologies and tools. The business and technology ben-efits of edge AI technology are being articulated and accentuated across.
AI-based video processing on clusters of servers in the cloud with high
latency is performed at high cost. Also, the electricity consumption andheavy heat dissipation into our fragile environment are on the higher side.Considering the various limitations widely discussed, the attention gets
turned toward running AI frameworks on multiple yet integrated devices
locally. That is, video data streams are run in IoT edge devices and serversin order to facilitate real-time data capture, storage, and processing in orderto extract insights in time.
Keeping things local and under the retailer’s direct control is exciting
shoppers to gain real-time personal experience. Through the breakthroughpower of edge AI, the retail field is experiencing a renaissance of edge-native,people-centric, and intelligent device services. There are a number of enablingtechnologies such as cloud-native computing, serverless computing, DevOps,
and reliability engineering. Cloud-native applications are being realized
through microservices architecture (MSA), containerization-enablementand container orchestration platform solutions. Especially the Kubernetes24 Pethuru Raj and Jenn-Wei Linplatform emerges as the most sought-after platform for running containerized
applications across different and distributed cloud environments that include
on-demand, online, off-premise/on-premise clouds. With the lighter versionsof Kubernetes, it is possible to create and manage edge clouds, which areformed out of edge devices (heterogeneous). Thus, edge device clouds hostingAI libraries collect and crunch device and environment data in order to pro-
duce real-time services.
Intelligent video analytics turn data into significant insights that translate
into better performance for retail operations. There are powerful and parallelprocessor architectures and intelligent camera solutions to do deep learning
tasks such as face recognition, image segmentation, object detection and
tracking, and classification for a host of applications such as
Customer flow analysis —This is for people counting and to gain a
deeper understanding of customers visits and their preferences.
Queue detection —This is for reducing waiting time and to empower
service staff with all the information to do their tasks efficiently.
Smart advertising —This is for enabling classification and association
tasks to guarantee targeted and personalized product promotion.
Inventory and Replenishment management —AI-inspired action-
able insights for visual merchandising, shelf management and stockmanagement.
Security & privacy —This is for ensuring the tightest security of people
and store assets. Access control and blockchain database for guaranteeing
unbreakable and impenetrable security and safety for stocks, bundles,
packets, etc. for
There are several innovations happening concurrently. Powerful processarchitectures including the classical CPUs, GPUs, TPUs, VPUs, etc.
Breakthrough processors for edge devices have laid down a stimulating
and scintillating foundation for sophisticated retail applications and servicesto attract customers to physical stores. The personal, social and industrial usecases of edge AI are steadily growing and drawing the attention of business
executives as well as IT professionals.
8. Edge AI for smarter cities
The implications of edge AI are greatly diversifying. With the accu-
mulation of multifaceted edge devices in our everyday environments such as
homes, hotels, hospitals, etc., the computing is all set to expand beyond its
jurisdiction. That is, computing moves from centralized cloud servers to25 Exploring the edge AI spacedecentralized edge devices. Besides proximate data processing, a number of
other benefits are being accrued through such a strategically sound transi-
tion. AI-instigated data processing is happening in edge devices in orderto build and deliver edge-native applications in time. The long-drawn auto-mation and augmentation tasks are being neatly accelerated through thesmart leverage of proven and potential data analytics platforms, products
and processes.
As we all know, there is a growing population of people migrating to
cities in search of several things. Therefore, our cities are getting thicklypopulated and therefore there are a number of challenges and concerns
widely expressed. Resource utilization has to be efficiently done. More facil-
ities have to be created with advanced social, cyber, healthcare, and connec-tivity infrastructures. Garbage, drainable, water and energy managementactivities have to be advanced in order to meet up the growing population.
Cities have to be made livable, lively and lovely. All kinds of citizen-centric
services have to be made available with all the care and clarity. Safety andsecurity, mobility and transportation, air quality, noise pollution, employ-ment opportunities, green spaces, etc. are all needed to sufficiently enhance
the quality of living. With the surge of edge AI-enablement methods, intel-
ligent and intimate services are being developed and delivered instanta-neously. Concierge applications will abound in the days to come. Cityprocess automation will pick up and gain speed with the technologicaladvancements.
With digitizing and edge technologies in plenty in and around us, every-
thing becomes digitized. Devices are getting connected with one anotherand with digitized entities. All kinds of ground-level disposables, implant-able, wearables, handhelds, portables, etc. are being digitized and integrated
with remotely held business workloads, IT services and databases. Especially
software applications deployed on on-demand, online, and off-premise/on-premise IT environments are empowering and energizing physicalobjects. Thus, digitized elements, connected devices, and cloud-hosted
applications combine well to automate most of the manual activities in set-
ting up and sustaining smart cities. With edge AI revolution, intelligentdevices become the new normal accelerating the fulfilment of smart cities.Sensors fuse with other sensors and actuators in the vicinity and with remoteentities through one or other networking options to create and sustain
next-generation capabilities. With real-time AI processing, an arsenal of
fresh services and applications will be unearthed and delivered to cityresidents.26 Pethuru Raj and Jenn-Wei LinEmergency services will get the technological boost. Incident or accident
response times will be very minimal. Inland security will be sufficiently
strengthened through the new breed of surveillance cameras, which areintrinsically enabled with advanced deep learning capabilities. Thus, securitycameras can collect a lot of data and analyses it quickly and automatically toemit out actionable insights in time. Edge devices are being empowered to
form dynamic, ad hoc, purpose-specific, and efficient clusters in order to
tackle complex problems. Especially data analytics consume a lot of compu-tation and storage. Through the quick formation of edge device clouds,complicated edge data analytics get performed in order to simplify and speed
up knowledge discovery. As discussed elsewhere, edge devices fitted with
computing, communication, data storage and analytics capabilities are layingdown a strong and sustainable foundation for envisaging sophisticated appli-cations. Edge analytics complements cloud-based data analytics. The speed,
scope, and sagacity of future applications will be clearly understood by
people.
The city traffic management will be digitally transformed. All kinds of
traffic crises can be predictably and pre-emptively handled to smoothen
the vehicle movement across city roads. IoT edge devices instrumented with
AI capability are to contribute immensely for intelligent traffic management.With edge AI, self-driving cars are to hit the road soon. The noteworthypoint here is that the technologies and tools are emerging for simplifyingthe transition of data to information and to knowledge.
Road signage is an essential tool for managing traffic in cities. Such an
arrangement alerts drivers to hazards and blockages and offers alternateroutes and updates on delays. Surveillance cameras can analyses road signageto enhance its accuracy. Edge analytics comes handy in automating several
aspects associated with road traffic. Poorly managed traffic is disastrous for
the fragile environment. Air quality is deteriorating, noise pollution is dan-gerous for city dwellers, precious time gets wasted due to the delays and traf-fic snarls, etc. With the random deployment of sensors along the road is
helping out in solving the traffic problem. So, data collection and processing
in real time using multiple and heterogeneous sensors and actuators go a longway in succulently surmounting the city problems. With edge AI, the peoplehealth and wellbeing are bound to go up significantly.
Citizen safety and security —Fire alarms, gas, pressure, temperature,
and humidity sensors and the faster spread of CCTV cameras across city areas
in conjunction with cloud-based city monitoring, measurement and man-agement systems are promising to enhance the security and safety of people27 Exploring the edge AI spaceand properties. AI-inspired video surveillance and analytics play a vital role
in appropriately enforcing law and order. The solidity of edge analytics and
increasingly sophisticated sensors are delivering more value to those who arefocused on keeping a city safe and secure.
Acoustic sensors can sense and send out alerts to road users in time. This is
in relation to the specific noises such as vehicle accidents, the sudden appli-
cation of car breaks, gunshots, glass breaking, etc. Edge analytics can monitor
a video stream and spot anomalies, unusual patterns, specific objects, and sus-picious behavior. The edge processing can then raise an alert or notificationto the concerned in time in order to avoid any kind of untoward incidents.
Public address systems can be integrated with sensors, cameras and other
devices to broadcast any kind of emergency situations such as the intrusionof criminals into the city areas.
Edge analytics can enable a smart city municipality to better manage and
conserve precious resources like energy, water and fresh air. Analytics on top
of IoT sensors in water systems and waste management systems will enablebetter monitoring and management. Innovative electric grids will increaseenergy efficiency for businesses and consumers alike. Edge analytics will also
help in the monitoring and controlling of building operations like HVAC,
lighting, and security to enable best possible living environment to thebuilding occupants.
Traffic flow, parking space availability, utility usage and public streetlight
management can be monitored by using IoT sensors on 5G network.
Authorities can leverage edge analytics to find practical solutions to conserve
energy, optimize water and power resources, and reduce environmentalimpact. Gradually, minimum traffic congestions and improved waste man-agement will entice new residents and hence increase economic opportuni-
ties within the community.
Edge analytics and Edge AI enable advanced and secure video, sensor and
communication systems to proactively monitor public spaces and law &order. Sensors embedded in critical infrastructure such as bridges and power
plants can monitor structural data to identify potential dangers, protecting
citizens and assets. Sensor-equipped drones can monitor vehicular traffic,crowds, construction sites and disaster areas to help monitor conditionscontinuously and support first responders.
Edge analytics will be the key enabler for the autonomous vehicles rev-
olution using the Internet of Vehicles. Vehicles on the road will communi-
cate with each other and with road infrastructures, improving overall roadsafety. It will also lead to reduced traffic congestions and enhanced driver28 Pethuru Raj and Jenn-Wei Lincomfort. Cars enabled with cloud connectivity can get a number of newer
features. Edge AI-enabled cars can exhibit several real-time capabilities.
Inertial and environmental sensors in smartwatches and fitness bands withdeep learning (DL) capability can respond to local events. Wearable devicescollect a lot of data on human activity, location, body parameters, etc. andalert their wearers. Video analytics within automobiles help in detecting and
alerting distracted and drowsy drivers through the eye position and the state
of the eye.
9. Edge AI for telecommunication
The Internet of Things (IoT) has matured fast to support and become
an inseparable part of the telecommunication industry. Self-driving vehicles
and the industrial IoT applications are the most prominent ones for substan-tiating the beneficial fusion between the IoT paradigm and the telecomdomain. The telecom industry, in fact, has adopted the IoT idea strongly
in order to be right and relevant to their customers and consumers. The
IoT has laid down a stimulating foundation for enabling machines anddevices to interact with one another to bring forth next-generation servicesto the humankind. Automated vehicles, augmented and virtual reality (AR/VR) and industry 4.0 applications will see the grand reality with the aspect of
edge intelligence gaining solid ground. The nexus of IoT devices, 5G com-
munication and AI frameworks can lead to hitherto unheard applications forindustry verticals and common people. On-device intelligence will becomethe new normal. Here are some global IoT trends that are going to influence
the telecom sector.
Telecommunication unifies Big and Streaming Data Analytics —
As indicated above, the IoT paradigm creates a massive amount of multi-structured data. Smart phones and connected devices generate a lot of data.
There are big and streaming data. Further on, there are big data analytics
(BDA) platforms in plenty from the open-source community. There arestreaming data analytics and streaming databases to emit out. Cloud serviceproviders are setting up cloud environments across the world and commu-
nication service providers are communication networks for integrating
ground-level IoT devices and sensors with cloud-based analytics applica-tions. Thus, streaming data emanating from IoT devices gets integratedcloud-based historical data to enable comprehensive data analytics to bringforth actionable insights for short-term as well as long-term decisions.29 Exploring the edge AI spaceThe arrival of 5G communication standard has clearly enhanced the
power and the value of IoT devices and data. Advanced 5G technologies
(CAT-M1 & NB-IoT) facilitate the integration of IoT devices and sensorswith faraway as well as nearby cloud platforms for data storage and analytics.Long-Term Evolution for Machines (LTE-M), an LTE upgrade supportslow complex CAT-M devices and ensures improved battery life.
Narrowband IoT (NB-IoT) is a Machine-To-Machine focused Radio
Access Technology (RAT) that enables huge IoT rollouts. It also ensureslast-mile connectivity using its extensive-range communication at lowpower. The use cases of these 5G technologies are exploding. For example,
NB-IoT is used for monitoring street lights, for waste management, and
remote parking. NB-IoT seamlessly tracks all kinds of pollution includingwater, land, and air for up keeping the environment’s health. NB-IoT scru-tinizes alarm systems, air conditioning, and complete ventilation system.
Telecom equipment may get affected due to extreme-level weather,
fire, or any other cause (natural or man-made). The physical security oftelecom assets is very important for ensuring the continuous delivery ofthe communication facility. The cyber s ecurity implications have to be also
calculated. With IoT sensors getting a ttached on these mission-critical arti-
facts, there is a huge scope for solid improvement. The blooming block-chain technology is also promising to ward attenuating c yberattacks. By
capturing all kinds of the IoT data and subjecting them locally andremotely, it is possible to extract value-added insights for preventive
and predictive maintenance of tele com equipment. By installing smart
cameras in and around the tower area and other locations housing telecomequipment, the physical security of telecom assets can be fully ensured.Edge AI contributes immensely for any thing that requirement real-time
decision-making and action.
Considering the huge impact of the path-breaking AI paradigm, light-
weight AI frameworks and libraries are plentiful these days. Such a transitionsimplifies the process of installing AI software into all sorts of IoT edge
devices and sensors. There are highly miniaturized and hugely improved
chipsets to specifically enable native AI processing. All these noteworthyadvancements ultimately result in real-time edge device data analytics.TinyML is emerging as a popular subject of study and research. EdgeAI is all about creating, evaluating and optimizing AI models for a
variety of systems and applications. Currently, modelling happens in cloud
environments as it requires a lot of computational resources. The tested and30 Pethuru Raj and Jenn-Wei Linrefined model is then taken to IoT edge device to do the inferencing on fresh
data. Creating and sustaining highly accurate prediction models is the most
common aspect of AI. Any kind of anomalies/outliers in telecom equipmentand networks can be proactively pinpointed to ensure their continuousfunctioning. By attaching appropriate IoT sensors, telecom assets can beremotely monitored, measured and managed. Remote access, theft exposure,
and fraud examination can be enabled through AI-attached IoT systems.
New-generation gadgets, drones, digital assistants, robots etc. are ade-
quately enabled through IoT sensors in order to be communicative and cog-nitive. Telecommunication plays a very vital role in shaping all kinds of
devices, appliances, equipment, machineries, etc. Device integration (local
and remote) is being empowered through connectivity and communicationcapabilities. AI-powered digital transformation is being speeded up throughambient, unified, and adaptive communication.
The telco industry is heavily using AI for a variety of improvements and
improvisations. Monetizing the edge capability, saving costs through hyperautomation and improving customer experience and engagement. AI hasbecome an integral part in conceiving and concretizing breakthrough tele-
com services. AI contributes in providing cognition-enabled connectivity
services.1.
Monetizing at the edge —Having understood the immense potential
of edge computing, communication service providers are keen onexploring and exploiting this unique phenomenon for their benefits.
IoT devices are emerging as the next-generation input/output (I/O)
device. Delivering a spectrum of low-latency telecom services in timeis enabled through a host of IoT devices. With the impending arrivalof 5G communication, IoT edge devices can deliver their services in
greater speed. For example, in the healthcare domain, with a mesh of
IoT devices and sensors, patients’ physical parameters can be collectedduring the ambulance ride and transmitted to the doctors’ devices so thatquick remedial actions can be contemplated and applied once patients
reach the hospital premises. Speed is the differentiator. Local analytics
to envisage correct and context-aware actions is being facilitated throughAI running on edge devices.
Further on, first responders can use intelligent drones to rapidly and remotelyassess emergency situations with real-time streaming video and audio even
before they arrive at the place. ATM machines can pre-emptively alert banks
if there is a fraudulent attempt. Thus, edge use cases are growing fast.31 Exploring the edge AI space2.Saving costs and boosting efficiencies —There are a number of
technological evolutions and revolutions such as containerization, con-
tainer orchestration platform solutions such as Kubernetes, resilient mic-roservices communication frameworks such as Istio, DevOps toolsets,etc. All kinds of log, operational, performance, security, health conditiondata of business workloads and IT services are meticulously collected and
analyzed deeply and automatically through the smart leverage of delec-
table AI advancements. AIOps is a new field of study and research and forcleverly using AI for automating and accelerating information and com-munication technologies (ICT) operations. Such a technological
empowerment brings in the much-needed productivity, affordability,
speed, and sagacity for service providers as well as consumers. Thereis a bright scope for the new idea of NoOps to flourish in the days tocome. That is, network infrastructure modules can be minutely moni-
tored and if there is any deviation from the prescribed limit, then a
proper notification will reach the concerned so as to plunge into appro-priate counter measures in time.
Network virtualization is enabling worldwide network service providers
remarkably. There is a consistent transition from physical network functions
(PNFs) to virtual network functions (VNFs) and cloud-native networkfunctions (CNFs). A number of advantages such as network flexibility,affordability, availability, reliability, scalability, and maneuverability arebeing accrued through such evolutions.
3.
Improving client engagement —Customer-centricity and propensity
become the new normal. Customer experiences and engagements go upsharply. The long-standing goals of customer delight and ecstasy arebound to see the grand reality with the tight integration with AI.
Edge computing delivers real-time experience for users. AI-enabled
chatbots can answer complicated questions correctly. Network distur-bances can be proactively understood to avoid any hiccups. AI systemscan process large amounts of data, in particular call detail records (CDR),
in the case of the telecommunication industry, identify patterns, detect
and predict network anomalies.
Virtual radio access network (vRAN) —As noted above, the concept of
virtualization (divide and conquer) is acquiring greater significance thesedays. It has permeated into everything significant. Like the virtualization
of network functions has enabled telecommunication networks to be mod-
ernized, the radio access networks (RANs) is also virtualized in order to availsignificant credits and profits. Through open and disaggregated RANs,32 Pethuru Raj and Jenn-Wei Lintelecom companies can simplify network operations and improve flexibility,
availability, and efficiency while serving a large number of end-user devices
and bandwidth-hungry applications. Cloud-native and open RAN solutionsoften lower costs, improve ease of upgrade and modification, and scale hor-izontally. As per the research reports, 6G communication will heavily relyupon the unprecedented successes of AI to be hugely distinguished in their
services.
Edge analytics and Edge AI add the data analytics capability to edge
devices. Edge analytics use cases are fast growing: cognitive devices, smarter
homes and cities, autonomous cars and industry automation.
10. Conclusion
Edge AI is all about bringing AI-inspired processing of edge device
data in order to emit out actionable insights out of data heaps in real time.The extracted intelligence can be used for producing real-time and intelli-
gent edge-native applications. Further on, edge devices are slated to become
cognitive in their operations, offerings and outputs through the tight inte-gration with AI. With intelligent edge devices abound in a system and envi-ronment, the era of firming up intelligent environments such as digitallytransformed homes, offices, manufacturing floors, retail stores, railway
stations, eating joints, entertainment plazas, shopping malls, etc.
Further reading
[1] Edge Analytics in 2022: What it is, Why it matters & Use Cases. https:/ /research.
aimultiple.com/edge-analytics/ .
[2] Edge Analytics –The Pros and Cons of Immediate, Local Insight. https:/ /www.talend.
com/resources/edge-analytics-pros-cons-immediate-local-insight/ .
[3] What Is Edge AI and How Does It Work? https:/ /blogs.nvidia.com/blog/2022/02/17/
what-is-edge-ai/ .
[4] Edge AI –Driving Next-Gen AI Applications. https:/ /viso.ai/edge-ai/edge-ai-
applications-and-trends/ .
[5] Edge-native applications: What are they and where are they used? https:/ /www.edgeir.
com/edge-native-applications-what-are-they-and-where-are-they-used-20210912 .
[6] Edge-native development best practices. https:/ /www.ibm.com/docs/en/eam/4.0?
topic¼reading-edge-native-development-best-practices .
[7] The role of cloud in edge-native applications. https:/ /www.f5.com/company/blog/the-
role-of-cloud-in-edge-native-applications .33 Exploring the edge AI spaceAbout the authors
Pethuru Raj working as a chief architect at
Reliance Jio Platforms Ltd. (JPL) Bangalore.
Previously. worked in IBM global Cloud
center of Excellence (CoE), Wipro consult-
ing services (WCS), and Robert Bosch
Corporate Research (CR). In total, I have
gained more than 20 years of IT industry
experience and 8 years of research experi-
ence. Finished the CSIR-sponsored Ph.D.
degree at Anna University, Chennai and
continued with the UGC-sponsored post-
doctoral research in the Department of
Computer Science and Automation, Indian Institute of Science (IISc),
Bangalore. Thereafter, I was granted a couple of international research fel-
lowships (JSPS and JST) to work as a research scientist for 3.5 years in two
leading Japanese universities. Focuses on some of the emerging technologies
such as the Internet of Things (IoT), Optimization of Artificial Intelligence
(AI) Models, Big, fast and streaming Analytics, Blockchain, Digital Twins,
Cloud-native computing, Edge and Serverless computing, Reliability engi-
neering, Microservices architecture (MSA), Event-driven architecture
(EDA), 5G, etc. My personal web site is at https:/ /sweetypeterdarren.
wixsite.com/pethuru-raj-books/my-bo oks https:/ /scholar.google.co.in/
citations?user ¼yaDflpYAAAAJ&hl ¼en.
Dr. Jenn-Wei Lin is currently a full profes-
sor in the Department of Computer Science
and Information Engineering, Fu Jen
Catholic University, Taiwan. He received
the M.S. degree in computer and informa-
tion science from National Chiao Tung
University, Hsinchu, Taiwan, in 1993, and
the Ph.D. degree in electrical engineering
from National Taiwan University, Taipei,
Taiwan, in 1999. He was a researcher at
Chunghwa Telecom Co., Ltd., Taoyuan, Taiwan from 1993 to 2001.
His current research interests are cloud computing, mobile computing
and networks, distributed systems, and fault-tolerant computing.34 Pethuru Raj and Jenn-Wei LinCHAPTER TWO
Edge computing:
Types and attributes
Sunku Ranganath
Intel Corporation, Hillsboro, OR, United States
Contents
1.Introduction 36
2.Internet of Things (IoT) edge 37
2.1 Device type and mobility 40
2.2 Service capabilities 40
2.3 Security and privacy 41
2.4 Latency requirements 41
3.On-premises edge 41
3.1 Disaggregation 42
3.2 Network requirements 43
3.3 Scalability 45
3.4 Security 45
3.5 Life cycle management 45
4.Wireless Access Edge 46
4.1 Scalability 46
4.2 Distributed RAN functions 46
4.3 Service models 47
4.4 Intelligent RAN 48
5.Network edge 48
5.1 Next Generation Central Office 49
5.2 Wireline fixed access edge 50
5.3 Physical locations 51
6.Challenges in edge computing 51
7.Multi-Access Edge Computing 55
References 61
About the author 62
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.03.00135Abstract
The chapter on “E d g ec o m p u t i n gt y p e sa n dA t t r i b u t e s ”provides introduction to var-
ious types of Edge computing by broadly classifying Edge in to four types, based onround trip latency requirements, (1) IoT Edge (2) Wireless Access Edge (3) On PremiseEdge (4) Network Edge. Requirements and attributes of each of these edge compute
types are discussed. Chapter then details the practical challenges across these Edge
deployments and further explores how ETSI MEC specifications helps address these
challenges.
1. Introduction
The paradigm of edge computing enables connectivity between
plethora of devices and core of the telecom data center in a manner that pro-vides low latency communication and enables multiple computing services
to be brought closer to the consumers. This requires edge compute archi-
tecture to be highly distributed and customized to be suitable and adaptablefor various device needs. Use cases such as immersive media, AugmentedReality (AR) and Virtual Reality (VR), etc., require low latency and high
bandwidth connectivity without loss of frame data. Industrial applications
such as autonomous robots, machine to machine interaction, etc., requirelow latency, real time computation and connectivity leveraging TimeSynchronous Networks (TSN). Vehicular communication such as autono-mous vehicles, UAV connectivity, drone communication, etc., require low
latency & distributed compute power spread across multiple geographic
points of interest. Use cases such as smart retail kiosks with facial recognitionor smart cameras with scene intelligence or smart cities that provide ubiq-uitous connectivity, etc., require heavily distributed compute power that
can simultaneously interact with multiple categories of devices and ability
to process huge volumes of data in short time while protecting privacyand identity of the end user. Use cases such as smart hospitals, remotehealthcare, intelligent medical devices, etc., require private wireless deploy-
ments with low latency communication while protecting patient confiden-
tiality and adhering to federal laws.
To facilitate various use cases, so me of which are mentioned above, the
edge computing infrastructure canno t be a monolithic architecture with
set of compute servers as in a datacenter model but needs to be dis-
aggregated into multiple layers of ac cess points based on some of these
aspects below:36 Sunku Ranganath–physical location
–security and privacy requirements
–round trip latency
–connectivity management
–scale requirements
–on premises requirements
–data locality
–real time communication requirements
–proximity to the end user
–service capabilities and many more.
The dissection of edge computing based on said factors heavily impacts the
architecture,
design, and deployment considerations for practical application
purposes. Its highly imperative that the end user considers these require-ments first before upgrading their existing infrastructure.
One easy way to understand various types of edge compute is based on its
proximity to the end device and round-trip latency to core of the datacenter,as indicated in Fig. 1 [1] . To broadly understand types of edge compute,
consider
the division in to following categories:
–Internet of Things edge
–On premise edge
–Access edge
–Network edge
2. Internet of Things (IoT) edge
This section of edge compute broadly categorizes plethora of devices
that are interconnected via common network infrastructure (either public orprivate) that enables these devices to be smart enough to communicate witheach other and possess possibility of enough intelligence to make autono-
mous decisions for its operations. Broad set of devices with connectivity
fall under this category, such as, intelligent and/or autonomous vehicles,mobile communication devices, autonomous robots, factory equipment,retail kiosks, smart cameras, connected city devices (such as intelligent
streetlamps, street sensors, smart parking meters, etc.), smart buildings, trans-
portation and logistics devices, intelligent cameras, smart energy devices,intelligent health care equipment and so on, as shown in Fig. 2 [2] . This type
of
edge is usually called “Far edge” from the perspective of core of the
telecom network.37 Edge computing: Types and attributesFig. 1 Types of edge compute and corresponding latency expectation.One of the primary requirements for segregating multiple devices in to
IoT Edge is its latency requirements. The typical round-trip latency for this
type of edge computing is less than 1 millisecond (ms). Primary advantages of
establishing IoT Edge:
–Accelerate development of innovative services that can interact and create
value from data using millions of devices at the edge
–Provide capability to test, build, run and scale AI/ML models for various
use cases
–Capability to operate locally to adhere to various city/state/country wide
regulations as the data does not have to cross the pre-determined boundaries
–Option to utilize innovative hardware infrastructure such as GPUs,
Infrastructure Processing Units (IPUs), Data Processing Units (DPUs), etc.
–Enables new service revenue models across industries as it brings together
Operational Technology (OT) and Information and Communication
Technology (ICT) companies
Fig. 2 Example of things and devices across IoT Edge.39 Edge computing: Types and attributes–Ability to remotely monitor and manage multiple edge devices through
an intelligent and automated way
–Provides ability to root cause and contain any major security or privacy
threats across the devices
Depending on the nature of the nature of the usage and nature of the busi-ness domains they are applied towards, following aspects become key part of
their architecture and design:
2.1 Device type and mobility
The IoT Edge vastly extends to many of the day-to-day devices that humanbeings use or interact with. With the shift towards automating majority of
the mundane tasks that humans perform, much of these devices are enabledwith various levels of intelligence to augment them with enough capabilitiesto make them independent. While devices such as mobile phones, facial rec-ognition devices, etc., are well known, the roll out of 5G is enabling
multiple new classes of devices across various sectors.
Autonomous vehicles are an example of mobile devices that require tre-
mendous amount of data transactions every second and very low latencyrequirements with V2X communications while managing the mobility
handoffs across geographical regions. Smart city type deployments utilize
multiple intelligent devices, like cameras that can detect riots on the streetsor smart lamp posts that provide ubiquitous connectivity, etc., which areinstalled in a single location and don’t have low latency mobility require-
ments and require aggregation points like gateways so that the data can
be analyzed and acted upon. The architecture for edge deployments highlydepends on the interaction among these devices and to the gateway node oredge server and communication protocols used between these entities.
2.2 Service capabilities
With the advent of 5G, devices today have ability to utilize frequency spec-trum that provide high bandwidth and can utilize Ultra Reliable LowLatency Communication (URLLC). Not only that the devices can have
intelligent capabilities, but new and innovative service models could be
enabled Over The Top (OTT) on these devices. For example, new expe-riences such as immersive media, Artificial Reality (AR) and Virtual Reality(VR) could be enabled as services on mobile devices or intelligent vehicles
or virtual reality goggles, etc. without constraints of limited set of geographic
boundaries. Things and devices could be broadly classified based on the40 Sunku Ranganathamount of service capabilities that they can enable across multiple business
domains of industrial, retail, autonomous transport, hospitality, entertain-
ment, health care, etc.
2.3 Security and privacy
Privacy becomes the key tenant in enabling services on the various devices
and things across IoT edge. Due to scale of number of devices that are con-
nected in thousands to millions at any given edge location, ability to protectidentity and user information of each individual device across multiple clas-ses of devices and the services that run on them is crucial. Security in terms ofauthorization and authentication across each of the layers of services con-
sumed and services offered need to be baked into each of the devices during
the lifetime of their connected existence. Edge deployments need to con-sider the scale at which the security and privacy needs to be enabled acrossthe devices.
2.4 Latency requirements
The nature of applications and service models at the IoT edge generally is toaggregate and process the data from multiple devices and things and provideactionable insights using which necessary actions could be triggered. The
nature of these data points generated could be once a second to minutes
or hours or days. The crucial aspect of IoT edge is to bring compute closerto these devices and perform necessary analytics to provide insights in realtime. Leveraging gateway-based aggregation points, efficient data transport
protocols such as MQ Telemetry Transport (MQTT) and 5G front haul,
latency requirement of 1ms round trip latency could be enabled.
3. On-premises edge
Consider an industrial manufacturing floor that has mix or autono-
mous and pre-programmed robots along with various sensors on an assemblyline that generate thousands of data points every second which needs to be
aggregated, stored, processed to detect any anomalies or failures or to pro-
vide actionable insights to ensure smooth operations. Transporting the dataacross public networks or using centralized cloud data center to process thedata would completely defeat the requirement of providing low latency and
fast processing of data there by losing the value of the data that was gener-
ated. On-premises edge-based architectures help enable these type of use41 Edge computing: Types and attributescases by having compute resources on customer premises and/or at various
points of presence at customer site. Latency considerations for On-Premise
Edge is generally targeted to be less than 5ms.
Businesses such as large enterprises, industrial manufacturing floors, large
retail operations, etc., benefit from these types of deployments to have abilityto process the data close to point of its origin while still having proprietary
rights of owning the necessary hardware. The business models could be that
the on-prem equipment is operated by service provider or utilize a licensingmodel to utilize the necessary virtualized workloads or have necessaryresources to life cycle manage the hardware and software stack. This type
of deployments is usually popular for central office or branch office or point
of presence type scenarios. Some of the important architectural consider-ations for deploying and utilizing an on-premises edge are share below.Primary advantages of On-premise edge:
–
Reducing capital expenditure by reducing the need to buy purpose-built,
vendor specific hardware
–Reducing operational expenditure through decreased running costs(less space needed to house all the equipment, less power needed to
run them etc.)
–Saving time on long procurement processes with many different vendors
–Lowering the risk of rolling out new services by allowing providers to
trial and roll back services, as the customer needs them
–Negate the need for engineer site visits through Zero Touch Provisioning
–Leverage universal set of APIs across multiple types of Edge clouds andCore Network Cloud for customers to develop value add applicationson top
–Ability to utilize public cloud or hybrid cloud infrastructure and utilize
any of the “as-a-Service” cloud models
3.1 Disaggregation
On-premise edge plays an important role in the distributed computing
paradigm bridging the device ecosystem to the network edge and core datacenter in large scale enterprise/retail/industrial scenarios. An importantassociated with this type of deployment is that the sensitive and proprietary
data is contained and secured on-premises while still can leverage con-
nectivity and scalability of cloud computing. Traditionally network func-tions are run in a virtualized environment which are moving toward42 Sunku Ranganathcloud native DevOps model of managing the on-prem network functions.
Universal Customer Premise Equipment (uCPE) is a prime example of
on-premise edge compute that provide combination of firewall, WAN opti-
mizer, router within a single equipment that is deployed on-premise, as
shown in Fig. 3 [3] . Due to advances in Network Function Virtualization
(NFV) and ability for a service provider to remotely provision, update,
upgrade and life cycle manage these network functions, on-premise edge
is an enticing appeal.
3.2 Network requirements
Due to the nature of on-premise edge being the main interface between IoT
edge and core of the datacenter, it needs the ability to interface with the IoT
devices wirelessly and the ability to aggregate and transport the data securely
to core the data center. The wireless connectivity requirements could span
from leveraging private 5G wireless network or utilizing Citizens
Broadband Radio Service (CBRS) spectrum or traditional Wi-Fi mesh net-
work to connect to the corresponding set of devices, sensors, things, etc. as
the data is aggregated, its packaged to be send to backend public or private
cloud in most cases or utilize network edge infrastructure which in turn
hauls the data to cloud environment. Software Defined—Wide Area
Network (SD-WAN) is a prime example of software defined management
of WAN services that allows enterprises to utilize combination of MPLS or
LTE or broadband internet services. Figure 4 [4] provides high level con-
nected architecture of SD-WAN with its open standard APIs providing
capability to do zero-touch service lifecycle management.
New everything can be controlled with a universal
CPE, placed in a central or branch location.
Fig. 3 Consolidation of multiple network functions on a uCPE.43 Edge computing: Types and attributesFig. 4 High level architecture of SD-WAN with its Open Interfaces.3.3 Scalability
The primary purpose of on-premise edge is to consolidate multiple network
functions into limited set of hardware and install them at specific set of points
of presence. The demand for data processing has to be gauged ahead ofdeploying these at appropriate locations in the overall network. WhileScaling the network services on demand within on-prem edge is a challenge,
much of the heavy lifting would be done using the connected cloud
environment to handle unexpected spike in the end user activity.
3.4 Security
The ability to remotely deploy and manage network functions on theon-prem equipment opens the doorway for multiple security-based net-work functions to be configured and managed at run time. Networkfunctions such as virtual Next Generation Firewall (vNGFW), virtualDeep Packet Inspection (vDPI), Denial of Service Attack prevention mech-
anisms, policy based secure access, etc., could all be delivered from cloud
to the on-prem equipment on-demand basis to secure the traffic per use casebasis. A combination of hardware-based security features, security networkfunctions and communication security are necessary to secure overall
deployment. Secure Access Service Edge (SASE) is a brand-new paradigm
that is evolving the security at the edge to move towards Zero Trust basedsecurity solutions. SASE is the combination of network security functions,such as SWG, CASB, FWaaS and ZTNA, with SD-WAN capabilities to
support the dynamic secure access needs of organizations. These capabilities
are delivered using as-a-Service model based on the identity of the entity,real time context and security 5and compliance policies.
3.5 Life cycle management
One of the primary advantages of utilizing on-prem edge is its ease of man-aging the life cycle of network functions either in cloud native microservicebased deployments or as virtualized deployment, by the service provider.
The end user could offload all the necessary tasks of upgrading, updating,
configuring, bootstrapping the infrastructure to the service provider andall of these can be done remotely once the equipment is installed. This appealof providing low latency data management on-premises and ability to con-
stantly update the network functions to the latest set of requirements, has
increased the adoption of on-premise edge computing.45 Edge computing: Types and attributes4. Wireless Access Edge
Using the advances in NFV, the once traditional Radio Access
Network (RAN) that was available as a fixed function device has now been
disaggregated to run as set of virtual functions in software. This reduces thecost by avoiding proprietary hardware and utilize Commercial Off The Shelf(COTS) servers. RAN is the crucial point that connects wireless devices to
the core the operator’s network. Concepts like virtual RAN (vRAN) and
industry initiatives such as Open-RAN and ORAN have enabled interfacesto manage these virtualized deployments similar to managing any other edgedevice. Move towards cloud native instantiation of RAN function is easing
the life cycle management of broad set of RAN deployments using
Continuous Integration (CI)/Continuous Deployment (CD) constructsleveraging DevOps models. The set of infrastructures necessary to deployand manage software RAN functions could be broadly termed as Access
Edge. Some of the architectural considerations for designing and deploying
Access Edge are as below.
4.1 Scalability
One of the most important benefit of dividing the RAN functions into indi-vidual software components is the reduction in costs when operating RANdeployments at scale. Few important aspects that help manage infrastructureusing NFV for RAN:
–
Ability to manage RAN infrastructure in an agile manner like
infrastructure-as-a-service or platform-as-a-service models
–Utilization of less hardware as these network functions could be servicechained and co-located in a performance and energy efficient manner
–Provides ability to instantiate, move or bring down workloads with min-
imal effort using suitable orchestrator
–Provides ability to elastically scale the necessary resources to address
changing network demands
4.2 Distributed RAN functions
3GPP specifications for 5G have enabled RAN into a functional split asRadio Unit (RU), Centralized Units (CU) and Distributed Units (DU).
CU takes of backhaul network connectivity to the 5G core network and
connects to DU as midhaul, while DU connects to RU and takes care offronthaul network. This has enabled network operators to deploy these units46 Sunku Ranganathin a customized fashion to satisfy scale requirements, real-time performance
management, optimize QoS for variety of service needs (for example, gam-
ing, voice, video, etc.) and ease the operations & life cycle management.Operators can now satisfy variety of latency tolerance levels and customizedthe transport for different deployment scenarios like rural or urban that havedifferent transport access like fiber.
ORAN consortium has laid out a standardize architecture that divides
the RAN into following sections with relevant components:–Radio side that includes Near-Real Time RAN Intelligent Controller(R
IC), O-RAN Central Unit, O-RAN Distributed Unit and O-RAN
Radio Unit
–Management side that includes service management and orchestrationframework
that contains Non-Real Time RIC
These entities are further interconnected using set of interfaces such as O1 or
E2
or F1, etc.
4.3 Service models
The disaggregated RAN model has now enabled various service models that
invites operators, service providers and third-party vendors to enable new ser-vice models providing value add on top of RAN deployments. O-RANintroduces concept of xApps that can run on Near Real-Time RIC utilizingthe xApp APIs. Some of the very important functionality set could now be
provided using xApps such as radio connection management, mobility man-
agement, QoS management, conflict mitigation among xApps, subscriptionmanagement, security functions, user policy management, etc. This createsan ecosystem of vendors that can just provide unique differentiating xApps
or combination of xApps and RIC functions or CU/DU to the operator.
Testing the interoperability between these different vendors supplied compo-nents would also be a major service revenue model that can be utilized.
Another aspect of Near Real-Time RIC is the shared data layer with
connectivity to appropriate database provides software vendors provide cus-tom database software to satisfy huge set of data volume requirements withreal time nature of the RIC. With move towards cloud native deployments,Non-Real Time RIC could now be implemented using industry’s defactochoice of orchestrators such as Kubernetes or OpenStack enabling new
service models to onboard, orchestrate, secure and life cycle manage the
functions spanning Non-RT RIC and near-RT RIC. Figure 5 [5] provides
a
reference of how various O-RAN components can be deployed using
Kubernetes based install.47 Edge computing: Types and attributes4.4 Intelligent RAN
One of the important tenets of moving to disaggregated RAN [R] is to imbibe
the aspect of “intelligence” which is inherently provided by means of Artificial
Intelligence (AI) algorithms. O-RAN approach not only opens interfaces
between the distributed units but allows to reach management interfaces such
as Radio Resource Management (RRM) and Self-Organizing Networks
(SON) functions that can control radio resources and network operations.
This enables service providers to implement AI models for radio network
automation customized to various use cases across small cells and macro cells.
Use cases such as intelligent Traffic Steering, QoE optimization, Massive
MIMO optimization and QoS based resource optimization are considered
for Phase 1 development to address immediate needs of the operators.
Phase 2 of the enablement of intelligent RAN are focused on RAN sharing,
RAN slice SLA Assurance, context based dynamic handover management for
V2X, flight path based dynamic UAV resource allocation, and radio resource
allocation for UAV applications.
5. Network edge
As the compute power is brought closer to the end users of devices and
things, data needs to be aggregated from across multiple IoT edges,
On-premises edge, and Access edge catering to the specific region before
connecting to the centralized data center that can span across vast set of
regions. This type of deployments could be broadly called “Network
Edge” or near edge in relation to the core data center of the service provider.
Fig. 5 Kubernetes based deployment of O-RAN components.48 Sunku RanganathThe set of computes across network edge is usually called Edge data cen-
ters. These are deployed usually as nano Data Center (nano-DC) or micro –
Data Center (micro-DC) consisting of few to many server racks. Key design
and architectural considerations for managing these customized data centersand running services include, but not limited to:–ability to support fixed function and dynamic functions that spawn over
edge
and cloud
–ability to utilize resources distributed across different pools in edge
and
cloud
–ability to onboard, execute and life cycle manage “Edge Native Services”
(ENS)
which are usually deployed in form of microservices that can
adhere and adapt to constraints of edge compute
–ability to adhere to network requirements that are access agnostic and
edge
aware
–provide resilient fault tolerant platforms
–provide an easy service delivery vehicle using cloud native constructs suchas
distributed service mesh, containers, etc.
Edge data centers could be further classified based on deployment types to
cater
to needs of type of end user such as enterprise deployments, telecom
providers, city, and public usage models, etc. The deployment types at
network edge could be broadly classified as below.
5.1 Next Generation Central Office
The Central Office (CO) is usually the final node in the network managedby a carrier, or it can be a headend or hub from a cable company perspectiveor can be a set of baseband unit pools from a wireless carrier perspective.
Since COs are where all the traffic is aggregated, it is usually the contention
point and can be bottleneck that affects latency and throughput ofcorresponding services. Leveraging NFV, virtualization of CO addressesvendor lock-in issues and disaggregates traffic enabling value add services
and controls costs. To effectively leverage the benefits from ability to deploy
as network functions, COs can now expand towards utilizing edge datacenters for network management, business & operation support systems(BSS & OSS) and analytics. Reference architectures such as CentralOffice Rearchitected as Datacenter (CORD) provides an integrated
platform to create an operational edge data center with in-built service capa-
bilities. Figure 6 [6] provides an example of a NGCO mini data center
by
Intel.49 Edge computing: Types and attributes5.2 Wireline fixed access edge
The traditional wireline broadband that is governed by standards from
Broadband Forum is undergoing industry shift towards convergence with
rest of the 5G infrastructure by utilizing COTS servers and leveraging
NFV. The wireline broadband edge broadly consists of networks that deliver
broadband services to the subscribers such as IPTV, VoIP, internet services,
etc. It is at the edge of the broadband network that Border Network
Gateways (BNGs) are used to perform subscriber management such as ses-
sion and circuit aggregation, Authentication Authorization & Accounting
(AAA) management, policy and traffic management functions, multiplexing
and demultiplexing of traffic to and from individual subscriber, etc. Multiple
edge network models are available based on distributed nature of BNG.
Virtualizing cable headend and Cable Modem Termination System
(CMTS) functions by dividing it into Control-Plane and User-Plane
Fig. 6 Architecture of NGCO mini data center by Intel.50 Sunku Ranganathfunctions provides an easy way to migrate to edge cloud and deliver services
from a data center enabling flexible, efficient, resilient, and scalable means of
operating broadband architecture. Further architectural changes acrosswireline fixed access edge infrastructure towards cloud native include virtualConverged Cable Access Platform (vCCAP). vCCAP provides a shifttowards Distributed Access Architecture (DAA) that combines headend func-
tions of CMTS and Edge Quadrature Amplitude Modulation (EQAM) and
implements these in a virtualized or cloud native microservice based infra-structure models, thereby increasing the capacity and throughput. vCCAPdeployment models could vary across centralized and distributed model based
on space availability, bandwidth consumption and server usage efficiency. The
move towards cloud native Kubernetes based deployment models has furthermade it simpler to leverage unified set of life cycle management architectures.
5.3 Physical locations
The edge data centers are usually located at the bottom of a cell tower, tel-
ecom central offices, headend facilities of a cable company deployments,
parking garages, building rooftops, etc. These are spread across closer tothe end consumer of edge services and sized up based on the volume ofservices and traffic requirements.
6. Challenges in edge computing
As the 5G and IoT infrastructure are transforming toward unified set
of architectures, they deliver brand new set of experiences and use cases
using unprecedented compute power and distribute the intelligence across
various devices. Across all the types of edge compute that have been sharedearlier, one common architectural and deployment pattern is the movetowards cloud native microservice based models. Traditionally limited set
of hardware vendors and service providers, controlled end to end deploy-
ment of communication infrastructure. With the transformation towardsdistributed architecture and 5G advances multiple new business modelsand service delivery models are being enabled across different types of edges.
Products and services from multiple infrastructure and software vendors
could need to be working together providing a cohesive interoperabilitybetween various network functions and seamless scale up and scale out fromacross edge to cloud infrastructure. This brings forth various challenges to
deploy, scale and manage the edge computing paradigm. Some of the
challenges in utilizing edge computing are described below.51 Edge computing: Types and attributes1.Software Infrastructure
Transformation towards virtualized and cloud native models has
essentially converted edge infrastructure to as-a-service deploymentmodel using industry standard cloud orchestrators such as Kubernetes.Proprietary network functions now have a necessity to evolve towardsmicroservice based architecture for service-oriented deployment models.
These network functions need to be onboarded, tested for interoperabil-
ity and scale and life cycle managed just like any other software applica-tion while maintaining the Service Level Agreement (SLA).
2.
Unified Manageability Across Edges
As we noted earlier that edge computing could be divided into mul-
tiple segments based on traffic type, applications being serviced, deviceconnectivity and point of presence, there is an explosion of number ofedge computing zones that span across geographical areas. Each of the
edge computing zone that addresses specific area of geographical region
or set of network bandwidth (that correlates to servicing specific set ofdevices) needs an interoperable mechanism with other edge computingzones to provide seamless connectivity. This arises the need for unified
orchestration and life cycle management across these multiple clusters
and cloud regions.
3.
Public and Private Cloud
The utilization of cloud native technologies across multiple edge
scenarios essentially puts the service provider to enable private cloud
clusters across these edges. Howev er, hyperscale cloud service pro-
viders such as the likes of Microsoft Azure or Amazon WebServices, etc., provide the opportunity of hyperscale economicsleveraging public cloud constructs. Infrastructure could now be scaled
in an intelligent and cost-efficient manner while leveraging unified
Application Programmable Interface (APIs) across the infrastructureprovided by public cloud provider. Hybrid approach of leveragingpublic cloud for agile and flexible service offerings while being vendor
agnostic and utilizing private cloud t o utilize already existing in-built
infrastructure is still an area that needs to be proven out in terms ofefficiency and scale.
4.
Security and Privacy
Due to diverse nature of various types of edge deployments, the needs
of a secure edge have evolved into multi-faceted set of approaches that
needs to be customized per edge deployment. There is no one size fits allpolicy that can satisfy requirements across the edge types. Ability to52 Sunku Ranganathprovide Authentication Authorization and Accounting (AAA) across the
distributed edge requires multiple levels policies to ensure every end user
is accounted for. Privacy of an end user or end device is another criticalaspect to maintain as the traffic flows across the edge. The security archi-tecture needs to abstract out end user/device information as the datatravels to the core of the network. Zero trust security architecture is
one the latest paradigm with the belief that no aspect of data communi-
cation is secure as there are no trusted personas while providing secureabstractions starting from hardware root of trust, service to service com-munication, data security and end to end control of data access across the
network. Implementing these with real time low latency requirements
continue to prove to be a major challenge.
5.
Hardware Abstraction and Utilization
Edge infrastructure’s increased utilization of COTS servers has
revolutionized each of the network function across the various edge
types. However, this means abstracting out all the hardware features,accelerators, and any other enhancements available for virtualized net-work functions or cloud native microservices. The aspect of utilizing
the underlying hardware features is left to cloud orchestrators that
expose and maps the features to the network functions at the deploymenttime. With the advent of various as-a-service hardware models suchas Graphical Processor Unit (GPU) as a service or InfrastructureProcessor Unit (IPU) or in general x Processor Unit (xPU) as a service,
additional intelligence needs to be enabled for latency sensitive network
functions to fully utilize various hardware features with the ability to scaleacross the edge deployments.
6.
Value of Data
Distributed and disaggregated edge computing paradigm brings out
huge amounts of data across the end-to-end edge infrastructure. IoTedge, Access edge, On-premises edge, Wireless edge, Fixed Accessedge all have connectivity to end users that provide gold mine of data
to be analyzed to provide value added OTT type services customized
for the end user. The challenge however is that the value of datadecreases as the latency increases farther from the origin point. Thus,an efficient and low latency data processing and analytics mechanismsare needed at the distributed edges that are closer to the end users.
7.
AI & ML Models for Edge
To derive the value of the data generated across the various edges,
huge amounts of data points associated with a single end user or a single53 Edge computing: Types and attributesdevice need to be processed and analyzed at a constant time interval
(for example every second in an Industrial automation use case).
This calls for customized ML and AI models that need to be tailoredfor each of the edge type and management system that can apply appro-priate model for the necessary use case. There is a huge scope for inno-vation and development in this space for various use cases across
the edges.
8.
Life cycle management
Software based network functions are deployed across edge infra-
structure necessitates the operator to deal with packaging, onboarding,
deploying the network functions; storing, updating, testing for interop-
erability, ensuring high availability with zero downtime, error freeupgrading on the fly, chaos testing, demand-based scaling, supplyingadequate infrastructure resources, detecting anomalies at run time
and so on, which all constitutes the aspects of life cycle management.
Due to the nature of complexity at each of the edge type, move towardsmicroservice based containerized deployments instead of virtualmachines has significant benefits over managing virtual machines.
Some of the following practices are required to be customized towards
each type of the edge computing:/C14
DevOps practices: to deploy software across 100s to millions ofdevices automation is the key. DevOps provides methodologiesfor CI/CD necessary to test, install, upgrade, and manage at scale.
/C14DevSecOps: Security must play an important role in the develop-ment and operations lifecycle. Automating security gates in theDevOps cycle ensure secure development and onboarding of theapplications.
9.Operational Telemetry
Telemetry consists of various set of metrics from applications and
infrastructure that can be exported to a database for them to be analyzedand derive meaningful insights. With the distributed nature of edge
computing, the traditional telemetry collection and analysis models don’t
apply as they add to the latency in data collection and data processingin a centralized location. Metrics generation, telemetry storage andprocessing need to be distributed across the individual set of edge typesbefore the data loses value because of latency. Combination of applica-
tion telemetry and infrastructure/hardware telemetry enables new set of
use cases such as closed loop automation, provide service assurance, QoSand QoE management, detect application and resource anomalies, etc.Analytics models are to be developed that can be customized and scale
across different types of edge deployments.54 Sunku Ranganath10.Policy Management
Policies at the edge are set of rules and constraints that control over
edge
services deployment by different personas, such as administrators,
service providers, application developers, service owners, operation
personnel, etc. These policies help cloud based orchestrators under-stand the constraints for each of the application type such as hardware
constraints, latency tolerance, application priority, run to completion
models, security requirements, scale requirements and so on. Policiesdiffer heavily based on type of edge. For example, policies at IoT edgeneed to consider low latency limitations while servicing large amount
of user data in a resource constrained environment, while policies at
Network edge need to ensure adequate resources are available forthe heavy network functions and consider the ability to scale betweenthe data center cloud and network edge. Ultimately a centralized policy
manager is required that can interact with individual policy managers
and enforcers at each of the edge types.
11.Network Automation
Managing and maintaining the distributed compute capacity and
varying
nature of network function requirements in edge environments
requires 100s of operations at any given instant. This gets a lot morecomplex in a 5G based architecture where the intelligence in the net-work is widely distributed. Automation of network operations is ahuge differentiating factor in owning, maintaining, and operating the
network at scale. Zero-touch automation is an emerging area that aims
to provide human free and interaction free automation of network issuesleveraging AI by taking preemptive actions against a predetermined set ofobjectives.
Some of these universal challenges across the edge continuum necessitate asimplified architecture that interconnects the interfaces between IoT edge,Wireless Access edge, Fixed Access Edge, On-premises and Network Edgein a seamless manner for cloud native application functions to be operated
across the infrastructure and in turn enable data movement across these
networks to core of the network. One such architectural concept isMulti-Access Edge Computing.
7. Multi-Access Edge Computing
The concept of Multi-Access Edge Computing (MEC) moves disag-
gregates the centralized computing of traffic and services in a central cloud to
the edge of the network and closer to the customer. As indicated in the
ETSI’s definition of MEC [ 7], it offers application developers and content55 Edge computing: Types and attributesproviders cloud-computing capabilities and an Information Technology
(IT) service environment at the edge of the network. This environment
is characterized by ultra-low latency and high bandwidth as well asreal-time access to radio network information that can be leveraged by appli-cations. The MEC initiative is an Industry Specification Group (ISG) withinETSI whose is to create a standardized, open environment which will allow
the efficient and seamless integration of applications from vendors, service
providers, and third parties across multi-vendor Multi-access EdgeComputing platforms. ETSI MEC provides set of standards, specificationsand API definitions that help enable various edge service models across
the different types of edge deployments. Figure provides high level frame-
work for MEC. Figure 7 [7] provides high level framework for MEC as
described
in ETSI GS MEC 003 v2.2.1.
To simplify the nomenclature and most importantly indicate adherence
of an edge deployment with MEC standards, the edge platform is usuallyreferred to as a MEC platform. Features of a MEC platform include, butnot limited to:–The importance of MEC is highlighted by the fact that it unites IT and
cloud
computing capabilities with telecom networks across the types of
edges described earlier.
–Enables new business models for applications and services to be hosted on
top
of mobile network elements such as RAN functions, gateway ele-
ments, customer premise equipment and so on.
–Creates set of unified APIs that application developers could leverageacross
both edge and cloud compute environment
–Provides mechanisms to reduce network congestion and accelerate net-work
and application performance enabling low latency type services to
IoT and mobile platforms
–Enables data processing to be done at the network edge closer to the userlocation
there by reducing the burden on network and cloud resources
–Provides new types of data security constructs with ability to process sen-
sitive
user data locally adhering to government or policy requirements
–Provides cost-effective way to deploy and manage private wireless and
wireline
networks through an intelligent allocation of assets
–Real time analytics using heuristics or AI models with a lower latencyturn
around
–Ability to adhere to various compliance requirements in a distributedmanner56 Sunku RanganathTo provide standard set of terminologies, interoperable interfaces, and
requirements around each of the MEC component, ETSI has published a
reference architecture as indicated in Fig. 8 [7] .
For application developers and service enablers, ETSI MEC has provided
few considerations for application to be deployed at the edge of mobile net-
works. The nature of requirements varies based on service, placement,
mobility requirements and so on and hence the requirements are classified
under varying modes of operations. Applications at the edge need to be
rearchitected to fit into edge requirements thereby making them Edge
aware. Using the Domain Name System services, it allows several applica-
tion requirements to be addressed causing few limitations in cases where
support is required for high user mobility in conjunction with the need
to frequently transfer application context between servers. In this case, there
is typically a need to inform, through notifications, the application about a
change of the server IP address. This calls for a change in the application
Fig. 7 Multi-access edge computing framework.57 Edge computing: Types and attributesFig. 8 MEC system reference architecture.logic, therefore requiring applications to be Edge aware. Few aspects of
edge-aware application enablement, as described in Harmonizing Standards
for edge computing—A synergized architecture leveraging ETSI ISGMEC and 3GPP specifications [8]:
–Edge awareness by server applications:
/C14Applications can offer enhanced user experience by if network services
such
as locations, QoS or traffic influence could be used in conjunction
with mobile networks
/C14Edge application may need to regi ster and discover each other’s
services or have capability to access common set of services atthe edge
/C14To realize low latency benefits, highly mobile devices need to be
connected
to most suitable edge clo ud requiring edge cloud platform
to assist in context migration to target edge cloud for stateful
applications
–Discovery mechanism between application client and server application
/C14DNS based: An authoritative DNS can, through IP address resolution,
perform
optimal routing to an edge cloud. However, the DNS options
have limitations for applications running on devices that are highlymobile or when the edge cloud is highly distributed. The enhance-ments to support mobility when DNS is used are still under studyin 3GPP.
/C14Device based: a device hosted client may also be used to facilitate the
discovery
of server applications and the optimal edge cloud, termed an
Edge Enabler Client (EEC) in 3GPP SA6 nomenclature. The same cli-
ent can also assist service continuity because it can subscribe andreceive information about mobility and possibly the decision to per-
form context migration during an application-level session. Edge
aware application clients on the device can directly interact with thisclient to benefit from all these advantages.
–Mobility and context transfer
/C14Context migration can utilize network that provides underlying
mobility
support
/C14Over the top migration through communication between edge cloudapplications
is the usual model
/C14Examples include, but not limited to, network exposure of mobility
events,
network offered ability to influence traffic steering, etc.59 Edge computing: Types and attributesThe above said capabilities of a MEC platform are derived from set of spec-
ification published by ETSI MEC ISG. MEC ISG has produced over
30 specifications establishing and describing APIs across multiple edge todevice connectivity and edge to network core scenarios and more. Someof the specifications published are listed as below:–
WLAN Access Information API
–Study on Inter-MEC systems and MEC-Cloud systems coordination
–General principles, patterns, and common aspects of MEC Service APIs
–UE Identity API
–API Conformance Test Specification; Part 1: Test Requirements and
Implementation Conformance Statement (ICS)
–API Conformance Test Specification; Part 2: Test Purposes (TP)
–API Conformance Test Specification; Part 3: Abstract Test Suite (ATS)
–Framework and Reference Architecture
–Edge Platform Application Enablement
–Multi-access Edge Computing (MEC) MEC 5G Integration
–General principles, patterns, and common aspects of MEC Service APIs
–Traffic Management APIs
–WLAN Information API
–V2X Information Service API
–Device application interface
–Application Mobility Service API
–Radio Network Information API
–Support for network slicing
–Edge Platform Application Enablement
–MEC Management; Part 2: Application lifecycle, rules, and requirements
management
–Study on MEC support for alternative virtualization technologies
–Location API
–Proof of Concept Framework
–Fixed Access Information API
–MEC Testing Framework
–UE application interface
–Support for regulatory requirements
–Framework and Reference Architecture
–General principles for MEC Service APIs
The viability of any new technology and its interfaces are put to test viabuilding a Proof of Concepts (PoC). ETSI MEC ISG has laid out set ofPoCs that provide a reference on possible use cases using MEC platforms.60 Sunku RanganathThe following PoCs are in various stages of development by its members
using the PoC Framework laid out by the ISG:
–Video User Experience Optimization via MEC - A Service Aware RANMEC
PoC
–Edge Video Orchestration and Video Clip Replay via MEC
–Radio aware video optimization in a fully virtualized network
–FLIPS—Flexible IP-based Services
–Enterprise Services
–Healthcare—Dynamic Hospital User, IoT and Alert Status management
–Multi-Service MEC Platform for Advanced Service Delivery
–Video Analytics
–MEC platform to enable low-latency Industrial IoT
–Service Aware MEC Platform to enable Bandwidth Managementof
RAN
–Communication Traffic Management for V2X
–MEC enabled OTT business
–MEC infotainment for smart roads and city hot spots
References
[1] Intel Corp, A smarter network: Creating a Platform for Innovation with Edge
Computing, intel.com .
[2] Martel Innovate, Together on the Edge, 2020. materl-innovate.com .
[3] Colt, What is Universal CPE, colt.net .
[4] Blueplanet, SD-WAN Automation, Blueplanet.com .
[5]L. Ji, Architecture, Integration and Testing, O-RAN, O-RAN wiki, 2020.
[6] R. Browne, P. Mannion, E. Walsh, Creating the Next Generation Central Office with
Intel
Architecture CPUs, whitepaper, intel.com .
[7]ETSI GS MEC 002 v2.2.1 (2020/C0 12), Muti-access Edge Computing; Framework and
Reference Architecture, ETSI ISG MEC, RGS/MEC-0003v221Arch, 2020.
[8]N. Sprecher, et al., Harmonizing standards for edge computing - A synergized architec-
ture
leveraging ETSI ISG MEC and 3GPP specifications, ETSI ISG MEC, 2020. ISBN:
979-10-92620-35-5.61 Edge computing: Types and attributesAbout the author
Sunku Ranganath is a Solutions Architect
for Edge Compute at Intel. For the last few
years, his area of focus has been on enabling
solutions for the Telecom domain, including
designing, building, integrating, and bench-
marking NFV based reference architectures
using Kubernetes & OpenStack components.
Sunku has been an active contributor to multi-
ple open-source initiatives. He serves as a
maintainer for CNCF Service Mesh Perform-
ance & CollectD Projects and participated
on the Technical Steering Committee for
OPNFV (now Anuket). He is an invited
speaker to many industry events, authored multiple publications and contrib-
uted to IEEE Future Networks Edge Serv ice Platform & ETSI ENI standards.
He is a senior member of the IEEE.62 Sunku RanganathCHAPTER THREE
Industry initiatives across edge
computing
Sunku Ranganath
Intel Corporation, Hillsboro, OR, United States
Contents
1.Linux Foundation Edge 64
1.1 Akraino 65
1.2 EdgeXFoundary 67
1.3 EVE 69
1.4 Fledge 69
1.5 Home Edge 72
1.6 Open Horizon 73
1.7 State of the Edge 74
1.8 Baetyl 74
1.9 eKupier 75
1.10 Secure Device Onboard 75
2.Linux Foundation for Networking 76
2.1 Open Network Automation Project (ONAP) 78
2.2 Anuket 78
3.O-RAN alliance 81
4.Open Network Foundation 86
4.1 VOLTHA 86
4.2 SEBA 87
4.3 Aether 87
4.4 SD-RAN ™ 88
4.5 SD-CORE ™ 90
5.3GPP 91
6.Small Cell Forum 92
7.Broadband Forum 93
7.1 Connected Home 93
7.2 5G 93
8.5G Alliance for connected industry and automation (5G-ACIA) 96
9.5G Automotive Association (5GAA) 97
10. Automotive Edge Computing Consortium (AECC) 98
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.03.0026311. Telecom Infra Project 101
11.1 OpenRAN 101
11.2 Connected City Infrastructure 102
11.3 5G Private Networks 102
12. IEEE International Network Generations Roadmap Edge Services Platform (ESP) 104
13. KubeEdge 105
14. StarlingX 106
15. Open Edge Computing Initiative 108
16. Smart Edge Open 110
17. Edge Multi Cluster Orchestrator (EMCO) 110
18. Global Systems for Mobile Association (GSMA) 113
References 114
About the author 115
Abstract
To accelerate the evolution and adoption of Edge computing various standard bodies,
open-source projects and industry consortia have come together in recent times torevolutionize Edge compute. This chapter goes through various initiatives across theworld that have major traction in terms of collaboration, collateral produced and indus-
try impact. Architecture, collateral produced and details of the projects involved are
described.
As we explore various types of edges, their constraints, challenges and
opportunities, various industry consortia have been formed to address someof the emerging challenges across these edge computing environments. Thissection describes these initiatives and the nature of work as it relates to type
of edge computing detailed earlier, apart from the ETSI MEC ISG described
earlier. There are multiple ongoing initiatives across edge ecosystem andnew alliances are forming as we write this book. Some of the major projectsthat are creating impact across the industry are shared. They are at various
stages of development in terms of maturity, from standardization, reference
implementation and live trials. Together these standard bodies, open-sourcesoftware and initiatives are shaping the future of 5G and Edge computing.
1. Linux Foundation Edge
Linux Foundation (LF) Edge has been formed to foster cross-industry
collaboration across IoT, telecom, enterprise and cloud ecosystems andenable organizations to accelerate adoption and innovation for edge com-
puting [1]. LF Edge provides broad umbrella of governance and organization
for 
various Edge projects to uniquely stand out against other projects in the64 Sunku Ranganathecosystem while providing necessary framework to interoperate with overall
edge ecosystem of projects. LF Edge divides edge compute types broadly
under following types:–Constrained Device Edge: the set of devices and things usually fall underInternet
of Things category consisting of devices with very limited set of
compute resources and power. Microcontroller based devices, embedded
devices, low power sensors, etc., fall under this category.
–Smart Device Edge: the set of devices that are usually mobile and consists
of
relative intelligence and/or compute capacity to perform complex
tasks. Smartphones, PCs, IoT gateways, etc., fall under this category
–On-Prem Data Center Edge: this edge is the connecting point betweenDevice
Edge and Access Edge and consists of micro-Data Centers or
on-prem data centers.
–Access Edge: this edge consists of wireless and wirelines access networks,
COs,
aggregation hubs, etc., that have server-based compute capacity.
–Regional Edge: multiple Access Edges feed into Regional Edges and
these
edges have set of data centers that caters to a specific geographical
region.
As shown in Fig. 1 [1] , LF Edge hosts multiple projects at various stages of
maturity
and evolution, at the time of writing this book. Brief description
of each of the projects is provided below.
1.1 Akraino
Considered to be a stage 3 impact project, it [2]provides open-source soft-
ware
stack that supports high-availability cloud services optimized for edge
computing systems and applications. It provides autonomous turn-key
solutions for service enablement, low latency placement, zero-touch provi-
sioning, etc., in the form of blueprints and automated deploymentstacks. Akraino is a set of open infrastructures and application blueprintsfor the Edge, spanning a broad variety of use cases, including 5G, AI,
Edge IaaS/PaaS, IoT, for both provider and enterprise edge domains.
These Blueprints have been created by the Akraino community and focusexclusively on the edge in all of its different forms. What unites all of theseblueprints is that they have been tested by the community and are ready foradoption as-is or used as a starting point for customizing a new edge blue-
print. Few of the blueprints and blueprint families provided by Akraino are:
–5G MEC System Blueprint Family
–AI/ML and AR/VR applications at Edge65 Industry initiatives across edge computingFig. 1 LF Edge Projects categorized against type of Edge deployment.–Automotive Area
–Edge Video Processing
–Integrated Cloud Native NFV/App stack family (Short term: ICN)
–Integrated Edge Cloud (IEC) Blueprint Family
–IoT Area
–KubeEdge Edge Service Blueprint
–Kubernetes-Native Infrastructure (KNI) Blueprint Family
–MicroMEC
–Network Cloud Blueprint Family
–Public Cloud Edge Interface (PCEI) Blueprint Family
–StarlingX Far Edge Distributed Cloud
–Tami COVID-19 Blueprint Family
–Telco Appliance Blueprint Family
–The AI Edge Blueprint Family
–Time-Critical Edge Compute
1.2 EdgeXFoundary
Also considered to be a stage 3 impact project, EdgeXFoundary [3]it pro-
vides
flexible open-source software framework that facilitates interoperabil-
ity between heterogenous devices and applications at the IoT Edge with
consistent framework for security and manageability. It collects data fromsensors and things at the Edge and acts as a dual transformation engine send-
ing and receiving data to and from enterprise, cloud and on-premises appli-
cations. EdgeX enables autonomous operations and intelligence at the Edgeas show in Fig. 2 . Top use cases provided by EdgeXFoundary [3]:
–Manufacturing use case: Remote monitoring of production equipment,
get
data from multiple sources and filter/transform it to react at edge
before sending to the cloud for aggregation, analysis and to optimize pro-
duction and maintenance.
–Retail use case: the Open Retail Initiative (ORI) promotes the EdgeX
fr
amework in retail to ingest data from cameras (OpenVino), POS systems,
RFID, etc., and use it at the edge for use cases like Loss Prevention and
Inventory Management.
–Building Automation use case- Edge Control (control devices via a com-mon
API), use edge data to control building environment (HVAC, light-
ing, access). Connect to the cloud to optimize power consumptionusing ML.67 Industry initiatives across edge computingFig. 2 Platform Architecture of EdgeXFoundary1.3 EVE
Considered stage 2 growth project, EVE [4]provides an open abstraction
en
gine that simplifies the development, orchestration and security of cloud-
native applications on distributed edge hardware. Supporting containers,
VMs and unikernels, EVE provides a flexible foundation for Industrial andEnterprise IoT edge deployments with choice of hardware, applications
and clouds. Fig. 3 shows high level architecture of EVE. The goal of
Pr 
oject EVE is to enable IoT edge computing deployments with the following
capabilities [4]:
–Access to hardware root of trust
–“Secure by default” deployment profile
–High efficiency and usage of device resources including remote control of
CPU,
memory, networking and edge device I/O ports
–Hosting of any combination of apps in virtual machines, containers andKubernetes
clusters
–Hosting of any guest operating system deployable in a virtual machine
–Ability to assign CPU cores and co-processing (e.g., GPU) tospecific
apps
–Ability to block unused I/O ports to prevent physical tampering
–Remote updates of entire software stack with rollback capability to pre-vent
bricking
–Automated patching for security updates
–Automated connectivity to one or more backends (cloud or on premises)
–Distributed firewall to securely route data over networks per policy
1.4 Fledge
Considered stage 2 growth project, Fledge [4]provides an open-source
framework
and community for the Industrial Edge. Architected for rapid
integration of any Industrial IoT (IIOT) devices, sensor or machine all usinga common set of application, management, and security REST APIs withexisting industrial “brown field” systems such as DCS (Distributed Control
Systems), PLC (Program Logic Controllers) and SCADA (Supervisory
Control and Data Acquisition) and the clouds. Fig. 4 provides high level over-
vi
ew of Fledge architecture. Some of the benefits of Fledge include [4]:
–Industrial Equipment Vendors—can build Your Next Generation
Machines
that can learn, maintain themselves, integrate with new cloud
services and data systems69 Industry initiatives across edge computingFig. 3 Project EVE high level architecture.Fig. 4 High level architecture of Fledge.–Industrial Operators can condition and perform predictive maintenance
on all machines
–Industrial System Integrators can leverage a framework for over IIoT
business
1.5 Home Edge
Considered stage 2 growth project, it [4]provides interoperable, flexible,
and scalable edge computing services platform with a set of APIs that can
also run with libraries and runtimes. It provides device and service manage-
ment in the home network, service offloading, scoring manager as the major
set of services. All the devices (TVs, fridges, washing machines, etc.) con-
nected into a Home Edge Network are considered Home Edge devices
and are assigned to Home Edge Nodes by the Home Edge Orchestrator.
The Orchestrator continuous scans the home network looking for new
devices and when it finds one, the device is assigned to a node or a new node
is created by the Orchestrator. Fig. 5 shows high level view of Home Edge.
Couple of top use cases of HomeEdge [4]:
Fig. 5 High level view of Home Edge.72 Sunku Ranganath–Service offloading in a home environment when device doesn’t have
required capabilities
–Distributed computing framework to maintain low latency and high data
privacy
1.6 Open Horizon
Considered stage 2 growth project, it [4]provides a platform for managing
the service software lifecycle of containerized workloads and related
machine learning assets. It enables management of applications deployed
to distributed web scale fleets of edge computing nodes and devices without
requiring on-premises administrators. Open Horizon simplifies the job of
getting the right applications and machine learning onto the right compute
devices, and keeps those applications running and updated. It also enables
the autonomous management of more than 10,000 edge devices
simultaneously—that’s 20 times as many endpoints as in traditional solu-
tions. Fig. 6 shows main components of Open Horizon. Key benefits of
Open Horizon [4]:
–Add new capabilities to a single-purpose device
–Enable your device to use other services (both nearby and cloud-based) to
enhance its existing capabilities
–Automate the hands-free management of workload lifecycle on the
device
–Automatically deploy applications to all devices where policies match and
an agreement is negotiated
Fig. 6 Main components of Open Horizon.73 Industry initiatives across edge computing1.7 State of the Edge
Considered stage 2 growth project, it [4]is an open-source research and pub-
lishing
project with an explicit goal of producing original research on edge
computing, without vendor bias. The State of the Edge seeks to accelerate
the edge computing industry by developing free, shareable research that canbe used by all. State of the Edge believes in four principles [4]:
–The edge is a location, not a thing
–There are lots of edges, but the edge we care about today is the edge of thelast
mile network
–This edge has two sides: an infrastructure edge and a device edge
–Compute will exist on both sides, working in coordination with the
centralized
cloud.
State of the Edge project produced following assets under the LF Edge
umbrella:–State of the Edge reports
–Open glossary of edge computing
–Edge computing landscape
1.8 Baetyl
Considered stage 1 at-large project, Baetyl [4]offers a general-purpose plat-
fo
rm for edge computing that manipulates different types of hardware facilities
and device capabilities into a standardized container runtime environment andAPI, enabling efficient management of application, service, and data flowthrough a remote console both on cloud and on prem. With modern con-
tainer and serverless design concepts and engineering tools optimized for
stand-alone and small multi-machines, Baetyl enables edge hardware andcloud native applications to work better and more efficiently together. It helpsdeliver stronger processing power to edge devices like smart home appliances,
wearables and other IoT devices. Fig. 7 shows the components of Baetyl pro-
j
ect and interaction between them.
Baetyl is committed to help build an open-source framework for the
edge that [4]:
–Abstracts different forms of hardware to a unified container environment,
from
IOT devices to distributed clusters, even embedded devices.
–Supports open application models, including plain Open ContainerInitiative
(OCI) container and serverless modes such as FaaS and
streaming.
–Provides a standardized remote management model with compatibility tok8s
primitives.74 Sunku Ranganath1.9 eKupier
Considered stage 1 at-large project, it [4]is an edge lightweight IoT data
analytics/streaming software to run at all kinds of resource constrained
edge devices. One goal of eKuiper is to migrate the cloud streaming
software frameworks to edge side. eKuiper references these cloud streaming
frameworks, and also considered special requirement of edge analytics, and
introduces rule engine for developing streaming applications at edge side. It
migrates cloud real-time cloud streaming analytics frameworks such as
Apache Spark, Apache Storm and Apache Flink to the edge. eKuiper refer-
ences these cloud streaming frameworks, and also considers any special
requirements of edge analytics, to introduce rule engine, which is based
on Source, SQL (business logic) and sink; rule engine is used for developing
streaming applications at the edge. Fig. 8 shows interaction among project’s
components and their interfaces.
1.10 Secure Device Onboard
Considered stage 1 at-large project, Secure Device Onboard (SDO) [4]is an
automated “Zero-Touch” onboarding service. To more securely and auto-
matically onboard and provision an edge device, it only needs to be
Fig. 7 Components and Interaction within Baetyl.75 Industry initiatives across edge computingdrop-shipped to the point of installation, connected to the network and
powered up. SDO does the rest. This zero-touch model simplifies the
installer’s role, reduces costs and eliminates poor security practices, such
as shipping default passwords. Secure Device Onboard provides easier,
faster, less expensive, and secure onboarding of devices. It expands TAM
for IOT devices, and in turn accelerates the resulting ecosystem of data
processing infrastructure. Most “Zero touch” automated onboarding solu-
tions require the target platform to be decided at manufacturer. Fig. 9 [4]
shows an example of provisioning with SDO. Key benefits of Secure
Device OnBoard include [4]:
–Enables Build-to-Plan Model—ODMs can build identical IOT devices
in high volume using a standardized manufacturing process. Reduces
inventories, supply cycle times, and costs.
–SDO “Late Binding”—allows the device’s target platform to be selected
“late” in the supply chain, at first power-on.
–It’s Open—means its service & cloud independent. Devices are bound to
target the ecosystem at install. Works with existing cloud services, it does
not replace them.
2. Linux Foundation for Networking
Linux Foundation for Networking (LFN) [5]aims to foster collabo-
ration and innovation across the open networking stack from the data plane
to the control plane enabling the necessary orchestration, automation, end-
to-end testing and more. LFN is building the required infrastructure
MQTTSQL / Rule Parser
SQL processors
SQL runtimeSinksMQTT
File
HTTP
StorageStreaming runtimeMQ
Sources
Fig. 8 eKupier Project Components and Interfaces.76 Sunku RanganathFig. 9 Provisioning with SDO.enablement and interoperability required for 5G leveraging hybrid approach
of both VNFs and CNFs. LFN integrates with edge compute leveraging the
following projects:
2.1 Open Network Automation Project (ONAP)
ONAP [6]is a comprehensive platform for orchestration, management, and
automation
of network and edge computing services for network operators,
cloud providers, and enterprises. ONAP orchestrates edge cloud onboardingfrom distributed edge locations. Fig. 10 provides example of how ONAP
helps 
orchestrate, analyze, and provide closed loop control in a centralized
way across the multiple types of Edges and Core Cloud. Following enhance-ments are in various stages of planning and execution for ONAP releases tobetter address the needs of Edge compute:–Optimization to address large number of edge-clouds
–Provide Mutual TLS with Edges, Secrets/keys protection (HSM/DHSM),
H
ardware rooted security, Verification of Edge stack (Attestation), and
centralized security for FaaS
–Adhere to geographical regulations such as GDPR
–Enhancements to performance using containerized deployments
–ONAP to expose API for Application providers—to create MEC Service,
in
stantiate MEC Service, provide MEC status, MEC analytics, etc.
–Ability for Zero Touch Provisioning (ZTP) systems to interact withONAP
to provide information about Edge inventory
–Aggregation of statistics & ML analytics for various edge deployments andprovide
infrastructure information to application providers.
2.2 Anuket
Anuket delivers a common model, standardized reference infrastructure
specifications, and conformance and performance frameworks for virtu-
alized and cloud native network functions. Anuket defines Edge CloudReference Architecture and provides a path to its realization usingReference Implementation. Anuket addresses a wide range of use cases from
Core all the way to the Edge. Anuket artifacts include integrated, tested, and
validated open software reference infrastructure used to design a confor-mance framework and verification program. The correlation between var-ious industry standard groups and Anuket’s Reference Models (RM),
Reference Architecture (RA), Reference Implementation (RI) and
Reference Conformance (RC) are explained in Fig. 11 [7] .78 Sunku RanganathFig. 10 ONAP enabling centralized control across Edge and Cloud.Fig. 11 End to End Technology Development by Anuket.3. O-RAN alliance
O-RAN alliance [8]has been founded by operators across the world,
AT&T,
China Mobile, Deutsche Telekom, NTT DOCOMO and Orange
and have become a world-wide community of mobile network operators,
vendors, and research & academic institutions operating in the Radio AccessNetwork (RAN) industry. O-RAN ALLIANCE’s mission is to re-shape
the RAN industry towards more intelligent, open, virtualized and fully
interoperable mobile networks. To achieve this, O-RAN ALLIANCE isactive in three main streams:–The specification effort /C21extending
 RAN standards towards openness
and intelligence
–O-RAN Software Community /C21development of open software for the
RAN (in cooperation with the Linux Foundation)
–Testing and integration effort /C21supporting O-RAN member companies
in testing and integration of their O-RAN implementations
O-RAN focuses on technical aspects of the RAN and stays neutral in anypolitical,
governmental, or other areas of any country or region. O-RAN
does its innovation and work through many of the technical workinggroups. The O-RAN specification work has been divided across technical
workgroups, all of them under the supervision of the Technical Steering
Committee. Each of the technical workgroups covers a part of theO-RAN Architecture. Brief description of technical workgroups is sharedbelow. Each of the Working Groups have published a set of specifications
that are listed below, at the time of writing this book.
WG1: Use Cases and Overall Architecture Workgroup .
 It has
overall responsibility for the O-RAN Architecture and Use Cases.Work Group 1 identifies tasks to be completed within the scope of
the Architecture and Use Cases and assigns task group leads to drive these
tasks to completion while working across other O-RAN work groups.Specifications produced by WG1:
/C14O-RAN Architecture Description 4.0
/C14O-RAN Operations and Maintenance Architecture 4.0
/C14O-RAN Operations and Maintenance Interface 4.0
/C14O-RAN Use Cases Detailed Specification 5.0
/C14O-RAN Use Cases Analysis Report 5.0
/C14O-RAN Slicing Architecture 4.0
/C14O-RAN Study on O-RAN Slicing 2.0
/C14O-RAN Information Model and Data Models Specification 1.081 Industry initiatives across edge computingWG2: The Non-real-time RAN Intelligent Controller and A1
Interface Workgroup . The primary goal of Non-RT RIC is to
support non-real-time intelligent radio resource management, higherlayer procedure optimization, policy optimization in RAN, and pro-viding AI/ML models to near-RT RIC. Specifications producedby WG2:
/C14O-RAN A1 interface
/C14O-RAN AI/ML Workflow Description and Requirements 01.02
/C14O-RAN A1 interface: General Aspects and Principles 2.02
/C14O-RAN A1 interface: Application Protocol 3.01
/C14O-RAN A1 interface: Transport Protocol 1.01
/C14O-RAN Non-RT RIC & A1 Interface: Use Cases andRequirements 3.00
/C14O-RAN Non-RT RIC: Functional Architecture 1.01
WG3: The Near-real-time RIC and E2 Interface Workgroup .
The focus of this workgroup is to define an architecture based onNear-Real-Time Radio Intelligent Controller (RIC), which enablesnear-real-time control and optimization of RAN elements and resources
via fine-grained data collection and actions over E2 interface.
Specifications produced by WG3:
/C14O-RAN Near-RT RAN Intelligent Controller Near-RT RICArchitecture 2.00
/C14O-RAN Near-Real-time RAN Intelligent Controller Architecture &
E2 General Aspects and Principles 1.01
/C14O-RAN Near-Real-time RAN Intelligent Controller, E2 Application
Protocol 1.01
/C14O-RAN Near-Real-time RAN Intelligent Controller E2 Service
Model 1.0
/C14O-RAN Near-Real-time RAN Intelligent Controller E2 Service
Model (E2SM), RAN Function Network Interface (NI) 1.0
/C14O-RAN Near-Real-time RAN Intelligent Controller E2 Service
Model (E2SM) KPM 1.0
WG4: The Open Fronthaul Interfaces Workgroup . The objective
of this work is to deliver truly open fronthaul interfaces, in whichmulti-vendor DU-RRU interoperability can be realized. Specificationsproduced by WG4:
/C14O-RAN Open Fronthaul Conformance Test Specification 3.00
/C14O-RAN Fronthaul Cooperative Transport Interface TransportControl Plane Specification 2.0
/C14O-RAN Fronthaul Interoperability Test Specification (IOT) 4.082 Sunku Ranganath/C14O-RAN Fronthaul Control, User and Synchronization Plane
Specification 6.0
/C14O-RAN Management Plane Specification 6.0
/C14O-RAN Management Plane Specification—YANG Models 6.0
/C14O-RAN Cooperative Transport Interface Transport Management
Plane Specification 2.0
/C14O-RAN Cooperative Transport Interface Transport ManagementPlane YANG Models 2.0
WG5: The Open F1/W1/E1/X2/Xn Interface Workgroup . The
objective of this work is to provide fully operable multi-vendor profile
specifications (which shall be compliant with 3GPP specification) for
F1/W1/E1/X2/Xn interfaces and in some cases will propose 3GPPspecification enhancements. Specifications produced by WG5:
/C14O-RAN O1 Interface specification for O-DU 1.0
/C14O-RAN O1 Interface specification for O-DU 1.0
/C14O-RAN O1 Interface for O-DU 1.0 - configuration Tables
/C14O-RAN Interoperability Test Specification (IOT) 02.00
/C14O-RAN Transport Specification 1.0
/C14O-RAN NR C-plane profile v4.0
/C14O-RAN NR U-plane profile v4.0
/C14O-RAN NR C-plane profile for EN-DC 2.0
/C14O-RAN NR U-plane profile for EN-DC 2.0
/C14O-RAN EN-DC C-Plane Table 1.0
WG6: The Cloudification and Orchestration Workgroup . The
cloudification and orchestration workgroup seeks to drive thedecoupling of RAN software from the underlying hardware platformsand to produce technology and reference designs that would allow com-
modity hardware platforms to be leveraged for all parts of a RAN
deployment including the CU and the DU. Specifications producedby WG6:
/C14O-RAN Acceleration Abstraction Layer General Aspects and
Principles 1.0
/C14O-RAN Orchestration Use Cases and Requirements for O-RAN
Virtualized RAN 2.0
/C14O-RAN O2 General Aspects and Principles Specification 1.0
/C14O-RAN Cloud Platform Reference Design 2.0
/C14O-RAN Cloud Platform Reference Design for DeploymentScenario B 1.01
/C14O-RAN Cloud Architecture and Deployment Scenarios for O-RANVirtualized RAN 2.0183 Industry initiatives across edge computingWG7: The White-box Hardware Workgroup . The promotion of
open reference design hardware is a potential way to reduce the cost
of 5G deployment that will benefit both the operators and vendors.The objective of this working group is to specify and release a completereference design to foster a decoupled software and hardware platform.Specifications produced by WG7:
/C14O-RAN Hardware Reference Design Specification for Indoor PicoCell with Fronthaul Split Option 6-1.0
/C14O-RAN Hardware Reference Design Specification for IndoorPicocell FR1 with Split Architecture Option 7-2 2.0
/C14O-RAN Hardware Reference Design Specification for IndoorPicocell FR1 with Split Architecture Option 8 2.0
/C14O-RAN Indoor Picocell Hardware Architecture and Requirement(FR1 Only) Specification 1.0
/C14O-RAN Deployment Scenarios and Base Station Classes for WhiteBox Hardware 2.0
/C14O-RAN Hardware Reference Design Specification for OutdoorMicro Cell with Split Architecture Option 7-2 1.0
/C14O-RAN Outdoor Micro Cell Hardware Architecture andRequirements (FR1) Specification 1.0
/C14O-RAN Hardware Reference Design Specification for FronthaulGateway 1.0
/C14O-RAN Outdoor Macrocell Hardware Architecture and
Requirements (FR1) Specification 1.0
WG8: Stack Reference Design Workgroup . The aim of this work-
group is to develop the software architecture, design, and release plan forthe O-RAN Central Unit (O-CU) and O-RAN Distributed Unit
(O-DU) based on O-RAN and 3GPP specifications for the NR proto-
col stack. Specifications produced by WG8:
/C14O-RAN Stack Interoperability Test Specification 1.0
/C14O-RAN Base Station O-DU and O-CU Software Architecture and
APIs 3.0
WG9: Open X-haul Transport Work Group . This workgroup
focuses on the transport domain, consisting of transport equipment,physical media and control/management protocols associated with thetransport network. Specifications produced by WG9:
/C14O-RAN Synchronization Architecture and Solution Specification 1.0
/C14O-RAN Management interfaces for Transport Network Elements 1.084 Sunku Ranganath/C14O-RAN WDM-based Fronthaul Transport 1.0
/C14O-RAN Xhaul Transport Requirements 1.0
/C14O-RAN Xhaul Packet Switched Architectures and Solutions 1.0
WG10: OAM Work Group. This workgroup is responsible for the
OAM requirements, OAM architecture and the O1 interface.
Further, O-RAN alliance has focus groups that deal with topics that are
over-arching
the technical workgroups or are relevant for the whole
organization.
SDFG: Standard Development Focus Group . SDFG [8]plays
the leading role on working out the standardization strategies of
O-RAN ALLIANCE and is the main interface to other Standard
Development Organizations (SDOs) that are relevant for O-RANwork, for which SDFG also coordinates incoming and outgoingLiaison Statements.
TIFG: Test & Integration Focus Group.
 TIFG [9]defines O-RAN’s
overall approach for testing and integration, including coordination oftest specifications across various WGs. This may include creating end-to-end test & integration specifications; profiles to facilitate O-RAN
productization, operationalization and commercialization; approaches
to meet general requirements; and specifications of processes for per-forming integration and solution verification. The TIFG plans and coor-dinates the O-RAN ALLIANCE PlugFests and sets guidelines for the 3rdparty Open Test & Integration Centers (OTIC). Specifications produced
by TIFG:
/C14O-RAN End-to-End Test Specification 1.0
/C14O-RAN Certification and Badging Processes and Procedures 1.0
/C14O-RAN End-to-End System Testing Framework Specification 1.0
/C14O-RAN Criteria and Guidelines of Open Testing and Integration
Centre
2.0
OSFG: Open-Source Focus Group. The biggest task that OSFG [9]
has accomplished was the successful launch of the O-RAN Software
Community. As most of open-source activities are happening directly in
the O-RAN Software Community, the OSFG remains in a dormantmode.
SFG: Security Focus Group.
 SFG[9]focuses on security aspects of the
open RAN ecosystem. Specifications produced by SFG:
/C14O-RAN Security Threat Modeling and Remediation Analysis 1.0
/C14O-RAN Security Protocols Specifications 1.085 Industry initiatives across edge computing4. Open Network Foundation
The mission of Open Network Foundation (ONF) [10]is to trans-
form access and edge networks by cross collaboration between academia
and industry to build next generation mobile and broadband infrastructure.
ONF provides solutions leveraging disaggregated and white box hardware,
open-source software for SDN, NFV and cloud technologies. Fig. 12 [10]
indicates current set of ONF Focus projects at the time of writing this
book. A brief description of these projects is provided below. ONF also pro-
vides set of reference designs leveraging these projects to build a deployable
platform.
4.1 VOLTHA
Virtual OLT Hardware Abstraction (VOLTHA) [10] is a project trans-
forming broadband infrastructure by providing a common, vendor agnostic,
GPON control and management system, for a set of white-box and vendor-
specific PON hardware devices using open-source software. Highlights of
VOLTHA, as indicated in Fig. 13 [10] below:
Mobile & 5G Solutions
AETHER
SD-FABRICSEBABroadband Solutions
Service Platform
Access Network
Core Network
SDN
Infrastructure
White Box HardwareONOS P4 PINS STRATUMSD-CORESD-RAN
SD-BNGVOLTHA
Fig. 12 ONF projects.86 Sunku Ranganath–Makes an access network look like an abstract programmable switch
–Works with legacy as well as virtualized devices. Can run on the device,
on general purpose servers, or in a virtualized cloud
–Provides unified, vendor/technology agnostic management interface
–DevOps bridge to modernization
4.2 SEBA
SEBA [10] a platform that utilizes open-source components to build a
virtualized PON network to deliver residential broadband and mobile back-
haul, as shown in Fig. 14 . It supports a multitude of virtualized access tech-
nologies at the edge of the carrier network, including PON, G. Fast, and
eventually DOCSIS and more. Highlights of SEBA:
–Kubernetes based
–High speed
–Operationalized with FCAPS and OSS Integration
4.3 Aether
Aether [10] is the first open source 5G Connected Edge platform for
enabling enterprise digital transformation. It provides mobile connectivity
ONOS
Client
(eg. OFAgent)
kafkaEtcdDevice
Management
Interface
OpenONU
AdaptorOpenOLT
Adaptor
OpenOLT
agent
ONLONU
ONUWhitebox OLTONIEDMI agentOpen
OMCIVOLTHA core
BAL
SDK(s)
Fig. 13 VOLTHA Component Diagram.87 Industry initiatives across edge computingand edge cloud services for distributed enterprise networks as a cloud man-
aged offering. Aether is an open-source platform optimized for multi-cloud
deployments, and simultaneous support for wireless connectivity over
licensed, unlicensed and lightly licensed (CBRS) spectrum [R]. As indicated
inFig. 15 [10] , some of the highlights of Aether project include:
–Provides predictive end-to-end performance with reliability and security
optimized for multi-edge sites
–Leverage CBRS spectrum and easy to deploy as WiFi
–Support AI/ML driven IoT, OT applications and mission critical edge
applications
–Optimized end-to-end slicing with spectrum management, allocating
dedicated slices for mission critical applications
4.4 SD-RAN ™
It is a 3GPP compliant software-defined RAN that is consistent with the
O-RAN architecture that is cloud-native and built on ONF’s projects.
SD-RAN ™[10] is developing a near-real-time RIC (nRT-RIC) and a
set of exemplar xApps for controlling the RAN. This RIC is cloud-native
and builds on several of ONF’s well established platforms including the
ONOS SDN Controller. The architecture for the SD-RAN nRT-RIC will
leverage the O-RAN architecture and vision. Fig. 16 [10] provides
Fig. 14 SEBA Architecture —high level overview.88 Sunku Ranganathreference of high-level view of SD-RAN architecture. Highlights of
SD-RAN project includes, but not limited to:
–As the SD-RAN ™project creates new functionality, all extensions and
learning that come from building the system will be contributed back to
the O-RAN Alliance with the intent that these extensions can help
advance the O-RAN specifications
Fig. 15 Aether architecture for 5G Connected Edge.
RUE2 E2 E2
N2 Mobile
Core CP
Mobile
Core UPN3F1-C
F1-UCU-UCU-C
DUxApp APIs
Policy xAppsSON xAppsRRM xApps
µONOS RICO1 A1
A1 O1
E2 RAN Config Service ModelsCertificate Mgr Data Store
TopologyRAN Control
Fig. 16 Architecture of SD-RAN implementation with μONOS RIC.89 Industry initiatives across edge computing–ONOS RIC is a cloud-native, carrier-grade SDN controller that enables
high performance, availability, scalability in a multi-vendor environment
–TheμONOS RIC uses a microservices architecture that includes certif-
icate manager, topology manager, configuration manager, RAN control
manager and distributed store
4.5 SD-CORE ™
The SD-Core ™[10]project is a 4G/5G disaggregated mobile core opti-
mized for public cloud deployment in concert with distributed edge clouds
and is ideally suited for carrier and private enterprise 5G networks. It exposes
standard 3GPP interfaces enabling use of SD-Core as a conventional mobile
core. It is also available pre-integrated with an adapter (part of the Aether
ROC subsystem) for those deploying it as a mobile core as-a-service solu-
tion. Fig. 17 [10] provides high level overview of SD-CORE architecture,
some of the project’s highlights include:
–Provides flexible, agile, scalable, and configurable dual mode 4G/5G core
network platform that builds upon and enhances ONF’s OMEC and free
5GC core network platforms to support LTE, 5G NSA and 5G SA
services.
–The SD-Core control plane provides the flexibility of simultaneous sup-
ports for 5G standalone, 5G non-standalone and 4G/LTE deployments.
Customer Portal Operator Portal
Adapter Adapter Adapter
APIs for configuration,
subscriber management,
telemetry
Internet /
Data NetworksgNB
eNBKubernetes APIs
for SD-Core
Container
ManagementRuntime
Operational
Control
5G Core
Control Plane
Dual Core User Plane Function (UPF)Sd-Core
Cloud Platform4G Core
Control PlaneThird-Party
Application
Fig. 17 SD-Core architecture.90 Sunku Ranganath–SD-Core provides a rich set of APIs to Runtime Operation Control
(ROC).
Operators can use these APIs to provision the subscribers in
the mobile core; control runtime configuration of network functions;
and provide telemetry data to third party applications. Third party appli-cations can leverage telemetry data to create applications for closed loopcontrol.
5. 3GPP
3rd Generation Partnership Project (3GPP) [11]unites seven telecom-
munications
standard development organization known as “Organizational
Partners” and provides their members with a stable environment to producethe Reports and Specifications that define 3GPP technologies. The projectcovers cellular telecommunications technologies, including radio access,
core network and service capabilities, which provide a complete system
description for mobile telecommunications. The 3GPP specifications alsoprovide hooks for non-radio access to the core network and for interworkingwith non-3GPP networks. 3GPP specifications and studies are contribution-driven, by member companies, in Working Groups and at the Technical
Specification Group level. The three Technical Specification Groups
(TSG) in 3GPP are below and the product:–Radio Access Networks (RAN),
–Services & Systems Aspects (SA),
–Core Network & Terminals (CT)
As a global initiative, 3GPP is well placed to build on its strong relationshipsand
collaborations with ETSI MEC and GSMA. 3GPP Release 17 is foun-
dational for edge computing but more will come in future releases given its
importance in mobile communications and as we gradually move beyond
5G. Artificial Intelligence and edge computing can both serve as buildingblocks but in different ways:–Network layer perspectives: AI can further optimize edge computing
applications.
–Application layer perspectives: Edge computing can be a building block
for
AI, e.g., offloading limited capabilities from the device to the
network.
SA2 currently responsible for [12] the 5G System and Evolved Packet
System
(EPS) Architectures including the 3GPP enhancements for91 Industry initiatives across edge computingmultimedia services (including emergency services), IoT, and other market
sectors/vertical industries related use-cases. TSG SA5 is currently responsi-
ble for [12].
1.Management and Orchestration which covers aspects such as operation,assurance,
fulfillment and automation, including management interac-
tion with entities external to the network operator (e.g., service pro-
viders and verticals)
2.Charging which covers aspects such as Quota Management and
Charging
Data Records (CDRs) generation, related to end-user and
service-provider
SA6 is currently responsible for application layer specifications, with empha-sis
on following [13]:
1.Critical communication applications (e.g., Mission Critical services forpublic
safety, railways)
2.Service frameworks (e.g., Commo n API Framework, Service Enabler
Architecture Layer, Edge Application enablement, Messaging enablement)
3.Enablers for vertical applications (e.g., automotive, drones, smartfactories)
6. Small Cell Forum
Small Cell Forum (SCF) [13]was founded with the aim of ensuring
scale
and, explicitly, diversity for the small cell industry. To ensure right con-
ditions for diversity following approaches are considered its priority:–Diversification of market applications
–Diversification of deployers
–Diversification of supply chains
SCF addresses requirements from service providers, enterprises, and tech-nology
providers—all of which will be driving the small cell-enabled 5G
future. SCF is helping enterprise, industry and government understandthe potential benefits of private networks and providing the technical workthat will help to enable them. SCF published a document SCF234 [13]that
correlates
the interworking between Small Cell Networks and Edge
Computing, architectural considerations and service models associated withprivate networks-based edge computing. Fig. 18 provides distinction of
edge 
locations, while Figure overall functional framework for edge comput-
ing correlating Mobile Network Operators (MNO) network functions andon-premise network functions.92 Sunku Ranganath7. Broadband Forum
BroadBand Forum (BBF) [14]is the communications industry’s lead-
ing organization focused on accelerating broadband innovation, standards,
and ecosystem development. Broadband Forum is an open, non-profit
industry organization composed of the industry’s leading broadband oper-
ators, vendors, and thought leaders who are shaping the future of broadband.
Its work to date has been the foundation for broadband’s global proliferation
and innovation. Broadband Forum’s projects span across 5G, Connected
Home, Cloud, and Access.
7.1 Connected Home
The IoT enhanced Connected Home [14]presents service providers with
unparalleled opportunities and challenges. This project provides the neces-
sary evolution of broadband with the standard—TR-369 or User Service
Platform (USP)—has been designed and built by the service providers
and vendors of Broadband Forum by leveraging the experience of deploy-
ment managed services through complex network environments. High level
architecture detailed in TR-369 is provided in Fig. 19 [15] .
7.2 5G
5G-Fixed Mobile Convergence (5G-FMC) is one of the areas of high pri-
ority to be addressed, to provide:
–Seamless service experience is key for users and drives the need for
full FMC
Fig. 18 Functional Framework for Edge Computing.93 Industry initiatives across edge computing–5G services are to be deployed in an access-independent context across
several
access technologies (incl. Wireless and wireline)
–On-demand network services, e.g., different levels of mobility may berequired
according to the application needs
–Multiple simultaneous attachments will be very common for certaindevices
and applications
BBF has produced following standards to help evolution of FMC towards5G:–5G Fixed-Mobile Convergence (SD-407)
–End-to-End Network Slicing (SD-406)
–Combined 3GPP and BBF functions (SD-357)
–5G requirements and enablers (SD-373)
Figure
20 [14] provides high level architecture of correlation between BBF
and
3GPP items necessary for FMC.Optional REST API
USP AgentOther
applications/automation
Application/PolicyEngine
Bulk DataCollectorEndpointAgent
Database
Message
Transfer
ProtocolControllerEndpoint
MTPProxy
Co-existingTR-069EndpointAgentEndpointMTP
Supported
Data
ModelOptionalHTTP BulkDataClient
InstantiatedData
Model
Service Elements
Network Interfaces, Managed Services,
Software Modules, Firmware, Proxied
Devices modeled in Device:2 (TR-181)Controller A
Controller BData
Collector
TR-069 ACS
Fig. 19 Architectural Framework of TR-369.94 Sunku RanganathFig. 20 BBF 5G FMC high level architecture.8. 5G Alliance for connected industry and automation
(5G-ACIA)
To make 5G a success for industrial applications, 5G-ACIA [16]
brings together widely varying 5G stakeholders to include organizations
o
f the Operational Technology (OT) (include industrial automation enter-
prises, machine builders, end users), Information and CommunicationTechnologies (ICT) (includes manufacturers, network infrastructure pro-
viders or mobile network operators), academic institutions and more.
Together, members discuss and evaluate technical, regulatory, and businessaspects with respect to 5G for the industrial domain through workinggroups addressing these issues. Following working groups are part of5G-ACIA [16]:
–Working Group 1: Use Cases and Requirements, discusses potential usecases
and requirements and defines a common body of terminology.
–Working Group 2: Spectrum and Operating Models, identifies and artic-ulate
specific spectrum needs of industrial 5G networks and explores new
operator models, for example for operating private or neutral host 5Gnetworks within a plant or factory.
–Working Group 3: Architecture and Technology,
 Considers the overall
architecture of future 5G-enabled ind ustrial connectivity infrastruc-
tures. This includes integration concepts and migration paths, theevaluation of key technologies e merging from 5G standardization
bodies.
–Working Group 4: Liaison and dissemination, takes care of the interac-
tion
with other initiatives and organizations by establishing liaison activ-
ities and initiating suitable promotional measures.
–Working Group 5: Validation and tests, deals with the final validation
of
5G for industrial applications. This includes the initiation of inter-
operability tests, larger trials, and potentially dedicated certification
procedures.
Various showcases have been created through 5G-ACIA spanning across
factory
automation, process automation, logistics, maintenance, human
machine collaboration. Fig. 21 [16] provides various industrial use case
and
corresponding service requirements.96 Sunku Ranganath9. 5G Automotive Association (5GAA)
The 5G Automotive Association (5GAA) [17] is a global, cross-
industry organization of companies from the automotive, technology, and
telecommunications industries (ICT), working together to develop end-
to-end solutions for future mobility and transportation services. Diverse
both in terms of geography and expertise, 5GAA’s members are committed
to helping define and develop the next generation of connected mobility
and automated vehicle solutions.
The transmission modes of shorter-range direct communications,
Vehicle to Vehicle (V2V), Vehicle to Infrastructure (V2I), Vehicle to
Pedestrian (V2P), and longer-range network-based Vehicle to Network
(V2N) communications comprising Cellular-V2X as defined by 3GPP
TR 22.885. An example of V2X architecture is described in Fig. 22 [9] .
5GAA has produced multiple artifacts that help enable future of automo-
tive industries. Some of the white papers published by 5GAA at the time of
writing this book:
Fig. 21 Industrial use cases according to communication requirements.97 Industry initiatives across edge computing–Safety treatment in V2X applications
–Cooperation models enabling deployment and use of 5G infrastructure
–Privacy by design aspects of C-V2X
–C-V2X use cases
–Vulnerable road user protection
–MNO network expansion mechanisms to fulfil connected vehicle
requirements
–5GAA efficient security provisioning
–Making 5G proactive and predictive for automotive industry
–Evaluation of available architectural options
–Edge computing for advanced automotive communications
–Cost-benefit analysis on C-V2X technology and its evolution to
5G-V2X
10. Automotive Edge Computing Consortium (AECC)
Automotive Edge Computing Consortium (AECC) [18] works
with leaders across industries to drive the evolution of edge network
Fig. 22 V2X communications.98 Sunku Ranganatharchitectures and computing infrastructures to support high volume data
services in a smarter, more efficient connected-vehicle future. The
Consortium is creating use cases and requirements on networking andcomputing for connected services in automobiles. Examples arehigh-definition map creation and distribution, intelligent driving, andremote diagnostic maintenance. Eventually our work will expand to
emerging mobile devices such as drones, robots and other types of vehicles.
Few of the consortium activities include:–Define specific automotive use cases and requirements with a focus onnetworking
and computing for automotive big data.
–Formulate a roadmap strategy from technology development to marketintroduction,
including the network evolution.
–Identify relevant communities for standardization and open-source soft-ware
development, and support these with inputs for use cases and
requirements with measures of success.
–Address efficiency issues in resource utilization such as communicationbandwidth,
computational power, and storage capacity. Examples of
solutions may include in-vehicle systems; edge computing; distributed
cloud; process/task migration; network virtualization (SDN/NFV)/con-
tainerization (micro-service); network interface/messaging; data centerfabric; and multiple-accesses (Wi-Fi/Cellular, etc.).
The AECC provides system architecture that interacts between vehicles,service
edge access network and necessary services such as AECC service
that is responsible for overall management of AECC system, as describedinFig. 23 [18] .
Following 
publications have been produced through AECC at the time
of writing this book [18]:
–Distributed Computing in an AECC System
–Break Down the Barriers to Automotive Edge Adoption
–Connected Cars: On the Edge of A Breakthrough
–Enabling the Connected Vehicle Market to Thrive
–Driving Data to the Edge: The Challenge of Traffic Distribution
–Operational Behavior of a High-Definition Map Application
–General Principle and Vision
–Use-case and Requirement Document99 Industry initiatives across edge computingAccess
networkMobility services
Primary path
Management path
System boundaryEnterprise
NetworkAECC Service
Computing
infrastructure
services
LEGEND
AECC SystemMSP and other AECC
system clients
AECC service, computing
infrastructure, and mobility
service clients
Vehicle system
Fig. 23 AECC system.11. Telecom Infra Project
The Telecom Infra Project (TIP) [19]is a global community of com-
panies and organizations working together to accelerate the development
and deployment of open, disaggregated, and standards-based technology
solutions that deliver the high-quality connectivity that the world needs.
To identify the best market opportunities for connectivity from operators
and other connectivity stakeholder, TIP operates as project groups and solu-
tion groups each producing set of deliverables and relevant software. Few of
the project groups and solution groups that are relevant towards edge
computing:
11.1 OpenRAN
OpenRAN’s mission [19]is to accelerate innovation and commercialization
in RAN domain with multi-vendor interoperable products and solutions
that are easy to integrate in the operator’s network and are verified for dif-
ferent deployment scenarios. TIP’s OpenRAN program supports the devel-
opment of disaggregated and interoperable 2G/3G/4G/5G NR Radio
Access Network (RAN) solutions based on service provider requirements.
Fig. 24 provides reference architecture of OpenRAN [ 19].
Key tenets of OpenRAN from TIP include:
–Disaggregation of RAN HW & SW on vendor neutral, GPP-based
platforms
Fig. 24 OpenRAN architecture.101 Industry initiatives across edge computing–Open Interfaces—Implementations using open interface specifications
between
components (e.g., RU/CU/DU/RIC) with vendor neutral
hardware and software.
–Multiple Architecture Options, including
–An all-integrated RAN with disaggregation at SW and HW level
–A split RAN with RU, BBU (DU/CU)
–A split RAN with RU, DU and CU
–A split RAN with integrated RU/DU, CU
–Flexibility—Multi vendor solutions enabling a diverse ecosystem for the
operators
to choose best-of-breed options for their 2G/3G/4G and 5G
deployments
–Solutions implemented on either Bare Metal or Virtualized orContainerized
Platforms
–Innovation via Adoption of New Technologies (AI/ML, CI/CD …)
–Supply Chain Diversity
11.2 Connected City Infrastructure
The Connected City Infrastructure project group [ 19] aims to develop a
configuration
with cost-efficient new and retrofitted street asset solutions
and different backhaul technologies. The group will publish a case studyto showcase a sustainable business model for street assets, managed bymunicipalities or public utilities, providing citizen services and generatingappropriate revenue streams. Fig. 25 [19] provides reference deployment
of
a connected city infrastructure.
The group is focused on urban connectivity solutions, specifically:
–The definition and validation of new construction and retrofitted mod-ular
street assets with LTE / 5G Small Cells and Public Wi-Fi E2E archi-
tectures. The PG members will deploy these street assets, with modularassembly, in a field trial supporting interchangeable backhaul and accesstechnologies.
–The creation of an anonymized business case for provision and operationof
operator service based on different backhaul & transmission services
(i.e., fiber, mmWave, microwave)—focusing on business driver con-structs required to drive a scalable solution. This will also be complemen-ted with deployment and operational guidelines.
11.3 5G Private Networks
The goal of the 5G Private Network Solution Project Group [19]is to make
5G
private networks accessible to a broad range of use cases and customers by102 Sunku RanganathFig. 25 Connected city reference solutiontransforming deployment from a bespoke special project to a standard prod-
uct with an appropriate cost structure. The group develops requirements for
the automated lifecycle management of an on-premises and edge cloudnative 5G Private Network solution based on a CI/CD toolset and open,disaggregated hardware components.
This solution group will develop a new approach to the implementation
of 5G Private Networks, resulting in:–Seamless integration of building blocks: 5G connectivity, Cloud NativeEdge
Computing and use cases
–Lower cost structure from the use of open & cloud native functions for
5G
private networks maximizing the use of open-source software
–More efficient and timely installation, maintenance, and upgrade of soft-
ware
through CI/CD automated life-cycle management (LCM) of all the
different units composing the architecture: CNP, tools, virtual network
functions, cloud native telco functions
–Improved security model and better performance (higher bandwidth,
lower
latency) through the use of local user plane breakout
–Develop requirements for the automated lifecycle management of an
on-premises
and edge cloud native 5G Private Network solution based
on a CI/CD toolset and open, disaggregated hardware components.
12. IEEE International Network Generations Roadmap
Edge Services Platform (ESP)
The purpose of the International Network Generations Roadmap
(INGR) [20]is to stimulate an industry-wide dialogue to address the many
facets
and challenges of the development and deployment of 5G in a
well-coordinated and comprehensive manner, while also looking beyond5G by laying out a technology roadmap with 3-, 5-, and 10-year horizon.
Fig. 26 shows the correlation of Edge Service Platform with various aspects
of 
software lifecycle management at the Edge.
The services to run on edge to cloud continuum is proposed as Edge
Service Platform Framework (ESPF) [21] which covers the edge service
optimizations
for low sub millisecond latencies and higher bandwidth and
capacities for 5G/IoT/WIFi6 and beyond. Edge Service includes the nec-
essary Platform and Applications that are distributed and delivered to con-
sumers and enterprises. The requirements to support privacy, security, and
proximity functionality for location-based delivery of services leads to touch104 Sunku Ranganathpoint of edges all over from on-prem, IoT Gateways, light poles, small cells,
macro cells, fronthaul, Micro Data Centers, midhaul, central office, provider
edge with microservices architecture, loosely coupled composable services
from service catalogues across the edge to cloud continuum.
13. KubeEdge
KubeEdge [22]enables cloud-edge synergy, computing at edge, and
easy access of a massive number of devices based on Kubernetes container
orchestration and scheduling capabilities. KubeEdge is an incubating project
under umbrella of Cloud Native Compute Foundation (CNCF).
As shown in Fig. 27 [22] the KubeEdge architecture consists of cloud,
edge, and device layers. The control plane is on the cloud. Edge nodes
are at the edge. At the cloud layer, the green box on the left represents a
Kubernetes master. It is a native Kubernetes control plane without any
changes. The light-yellow box on the right represents CloudCore. It con-
tains EdgeController and DeviceController, which process data from the
control plane, and Cloud Hub, which sends the data to EdgeHub at
the edge. The edge enables application and device management. In the
light-yellow box, on the left is Edged for application management. On
the right are DeviceTwin and EventBus for device management. The green
box on the left represents DataStore. It enables local autonomy. Specifically,
when the data of an application or device is distributed from the cloud
through EdgeHub, the data is stored in a database before it is sent to
OnboardingContent CachingOn-Prem ClientMobile-Client
Service
Access & Availability Security & IdentityEdge Cluster
InfrastructureEdge Infra portability
& Interop
AutoScale, PoliciesPlatformFederation &
AutomationEdge Service
PlatformOpen Service Broker
Public Cloud
Private CloudDistributed
Management
Edge CatalogID Mgmt./Security Binding
Control LoopsService Abstraction
S/W Offload
H/W Offload/ Acceleration/Taints
Fig. 26 IEEE INGR ESP framework.105 Industry initiatives across edge computingEdged or the device. In this way, Edged can retrieve metadata from the data-
base and the service recovers even when the edge is disconnected from the
cloud or when the edge node restarts.
14. StarlingX
StarlingX [23]is a complete cloud infrastructure software stack for the
edge used by the most demanding applications in industrial IOT, telecom,
video delivery and other ultra-low latency use cases. Fig. 28 [23] show high
level overview of StarlingX project. With deterministic low latency required
by edge applications, and tools that make distributed edge manageable,
StarlingX provides a container-based infrastructure for edge implementations
in scalable solutions that is ready for production. StarlingX is a project hosted
under Open Infrastructure Foundation [24]. The project builds on existing
services in the open-source ecosystem by taking components of projects
such as Ceph, OpenStack and Kubernetes and complementing them with
K8S
API ServerControllers
EdgeController
DeviceController
Cloud Hub
EdgeHub
MetaManager NodeLevel
DataStore
Docker
Pod Pod Pod Mapper (Protocol-1)Volume Configmap Pod Prober Event ...
Mapper (Protocol-2)containerd CRI-O
MQTT BrokerEdgedEventBusDeviceTwinServiceBushttp://
APPCloud
Edge
DeviceCloudCore
EdgeCore
Fig. 27 KubeEdge architecture concept.106 Sunku RanganathFig. 28 StarlingX architecture.new services like configuration and fault management with focus on key
requirements as high availability (HA), quality of service (QoS), performance
and low latency.
Some of the highlights of StarlingX software:
–Reliability: Fault management, fast secure VM failover and live migrationminimizes
downtime
–Scalability: Deployable on one to thousands of distributed nodes allowingfor
a single system to be used from edge to core
–Small footprint: Providing a platform for edge and IoT use cases even forenvironments
with tight resource constraints
–Ultra-low latency: Deterministic, tunable performance optimized for theuse
case
–Secure: Software security to avoid tampering at the edge, where physicalsecurity
may be limited
–Lifecycle management: Simplified deployment and operations with fullsystem
management through comprehensive orchestration suited for
the edge
15. Open Edge Computing Initiative
The Open Edge Computing Initiative [25] is a collective effort by
multiple
companies, driving the business opportunities and technologies
surrounding edge computing. The Open Edge Computing Initiative isshaping the global eco-system around edge computing by:–Driving the convergence of edge computing platforms and services on a
global
scale
–Providing attractive edge applications for live edge demonstrations
–Running a real-world edge computing test center (called the Living Edge
Lab)
for user and technology trials
–Driving the adoption of Open Edge Computing with edge applicationproviders,
telecom operators, and cloud service providers
–Tackling key technical challenges of edge computing with its academicpartner
Carnegie Mellon University
The Living Edge Lab provides an open proving ground for edge computing,as
described in Fig. 29 [25] .
Few
of the publications [25]produced by Open Edge Computing:
–Impact of Delayed Response on Wearable Cognitive Assistance
–The Role of Edge Offload for Hardware-Accelerated Mobile Devices
–OpenRTiST: End-to-End Benchmarking for Edge Computing108 Sunku Ranganath–Simulating Edge Computing Environments to Optimize Application
Experience
–Edge Computing for Legacy Applications
–Towards Scalable Edge-Native Applications
–Seeing Further Down the Visual Cloud Road
–The Seminal Role of Edge-Native Applications
–Towards Drone-sourced Live Video Analytics for the Construction
Industry
–Towards a Distraction-free Waze
–EdgeDroid: An Experimental Approach to Benchmarking Human-in-
the-Loop Applications
–Edge-based Discovery of Training Data for Machine Learning
–Bandwidth-efficient Live Video Analytics for Drones via Edge
Computing
–Experimental Testbed for Edge Computing in Fiber-Wireless Broadband
Access Networks
–An Application Platform for Wearable Cognitive Assistance
–You Can Teach Elephants to Dance: Agile VM Handoff for Edge
Computing
–Live Synthesis of Vehicle-Sourced Data Over 4G LTE
–Assisting Users in a World Full of Cameras: A Privacy-aware
Infrastructure for Computer Vision Applications
–An Empirical Study of Latency in an Emerging Class of Edge Computing
Applications for Wearable Cognitive Assistance
–Edge Computing for Situational Awareness
Fig. 29 Working Process and Output from Living Edge Lab.109 Industry initiatives across edge computing16. Smart Edge Open
Smart Edge Open [26]is a MEC software toolkit that enables highly
optimized
and performance edge platforms to on-board and manage appli-
cations and network functions with cloud-like agility across any type of net-
work. This open-source distribution is designed to foster open collaborationand application innovation at the Network Edge and On-Premise Edge,
making it easier for cloud and Internet of Things (IoT) developers to engage
with a worldwide ecosystem of hardware, software and solutions integratorsto develop solutions for 5G and Edge. A typical Smart Edge Open-baseddeployment consists of a Kubernetes Control Plane and Edge Node.
Fig. 30 [26] shows logical components of Smart Edge Open Developer
Experience
Kit solution that is built on Kubernetes.
Smart Edge Open offers unique capabilities to accelerate application
development at the Edge:
–Abstracts out the network complexity for Cloud and IOT developers
making
migration of applications from the cloud to the edge easier
–Enables secure on-boarding and management of applications with anintuitive
web-based GUI
–Built on a modular, microservices based architecture, it provides the
building
blocks for various functionalities such as access termination, traf-
fic steering, multi-tenancy for services, service registry, service authenti-
cation, telemetry, application frameworks, appliance discovery andcontrol
–It is built on top of consistent and standardized APIs exposed to the devel-oper
community
17. Edge Multi Cluster Orchestrator (EMCO)
The Edge Multi-Cluster Orchestrator (EMCO) [27], is a software
framework
for intent-based deployment of cloud-native applications to a
set of Kubernetes clusters, spanning enterprise data centers, multiple cloud
service providers and numerous edge locations. It is architected to be flex-
ible, modular, and highly scalable. It is aimed at various verticals, includingtelecommunication service providers. The mission of the Project is to createa universal control plane that helps organizations to securely connect and
deploy workloads across public clouds, private clouds, and edge locations,
with end-to-end inter-application communication enabled. Fig. 31 [27]
provides logical architecture of EMCO.110 Sunku RanganathFig. 30 Smart Edge Open Developer Experience Kit.Number of K8s clusters (Edges or clouds) could be in tens of thousands,
number of complex applications that need to be managed could be in hun-
dreds, number of applications in a complex application could be in tens of
thousands and number of micro-services in each application of the complex
application can be in tens of thousands. Moreover, there can be multiple
deployments of the same complex applications for different purposes. To
reduce the complexity, all these operations are to be automated. There shall
be one-click deployment of the complex applications and one simple
dashboard to know the status of the complex application deployment at
any time. Hence, there is a need for Multi-Edge and Multi-Cloud distrib-
uted application orchestrator.
Compared with other multiple-clusters orchestration, EMCO focuses
on the following functionalities:
–Enrolling multiple geographically distributed Smart Edge Open clusters
and third-party cloud clusters.
–Orchestrating composite applications (composed of multiple individual
applications) across different clusters.
–Deploying edge services and network functions on to different nodes
spread across different clusters.
–Monitoring the health of the deployed edge services/network functions
across different clusters.
–Orchestrating edge services and network functions with deployment
intents based on compute, acceleration, and storage requirements.
–Supporting multiple tenants from different enterprises while ensuring
confidentiality and full isolation between the tenants.
Enterprise
EdgesEdges
CloudsNetwork
EdgesSoftware Platforms
Telco CO
EdgesMonitoringSecure Mesh
Controller Resource
SynchronizerNetwork
Configuration
ManagementCluster
Registration
ControllerDistributed
Application
scheduler
Secure WAN
ControllerDistributed
Cloud ManagerHardware Platform
Aware Controller
Pub/Pvt
CloudsCLI/GUI
EMCO
Fig. 31 EMCO architecture.112 Sunku Ranganath18. Global Systems for Mobile Association (GSMA)
The Global Systems for Mobile Association [28], usually referred to as
Global
Systems for Mobile Communications, or GSMA, represents the
interests of mobile operators worldwide, uniting more than 750 operators
with almost 400 companies in the broader mobile ecosystem, includinghandset and device makers, software companies, equipment providers and
internet companies, as well as organizations in adjacent industry sectors.
The GSMA represents its members via industry programs, working groupsand industry advocacy initiatives.
The Operator Platform Group (OPG), within GSMA, has defined the
Edge Cloud functionality in the OPG.01 Operator Platform Telco EdgeProposal and the Telco Edge Cloud (TEC) taskforce works on launchinga global telco edge computing service that implements it. High level refer-ence architecture of Telco Edge as shared by Operator Platform Telco Edge
proposal (OPG.01) is shared in Fig. 32 [28] . The TEC group has produced
following
artifacts:
–Operator Platform Telco Edge Proposal
–Telco Edge Cloud: Edge Service Description and Commercial Principles
–Operator Platform Telco Edge Requirements
–Edge and Cloud Developments in Europe
The Telco Edge Cloud (TEC) Pre-Commercial Trials Foundry project
[28], that is part of GSMA, aims to bring the industry together to collaborate
NorthBound
Interface
East-WestBound
Interface
User-Network
Interface
APP
APIsSouthBound
InterfaceApplication Provider
OP 1 OP 2
Edge computing platform Users
Fig. 32 High-level Reference architecture.113 Industry initiatives across edge computingaround a number of pre-commercial trials of Telco Edge Cloud capabilities
with app developers and service providers sponsored by the industry partners
and supported by MNOs. The potential for Independent software vendors(ISVs) is a pilot opportunity with at least two operators in a pre-commercialtrial in the second half of 2021. In doing so, generate proof points and pro-mote the benefits of using Telco Edge Cloud infrastructure to stimulate the
development of a wider ecosystem.
References
[1] LF Edge, The New Open Edge IOT+Telecom+Cloud+Enterprise+Industrial,
LFEdge.org .
[2] Akraino, Welcome to Akraino Wiki, akraino.org .
[3] Edgexfoundary, EdgeX Foundary Project Wiki, edgexfoundary.org .
[4] LFEdge Projects, Project EVE, LFEdge.
[5]
Linux Foundation Networking, home page. lfnetworking.org .
[6] ONAP, home page, onap.org/about .
[7] Anuket Artifacts, anuket.io/artifacts.[8]
ORAN Alliance, homepage, o-ran.org .
[9] D.Sabella, et al., “Toward Fully Connected Vehicles: Edge Computing for Advanced
Automotive
Communications, 5GAA_T-170219.
[10] Open Networking Foundation, homepage, opennetworking.org .
[11] 3GPP, about-us. 3gpp.org/about-3gpp .
[12] 3gpp, 3gpp Specifications Group.
[13]
Small Cell Forum. smallcellforum.org .
[14] Broadband Forum, about us, broadbandforum.org/about-bbf .
[15] Qacafe, An Overview of The User Services Platform (USP/TR-369), Resources by
Qacafe.
[16]
5G Alliance for Connected Industries and Automation, about-us, 5GACIA.
[17] 5G Automotive Association, about-us, 5GAA.
[18] Automative Edge Computing Consortium, About, AECC.
[19] Telecom Infra Project, homepage, TIP.
[20] IEEE Future networks, International Next Generation Roadmap, futurenetworks.ieee.
org/roadmap.
[21] R. Sunku., Edge Automation Platform, International Network Generations
Roadmap—2021
Edition.
[22] KubeEdge Maintainers, KubeEdge: Cloud Native Edge Computing, Cloud Native
Computing Foundation Blogs.
[23] StarlingX, homepage, starlingx.io.
[24] Open Infrastructure Foundation, homepage, openinfra.dev.
[25] Open Edge Computing Initiative, Homepage, Openedgecomputing.[26] Smart Edge Open, github.io.
[27] ONAP, EMCO Architecture & Design, wiki.
[28] GSMA, aboutus, GSMA.114 Sunku RanganathAbout the author
Sunku Ranganath is a Solutions Architect
for Edge Compute at Intel. For the last few
years, his area of focus has been on enabling
solutions for the Telecom domain, including
designing, building, integrating, and bench-
marking NFV based reference architectures
using Kubernetes & OpenStack components.
Sunku has been an active contributor to mul-
tiple open-source initiatives. He serves as a
maintainer for CNCF Service Mesh Per-
formance & CollectD projects and as an elec-
ted member on the Technical Steering
Committee for OPNFV (now Anuket). He
is an invited speaker to many industry events, authored multiple publica-
tions, filed eight patents, and contributed to IEEE Future Networks Edge
Service Platform & ETSI ENI standards. He is a senior member of the IEEE.115 Industry initiatives across edge computingThis page intentionally left blankCHAPTER FOUR
IoT-edge analytics
for BACON-assisted multivariatehealth data anomalies
Partha Pratim Ray
Department of Computer Applications, Sikkim University, Gangtok, India
Contents
1.Introduction 118
2.Related works 119
3.System design 120
3.1 BACON algorithm for selection of multivariate outliers nomination 121
3.2 Initial basic subset of regression data algorithm 123
3.3 BACON robust regression algorithm 124
3.4 IoT-BACON-EEM algorithm 126
4.Results 128
4.1 Robust distance wise analysis 128
4.2 Robust linear regression analysis for HRV 129
4.3 Coefficients 132
5.Conclusion 134
References 134
About the author 137
Abstract
Anomaly detection in Internet of Things (IoT)-enabled systems can significantly improve
the quality of the deployed systems. Though existing techniques can detect anomaliesfrom a dataset, more efficient algorithms can be used to minimize the burden of exces-sive computational overhead on the resource-constrained IoT-edge device pool. In thischapter, we implement the blocked adaptive computationally efficient outlier nomina-tors (BACON) algorithm along with the estimated-expectation/maximization (EEM)method to improve the anomaly nomination for IoT-based health dataset. We deploy
the weighted variant of the BACON algorithm package —“wbacon ”from the
R repository to validate the utilization of anomaly nomination for an IoT-edge-enabled
health dataset. The deployed scheme advocates the importance of distance wiseMahalanobis statistical tool for efficient nomination of potential outliers from the IoTdataset.
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0021171. Introduction
Anomaly nomination refers to a process to potentially identify a set of
observations from a large dataset which can be termed as the outliers [ 1–3].
The
outliers are the data points that form a dataset that behave abnormally
or present discrepancies in terms of behavior of homogeneity with other
existing observations [ 4,5]. Such anomalies can significantly disturb the
analytical
perspective when not identified during the early stage of data
gathering [ 6–8], especially for the sensor-oriented ubiquitous systems where
misinterpretation
of potential observation can significantly damage the
performance of the ecosystem in later stages [ 9–11].
In
recent years, IoT has emerged as a key enabler of smart applications.
The key focus of IoT-edge-enabled systems is to provide minimal delayand maximum throughput of the prospective use cases at the extremeedge (edge) of the existing networking infrastructures [ 12–14]. Herein, a
number
of sensors generate data streams and propagate toward micro-
controllers which is predefined with a set of algorithms to execute smarttasks. Normally, most of the data is sent to the remote cloud computingplatforms for permanent storage where a data analyst can perform givenanalytical jobs in future [ 15,16]. But, this domain of technology is currently
facing
a serious challenge as follows. Due to excessive use of sensors in
IoT-edge-assisted use cases, gradual dumping of huge piles of data is beingmade to the cloud platforms. Thus, it is regularly consuming the costlynetwork bandwidth and increasing the data traffic. It is also populating cloud
storage systems with big sensor data which are often not properly diagnosed.
It would have been better if we could imply light-weight edge analyticssolutions at the IoT devices that could take instantaneous decisions aboutan IoT data whether to send it to a remote cloud or not. In doing so, band-
width consumption and traffic generation could have been minimized that
would in turn make the cloud storage systems more useful for other impor-tant data saving aware tasks [ 17–19].
This
chapter presents the implementation of such a scenario with help of
the amalgamated BACON-EEM methods for the IoT-edge ecosystem. Wedeploy the “wbacon” package [ 20] from the R repository to identify and
nominate 
the potential outliers from an IoT health dataset [ 21]. The IoT
health
dataset contains six variables each with 617 rows. We use human skin
conductance (C), saturated oxygen (SO), body core temperature (T), blood
pressure low (BL), blood pressure high (BH), and heart rate variability118 Partha Pratim Ray(HRV) as the variables. Each of these variables contains some intentionally
incorporated outliers so as to enable the deployed BACON-EEM algorithm
to nominate such observations as the potential outliers. We investigate theefficiency of this algorithm for the IoT-based scenario due to the fact thatsuch provisions are highly resource constrained in nature. We aim to studyhow the BACON-EEM algorithm combination can be useful to efficiently
nominate the introduced outliers from the given IoT health dataset. It is
expected that the algorithm can minimize the computational overheadfrom the IoT devices, thus giving a space for performing other essentialcomputations. We choose the statistical tools instead of conventional
machine or deep learning aware algorithms owing to the fact that such
schemes are normally computationally intensive thus not always suitablefor the IoT-edge deployment.
The major contributions of this chapter can be summarized as follows.
To implement the BACON-assisted EEM algorithm for an IoT-edgeecosystem
To analyze the results in terms of robust distances and regression analytics
To deploy the Mahalanobis distance and distance from median appro-
aches
for anomaly nomination
To assess the basic subset formation and increment for the regression data
To present regression use case for the HRV against C, SO, T, BL, and
BH
variables
2. Related works
In Ref. [ 22], an algorithm is proposed called “FiRE.1” to sketch the
linear-time
centric global anomaly identification. New outlierness criteria
are also proposed herein this paper.
A landmark-based outlier detection approach is presented in Ref. [ 23].
Such
a soft mechanism is based on the candidate and the confirmed outliers.
It adopts a sliding window landmark model when a potential candidate isassumed as an anomaly.
An overview of swarm intelligence aware outliers detection methods is
discussed in Ref. [ 24]. A systematic review on security logs for data leak
detection 
is performed in Ref. [ 25].
A
scalable density aware clustering algorithm for anomaly detection is
proposed in Ref. [ 26] where large size datasets can be applied. The algorithm
is
linear in terms of time complexity.119 IoT-edge analytics for BACON-assisted multivariate health data anomaliesStudy of anomalies in the multivariate dataset for Alzheimer’s disease
detection is investigated by Ref. [ 27]. The study uses the parallel indepen-
dent
component analysis, correlation analysis, sparse partial test, genome-
wide association, and sparse reduced rank regression out of many such
techniques.
Anomaly detection method is investigated in Ref. [ 28] for identification
of
intrusions by using a machine learning technique. A parallel tensor
factorization based on rational learning is proposed in Ref. [ 29].
Anomaly
detection scheme is also linked with the zero-shot learning for
study of signal recognition by using convolution neural network [ 30].
Privacy
for the cyberphysical world is an important factor that must be
preserved against intrusions. Thus in Ref. [ 31], outliers are detected from
the
intrusion recognizing framework.
Deep neural networks have the potential to detect anomalies in a
network. In Ref. [ 32], an anomaly detection technique is employed to
identify
faults in an unmanned aerial vehicles network.
Fraud in healthcare-related information spreading can be dangerous for
the patients at a large. A sub peer group analysis aware outliers detection
method is implied to find fraud in healthcare information sharing [ 33]. In
Ref.
[34], outliers detection technique is implemented for an electoral
dataset
to find the mixed-effects regressions.
Lessons Learned: We learned that none of the works in literature uses the
BACON-EEM algorithm for nomination of anomalies for an IoT-edge-enabled health dataset. The novelty of this chapter is as follows: (i) use of
BACON-EEM methods for IoT-based ecosystem, (ii) minimization ofprocessing overhead in IoT devices, and (iii) investigation of the robustMahalanobis distances and distance from medians as key enablers of outliers
nomination.
3. System design
The implied system design depends on several factors considering
the optimality, breakdown, and equivariance of the observations [ 35–37].
For
example, we can take a case having an IoT data set with nnumber
of data points related to pvariables. In this IoT dataset, there exists k
which is an unknown value. The unknown value k<n
2. In a normal
scenario, one can perform a brute-force search to find the outliers having
a subset of size k¼1, 2, 3…,n
2. This task is computationally expensive for120 Partha Pratim Raya resource-constrained IoT-based system due to the large number of
subsets, i.e.,P n
2
k¼1n
k/C0/C1
which require huge amount of processing and mem-
ory capacity [ 38–41]. Moreover, this process may not be successful to find all
the
subsets nor exclude the anomalies from the IoT dataset.
Nomination of anomalies from an IoT dataset can be done in many
ways [ 42–44], for instance, use of minimum covariance determinant
(MCD)
and minimum volume ellipsoid (MVE). Thus, we need to first
compute then
k/C0/C1
, i.e., volume of ellipsoids [ 45–47]. Second, we can
choose
a subset that provides minimum determinant or volume where
n
k/C0/C1
≪Pn
2
k¼1n
k/C0/C1
and h¼n+p+1
2. Again, this method is not feasible for an
IoT-based environment due to a large computational complexity.
Use of estimators can improve the optimality condition for both anomaly
detection and robust regression environment [ 48,49]. An estimator can be
expressed
as in Eq. 1 to follow an affine equivariant, where Tis an estimator
and Aand bare nonsingular matrix and vector, respectively [ 50,51].
TðXA+bÞ¼TðXÞA+b (1)
Normally,
the brute-force technique is essentially an affine equivariant
due to the fact that both covariance matrix and multivariate mean possess
similar characteristics. In practice, it is very difficult to find an affine
equivariant technique that has a very high breakdown point. In most of
the cases, the breakdown point is bounded by1
p+1.
In this section, we present two techniques, e.g., (i) multivariant and
(ii) regression data that can be applied based on two versions of algorithms.
Version 1 is nearly affine equivariant and version 2 is affine equivariant.
Version 1 has high breakdown points but version 2 has lower breakdown
points.
3.1 BACON algorithm for selection of multivariate outliers
nomination
We deploy the BACON algorithm [ 8] to enable the computation more
efficiently
for an IoT-based environment. This algorithm can be applied
to the large IoT datasets. The methods herein presented have negligible sen-sitivity to the starting point. Though a robust start can give higher assurance
of very high breakdown, robust initial points do not conform to the affine
equivariant in its successive iterations.121 IoT-edge analytics for BACON-assisted multivariate health data anomaliesThe algorithm begins with a small subset of data which is presumed to
be free of anomaly observations. Later, the subset is increased in size to
include all normal data points. The data points that are excluded can benominated as potential outliers. To save computational chaos in the IoTdevices, the basic subset is expected to grow rapidly and validate against aspecific criteria. This algorithm removes the need of (i) sorting of large
arrays of discrepancies, (ii) minimization of the number of covariance
matrices, and (iii) reduction in the number of inverted covariance matrixcomputations. Algorithm 1 presents the detailed step-by-step methods of
this
algorithm. An iteration is imposed to increase the size of the basic subset.
ALGORITHM 1 BACON Algorithm for Identification of
Multivariate Anomaly Nomination.122 Partha Pratim RayIt also helps to minimize the memberships to the data points against the
current basic subset resulting in a state of nonanomalous points. Bigger
the basic subset size, more the possibility to yield reliable estimates.
Assume the scenario where there exists a Xmatrix containing the total
nxpnumber of IoT data points, where columns are variables and rows are
observations. The BACON algorithm needs to find the initial basic subset
with size mgreater than p. The subset can be developed by the BACON
algorithm itself or can be supplied as an external input. The initial basicsubset must be “clean” where no anomaly can be present. Now, estimationcan be done only when the basic subset size is substantially large. Hence,
ifm¼cp, the estimation can be based on minimum cobservations per
parameter of the model.
We can use two versions to create the initial basic subset, e.g., (i) version
1—Mahalanobis distance and (ii) version 2—distance from medians. The
Mahalanobis distance-based method is not robust but an affine equivariant,
whereas the distance from medians method is robust but not an affineequivariant due to the fact that the coordinate wise median is not consideredto be an affine equivariant.
The BACON algorithm helps to nominate the subset of data points as
outliers by using either version 1 or 2. Propagating in this way, it laternominates a new subset of central observations and so on. When the initialsubset is not closer to the middle of the normal data points, successiveprogression of iterations tends to drift to the center of the data set. When
the basic subset grows more and more, the covariance matrix and mean
get more stable.
3.2 Initial basic subset of regression data algorithm
Let us now consider that y¼Xβ+Erepresents a generic linear model
where Xis an IoT-based nxpdata matrix where prefers to variables
having p<n. The yrepresents the n-vector of responses. The βrepresents
thep-vector of several unknown variables and Edenotes the random errors
having n-vectored size. In this context, mean E(EjX)¼0 and variance
var(EjX)¼σ2In, where σ2and Inrefer to unknown parameter and
identity matrix with nth order. The least-square estimate (LSE) of βand
σ2is given by bβ¼ðXTXÞ/C01XTy. The residual mean square (RMS) is
given bybσ2¼SSE
ðn/C0pÞ. The ecan be obtained by e¼(In/C0P)ythat is essentially
a vector having generic residuals. The SSE ¼eTerepresents the residual
sum of squares. We can also state that P¼X(XTX)/C01XT. We also assume123 IoT-edge analytics for BACON-assisted multivariate health data anomaliesthat yb,Xbare the basic subsets where each of the IoT-based observations
is indexed by b,bis a set of indices. The bβbcan be assumed as the
estimated regression coefficients upon fitting the model on b. Herein, we
can assume that SSE bandbσ2
brefer to residual sum of squares and residual
mean square, respectively.
Algorithm 2 presents the basic subset formation for regression IoT data.
At
first, the diis computed based on the ymandXmwhere the mnumber of
observations are present with smallest possible values. When the Xmis not
found to be of full rank, the algorithm increases the basic subset while
incorporating the smallest possible values of dið/C22xb,CbÞ. This process con-
tinues until it has a full rank. Next, the algorithm computes ti(ymXm) and
identifies the p+ 1 data points where jti)ym,Xmjis considered as the smallest.
Later, it declares those as the basic initial subset. The piiis referred to as the
diagonal elements of the projection matrix P¼X(XTX)/C01XT. The scaled
ordinary least squared residuals are obtained when xiXband the scaled
prediction errors are generated when xi62Xb,ti(yb,Xb).
The method starts with m¼ðn+p+1Þ
2by using the distances to define
discrepancies. The initial subset is p+ 1 and it increases until it reaches to
the size ofðn+p+1Þ
2data points. It can increase beyond this limit but
gets restricted when the r+ 1 smallest possible discrepancies aim to
exceed tðα
2ðr+1Þ,r/C0pÞ. Herein, ris the size of the current basic subset for each
step and t(α,r/C0p)refers to 1 /C0αpercentiles of t-distribution. The degree
of freedom used herein is r/C0p.
This algorithm tries to repeatedly fit the regression model. However, the
sorting of discrepancies at each loop step is a computationally intensiveprocess. Algorithm 3 is implied herein to grow the basic subset in blocks.
Thus, 
it can retain the capability for adaptation of the IoT data. It can
overlook the unwanted computations within the process, i.e., a robustdesign is a must.
3.3 BACON robust regression algorithm
TheAlgorithm 3 aims to nominate the observations which are not included
in
the final basic subset as the potential anomalies. This algorithm minimizes
the regression computations. It also reduces the evaluations of the discrep-ancies. There is no significance to order or sort the discrepancies rather just
simple check against the given constant value. Such checking requires only
nnumber of operations.124 Partha Pratim RayALGORITHM 2 BACON Algorithm for Basic Subset Formation
for Regression Data.125 IoT-edge analytics for BACON-assisted multivariate health data anomalies3.4 IoT-BACON-EEM algorithm
Algorithm 4 aims to nominate the outliers with a combination of the
BACON
and estimated expectation/maximization (EEM) method [ 9].
The
EEM acts as the basis of this algorithm where X¼Xo[Xm, where
Xoand Xmrefer to observed and missing values from an IoT data set X.
Here, it is assumed to have the missingness as ignorable and independent
of sampling [ 22,23]. Herein, the data log-likelihood can be presented as
l(θjX)¼ηθT/C1T(X)+Ng(θ)+c, where cis a constant. Such an expression
is derived from the factorization of P(Xjθ)¼P(Xojθ)P(XmjXo,θ) with
the normal log-likelihood as l(θjX)¼l(θjXo)+ log(P(XmjXo,θ)) + c.
The unknown average P(XmjXo,θ) captures the interdependence where
over l(θjX)a n d P(XmjXo,θt). The θtrefers to the initial estimate on an
unknown parameter, whereas the θ(t+1)maximizes the next level expec-
tation. The ηðθÞ¼η1ðθTÞ,η2ðθTÞ,η3ðθTÞ,…,ηkðθTÞpresents the cano-
nical form of θwith additive statistic TjðXÞ¼PN
i¼1hjxi. The linearity
ofl(θjX) can be replaced by the E(Tj(X)jXo,θt) for a multivariate normal
distribution X¼X1,X2,X3,…,Xp.W ec a nu s eav e c t o rT ðXÞ¼
½T1ðXÞ,T2ðXÞ,T3ðXÞ,…,TkðXÞ/C138T. We can also inferPN
i¼1xk
iandPN
i¼1xk
ixlias the sums and sum of products, respectively, with k,p/C211.ALGORITHM 3 BACON Robust Regression Algorithm.126 Partha Pratim RayThe sum of products can be analogously presented as EPN
i¼1xk
ijXo,
θt¼PN
i¼1Exk
ijXo,θt¼PN
i¼1Eðxk
ijxobsi,θtÞ, where p/C21k/C211. We can also
use the Horvitz– Thompson estimators as Tk0¼P
swiEðxk
ijXobsi,θtÞand
Tkl¼P
swiEðxk
ixlijxobsi,θtÞ.T h e Tklis estimated from the EðPN
i¼1xk
ixlijXo,θtÞ.
With this the EE-step is concluded. Now, for the M-step we can consider
θðt+1Þ¼sop½0/C138TklP
swiwhere k/C210,p/C21landsop[0] refers to the sweep operator
for the first column of the IoT matrix.
Algorithm 4 presents the algorithm where BACON and EEM are
combined
with an IoT-based dataset to roughly estimate the BACON
at each iteration. This results in a minimal number of iterations to be
involved. Herein, the Mahalanobis distance is assumed as the marginal
case with a factorp
qwith nonmissing number of variables q¼P
krik. The
partitioning of xis done as ½xT
o,xTm/C138Tand C00refers to the part of the
covariance matrix against xo.ALGORITHM 4 IoT-BACON-EEM Algorithm.127 IoT-edge analytics for BACON-assisted multivariate health data anomalies4. Results
In this section, we discuss the results obtained from the study. First, we
discuss the robust distance wise analysis and Second, we present robust linear
regression analysis for HRV as a case study.
4.1 Robust distance wise analysis
4.1.1 Robust distance
The outliers present in the IoT dataset are clearly shown in Fig. 1 . The upper
plot shows the robust Mahalanobis distances against the index of IoT-based
observations. The robust distance is provided within the range of 5 –30. An
observation wise plotting is done in this graph. Also, we present the robust
distance against the univariate projections in the lower plot. Both plots show
32 potential nominations of observations as outliers shown in red circles.
We can also see that there are some black boundary circles near the distance
line at 5. It depends on the data analysts to decide whether to consider
these circles as potential anomalies.
Fig. 1 Robust distance of IoT data points.128 Partha Pratim Ray4.2 Robust linear regression analysis for HRV
We perform regression between the HRV and other variables of the IoT
dataset. We obtain several regression information from the experiment as
discussed below.
4.2.1 Residual-fitted
We perform the residual vs fitted values for five different regression studies,
e.g., (i) HRV vs C + SO + T + BL + BH, (ii) HRV vs C+ SO + BL + BH,
(iii) HRV vs C + BL + BH, (iv) HRV vs BL + BH, and (v) HRV vs BH.
Fig. 2 shows all the charts for different regression aspects. Various fitted
values are obtained on the x-axis, whereas the residuals are plotted on the
y-axis with a range of /C03t o3 .
4.2.2 Normal Q –Q
We perform the Q –Q plots for five different regression studies, e.g.,
(i) HRV vs C + SO + T+ BL + BH, (ii) HRV vs C + SO + BL + BH,
(iii) HRV vs C + BL + BH, (iv) HRV vs BL + BH, and (v) HRV vs
BH. Fig. 3 shows all the charts for different regression aspects. Various
Fig. 2 Residual vs fitted analysis for HRV.129 IoT-edge analytics for BACON-assisted multivariate health data anomaliestheoretical quantile values are obtained on the x-axis, whereas the standard-
ized residuals are plotted on the y-axis with a range of /C03t o4 .
4.2.3 Scale location
We perform the scale locations for five different regression studies, e.g.,
(i) HRV vs C + SO + T+ BL + BH, (ii) HRV vs C+ SO + BL + BH,
(iii) HRV vs C + BL + BH, (iv) HRV vs BL+ BH, and (v) HRV vs
BH.Fig. 4 shows all the charts for different regression aspects. Various fitted
values are obtained on the x-axis, whereas the standardized residuals in
square root are plotted on the y-axis with a range of 0 to 1.5.
4.2.4 Robust Mahalanobis distance
We perform the robust Mahalanobis distances for five different regression
studies, e.g., (i) HRV vs C + SO + T + BL + BH, (ii) HRV vs C + SO +
BL + BH, (iii) HRV vs C + BL + BH, (iv) HRV vs BL + BH, and
(v) HRV vs BH. Fig. 5 shows all the charts for different regression aspects.
Various robust distances are obtained on the x-axis, whereas the standardized
residuals are plotted on the y-axis with a range of /C02t o6 .
Fig. 3 Normal Q –Q plot analysis for HRV.130 Partha Pratim RayFig. 4 Scale location analysis for HRV.
Fig. 5 Robust Mahalanobis distance analysis for HRV.131 IoT-edge analytics for BACON-assisted multivariate health data anomalies4.3 Coefficients
Table 1 presents the comparison of regression residuals for five regression
studies
on HRV. We find almost similar values of Min, 1Q, Median, 3Q,
and Max parameters. Median value of all the regression studies lies within
/C00.01206 to /C00.01891. However, Table 2 presents the residual square
error
(RSE), multiple R2, adjusted R2, F-statistic, and p-value. We find
minimum RSE for the HRV vs BH regression study. However, the differ-ence of values of the RSE with respect to other regressions is very muchsimilar. Other parameters are also smallest for the HRV vs BH regressionstudy. It has the highest p-value, i.e., 0.9976. It infers that HRV is dependent
on all the other variables but mostly on the BH and combination of C + SO
+ T + BL + BH than others.
We also perform a comparison of parametric values of regression studies
as shown in Table 3 . Significance is found for all the regressions except HRV
vs
C + SO + T + BL + BH. Cumulative standard error is higher for this
regression than others. In every regression study, the BH has the highest
involvement on the dependence factor over HRV.
Table 1 Comparison of regression residuals.
Regression Min 1Q Median 3Q Max
HRV—C+SO+T+BL+BH /C02.48434 /C00.58511 /C00.01206 0.64300 2.42821
HRV—C+SO+BL+BH /C02.48276 /C00.58523 /C00.01891 0.63423 2.42865
HRV—C+BL+BH /C02.4782 /C00.5817 /C00.0134 0.6268 2.4206
HRV—BL+BH /C02.47745 /C00.58098 /C00.01395 0.63766 2.41625
HRV—BH /C02.46691 /C00.58574 /C00.01708 0.64618 2.43519
Table 2 Comparison of estimations of regression coefficients.
Regression RSE Multiple R2Adjusted R2F-Static p-value
HRV—C+SO+T+BL+BH 0.7361 0.002141 /C00.006038 0.2618 0.9338
HRV—C+SO+BL+BH 0.7356 0.001893 /C00.004642 0.2897 0.8847
HRV—C+BL+BH 0.7352 0.001369 /C00.003526 0.2797 0.8401
HRV—BL+BH 0.7346 /C00001177 /C00.002089 0.3589 0.6986
HRV—BH 0.7345 1.447e/C0 08/C00.001629 8.882e /C006 0.9976132 Partha Pratim RayTable 3 Comparison of parametric values of regressions.
Estimate Std. Error tvalue Pr( >|t|)
HRV—C + SO + T + BL + BH
(Intercept) 6.8176680 4.1625086 1.638 0.102
C 0.0167545 0.0479677 0.349 0.727
SO /C00.0118980 0.0218045 /C00.546 0.585
T /C00.0137873 0.0353746 /C00.390 0.697
BL /C00.0082075 0.0099504 /C00.825 0.410
BH /C00.0002322 0.0072381 /C00.032 0.974
HRV—C + SO + BL + BH
(Intercept) 5.5004269 2.4281514 2.265 0.0238 *
C 0.0172364 0.0479184 0.360 0.7192
SO /C00.0123191 0.0217626 /C00.566 0.5716
BL /C00.0082758 0.0099420 /C00.832 0.4055
BH /C00.0001456 0.0072297 /C00.020 0.9839
HRV—C + BL + BH(Intercept) 4.3106408 1.2151324 3.547 0.000419 ***
C 0.0167544 0.0478843 0.350 0.726539
BL /C00.0082686 0.0099364 /C00.832 0.405649
BH /C00.0002145 0.0072247 /C00.030 0.976319
HRV—BL + BH
(Intercept) 4.3888818 1.1935249 3.677 0.000256 ***
BL /C00.0084056 0.0099216 /C00.847 0.397212
BH /C00.0002168 0.0072195 /C00.030 0.976056
HRV—BH
(Intercept) 3.6994641 0.8728984 4.238 2.6e /C005***
BH /C00.0000215 0.0072141 /C00.003 0.998
Significance codes: 0 “ ***” 0.001 “ **” 0.01 “ *”.133 IoT-edge analytics for BACON-assisted multivariate health data anomalies5. Conclusion
In this chapter, we implement the BACON-EEM algorithm for
the IoT-edge-assisted health dataset. The main focus of this paper is to
enable IoT-based devices to nominate the potential outliers present in thegiven IoT health dataset. We deploy the robust Mahalanobis distanceand distance from median schemes to nominate the anomalies from the
IoT health dataset. Results show that the BACON-EEM algorithm can
minimize the overhead of processing capabilities of the IoT devices.
References
[1]A.S. Hadi, Identifying multiple outliers in multivariate data, J. R. Stat. Soc. B
54
(3) (1992) 761– 771.
[2] D. Donoho, Breakdown properties of multivariate location estimators, (Ph.D. qualify-
ing
paper), Department of Statistics, Harvard University 1982.
[3]A. Atkinson, Stalactite plots and robust estimation for the detection of multivariateoutliers,
in: S. Morgenthaler, E. Ronchetti, W. Stahel (Eds.), Data Analysis and
Robustness, Birk €auser, 1993.
[4]R.J.A. Little, D.B. Rubin, Statistical Analysis With Missing Data, John Wiley & Sons,Inc.,
New York, 1987.
[5]R.A. Maronna, Robust M-estimators of multivariate location and scatter, Ann. Stat.
4
(1976) 51 –67.
[6]J. Schafer, Analysis of Incomplete Multivariate Data, Volume 72 of Monographs on
Statistics
and Applied Probability, Chapman & Hall, 2000.
[7]D. Rocke, D. Woodruff, Computation of robust estimates of multivariate location and
shape,
Stat. Neerlandica 47 (1993) 27 –42.
[8]N. Billor, A.S. Hadi, P.F. Vellemann, BACON: blocked adaptive computationally effi-
cient
outlier nominators, Comput. Stat. Data Anal. 34 (2000) 279 –298.
[9]C. B /C19eguin, B. Hulliger, The BACON-EEM algorithm for multivariate outlier detec-
tion in incomplete survey data, Surv. Methodol. 34 (2008) 91 –103.
[10] J.F. Magnotti, N. Billor, Finding multivariate outliers in fMRI time-series data,
Comput.
Biol. Med. 53 (2014) 115– 124.
[11] B. Du, L. Zhang, Random-selection-based anomaly detector for hyperspectral imagery,
IEEE
Trans. Geosci. Remote sens. 49 (5) (2010) 1578 –1589.
[12] M. Iqbal, M. Riaz, W. Nasir, Multivariate outlier detection: a comparison among two
clustering
techniques, Pakistan J. Agric. Sci. 54 (1) (2017) 227 –231.
[13] J.A. Jablonski, T.J. Bihl, K.W. Bauer, Principal component reconstruction error forhyperspectral
anomaly detection, IEEE Geosci. Remote Sens. Lett. 12 (8) (2015)
1725 –1729.
[14] K.M. Sunderland, D. Beaton, J. Fraser, D. Kwan, P.M. McLaughlin, M. Montero-
Odasso,
A.J. Peltsch, F. Pieruccini-Faria, D.J. Sahlas, R.H. Swartz, S.C. Strother,
The utility of multivariate outlier detection techniques for data quality evaluation in
large studies: an application within the ONDRI project, BMC Med. Res.Methodol. 19 (1) (2019) 1 –16.
[15] A. Cerioli, A. Farcomeni, M. Riani, Strong consistency and robustness of theforward
search estimator of multivariate location and scatter, J. Multivar. Anal. 126
(2014) 167– 183.134 Partha Pratim Ray[16] D. Neira-Rodado, C. Nugent, I. Cleland, J. Velasquez, A. Viloria, Evaluating the
impact
of a two-stage multivariate data cleansing approach to improve to the perfor-
mance of machine learning classifiers: a case study in human activity recognition,Sensors 20 (7) (2020) 1858.
[17] D. Pelleg, A. Moore, Active learning for anomaly and rare-category detection, Adv.Neural
Inf. Process. Syst. 17 (2004) 1073 –1080.
[18] J. Gao, F. Liang, W. Fan, C. Wang, Y. Sun, J. Han, On community outliers and theirefficient
detection in information networks, in: Inproceedings of the 16th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining,2010, pp. 813– 822.
[19] L. Gao, Q. Guo, A.J. Plaza, J. Li, B. Zhang, Probabilistic anomaly detector for remotelysensed
hyperspectral data, J. Appl. Remote Sens. 8 (1) (2014) 083538-1 –083538-20.
[20] WBACON R Package, Available Online, https://cran.r-project.org/web/packages/wbacon/index.html,
2021. accessed 28.07.21.
[21] IoT Health dataset, Available Online, https://github.com/ParthaPRay/IoTHealthDataSet/blob/main/IoT-multivar-anomaly-with-SO.csv,
2021. accessed 03.08.21.
[22] P. Gupta, A. Jindal, D. Sengupta, Linear time identification of local and global outliers,
Neurocomputing
429 (2021) 141– 150.
[23] K. Kolomvatsos, C. Anagnostopoulos, Landmark based Outliers Detection in Pervasive
Applications,
in: 2021 12th International Conference on Information and
Communication Systems (ICICS), IEEE, 2021, pp. 201 –206.
[24] S. Mishra, R. Sagban, A. Yakoob, N. Gandhi, Swarm intelligence in anomaly detectionsystems:
an overview, Int. J. Comput. Appl. 43 (2) (2021) 109 –118.
[25] R. A´vila, R. Khoury, R. Khoury, F. Petrillo, Use of security logs for data leak detection:a systematic literature review, Secur. Commun. Netw. 2021 (2021) 1 –29.
[26] S.A.N. Nozad, M.A. Haeri, G. Folino, SDCOR: Scalable density-based clustering forlocal
outlier detection in massive-scale datasets, Knowl.-Based Syst. 228 (2021) 107256.
[27] J. Sheng, L. Wang, H. Cheng, Q. Zhang, R. Zhou, Y. Shi, Strategies for multivariate
analyses
of imaging genetics study in Alzheimer’s disease, Neurosci. Lett. 762 (2021)
136147.
[28] T. Rincy N, R. Gupta, Design and development of an efficient network intrusion
detection
system using machine learning techniques, Wireless Commun. Mobile
Comput. 2021 (2021) 1 –35.
[29] F. Al-Obeidat, A. Rocha, M.S. Khan, F. Maqbool, S. Razzaq, Parallel tensor factori-zation
for relational learning, Neural Comput. Appl. (2021) 1 –10. Early access.
[30] Y. Dong, X. Jiang, H. Zhou, Y. Lin, Q. Shi, SR2CNN: zero-shot learning for signal
recognition,
IEEE Trans. Signal Process. 69 (2021) 2316 –2329.
[31] I.A. Khan, D. Pi, N. Khan, Z.U. Khan, Y. Hussain, A. Nawaz, F. Ali, A privacy-
conserving
framework based intrusion detection method for detecting and recognizing
malicious behaviours in cyber-physical power networks, Appl. Intell. 51 (2021) 1 –16.
[32] A. Alos, Z. Dahrouj, Using multiple deep neural networks platform to detect differenttypes
of potential faults in unmanned aerial vehicles, J. Aerosp. Technol. Manag.
13 (2021) e1321.
[33] L. Settipalli, G.R. Gangadharan, Healthcare fraud detection using primitive sub peergroup
analysis, Concurr. Comput. Pract. Exp. 33 (23) (2021) e6275.
[34] A.M. Di Brisco, S. Migliorati, A spatial mixed-effects regression model for electoraldata,
Stat. Methods Appl. 30 (2) (2021) 543– 571.
[35] Y. Altmann, S. McLaughlin, A. Hero, Robust linear spectral unmixing using anomalydetection,
IEEE Trans. Comput. Imaging 1 (2) (2015) 74 –85.
[36] A. Arfaoui, A. Kribeche, S.M. Senouci, M. Hamdi, Game-based adaptive anomalydetection
in wireless body area networks, Comput. Netw. 163 (2019) 106870.
[37] P. Sawant, N. Billor, H. Shin, Functional outlier detection with robust functional prin-cipal
component analysis, Comput. Stat. 27 (1) (2012) 83 –102.135 IoT-edge analytics for BACON-assisted multivariate health data anomalies[38] A.J. Messer, K.W. Bauer Jr, Method of sensitivity analysis in anomaly detection algo-
rithms
for hyperspectral images, in: Algorithms and Technologies for Multispectral,
Hyperspectral, and Ultraspectral Imagery XXIII, vol. 10198, International Societyfor Optics and Photonics, 2017, p. 101980U.
[39] S. Sandbhor, N.B. Chaphalkar, Impact of outlier detection on neural networks basedproperty
value prediction, in: Information Systems Design and Intelligent Applications,
Springer, Singapore, 2019, pp. 481– 495.
[40] K.M. Kirtland, Outlier Detection and Multicollinearity in Sequential VariableSelection:
A Least Angle Regression-Based Approach, Cornell University, 2017.
[41] S.A. Tomlins, D.R. Rhodes, J. Yu, S. Varambally, R. Mehra, S. Perner, F.
Demichelis,
B.E. Helgeson, B. Laxman, D.S. Morris, Q. Cao, The role of SPINK1
in ETS rearrangement-negative prostate cancers, Cancer cell 13 (6) (2008) 519 –528.
[42] R.J. Johnson, J.P. Williams, K.W. Bauer, AutoGAD: an improved ICA-based hyper-
spectral
anomaly detection algorithm, IEEE Trans. Geosci. Remote Sens. 51 (6) (2012)
3492 –3503.
[43] A.S. Hadi, A.R. Imon, M. Werner, Detection of outliers, Wiley Interdiscip. Rev.Comput.
Stat. 1 (1) (2009) 57 –70.
[44] B. Du, L. Zhang, D. Tao, D. Zhang, Unsupervised transfer learning for target detectionfrom
hyperspectral images, Neurocomputing 120 (2013) 72 –82.
[45] I.H. Naqvi, Outlier/Event Detection Techniques in Wireless Sensor Networks, 2012.
[46] H. Eldardiry, K. Sricharan, J. Liu, J. Hanley, B. Price, O. Brdiczka, E. Bart,
Multi-source
fusion for anomaly detection: using across-domain and across-time
peer-group consistency checks, J. Wirel. Mob. Netw. Ubiquitous Comput.
Dependable Appl. 5 (2) (2014) 39 –58.
[47] A. Cerasa, A. Cerioli, Outlier-free merging of homogeneous groups of pre-classified
observations
under contamination, J. Stat. Comput. Simul. 87 (15) (2017) 2997 –3020.
[48] S.G. Beaven, G.G. Hazel, A.D. Stocker, Automated Gaussian spectral clustering of
hyperspectral
data, in: Algorithms and Technologies for Multispectral, Hyperspectral,
and Ultraspectral Imagery VIII, vol. 4725, International Society for Optics andPhotonics, 2002, pp. 254– 266.
[49] A.J. Messer, On the development of robust anomaly detection algorithms with limitedlabeled
data, AIR FORCE INSTITUTE OF TECHNOLOGY WRIGHT-
PATTERSON AFB OH WRIGHT-PATTERSON AFB United States, 2017.
[50] H.H. Pajouh, G. Dastghaibyfard, S. Hashemi, Two-tier network anomaly detectionmodel:
a machine learning approach, J. Intell. Inf. Syst. 48 (1) (2017) 61 –74.
[51] D. Verma, R. Kumar, A. Kumar, Survey paper on outlier detection using fuzzy logic
based
method, Int. J. Cybern. Inf. Sci. (IJCI) 6 (1/2) (2017) 29 –35.136 Partha Pratim RayAbout the author
Partha Pratim Ray has completed bache-
lors and masters degree in Computer
Science and Engineering and Electronics
and Communication Engineering, respec-
tively from West Bengal University of
Technology, India. He is a senior member
of IEEE. He has published more than
70 research articles in various research ave-
nues. His area of interest is Internet of
Things and Next Generation Computing.137 IoT-edge analytics for BACON-assisted multivariate health data anomaliesThis page intentionally left blankCHAPTER FIVE
The edge AI paradigm:
Technologies, platformsand use cases
Pethuru Raja, J. Akilandeswarib, and M. Marimuthuc
aSite Reliability Engineering (SRE) Division, Reliance Jio Platforms Ltd. (JPL), Bangalore, India
bDepartment of Information Technology, Sona College of Technology, Salem, TN, India
cResearch Scholar, Sona College of Technology, Salem, TN, India
Contents
1.Introduction 140
2.Delineating the two paradigms 140
3.Tending toward the digital era 143
4.The key connectivity technologies 148
5.The 5G use cases and benefits 149
6.About edge computing 151
7.Edge computing architecture 153
8.Edge cloud infrastructures 155
9.Edge analytics 157
10. The key benefits of edge computing 159
11. Tending toward edge AI 162
12. Artificial intelligence (AI) chips for edge devices 164
13. The noteworthy trends toward edge AI 165
14. Why edge processing? 167
15. Edge-based AI solutions: The advantages 168
16. Applications that can be performed on edge devices 170
17. Edge AI use cases 174
18. Conclusion 179
About the authors 180
Abstract
Two of the most interesting topics in the technology world today are edge computing
and artificial intelligence (Ai). Individually each of them has done exceedingly well incontributing for the betterment of the society. Now if they converge, we can solidlyexpect a paradigm shift and the impacts will be mesmerizingly profound and phenom-enal. The fusion of these two popular technologies is laying down a spectacular foun-
dation for creating new kinds of experiences and opportunities for people. Newer
possibilities can easily come up across business verticals. There will be premium and
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.003139people-centric services emerging and evolving for automating and accelerating a
number of manual activities in our daily lives. This unique combination can result in
fructifying the longstanding demand of people IT. These also smoothen the pathand clear the route for the age-old transition from business IT to people IT. This conver-gence directly influences peoples ’lives in a distinguishing and deft manner. This can
save time, improve privacy, reduce network traffic, and enable applications or devicesto be optimized for specific environments. Precisely speaking, the convergence of thesetwo strategically sound technologies can bring forth noteworthy advancements andaccomplishments for the total society. In this chapter, we are to see how this uniquelinkage is going to be hugely beneficial for institutions, individuals and innovators.The union is being termed as “Edge AI. ”
1. Introduction
Two of the most interesting topics in the technology world today are
edge computing and artificial intelligence (Ai). Individually each of them hasdone exceedingly well in contributing for the betterment of the society.Now if they converge, we can solidly expect a paradigm shift and the
impacts will be mesmerizingly profound and phenomenal. The fusion of
these two popular technologies is laying down a spectacular foundationfor creating new kinds of experiences and opportunities for people.Newer possibilities can easily come up across business verticals. There will
be premium and people-centric services emerging and evolving for auto-
mating and accelerating a number of manual activities in our daily lives.This unique combination can result in fructifying the longstanding demandof people IT. These also smoothen the path and clear the route for theage-old transition from business IT to people IT. This convergence directly
influences peoples’ lives in a distinguishing and deft manner. This can save
time, improve privacy, reduce network traffic, and enable applications ordevices to be optimized for specific environments. Precisely speaking, theconvergence of these two strategically sound technologies can bring forth
noteworthy advancements and accomplishments for the total society. In this
chapter, we are to see how this unique linkage is going to be hugely ben-eficial for institutions, individuals and innovators. The union is being termedas “Edge AI.”
2. Delineating the two paradigms
In this section, we are to dig deeper and dwell at length to provide
detailed information on the two participating technologies: artificial140 Pethuru Raj et al.intelligence (AI) and edge computing. Further on, how their union is going
to be a game-changer for businesses as well as people in the ensuing digital
era. Leading market analysts and researchers are in unison in insisting andillustrating that AI is the future not only for IT but also for all business ver-ticals. In fact, every institution, innovator and individual are to benefitimmensely through the distinct developments happening in the AI space.
Fresh digital life services and applications can be conceived, concretized
and delivered to customers, clients and consumers with the faster maturityand stability of AI technologies and tools.
With the ready availability of big data, the flourish of virtually infinite
and affordable cloud storage and computation facilities, the emergence of
path-breaking machine and deep learning (ML/DL) algorithms, and highlyminiaturized yet performant semiconductors, the AI field is bound to shakeup the world in the days to unfurl. Every data-generating domain is to ben-
efit enormously with the breakthrough improvisations happening in the AI
space. The process of transitioning data into information and knowledge isbeing optimized and automated through a host of technologies and tools.Thus, knowledge discovery and dissemination aspects gain prominence.
There are a plethora of pioneering personal as well as industrial use cases crit-
ically leveraging various AI distinctions. Not only business workloads butalso IT systems/services become intelligent in their decisions, deals anddeeds with the unprecedented AI power. Digitized entities and connecteddevices become cognitive in their actions and reactions. There are open
source and commercial-grade AI platforms, frameworks, libraries, accelera-
tors and tools in plenty in producing AI model creation, evaluation, andoptimization. AI models creation, evaluation, optimization and refinementhave become the new normal for worldwide enterprises in order to be ahead
of their immediate competitors and to retain the yearned edge in brand
value. Precisely speaking, empowering computers, communicators, con-sumer electronics, handhelds, wearables, portables, implantable devices,gadgets and gizmos with human-like intelligence is the ultimate goal of AI.
The second one is none other than the edge computing paradigm, which
is to fulfill the idea of long pending decentralized and distributing comput-ing. That is, computing has to be performed by all the participating devicesin a network. In other words, every resource-intensive node in any networkhas to have the computing and communication capability to team up with
one another in the vicinity and with remote ones through networking. The
brewing idea is to envisage and ensure state-of-the-art services to businessesas well as commoners. This is quite opposite to the much-trumpeted cloud141 The edge AI paradigmcomputing, which is typically consolidated, centralized, automated and
shared. Client machines and devices from every nook and corner of the
whole world can connect and get the things done through cloud servers,which are increasingly commoditized and cheaper. In the extremely con-nected world, the computing is done across an entire network instead ofcentralizing it in cloud environments. That is, not only server machines
but also client devices participate and contribute for a bigger purpose.
Toward further clarification, take this renowned example. A security
camera put up in a critical junction is capturing and streaming all the videocontent captured to a nearby or faraway cloud center (public or private) over
a network to be analyzed for fulfilling certain requirements such as people
and object detection. Now with advanced cameras, that video analysis canbe performed locally within the camera itself. This saves a lot. First, it dra-matically reduces the amount of data to be transmitted over the network
thereby network bandwidth gets preserved. This results in reducing network
congestion. Precisely speaking, real-time data capture, storage, processing,decision-making and actuation gets accomplished through edge/fog com-puting. Data privacy and security are being accomplished through the gran-
diose participation of networked embedded devices, which also contribute
in proximate, timely and trustworthy computation, analytics, knowledgediscovery, and action.
There are a number of versatile technological upgrades. We have pow-
erful chips such as multicore CPUs, GPUs, FPGAs, VPUs, TPUs, ASICs
and other SoCs. Another interesting development is the open ISA
(Instruction Set Architecture) RISC-V. RISC-V provides another optionbeyond the traditional general-purpose architectures, such as x86 or Arm.With RISC-V, it is possible to customize the instruction set for specialized
or extreme application demands, such as artificial intelligence (AI) algo-
rithms. In the case of the surveillance example above, the video processinghappens in the camera itself thereby real-time decision and action can beinitiated and implemented. Such proximate processing could dramatically
reduce the amount of data (video frames) being transmitted to cloud envi-
ronments over dedicated or the open Internet. By deploying analytics librar-ies in client devices, which are adorned with specialized and adroitprocessors, the local and real-time computing gets accomplished. If AI isdone right at the device level, then the result is altogether different.
The combination of the versatile AI-specific semiconductor components
and the path-breaking AI algorithms in conjunction with the fast, low-costand local storage offered by flash memory brings forth adequate computing142 Pethuru Raj et al.and analytics power. This transition empowers networked edge devices to
accomplish proximate data processing in order to emit out instantaneous
insights and results. Thus, the new domain of “Edge AI” has blossomedin the recent past. AI pundits, pioneers and proponents are working intentlyin unearthing people-centric and industrial use cases for edge AI as a wayforward to popularize its richness and reach.
3. Tending toward the digital era
There are path-breaking digital technologies and tools emerging to
fulfill the digital transformation needs across industry verticals, governments,organizations, and institutions. Every tangible thing in our everyday envi-
ronments is getting methodically digitized through the application of
digitization and edge technologies. And when these digitized entities inter-act with one another, there is a tremendous amount of digital data getsgenerated. As widely reported, data has become the new oil or fuel for real
and sustainable business transformation. Data is being termed as the strategic
asset for any organization to plan and surge ahead. Decision-makers, exec-utives and stakeholders leverage digital intelligence to steer their organiza-tions in the right direction to the destination. The power of digital
technologies comes handy in transitioning digital data to information and
to knowledge. There are influential technologies and tools for enabling datacapture, integration, virtualization, ingestion, wrangling, masking, pre-processing, storage, analytics, and visualization capabilities. The brewingtrend and transition is that we are heading toward data-driven insights
and insights-driven decisions and deeds. We have listed out the prominent
digital technologies below for the benefit of our esteemed readers.
For business transformation, the following activities are being insisted by
enterprise architecture (EA) architects, consultants and advisors.
1.
Technology Choice
2.Architecture Assimilation
3.Process Excellence
4.Infrastructure Optimization
5.Data Analytics
Digitization Technologies : Let us start with digitization technologies. There
are digitization, connectivity and edge technologies. These are collectivelycalled as the Internet of Things (IoT) technologies. These are for
transitioning ordinary items into extraordinary artifacts. Edge technologies
such as sensors, actuators, stickers, smart dust, specks, tags, barcodes, chips,143 The edge AI paradigmmicrocontrollers, beacons, LED lights, etc. are being internally or externally
attached on important assets and this nuanced attachment enable edge tech-
nologies to acquire the assets’ operational and environmental data. This is theway all kinds of physical, mechanical and electrical systems in our personal,social and industrial environments get digitized. Dumb objects becomeanimated entities. Commonly found and concrete objects get methodically
digitized. Digital objects can do small-scale and real-time computing and
are stuffed with communication modules. Digitized objects can share theirhealth condition, performance, security, operational, and log data. Furtheron, digitized assets at the ground level gets hooked to cloud-based software
applications, services and databases over the Internet. This phenomenon is
being termed as cyber physical systems (CPS). Thus, the concepts such asdigitization, connectivity, automation and orchestration are acquiring spe-cial significance in the digital era. Thus, the Internet of Things (IoT) tech-
nologies are destined for establish and sustain the digital living. Digital
entities connect, communicate, collaborate, corroborate and correlate withone another. This produces a tremendous amount of multi-structured data.In the subsequent sections, we are to write about how digitalization tech-
nologies come handy in squeezing out viable and venerable insights out
of such digital data.
The Explosion of Digital Entities : As per the leading market watchers and
analysts, there will be trillions of digitized entities, billions of connecteddevices and millions of microservices in the years to come. With the faster
maturity and stability of path-breaking miniaturization technologies, we
have state-of-the-art micro- and nanoelectronics products flourishingeverywhere. As indicated above, there are competent digitization and edgetechnologies. The important digitization technologies are disappearing sen-
sors and actuators, which are plentifully and purposefully occupying most of
our everyday environments these days. Sensors and actuators are also knownas implantable devices. By applying these powerful digitization and edgetechnologies, all kinds of casual and tangible things in our living and working
spaces are entitled to become smart objects. In other words, ordinary items
in our daily places become extraordinary. Any tangible/concrete object isbound to become sentient material through the smart application of theabove-mentioned edge and digitization technologies.
What is the result of all these empowerments and enablement? All sorts of
physical, mechanical, electrical and electronics systems in our buildings,
manufacturing floors, entertainment plazas, eating joints, railway stations,air and sea ports, auditoriums, sports stadiums, microgrids, nuclear144 Pethuru Raj et al.establishments, shopping malls, etc. are being fruitfully digitized with much
care and clarity. These distinct, deeper and decisive digital technologies are
intrinsically empowering the whole world to have trillions of digitizedentities in the coming years. It is therefore indisputably correct to state thatthe digitization movement is in full speed. Our shirts become e-shirts, ourdoors, cots, windows, chairs, tables, wardrobes, kettles, wares, utilities, uten-
sils, etc. will become smart through the formal attachment of digitization and
edge technologies. Especially multifaceted sensors and actuators play a veryvital role in shaping up the digital world and living. Everything becomesdigitized.
Already all our computers (laptops, tab lets, desktops, and server machines)
are integrated with the Internet commu nication infrastructure in order to
access web content and services. These d ays our communicators (smartphones)
are web-enabled in order to fulfill anyt ime, anywhere, any network, any device
access of web content and serv ices. Additionally, sens ors-attached physical,
mechanical, electrical and electron ics are hooked into the Web in order to
be remotely empowered. Such an empo werment makes our everyday devices
and machines to be ready for contributing copiously for the realization of dig-
ital societies. Thus, physical devices ar e set to become digital devices, which are
intrinsically revitalized to perform edg e computing. Digital elements are called
edge devices.
The Proliferation of Connected Devices : We talked about ordinary things
getting transformed into digital elements through a few pioneering technol-
ogies. Now come to the device world. With the fast-growing device eco-
system, we are being bombarded with a variety of slim and sleek, handy andtrendy, resource-constrained and intensive, purpose-specific and agnostic,multimedia, multimodal, and multifaceted devices. There is a growing array
of handhelds, mobiles, portables, wearables, nomadic, and fixed devices.
Further on, we have consumer electronics, medical instruments, homeappliances, communication gateways, robots, drones, cameras, game con-soles, machineries, equipment, medical instruments, single board computers
(SBCs), programmable logic controllers (PLCs), SCADAs, etc. yearning to
be digitized and connected to attain all the originally expressed benefits. It isforecast that there will be billions of such higher-end devices soon. With thesurging popularity of the Internet of Things (IoT) paradigm, every digitizedentity and device is solemnly readied to be Web-enabled. Now with the
overwhelming adoption of the cloud idea, every device in and around us
is ordained to be cloud-enabled. In other words, every electronics is slatedto be connected. The number of connected devices is expected to be in145 The edge AI paradigmbillions soon. Device-to-device (D2D) and device-to-cloud (D2C) integra-
tion experimentations and scenarios are gaining momentum due to the
steady growth of several implementation technologies.
Precisely speaking, digitized enti ties and connected devices are stuff-
ing and saturating our everyday env ironments (personal, social and
professional). For enabling context-awareness and multidevice com-
puting applications, the new connected era beckons and dawns upon
us. These technologically enabled d evices are typically called as edge
or fog devices because they are at the edge of the network. They arevery near to us. We can see, touch and feel them. For example, the point
of sale (PoS) devices in a retail store is an edge device, cameras in our
homes, hotels, and hospitals are edge devices, robots in a happeningp l a c es u c ha sas u r g e r yr o o mi n s i d eah o s p i t a li sa ne d g ed e v i c e ,t h el i s tgoes on and on.
Now comes the twist. Edge devices generally generate a lot of data every
hour. That is, they collect a lot of useful and usable data about themselves,their environments, owners, users, etc. Edge devices collect operational, log,health condition, performance, and security data. Edge devices are touted as
the primarily data collectors about the various temporal, special and behav-
ioral aspects. That is, edge devices plus empowered systems (systemsempowered by edge devices) individually and collectively throw a lot of dataof their capabilities, capacities, states, change of states, etc. Compared todigital elements, connected devices are computationally powerful. Thus,
digital elements are primarily data generator and transmitter. However,
connected devices are capable of receiving and processing any amount ofdigital data in real time.
The Continued Adoption of Cloud Applications, Services and Data Sources :
We have discussed about edge devices and their explosion. At one end,
we have zillions of digitized entities being made out of physical elements.The second category in the hierarchy is scores of versatile electronics devicescapable of capturing, storing, processing, and mining digital data emitted
by exponentially growing digitized entities. Now, at the top of the spec-
trum is none other than cloud assets and applications. Real-time data ana-lytics is being accomplished through electronics devices. Data collectionhappens through digitized entities at the lower end. Increasingly electronicsdevices (alternatively termed as edge or fog devices) support in-device AI
processing. That is, lightweight AI toolkits are deployed on edge devices
to subject digital data to a variety of deeper and real-time investigationsin order to extract real-time insights that can be looped back to146 Pethuru Raj et al.decision-making systems and people to initiate the process of pondering
about the next course of actions with all the clarity and confidence.
In cloud environments (private, public or hybrid), historical and com-
prehensive data analytics through integrated data analytics platforms andAI frameworks is being done. Thus, the role of cloud infrastructures andplatforms in this increasingly data-engulfed world can’t be undermined.
Especially big data analytics through batch processing can be comfortably
done in traditional cloud environments.
The Arrival of Edge Clouds : Considering the need for real-time insights
and applications for establishing and sustaining intelligent enterprises, indus-
try houses and cloud service providers are setting up miniaturized cloud
environments in their offices and campuses. Telecommunication serviceproviders are using their base stations to have a small-scale cloud centersto ensure real-time customer experience.
Especially with edge devices becoming powerful, edge device clusters/
clouds are being formed in a dynamic and ad hoc manner to perform specifictasks. With the forecast of billions of connected devices, there will be a big-ger focus on forming and using edge device clouds in order to build and
release location-specific, context-aware and real-time applications. Thus,
there will be a combination of traditional and modern cloud centers in visu-alizing and realizing next-generation business and people applications in theyears ahead. With devices contributing their unique capabilities, the scopeand sophistication of software applications will be deeper, deft, and decisive.
Digitalization Technologies : In this section, we are to see how digital trans-
formation being simplified and streamlined by digital technologies has laiddown a stimulating and sparkling foundation for business transformation.Lately, we are being bombarded with multifaceted digitized entities, con-
nected devices and microservices. Because of the accumulation of digitized
assets, a massive amount of multi-structured digital data gets produced, gath-ered, stocked and subjected to a variety of deeper investigations. In short, thefaster adoption of digitization technologies results in digitized elements,
which generate big data. Now with the grandiose arrival of digitalization
technologies, making sense out of digital data gets accelerated. Digital datais being actually gleaned from different and distributed digital entities anddevices. That is, the process of transitioning of raw data into informationand into knowledge is being automated through a host of revolutionary dig-
italization technologies such as:
1.
Artificial Intelligence (AI) Algorithms
2.Integrated Analytics (Big, Fast and Streaming Data) Platforms147 The edge AI paradigm3.Blockchain for Device and Data Security
4.Digital Twins
The above-mentioned technologies are primarily for extracting actionable
insights in time out of digital data. There are other supporting technologicalinnovations and disruptions for speeding up the process of developing anddelivering highly scalable, available, reliable, and portable digital applications
that can run on multiple systems including personal devices, enterprise and
cloud servers.
1.
Data Virtualization
2.Databases, Data Warehouses and Data Lakes
3.Knowledge Visualization Dashboards
4.Cloud-native Computing
5.Fog/Edge Computing
6.Serverless Computing
7.Microservices Architecture (MSA)
8.Event-driven Architecture (EDA)
9.Message Brokers and Queues
10.Event Meshes
11.Cybersecurity
12.Process Automation
13.Workflow Orchestration
Thus, data to information and knowledge gets facilitated through digita-lization technologies. The combination of digitization and digitalization
technologies is generally termed as digital technologies.
4. The key connectivity technologies
There are wireline and wireless communication technologies. Lately,
5G is the widely talked about communication technology. This section is
allocated to throw more light on the 5G ecosystem. 5G is the new cellularcommunication standard bringing forth a new kind of network that canconnect everyone and everything including digitized entities/smart object/
sentient materials, consumer electronics, medical instruments defense
equipment, manufacturing machineries, etc. 5G is being positioned andproclaimed as a new wireless communication technology capable of deli-vering data transfer of gigabits per second. It facilitates ultra-low latency,
higher reliability, and massive network capacity/bandwidth. It has the
inherent power to accommodate higher density of devices. Higher148 Pethuru Raj et al.performance and heightened efficiency are being seen as the key motiva-
tions for envisaging and realization of newer business models and industry
use cases.
5G is based on OFDM (Orthogonal frequency-division multiplexing).
The OFDM method is famous for efficiently modulating a digital signalacross different channels to reduce interference. 5G uses the 5G new radio
(NR) air interface and uses wider bandwidth technologies. The 5G NR air
interface can enhance OFDM to deliver a much higher degree of flexibilityand scalability. This is the key reason for 5G to give access to more peopleand things for implementing hitherto unheard use cases. 5G can operate in
lower bands (e.g., sub-6 GHz) as well as mmWave (e.g., 24 GHz and up).
This unique capability will bring in extreme capacity, multi-Gbps through-put and the much-insisted low latency. Mission-critical communications areto be accomplished through the leverage of the 5G standard. With the surg-
ing popularity of IoT systems, solutions and services, the role and responsi-
bility of 5G communication acquires special significance. For enabling IoTsensors and devices to talk to one another in the vicinity as well withremotely held cloud-hosted software services and applications, 5G plays a
very vital role. The below figure tells all about the distinctions of the 5G
communication.
5. The 5G use cases and benefits
Empowering Enhanced Mobile Broadband : Compared to the previous
generations, 5G will supply enhanced mobile broadband. The user experi-
ences will be better. For example, virtual and augmented reality (VR and
AR) experiences will be more intuitive and immersive.
Enabling New Capabilities : With the faster proliferation of IoT devices and
services, 5G contributes immensely in realizing context-sensitive, time-
critical and people-centric applications. As indicated elsewhere, as per the
reports of leading market watchers and analysts, there will be billions ofIoT devices and trillions of IoT sensors in the years ahead. For the forthcom-ing connected era, 5G guarantee critical connectivity infrastructure. 5G also
simplifies and streamlines the aspect of linkage of ground-level IoT systems
and cloud IT infrastructures. The Vehicle-to-Everything (V2X) communi-cation will be facilitated. The V2X includes Vehicle-to-Vehicle (V2V),Vehicle-to-Infrastructure (V2I), and smart cars. Telesurgery will become
the new normal with the maturity and stability of 5G communication.
Telemetry data gets captured and crunched to squeeze out actionable149 The edge AI paradigminsights in time. With right and relevant insights at hand, a variety of
intelligent offerings can be designed, developed and deployed.
Virtual networks (5G slicing) tailored to each use case : 5G will be able to sup-
port all communication needs from low power Local Area Network(LAN) –like home networks, for example, to Wide Area Networks
(WAN), with the right latency/speed settings. This is addressed today is
by aggregating a variety of communication networks (Wi-Fi, Z-Wave,
LoRa, 4G, etc.).
The 5G networks under deployment feature lower latency, higher capac-
ity, and increased bandwidth. These improvements will have far-reaching
implications on how people across the world live, relax, and work.
Network Speed Increase : 5G networks can give the speed up to 10 Gbps,
which is almost 100% increase compared to 4G communication.
Ultra-Low Latency : Latency measures the time it takes for your phone to
send a message and get a response. Shorter latency enables quick response
interactions. Low latency 5G networks open up lots of new possibilitiesfor services that demand nearly instant response time. That includes tele-medicine, self-driving vehicles, remote monitoring, measurement and
management of mission-critical assets, etc.
Agriculture, manufacturing, and logistics will benefit from 5G-inspired
lower latency. The combination of high speed and minimal lag is also goodfor virtual reality (VR) and augmented reality (AR) applications, which arelikely to be used extensively as connectivity improvements create a more
seamless, immersive experience.
Enhanced Network Capacity : 5G networks are poised to deliver up to
1000 times more capacity than the currently pervasive 4G networks. Dueto the participation of a large number of networked embedded devices,
smartphones, connected machineries, etc. in an industrial environment,
5G is being seen as the powerhouse for the ensuing IoT era. Smart factories,cities, farms, university campuses, warehouses, airports, and other mission-critical places are to benefit immensely with 5G networks. With 5G, one
million IoT sensors and devices can be accommodated within a square kilo
meter surroundings.
High Reliability : Global Wireless Solutions, Inc. (GWS) has found that
AT&T has the most reliable 5G nationwide network, completing 99.5%of the data transfer tasks performed on 5G networks. Verizon Wireless
finished second at 98.8% and T-Mobile third at 97.3%.
The next-generation computing needs pioneering communication
capacity and capability. The arrival of 5G communication facility is to fairly150 Pethuru Raj et al.enthuse pundits and pioneers to ponder about fresh business, technical and
user cases. Especially for fulfilling the ideals of digital transformation, the 5G
connectivity is indispensable.
6. About edge computing
Edge computing is the computing capability and facility being pro-
vided by edge devices. With edge sensors and devices are projected to be
in billions in the years to come, the amount of data getting generated by edge
devices is humungous. As there is a realization that every type of edge datacomprises some useful and usable knowledge, edge data processing gainsprominence. It is mandated for extracting actionable insights out of data
heaps in time. Data value goes down when time elapses and hence data
has to be processed immediately in order to extract time-sensitive businessvalue. Edge computing follows the proven and potential distributed archi-tecture. Cloud computing is quite centralized whereas edge computing is a
distributed model. Multiple heterogeneous devices voluntarily cooperate
with one another to do edge data collection, storage and processing. Thedevice heterogeneity is being taken care of through containerization. Andmultiple devices participating in business computing is being fully facilitated
through container lifecycle management platforms such as Kubernetes.
Edge devices, instead of sending data to nearby or faraway cloud servers,
are being empowered to collect, cleanse and crunch their data instanta-neously if they have the requisite data storage and processing power. Thebrewing trend, as articulated above, edge devices are instrumented with
higher processing and memory/storage power. Thereby edge devices
innately readied to contribute for real-time computing.
Further on, edge devices can intrinsically form an ad hoc and dynamic
clusters/clouds to accomplish bigger and better things. That is, device clus-
ters and clouds are being formed out of edge devices. This technologically
enabled transition is seen as a game changer towards establishing and sustain-ing people IT. Thus far, businesses have been immensely benefiting out of allthe noteworthy achievements in the IT space. Now every individual is set to
benefit immeasurably through the evolutions and revolutions happening in
the IT space. As there are billions of connected devices, the amount of mem-ory, processing power, storage and IOPS capacities are turning out to besimply phenomenal and hence the future certainly belongs to the pioneering
edge computing model. There are edge gateways/servers in order to con-
tribute as a master or control server whereas other devices in the network151 The edge AI paradigmedge may help in data collection, storage and processing. Edge devices may
be classified into categories: resource-constrained and intensive. Edge
devices such as sensors and actuators are typically blessed with less processingand memory power and there are other edge devices stuffed with more com-puting capability and storage capacity. The resource-intensive edge devicesare normally termed as edge servers as they can participate in data processing
captured by edge devices.
In a smart home environment, there may be several single-purpose and
multipurpose sensors ranging from gas, temperature, humidity, presence,pressure, and other sensors enabling home-automation activities. These sen-
sors are termed as edge devices. Additionally, there are resource-intensive
connected devices such as consumer electronics, Wi-Fi gateways, kitchenutensils, etc. within the home. These powerful devices are being called as edgeservers as they can do data processing individually and also, they can form a
temporary and purpose-specific device cluster dynamically and swiftly to
accomplish bigger and better needs for the home owners and occupants.
Edge computing, in a way, decentralizes processing power to ensure
real-time processing without the much-maligned network latency while
reducing network bandwidth and storage requirements. Edge computing
makes it possible to visualize and realize a number of next-generationpeople-centric services and applications. Industries also can get a lot of man-ual things getting automated and accelerated. This newer computing modeloffers a range of value propositions for producing and running smart IoT
applications and use cases across industry verticals. The prime advantages
of edge computing are given below:
Low latency : Edge servers/devices are calculatedly instrumented to do
the much-demanded proximate data processing in real time as they carry
the advantage of being situated near data sources. The computation
logic, once centralized, now moves to edge devices to accomplishdecentralized and distributed computing. Edge devices are digitizedand destined to capture and transmit their own operational state and also
their surroundings data to nearby edge servers. Edge servers are close to
edge devices and hence data processing happens immediately and deci-sions are taken quickly.
Scalability : For catering more data, additional devices can be provi-
sioned and configured to accomplish real-time data analytics. This is
due to the accumulation of edge devices in any important environment.
Device clusters and clouds will become the new normal in conceivingand concretizing multidevice, process-aware and composite applications152 Pethuru Raj et al.Security and Privacy : Devices and transacted data are restricted to the
particular environment. There is no need to transmit data over the public
and open Internet. Data transmission is within the Intranet and hencedata security and privacy are inherently ensured. Additionally, theproven blockchain technology is being utilized in order to bring in anadditional layer of security as cyber-attacks and terrorisms are consis-
tently on the rise in the extremely connected world.
Bandwidth Efficiency : The first-level data analytics and filtering
happen at the edge devices and hence only useful data gets transmittedto cloud environments for comprehensive analytics. That means the
expensive and scarce network bandwidth gets saved through such
source-level data analytics.
Robust connectivity : Even if the cloud connectivity is unavailable,
edge computing happens without any issue.
Real-time Computing : Data capture and processing happen locally
and quickly. Edge computing delivers the real meaning and value forreal-time applications and services. Real-time and event-driven enter-prises will become the new normal.
Thus, leveraging one or more edge devices to do real-time implementationof advanced and people-centric applications is gaining momentum with thematurity and stability of edge computing technologies and tools.
7. Edge computing architecture
The key information is that edge computing fully complies with the
distributed computing patterns. As articulated above, edge devices are all setto become pervasive and persuasive. Every industry vertical is embracing thisstrategically sound technology in order to be right and relevant to their cus-
tomers, consumers and employees. Devices are being increasingly stuffed
with more processing power and memory capacity and hence edge devicesare ceaselessly penetrating into every business domain to bring in deeper anddecisive automation. In an edge architecture, there are three types of devices
contributing immensely. They are edge sensors and actuators, edge devices
and gateways.
Edge Devices (Sensors and Actuators) : As accentuated above, edge
sensors are not blessed with high-end memory, processing and storage
power. They are for sensing a variety of things and to respond accord-
ingly. We know about gas, heat, pressure, presence, movement,153 The edge AI paradigmgestures, oscillation, and humidity sensors. Hence, they typically collect
data about their surroundings and send it to nearby edge servers. Sensors
are miniaturized and hence becoming disappearing. On the other hand,extracted information from aggregated sensor data gets supplied toactuators in time to act with all the confidence and clarity. Sensorsand actuators are generally not for data processing. Sensors are sensitive
whereas actuators are for real action based on the sensed data.
Edge Servers : These are resource-intensive systems and hence they can
run operating systems such as Android, iOS, Raspbian , etc. Further on,
they can run data analytics platforms/tools/accelerators in order to do
real-time data analytics. They can do streaming analytics of edge sen-
sor/actuator data in real time. Time-series data is more prevalent andrelevant for edge intelligence. On-device learning is bound to go upwith the respective technologies are growing in maturity and stability.
Further on, artificial intelligence (machine and deep learning (ML/
DL) algorithms is drawing a greater attention from researchers and pro-fessional. The most important contributions of AI are computer vision(CV) and natural language processing (NLP). With the availability of
enabling AI toolkits and frameworks, edge devices are being vision-
enabled. That is, the concepts of machine vision and intelligence areemerging and evolving fast. Machines are also able to understand humanspeech and respond insightfully. On-device AI processing could producepredictive, prescriptive and personalized insights in time. These devices
are capable of data processing and can talk directly to faraway cloud
servers and services. Or they can connect to edge gateway/broker to talkto cloud applications. These devices form quick clusters (called as edgedevice clouds) to fulfill special needs. Edge clouds are for proximate
processing.
Edge Gateways also run any full-fledged operating system. They have
unconstrained power supply, CPU power, memory and storage.Therefore, they can act as intermediaries between the cloud and edge
devices. These offer the standardized middleware functionalities.
Cloud Environments : We have private, public and hybrid clouds to
host business workloads and IT services. These centers involve a largenumber of commodity server machines, storage appliances and network-ing solutions in order to host and run web, mobile, and IoT applications
and services. Not only applications, databases, platforms, and mid-
dleware solutions are also being installed on cloud infrastructures.Simply speaking cloud environments are the highly optimized and154 Pethuru Raj et al.organized IT environments to run enterprise-scale, mission-critical,
service-oriented, event-driven, operational, analytical and transactional
applications.
The beauty is that all the cloud resources are optimally used for runningmultiple applications concurrently in order to achieve resource efficiency.That is, resource utilization has to be on the higher side in order to pass
on the monetary benefits to consumers. Further on, all kinds of cloud-
inspired services can be supplied to multiple users at the same time. Thus,sharing, automation and orchestration capabilities introduced by the cloudparadigm is still sustaining the interest toward cloudification. All these dis-
ruptions bring down the cloud costs drastically. With the leverage of orches-
tration tools, most of the simple and complex administration activities getautomated and accelerated. With the massive scalability, the leverage ofcheaper hardware, deeper automation and automated orchestration through
the containerization movement, the cloud concept is pitched for affordabil-
ity, agility, and adaptivity. It is projected that more than the 80% of softwareapplications will reside in cloud environments in the years ahead.
We have discussed the prominent building-blocks of edge architecture.
Besides sensors and actuators, devices and device gateways, we need cloud
environments in order to stock all kinds of edge data. Further on, in syn-chronization with current edge data, a kind of comprehensive and historicaldata analytics can be accomplished in cloud environments. The increasedadoption and adaptation of digital twins is being presented as one of the
key drivers for the spread of edge computing across industry verticals.
Cyber physical system (CPS) is a related discipline gaining significantmomentum. All kinds of ground-level physical systems get hooked tocloud-based applications and platforms to envisage smart systems and ser-
vices. That is, digital systems in our everyday environment are all set to
be artistically empowered to exhibit adaptive behavior.
8. Edge cloud infrastructures
Any computing and analytics activity needs a solid IT infrastructure.
For certain scenarios, on-device AI capability is needed. That is, a single
edge device can suffice. For complicated activities, there is a need for clus-tering multiple heterogeneous devices together to form a kind of devicecloud dynamically. Clustered or cloud environments are mandatory for
doing extremely powerful data analytics. As we all know, for certain needs,
web-scale companies use thousands of compute nodes to do comprehensive155 The edge AI paradigmdata analytics. For performing edge data analytics, we need distributed edge
infrastructures. Thus, edge computing infrastructures include edge devices
and their clouds. Some activities are being offloaded to faraway cloud envi-ronments. Real-time computing and proximate data processing needs arepredominantly being done through edge devices locally.
As indicated above, we will have zillions of digitized entities and con-
nected devices in our earth planet soon. These are subdivided into two
major categories: resource-constrained and intensive. Resource constraineddigitized entities are focusing on data gathering. Because of the externally orinternally embodied communication modules, they can transmit a variety of
data to nearby device middleware, which are also called as device bus, IoT
gateway and broker, etc. These devices/intermediaries are typically power-ful and can comfortably do all kinds of middleware activities. Data fusion,message enrichment, intermediation, security, routing, filtering, tunneling,
encapsulation, etc. can be accomplished by IoT gateway software. Further
on, they send data to faraway cloud environments in order to be crunched bycloud-hosted data analytics and AI toolkits to extract actionable insights.However, resource-constrained edge devices are not participating in data
processing whereas resource-intensive edge devices are capable of not only
collecting but also involving themselves in data processing, analytics, miningand learning. Resources-intensive devices can therefore cluster themselvesto form ad hoc, dynamic and purpose-specific device clouds in time toaccomplish real-time data analytics.
Considering the persistent demands for cloud centers near users,
renowned data center service providers are collaborating with telecommu-nication service providers to set up edge clouds, which are the miniaturizedversion of traditional clouds (public and private). However, with the ready
availability of powerful devices in large number and the maturity of device
cloud formation technologies and tools, device clouds can be formed easilyand quickly and they are being utilized for a variety of edge computing usecases in a risk-free, affordable, and secure manner.
The prominent reasons for setting up edge cloud centers are as follows.
The scalability issues, excessive power consumption, bandwidth wastage,high latency and privacy are some of the critical factors that are drivingthe demand for edge cloud infrastructure in the form of micro data centers,cloudlets, or edge clouds/clusters. Edge cloud centers support distributed
computing architecture.
There are a few important reasons for the surging popularity of
edge clouds. Real-time data collection, storage, processing, analytics,156 Pethuru Raj et al.decision-making, and action are being made possible with the realization of
edge cloud infrastructures. With the accumulation of sensors and actuators,
there is a steady stream of sensor and device data. With edge clouds in place,all kinds of sensor data can be captured and subjected to a variety of deeperinvestigations to uncover hidden patterns in data streams. Real-time cus-tomer engagement and fulfillment is being guaranteed through edge cloud
centers. Streaming data analytics is becoming one of the most influencing use
cases of edge computing.
While Raspberry Pi has long been the gold standard for single-board
computing, powering everything from robots to home appliances. This
edge computer has a PC-compatible performance, plus the ability to output
4K video at 60Hz or power dual monitors. Its close competitor, theIntel Movidius Myriad X VPU has a dedicated neural compute enginefor hardware acceleration of deep learning (DL) inference at the edge.
Google Coral adds to the competition offering a development board to
quickly prototype on-device ML products with a removable system-on-module (SoM). There is a transition here from servers-centric cloud envi-ronments to devices-centric cloud environments. As there are billions of
edge devices, the future belongs to device clouds. Thus, the mass production
of single board computers (SBCs) is being touted as the valid reason to rapidadoption of edge computing.
9. Edge analytics
Analytics is being positioned as the key differentiator for the entire
human society. With the number of edge devices going up rapidly in and
around us, we can expect more device interactions, collaborations and cor-relations and thereby the amount of device data getting generated is also big.That is, the device world is supplying big and streaming data. The need is to
do real-time analytics of device data in order to emit out actionable insights.
There are well-known batch and real-time data processing. Similarly, thereare big and streaming data analytics methods and platforms. In the recentpast, we hear about high-end platforms to complete big data analytics
quickly. That is, the real-time analytics of big data is being fulfilled through
a host of advancements in the IT space. The unprecedented explosion of IoTdevices and the adoption of analytical methods have led to the surging pop-ularity of the IoT analytics domain. Without proper and timely analytics of
IoT device data, enabling IoT devices to be intelligent in their operations,
outputs and offerings is a tough affair.157 The edge AI paradigmWith devices individually or collectively contributing for massive
amount of multistructured data, data analytics acceleration engines and algo-
rithms are gaining prominence. Customers’ needs are constantly changingand meeting their demands mandate for edge cloud and computing. Witha surge in IoT sensors and devices in our work spots, living and leisure places,real-time monitoring, measurement, and management turn out to be an
important factor for the intended success of IoT devices. By doing
real-time analytics on IoT device transaction data, the safety, sagacity andsecurity of IoT sensors and devices are being ensured. Real-time data ana-lytics is essential for creating and delivering real-time applications. Real-time
computing is being made possible through edge computing. Newer possi-
bilities and opportunities will emerge with the greater awareness of edgecomputing and its usefulness. A host of edge-centric services and applica-tions will see the light with the faster maturity of edge-inspired innovations
and the penetration and pervasiveness of 5G communication.
The edge analytics paradigm has become more feasible as edge devices
are being stuffed with enough firepower. Also, 5G and Wi-Fi 6 communi-cation technologies enable last mile connectivity. In addition, real-time ana-
lytics and applications are mandated and hence edge-based data analytics is
being taken up with all the sincerity.
Edge analytics optimizes the whole process by handling the bulk of anal-
ysis on-site. Edge devices individually and collectively embark on data ana-lytics to uncover hidden patterns and insights in time. The goal is to provide
real-time or as close to real-time insights as possible. Additional devices can
be easily accommodated to tackle growing data volumes. For example, inhealthcare, there is a convincing use case for edge analytics. If we wear apulse meter and blood sugar monitoring sensor in our body, if there is a sud-
den spike while walking or exercising, then a notification or alert event is
created and an appropriate message gets communicated to our spouse orfamily doctor immediately. The smartphone in our pocket acts as the pow-erful intermediary in capturing and conveying the message to the concerned
person to enable them to ponder about the counter measures in quick time.
This is a streaming analytics phenomenon. A few rules are formed andstocked inside the body-attached device along with a rule engine to takedecisions instantaneously.
This is a kind of outlier/anomaly detection in real time. Such a require-
ment is being visualized across industry verticals. This real-time monitoring,
measurement and management of physiological parameters is being facili-tated through edge devices (sensors, body area network (BAN) and158 Pethuru Raj et al.smartphone). These digitized entities, connected devices, and microservices
do a fast calculation based on some rules to articulate what is found. If there is
a break-in in the threshold value, immediately the right information isformed as a message and gets delivered to the correct person to initiatethe proper actions with all the alacrity and clarity.
Another example is as follows. A device that controls the temperature of
a refrigerator at a supermarket detects a dangerous change in internal tem-
perature. That change can cause damage to the products inside in seconds. Inthe traditional case, that message would have traveled to a central or cloudserver and it gets parsed and processed there. The result is then relayed back
to the sensor fit inside the fridge. By that time, the refrigerator’s goods would
have spoiled and fit for nothing. With edge analytics, the same problemcould be resolved in a matter of a few seconds with the sensor instantly relay-ing the problem and implementing a viable solution therein. Similarly,
manufacturing plants can use edge analytics to keep better track of machin-
ery health, production output, and be ready to deal with any crisis that arisesin seconds instead of minutes.
Similarly, in other industry verticals, such kinds of analytics of streaming
data using edge devices or clouds are gaining speed and visibility. Therefore,
edge analytics may be an exciting area of great potential, but it should not beviewed as a full replacement for central data analytics. Both can and will sup-plement each other in delivering data insights and add value to businesses.
Processes are increasingly being automated and accelerated through a
host of breakthrough technologies across industry verticals. Processes are
being optimized through rationalization and competent technologies con-tribute in process excellence. Process orchestration is the latest buzzword.Multiple automated process steps are being combined and sequenced to
bring in deeper and mission-critical automation. Edge computing is one
of the promising phenomena in radically removing process flabs to arriveat highly optimized processes. Edge analytics is seen as an indispensableact for the digital living.
10. The key benefits of edge computing
Businesses can benefit immeasurably through the smart application of
edge computing. Here come the prominent and potential advantages.
Real-time Decisions : Due to the participation of a large number of hetero-
geneous devices in any environment (home, hospital, hotel, etc.), the data
creation, capture, storage and processing happen locally and in a distributed159 The edge AI paradigmmanner. Edge devices are the IT infrastructure here. As everything happens
near the sources of the data, the transaction time is very less as there is no trip
to cloud and back. Mission-critical machine operations are saved from anykind of break or slow down through edge computing. Hazardous incidentsare being proactively barred from taking place. Real-time computing is thefirst and foremost facet of edge computing.
Reliable operations even with intermittent connectivity : Internet connectivity is
generally unreliable in remote and rough environments. If the networkconnectivity is lost, then remotely monitoring and managing criticalinfrastructures and assets such as oil wells, farm pumps, solar farms, wind-
mills, etc. can be difficult. With proximate processing, there is no data loss
or operational failure in the event of any disconnect. Required operationshappen locally and fast to fulfill all mandated obligations without flop, falterand fumble.
Security and compliance : Due to local computing, a lot of data transfer
between edge devices and cloud environments (private or public) overthe porous, public, and open Internet can be avoided. It’s possible to filterout sensitive information locally. And only important information gets
transmitted to cloud storage for performing historical and comprehensive
data analytics.
Renaissance : Edge devices can form clusters/clouds in order to do bigger
operations. Local storage, computation, decision-making and actuationeventually reduce operational costs. Innovations will thrive in such an open
environment. People-centric services and applications can be visualized and
affordably realized.
The Explosion of Edge Services : Microservices architecture (MSA) and
event-driven architecture (EDA) are pronounced as the most impactful
and extensible architecture patterns and styles for constructing enterprise-
class software applications. That is, synchronous and asynchronous micro-services are being proclaimed as the most optimal building-block forcreating and sustaining software solutions.
Actually, multiplicity and heterogeneity lead to heightened complexity.
MSA is for lessening the rising device complexity. That is, heterogeneousdevices can be exposed as device services with APIs. Thus, any disparatenessand deficiency of devices can be conveniently hidden from the outsideworld. The much-needed interoperability between current and emerging
devices is being facilitated. IoT edge devices can be remotely monitored
and managed. Also, device-to-device (D2D) and device-to-cloud (D2C)integration phenomenon has are being simplified and streamlined.160 Pethuru Raj et al.Through service-enablement, different and distributed edge devices can
be shrewdly combined to formulate competent and cognitive solutions for
complicated and sophisticated applications. Such a smooth and risk-freeintegration and orchestration of edge devices is seen as an amplifying plat-form to envisage and produce fresh business opportunities and possibilities.Innovations are bound to thrive in such uncomplicated and open environ-
ments. Instrumented and interconnected devices are the initial requirements
for empowering devices to be intelligent in their actions. The nuancedservice paradigm has penetrated into the edge space toward intelligent,real-time and cloud-native applications.
Improved equipment uptime : Devices’ performance and health condition
are being constantly monitored and if there is any deviation, it can be pro-actively and pre-emptively attended. This directly contributes in elongatedlifeline for devices, which are contributing for a mission-critical task.
Preventive maintenance is enhanced with predictive maintenance with
the edge analytics capability enshrined in edge devices. The return on invest-ment (RoI) goes up while the total cost of ownership (TCO) is kept low.The optimal utilization of industrial assets and machineries goes up signifi-
cantly with edge computing.
Failure prediction : Every module contributing for the process implemen-
tation, integration, automation and orchestration is being monitoredthrough automated monitoring and observability tools and all the data thuscollected is being subjected to a variety of deeper and decisive investigations
in order to understand how each component performs and what is its
remaining lifetime? As indicated elsewhere, machine/device data volumeis exponentially growing. Edge analytics comes handy in mining data inneatly understanding the health condition of devices and machines. Also,
learning from data is another interesting phenomenon gaining the confi-
dence of people through the distinct advancements of machine and deeplearning algorithms.
While edge computing has the advantages of local computation and
faster decision-making, the traditional cloud gives an illusion of infinite
compute power and data storage. Big data analytics can be performed outof cloud environments. The onset of pioneering artificial intelligence (AI)algorithms for emitting out personalized, prognostic, predictive and pre-scriptive insights out of big data goes hand in hand with cloud environments.
The future holds good for the hybrid model. The real-time computing of
edge devices in conjunction with massive computing capability offeredby cloud environments is definitely the future. Manufacturers are looking161 The edge AI paradigmfor ways and means to enhance the productivity and the responsiveness of
their production systems further by leveraging edge computing. Smart
manufacturing is the vision of manufacturing industries. Sensing andresponding (S & R) capability is being incorporated in assembly linesthrough edge computing.
Smart manufacturing envisions a future wherein factory equipment can
make real-time and autonomous decisions based on what’s happening on thefactory floor. With multifaceted sensors, actuators, instruments, equipmentand machineries joining in the manufacturing process, a number of activitiescan be measured minutely and linked together to bring in additional astute-
ness and alacrity. All aspects of manufacturing, supply chain, operations,
product design, etc. can be intertwined to bring in flexibility and adaptivity.The solidity of the IoT paradigm, artificial intelligence (AI) advancements,edge/fog computing, 5G communication, blockchain technology, digital
twins, software-defined cloud environments, and integrated data analytics
supply all that are needed to fulfill the traits and tenets of smart manufactur-ing. All these improvisations will ultimately lead to the realization of theindustry 4.0 vision.
Edge Computing for Smart Retail : As 5G, edge computing, and AI tech-
nologies stabilize, we’re going to experience a series of hitherto unhearddigital innovations and disruptions. Real-time data analytics is the key dif-ferentiator of edge computing. Real-time services and applications can bebuilt and supplied to people. Augmented reality (AR) is a newly incor-
porated phenomenon gaining prominence with the success of edge com-
puting. The fast-evolving 5G is getting empowered through edgecomputing, which is also enabled through the power of 5G cellular commu-nication. ( https:/ /venturebeat.com/2020/01/31/as-retail-evolves-5g-and-
edge-computing-keep-you-in-the-express-lane/ ).
The customer experience in retail stores is bound to go up sharply.
Customers’
needs are understood beforehand and fulfilled precisely.
Inventory and replenishment management tasks are getting automated.
Data-driven insights and insights-driven decisions and deals will become
the new normal. Computer and machine vision, natural language processing(NLP), human and machine interfacing (HMI), 5G etc. consciously fulfillthe widely demanded customer delight.
11. Tending toward edge AI
The unprecedented growth of edge computing will pave the way for a
host of sophisticated services and applications at the edge. Edge devices and162 Pethuru Raj et al.clouds are capable of enabling edge data analytics. Especially streaming data
analytics can be greatly optimized with the smart application of artificial
intelligence (AI) algorithms. In Edge AI, the AI algorithms are used locallyon a hardware device, without requiring any connection to the outsideworld. It uses data that is generated from the device and processes it to givereal-time insights in less than a few milliseconds.
Your iPhone has the innate ability to register and recognize your face to
unlock your phone in a fraction of seconds. Take the example of self-drivingcars. Computer vision algorithms play a very vital role in realizing autono-mous vehicles. Real-time decision-making capability comes handy for cars
to traverse in expressways and city roads without any incident or accident.
Vehicle to vehicle (V2V) communication is being simplified through local-ized computing. The computing happens at the edge wherein data gets gen-erated, captured and crushed. Therefore, edge use cases are growing steadily
and sagaciously.
From Google maps alarming you about bad traffic to your smart refrig-
erator reminding you to buy some missing dairy stuff, AI is pervasive thesedays. We are quietly moving from AI toolkits deployed in cloud and enter-
prise servers to AI-enabled edge devices. Smartphones, robots, consumer
electronics, drones, cameras, etc. are being activated to be intelligent in theiroperations through AI algorithms. Handhelds, wearables, portables, andgadgets are being embedded and empowered with AI toolkits. Everyday fac-tors are driving AI processing to edge devices, which are increasingly stuffed
with more processing, memory and storage power. Applications like auton-
omous driving and navigation mandate for sub-millisecond latency require-ments that can be made possible only through edge processing. Speechrecognition, object detection, and other real-time operations need edge
processing.
Computer vision algorithms are being deployed in machines and devices
these days in order to enable them to have a clear view and understanding ofthings around them. This technology empowerment enables devices to
understand of personal, professional, location and social needs of people
in an unambiguous manner, produce the right and relevant device servicesand deliver them in an unobtrusive fashion in real time. That is, the newconcepts of machine vision and intelligence are being insisted in order toaccomplish certain real-time decision-making and action. Deep learning
algorithms, a grandiose part of artificial intelligence (AI) domain, are show-
ing a lot of promise in fulfilling the goals of machine vision and intelligence.The deployment model is as follows. Deep learning models are being pro-duced, trained and tested in cloud environments but the inferencing is being163 The edge AI paradigmperformed in edge devices. This way, the much-abhorred network latency is
significantly reduced in order to deliver right services to right people at right
at right place. Making and refining learning models using cloud infrastruc-tures and platforms are the way forward as cloud centers are being stuffedwith a lot of computing, and storage resources. Inference is a relatively sim-pler task to be performed through edge devices in real time. Also, results can
be obtained through edge-based inferencing in time. When new datasets are
given to an already curated and corrected model, a new upgraded model getscreated and passed on to the edge devices to infer accordingly.
There are formal AI model optimization methods such as compression,
pruning, quantization, sparsity, transfer learning, attention, federated learn-
ing, knowledge distillation, etc. for facilitating running AI models on edgedevices. There are works going on to enable model creation and upgradedirectly on edge devices. There are AI-centric processor architectures and
implementations for doing on-device AI processing. In short, the edge AI
paradigm is all set to flourish with the proper nourishment from semicon-ductor companies and researchers focusing on efficient, explainable andedge AI models. Resultantly, the prediction, anomaly detection, uncovering
of hidden patterns and associations, visualizing fresh possibilities and oppor-
tunities can be simplified and speeded up.
12. Artificial intelligence (AI) chips for edge devices
With a series of noteworthy advancements, it is becoming easier to run
AI toolkits at the edge today. Edge devices are becoming powerful.
AI-enabling chips are being manufactured in large quantities. And the eraof in-device AI is all set to drastically transform our everyday life. That is,intelligence is being achieved at the edge. In other words, edge intelligence
is becoming the new normal. Devices will become innately intelligent.
Intelligent devices will be pervasive and persuasive.
Edge-based AI chipsets and accelerators are being produced and installed
in smartphones, connected speakers, head mounted displays (HMDs), auto-
motive systems such as car, vehicles, trucks, etc., laptops and tablets, robots
and drones, cameras, game consoles, consumer electronics, kitchen waresand appliances, etc. Depending on the AI application and device category,there are several hardware options for performing AI-inspired processing in
the edge. There are multiple options including central processing units
(CPUs), graphical processing units (GPUs), tensor processing units164 Pethuru Raj et al.(TPUs), vision processing units (VPUs), application-specific integrated
circuits (ASICs), field programmable gate arrays (FPGA) and system-
on-a-chip (SoC) accelerators.
Edge AI chips are physically smaller, relatively inexpensive, use much less
power, and generate much less heat. These special properties make themsuitable and sustainable for both consumer and non-consumer devices. As
articulated above, by doing local computation leveraging highly sophisti-
cated chips, a number of benefits can be accrued. Sending large-scaleunprocessed data to faraway cloud servers to be processed is fully avoided.Data security and privacy are being fulfilled. Personal, social and professional
data get secured through edge computing. Local computations always con-
tribute for time-critical applications. Some scenarios need data processing bypowerful servers. Thus, the future belongs to the hybrid model.
Previously for enabling concurrent operations, GPUs have gained
immense popularity. These general-purpose chips are more appropriate
for cloud and enterprise servers. Machine and deep learning models wererun on GPUs without any hitch or hurdle. But running GPUs directly inedge devices needs some refinement. The number of connected devices
is growing exponentially and edge devices are gradually empowered to join
in the mainstream computing. Edge devices are also hooked to cloud-hostedservices, platforms, and databases. There is a need for empowering edgedevices to host and run microservices. Hence there is a race to bring forthfresh chip-making approaches and products. For factory automation, smart
city applications, next-generation retail experience, etc., companies across
the world are working on face recognition, object detection, replenishmentmanagement, autonomous vehicles, traffic congestion avoidance, etc.
13. The noteworthy trends toward edge AI
A number of technological innovations and disruptions have laid
down a scintillating foundation for visualizing and realizing context-awareand cognitive edge AI services and applications. AI-specific chips are beingmanufactured in plenty these days and this turns out to be a huge dif-
ferentiator for the unprecedented success and motivation for edge AI.
There are several ways being debated and disserted for pushing AI capabilityto the edge with the aim of taking edge computing to the next level. Edgedevices are becoming powerful and can find, bind and interact with one
another in the vicinity as well as remote ones through a network. Also,
devices are being hooked into cloud-hosted applications and data sources.165 The edge AI paradigmSometimes one device may not have all the resources in place in order to
finish a task. That means, multiple devices (heterogeneous) devices form
a dynamic and ad hoc cluster/cloud to accomplish complicated tasks.One or more devices contribute as master/control device whereas otherdevices play the role of data storage and processing tasks. That is, the forma-tion of device clusters/clouds has laid down a robust and versatile foundation
for the surging popularity of the edge AI concept. At the chip level, there are
praiseworthy breakthrough processors emerging to accomplish AI opera-tions fast within an edge device. In this section, we are to discuss the keymotivations for the intended success of the edge AI paradigm.

Distributed computing : There is a steady movement from centralized
computing to distributed computing in order to gain business, technical,and, user advantages. By leveraging the distinct ideals of distributedcomputing, process and data-intensive AI workloads get divided and
distributed across a number of machines to speed up their execution.
As we all know, blockchain technology is leaning upon the distributedand decentralized computing models to be efficient and effective for avariety of business verticals. Business problems are dismantled to arrive
at competent software solutions, which are being built as a collection of
interoperable and portable services. Such disaggregation and aggregationgo a long way in agile and risk-free software engineering. That is,enterprise-grade applications are being made through multiple easilymanageable services. Each service and its instances and versions/editions
are being run on independent runtimes to facilitate service composition.
Composite services are process-aware and fulfilling business require-ments smoothly. By picking and packing up certain services, it is possibleto create different applications. Thus, we are on the road towards
next-generation software engineering through configuration, customi-
zation and composition methods. In a distributed setup, the communi-cation among participating service components happens via networkcalls. Thus, applications can be partitioned, data volumes can be sub-
divided into small pieces, and infrastructure resources are also virtualized
and containerized to enhance resource utilization efficiency. Thus, dis-tributed computing is the futurist model. Edge AI is to see a lot of successas it is natively distributed.
AI-centric Processors : The chipset domain is going through a number
of innovations and disruptions. As inscribed above, there are generic and
specific chipsets emerging and evolving in order to simplify and speed upAI processing at the edge.166 Pethuru Raj et al.Advanced Algorithms and Toolsets : There are several machine and
deep learning (ML/DL) algorithms in the industry now in order to bring
out personalized, predictive and prescriptive insights in time. A numberof state-of-the-art algorithms mimicking human brain function is alsobeing unearthed and experimented. With delectable advancements inthe deep learning domain, computer vision (CV) and natural language
processing (NLP) requirements are being met comfortably. Now AI
algorithms are being taken to edge devices in order to squeeze outreal-time insights. There are a litany of enabling frameworks and toolkitsto make AI-enabled edge processing simple, smart and swift.
AI Model Compression : AI models are being compressed to reduce
model size considerably so that the required computational resourcescan come down. Also the time being taken to model also comes downsharply. Further on, the power energy consumption goes down thereby
the heat dissipation level also reduces. Above all, compressed AI models
can comfortably run on edge devices. Thereby, edge clouds formed outof edge devices can be the new environment for AI model creation, eval-uation, optimization, etc. With fresh data coming in, edge devices
quickly redo the training, testing and deployment.
Implementing AI algorithms closer to where personal, social and profes-
sional operations is being termed as the futuristic step. It is all about per-forming time-critical decisions at the edge and refer to the cloud whereintensive computation and historical analysis are needed.
Business behemoths has to address the perpetual AI data management
challenges by setting up powerful and highly available edge systems.Generally big AI applications require large and expensive enterprise-scaleservers. Or in public cloud centers, commodity servers are being clustered
to run AI applications. But in the recent past, we are being bombarded with a
plenty of resource-intensive multifaceted devices. By leveraging the provenand potential distributed architecture, AI processing is being done at theedge comfortably. With the growing popularity of connected devices, many
industries such as retail, manufacturing, transportation, and energy are gen-
erating vast amounts of data at the edge of the network. As indicated above,edge analytics helps to arrive at a cognitive decision.
14. Why edge processing?
The answer is the data privacy and security and the need for real-time
analytics of edge data. The real-time feature may be lost if we take edge data167 The edge AI paradigmto cloud servers to be processed and preserved. That is, proximate processing
is mandated for extracting actionable insights in time. Therefore edge
processing is getting the attention.
Google’s auto suggest application on mobile phones mandates for a
200-ms latency requirement. Similarly, vision-based applications like aug-mented reality (AR), virtual reality (VR), and mixed reality (MR) need high
bandwidth. For these bandwidth-intensive applications, the bandwidth
requirement is set to grow from 20Mbps today to 50Mbps in the nearfuture. Besides real-time data processing, the cost of doing edge-based dataprocessing turns out to be cheaper than cloud-based data processing. The
network bandwidth cost is also on the lower side.
Further on, there are several technological innovations speeding up and
facilitating edge side processing. Data scientists produce powerful yet highlyminiaturized deep neural network (DNN) models that can run on edge
devices without consuming much resources. Efficient implementations of
AI algorithms, lightweight AI toolkits, application-specific AI chips, edgecloud formation, etc. enable edge processing. Several frameworks and tech-niques support model compression, including Google’s TensorFlow Lite,
Facebook’s Caffe2Go, Apple’s CoreML, Nervana’s Neural Network
Distiller, and SqueezeNet.
Edge cloud formation and management platforms are emerging fast. There
are lightweight and edge-specific Kubernetes platform implementations.Kubernetes is famous for deploying, managing and maintaining containerized
applications across a cluster of nodes. Auto-scaling, auto-healing, declarative
APIs, stateful apps through persistent volumes, etc. are the key benefits ofcontainer orchestration platforms. Now the widely adopted cloud-nativeprinciples are being taken to edge devices.
Thus, there are a plethora of technological revolutions to implement the
edge AI vision.
15. Edge-based AI solutions: The advantages
Building and running edge-based AI solutions are not straightforward.
As illustrated above, there are limitations on every side. So, there are a few
critical decisions to be consciously made. Especially expert decisions oughtto be made in data collection and preparation, algorithm selection and train-ing the algorithms continuously, deploying and refining the models, etc.
One widespread trend is that training and testing models can be done at
cloud environments. Then run the trained and tested model in edge devices168 Pethuru Raj et al.to make appropriate inferences in time. The processing/storage capacity at
the edge also needs to be taken into consideration while finalizing the edge
solution design.
Intelligent enterprises consistently insist for real-time computing. It is
indisputably clear that real-time analytics, decision-making, and operationscan see the grand reality only through edge computing. Therefore, edge
solutions are gaining significance for crafting real-time applications.
Further on, to build intelligent devices and applications, AI capabilitiesare being embedded in edge devices. Edge devices can collectivelyself-learn, adapt and act cognitively by the sheer power of AI algorithms.
And this phenomenal transition is seen as the clear game-changer for bring-
ing forth real business transformations. With cognitive edge devices, a vari-ety of smarter and situation-aware services, systems, and environments canbe realized and deployed through the fast-exploding edge AI conundrum.
The unique AI competency can be applied on edge system data (log, oper-
ational, performance, health condition, security, etc.) in order to bring outactionable insights in time. Experts have pointed out the following advan-tages of AI-driven edge devices.
1.
Edge-based AI is primarily used for constructing sensitive and responsive
(S & R) systems. Insights are immediately extracted, delivered andprocessed in order to fulfill the ideals of real-time operations.
2.Edge-based AI guarantees greater security for edge data as data does nottravel outside before taking tactical and time-bound decisions. AI-driven
security measures can be applied in order to ensure heightened security.
3.Edge-based AI is highly flexible. Multiple industry domains ranging
from smart lighting, energy management, and retail to smart cities areto get immensely benefited out of the advancements happening in the
cusp of edge computing and AI.
4.Edge-based AI solutions can be self-contained. With the availability of
advanced machine and deep learning algorithms, computer vision andnatural language processing applications can be realized. This makes edge
AI systems to be autonomous and articulate.
5.Edge-based AI guarantees superior customer experiences. We will fully
realize context-aware, people-centric and adaptive services. Owners,operators, and occupants will enjoy the technology-inspired innova-tions. The customer experience is bound to go up significantly. Trust
on technology solutions will see a strong boost.
As we tend toward the deeply connected digital economy, intelligence has
to move to the edge. Thus, the powerful combination of AI and the IoT169 The edge AI paradigmopens up new vistas for organizations to truly sense and respond to events
and opportunities around them. Remote and rough locations such as coun-
tryside farms, forest areas, oil fields, etc. can benefit immensely from thecutting-edge AI technologies. As IoT moves into more eccentric and dis-connected environments, the necessity of edge or fog computing willbecome more prevalent.
16. Applications that can be performed on edge devices
There are certain types of applications that can be performed on edge
devices.
Machine Learning (ML) at the Edge :ML algorithms are extensively used in
enterprise and cloud environments in order to make accurate predictions
through classifications, clustering, regression, association, etc. algorithms.Now, ML-based prediction capability moves to the edge. Edge devicesare being showered with more power and AI-specific chipsets are hitting
the market. Therefore, running AI algorithms on everyday machines and
devices is gaining momentum. There are several real-world use cases beingpublished by researchers. Intensive care is one area wherein edge-basedML plays an important role. Real-time data capture, processing and
decision-making are important for closed-loop systems. There is a need
to maintain critical physiological parameters, such as blood glucose levelor blood pressure, within specific range of values. Edge AI plays an essentialrole here. There are several verticals keenly exploring and experimentingwith edge AI in order to be ahead of their competitors and to retain the
yearned edge of their brand value. In the subsequent sections, we are to dis-
cuss some domains wherein the edge AI capability is to bring in distinctadvancements.
Machine Learning (ML) Frameworks for Edge Devices : Integrated platforms,
in-memory databases, enabling frameworks and accelerators collectively
contribute for speeding up AI-inspired data processing at the edge. Likeframeworks for accelerated software engineering, ML frameworks arestuffed with pre-trained and tested models for the difficult and repetitive
tasks such as speech and face recognition, object detection, natural language
processing (NLP), etc. These facilitate the development of customML models from the scratch also. We all would have heard GoogleTensorFlow for AI processing at cloud servers. But with the increased usage
of edge devices for real-time computing, Google has released a shrunken
version and named it as TensorFlow Lite, which has application170 Pethuru Raj et al.programming interfaces [APIs] for many data science-centric programming
languages such as Python, Java, etc. This lightweight version is optimized for
on-device applications and comes out with an interpreter tuned foron-device machine learning.
Custom models are generally converted in TensorFlow Lite format, and
their size is also optimized to increase the much-needed efficiency. ML for
Firebase targets mobile platforms and uses TensorFlow Lite, Google Cloud
Vision API, and Android Neural Networks API to provide on-device MLfeatures, such as facial detection, bar-code scanning, and object detection,etc. PyTorch Mobile targets the two major mobile platforms and deploys
on the mobile devices models that were trained and saved as torchscript
models.
It is found that reducing the number of parameters in deep neural net-
work (DNN) models helps decrease the computational resources needed for
model inference. Some popular models which have used such techniques
with minimum (or no) accuracy degradation are YOLO, MobileNets,Solid-State Drive (SSD), and SqueezeNet.
As articulated above, model compression is a viable technique to take
ML models to edge devices Using several compression techniques and cach-ing intermediate results to reuse iteratively, researchers have improved theexecution speed of deep neural network models. DeepMon is one such MLframework for continuous computer vision applications at the edge device.More details can be obtained from this page ( https:/ /blogs.sap.com/
2019/10/16/why-machine-learning-at-the
-edge/ ).
With AI capability is being accomplished through edge devices, there
can
be multiple openings and opportunities waiting to explode. The IoT
device ecosystem will expand further. AI algorithms will be refined to be
fit in edge devices. The market for AI-specific chipsets will enlarge cease-
lessly. Newer business, technical and user cases will emerge and evolve.The onset of 5G connectivity boosts the deployment of the edge AI para-digm. Forming edge device clouds will be simplified through scores of
technological solutions.
Deep Learning at the Edge : One of the most promising domains of inte-
grating deep learning and edge analytics is to achieve computer vision
and video analytics. Edge analytics implements distributed structured videoprocessing, and takes each moment of camera recordings and performs
analysis in real time. A camera can do this today and when multiple cameras
in the vicinity get synchronized, the computational efficiency alongwith real-time knowledge discovery and dissemination get a strong boost.171 The edge AI paradigmThe surveillance and security tasks are getting automated through the
edge AI concept. Emergency incidents can be tackled efficiently in real time
through such kinds of AI-enabled cameras and their clusters. Transmittingvideos to faraway cloud storage appliances to initiate and implement videoanalytics does not solve the real-time requirement. Thus, with the speedyprogress of edge AI, real-time computations steadily move to edge
environments.
Deep learning neural networks bring in additional automation in order
to produce models that substantially improve the prediction accuracy.But the accuracy involves higher computation and memory consumption.
A deep learning model generally consists of layers of computations where
thousands of parameters are being computed in each layer and passed tothe next in an iterative fashion. If the input data has higher dimensionality(that is, if the input data is a high-resolution image), then it is obvious that it
needs extra computational capability. GPU farms are being used in cloud
environments in order to do this kind of massive computation. Migratingto and performing such a huge computational task is edge devices is defi-nitely not an easy task. However, there is a glimmer of hope with a number
of technological innovations simultaneously happen in this space.
Due to the resource-constrained nature of edge devices, the operating
model is being tweaked. That is, the deep learning models are trained andtested in powerful on-premises or off-premise cloud servers and then it getsdeployed on the edge devices to just make inferences on new data. Inferencing
is not a labour-intensive task. The turnaround approach is to design
power-efficient ML/DL algorithms, develop specialized hardware, andleverage distributed architecture that predominantly suits for IoT devices.The device distribution and interaction are being supported through the
5G connectivity, which ensures ultra-reliable and low-latency communica-
tion. 5G also comes to the rescue in deploying a large number of IoT deviceswithin a crowded environment.
Edge-based Inferencing : This will become a new normal soon. IoT sensors
and devices generate a lot of poly-structured data. And powerful ML and DL
models are being trained, tested and used in powerful and scalable cloudenvironments. The refined, curated and refreshed models can, then, be usedin edge devices to make accurate inferences on fresh data. AI Edgeprocessing today is focused on moving the inference part of the AI workflow
to edge devices. The newly captured data resides in edge devices only and it
does not travel to cloud and hence the data privacy and security are fullyguaranteed through this futuristic approach. The impact of model172 Pethuru Raj et al.compression techniques like Google’s Learn2Compress that enables squeez-
ing large AI models into small hardware form factors is also greatly contrib-
uting to the rise of AI edge processing. Federated learning andblockchain-based decentralized AI architectures are also part of the shift.With edge device clouds stuffed with edge AI chips, even the complicatedtraining part may move to the edge in the days to unfurl.
Computer Vision (CV) at the Edge :Undoubtedly, machine vision is the
next target. Vision-based applications are gaining the popularity. We allknow that computer vision (CV) is one of the prime applications of theAI domain. The field of computer vision has grown up solidly and its
adoption rate is high with the maturity of deep learning algorithms.
Vision-enabling platforms and applications are increasingly deployed andmanaged in enterprise and cloud servers. Video and vision data analyticshave become the new normal for extracting actionable insights out of static
and dynamic images. However replicating the same in edge devices is beset
with resource challenges and concerns.
The idea is to empower edge devices to view, perceive, understand and
act. Due to the resource constraints, there is a need for some unique methods
to comfortably run deep learning models in edge devices. Many researchers
and experts have come out with breakthrough solutions. One standout wayis to use the method of model optimization, which turns out to be the finesttechnique to make deep learning models to work on low power and costdevices. That is, optimization techniques come handy in fulfilling the need
for running deep learning models on edge devices. These techniques include
optimizing hyperparameters such as input size, Batch Normalization,Gradient descent, Momentum, etc. The other popular optimization tech-nique is Adam Optimization, which is a method that helps to optimize
model performance and loss value during training the model. It computes
one’s learning rate with different parameters. The Adam optimization algo-rithm is a combination of gradient descent with momentum and RMSpropalgorithms. There is a practical implementation and the details are shared
in this page ( https:/ /medium.com/datadriveninvestor/edge-ai-computer-
vision-on-the-edge-dfa4ad604651 ).
Natural Language Processing (NLP) at the Edge: Artificial intelligence’s
resurgence
is widely credited to the significant advancements in realizing
large computing power. There are and game-changing chipsets such asCPUs, GPUs, TPUs, and VPUs. Then, through compartmentalization
(virtualization and containerization), physical machines could get segmentedinto a number of virtual machines (VMs) and containers. On the other hand,173 The edge AI paradigmhundreds of distributed VMs and containers can be easily clustered together
to realize virtual supercomputers. That is, we have parallel and high-
performance computers. The cloudification aspect has made computingas the fourth social utility. These breakthrough improvements have simpli-fied and strengthened the AI processing. Edge devices are being stuffed withpowerful processors and hence all the innovations happening in the enter-
prise and cloud spaces are being replicated in the edge space also.
Edge solutions and systems are being designed with distributed architec-
tures to support data analytics. When analytics becomes the core and centralaspect, any edge system can be categorized as cognitive systems. Companies
are trying to gain more intimacy and ecstasy with their customers by offering
premium services. Right applications are being produced and delivered toright people at right time at right place with all the trustworthiness.
NLP capabilities are being demanded across industry verticals these days.
In the healthcare domain, NLP can really help patients in clearly articulating
their sentiments and symptoms to care providers. There is no need to sendanything to cloud to be processed thereby it is possible to avoid any privacyand security dangers of personal data. The freshly attached human-machine
interfaces (HMI) empower edge devices to interact with humans in an
intelligent and intimate manner. Edge devices are being enabled to under-stand human instructions and act upon them with all the alacrity and clarity.If there are any issues with machines, they can express them in an unambig-uous manner to professionals. In short, edge devices are also made to have
natural interfaces through the incorporation of the NLP capabilities.
One of critical AI components is natural language processing (NLP),
which can create intelligent interactions between machines and humans.To fill the gap between people and machines, NLP leverages machine learn-
ing, computational linguistics, and even computer science to help under-
stand and even manipulate human languages. This allows for the correctcomprehension of the syntax, semantics and structure of various humanlanguages. With this empowerment, it is easy for machines to interact with
men in a natural style. The prominent and dominant examples of NLP in
action include intelligent assistants like Apple’s Siri, Microsoft’s Cortana,and the Amazon Echo (Alexa).
17. Edge AI use cases
Machine Learning for Smarter Homes : Smart systems and environments
will be realized and sustained with the proliferation of intelligent devices.174 Pethuru Raj et al.For creating sophisticated home applications, multiple devices within a
home have to be linked up. Devices need to react for local events. For exam-
ple, the GPS data from your smartphone or connected car can trigger thesmart thermostat at the home to the desired setting when you areapproaching your home. Using visual recognition, smart locks in conjunc-tion with smart doorbells can also recognize you, unlock the door, trigger
the lights and even start playing music. That is one event or action trigger a
whole lot of things in order to facilitate digital living. Such intelligent coor-dination among home devices is to result in newer experiences for people.That is, machines have to be trained and continuously refined for cognitively
reacting to various events. Datasets for training and testing are plentifully
available these days. Therefore, empowering machines and devices to learnfrom datasets is gaining the speed. This empowerment makes devices to beintelligent in their operations, offerings and outputs when new incidents or
accidents happen.
Smart Manufacturing : We may have a plenty of multifaceted and differ-
ently abled machines in manufacturing floors and product assembly lines.These critical assets have to be minutely monitored, measured and managed.
There are observability platforms and tools with real-time analytics capabil-
ity. Any worthwhile deviation and deficiency can be accurately and proac-tively identified through such monitoring software. If not identified in time,there can be a huge financial and face loss for the company. When partic-ipating machines are inherently enabled to learn their current state, the avail-
ability, reliability and versatility of machineries can be succulently increased.
Machine learning comes handy in empowering predictive and preventivemaintenance of machines. The manufacturing processes can be continuouslyoptimized toward process excellence. The incorporation of edge devices and
the power of AI algorithms combinedly contribute for factory automation
towards fulfilling the distinct industry 4.0 vision.
Smart Environments : In enterprise and cloud IT environments, the new
buzzword of AIOps is gaining momentum in order to optimally run
compute machines. Similarly, AIOps capability can be applied on machine
and device data (log, operational, performance and security) in order tocorrectly judge the device throughput and the remaining lifetime. Thus,besides machine intelligence, operational intelligence goes a long way insetting up and sustaining smarter environments across industry verticals.
Smart railway stations, airports, warehouses, hospitals, hotels, retail stores,
etc. will be easily established and enhanced through edge AI products andservices.175 The edge AI paradigmSurveillance and Monitoring: This is one of the well-known use cases for the
runaway success of edge computing. CCTV cameras are widely used in
important junctions and stations for the safety and security of people andproperties. Typically images and videos get captured and transmitted tocloud storages in order to be processed to extract any important information.AI toolkits are directly installed and run in cameras. Whenever there is
something suspicious and noteworthy, immediately an alarm gets raised
and the concerned people can swing into appropriate action to nip any unto-ward things at the budding stage itself. Real-time processing and notifica-tions go a long way in safeguarding people and properties.
As populations grow, age, and become wealthier, healthcare systems
struggle to provide the care needed by people. Digital health is the way for-ward. The increased deployment and usage of the IoT devices and sensors,cloud infrastructures for healthcare data storage and analytics, path-breaking
AI algorithms, edge AI systems, etc. represent the next-generation digital
healthcare.
Connected Ambulance : In the healthcare domain, every second and minute
is precious. In an emergency situation, the near-zero latency, mobility and
proximate data processing capabilities of edge computing can enable faster
and more accurate diagnosis and medication by paramedics onsite.Location-based and context-aware services can be made and delivered tovarious patients and providers. It is possible to do live streaming of processedpatient data, such as heart rate from sensors and monitors or live video feeds
from first responder body cameras to the hospital with the soaring and roar-
ing 5G power. This gives hospital staff right information on incomingpatients’ state. This acts as an initial information on vigorously visualizingpotential treatment processes. Also, it is possible to do correct analysis of
patients’ vitals (such as blood pressure, pulse rate, etc.) at the edge for
real-time diagnosis and recommendations by first responders.
Further on, augmented reality (AR) glasses (head mounted devices
(HMDs) at the edge help to display information about patient history and
complex treatment protocols to accordingly empower paramedics.
Haptic-enabled diagnostic tools enable remote diagnostics by specialists.More details can be obtained from this page ( https:/ /stlpartners.com/
digital-health-telecoms/digital-health-at-the-edge/ ).
In-hospital patient monitoring :
 As inscribed several times, patients’ data
privacy and security are paramount. Today healthcare sensors and monitors,medical electronics, instruments, and equipment are generating a lot ofuseful data but it is getting transmitted to and stocked in cloud176 Pethuru Raj et al.environments. Also with time delay, the data value also diminishes. In
other words, for emitting out correct insights, data timeliness and trust-
worthiness are very essential. Transmitting highly confidential data on openand public network (the Internet communication infrastructure) and storingit in third-party cloud storage appliances are definitely not wanted bypatients. Therefore, edge devices forming clouds for accomplishing
real-time analytics are being demanded these days. An on-premise edge
device clusters within the hospital building or campus could process datalocally. This setup ensures the unbreakable and impenetrable security forpatients’ health data. Edge analytics through an embedded streaming analyt-
ics platform or a ML/DL algorithm implementation or an AI toolkit enables
real quick decision-making and notifications to the concerned of anyabnormality.
Remote monitoring and care : With the noteworthy technological advance-
ments, remote diagnoses and monitoring are becoming new normal.
Teleconsultation and medication are happening increasingly. In thisCovid-19 situation, it is quite difficult to gain personal and physical attentionfrom doctors. Also, chronic and non-communicable diseases are increasingly
spreading. With more healthcare issues induced by damaged environments,
we have more people who need medical attention often. Consideringthe growing load, we need more number of care givers and clinicians.Also multispeciality hospitals are being situated in urban areas. Ruralpeople would find it difficult to travel to city areas to have them properly
tested. Life expectancy is increasing globally with wealthier people.
Elderly people.
Remote patient monitoring is therefore coming handy in alleviating
some of this strain. There are competent sensors for minutely and meticu-
lously measuring different body parameters. These sensors can network
themselves (body area network (BAN) in order to arrive aggregated value.We have multifaceted wearables, handhelds, smartwatches, etc. Oursmartphone can act as an IoT gateway to transmit acquired data to doctors
and specialists in time. Cameras can contribute here in enabling monitoring
patients remotely. Ambient assisted living (AAL) is an interesting domain.Debilitated and diseased people who are living alone in an isolated situationcan be remotely and rewardingly monitored by care givers and family mem-bers. Post-surgery care management is also done remotely. We have break-
through IoT devices and sensors ably supported by 5G connectivity. To
ensure local intelligence, IoT devices are empowered with AI chips andtoolkits. Thus, data capture and crunching get accomplished locally and fast.177 The edge AI paradigmWith highly optimized machine learning models incorporated in edge
devices, health-related prediction and prescription can be realized in order
to ponder about appropriate measures.
Mining, oil, and gas and industrial automation : The business value of
edge-based ML becomes obvious in the oil, gas, or mining industry.Employees work in remote and rough environments. AI-enabled robots
and machineries capture a variety of decision-enabling data and accurate
decisions are being made in time. Such a capability goes a long way in ensur-ing the safety and security of assets. Critical assets across industry verticals arebeing technologically monitored so that any kind of deviation and distur-
bance can be pre-emptively pinpointed and suitable counter measures can
be considered and activated.
Ambient Intelligence (AmI) : This is quite an old concept. This is a technical
domain for achieving computational intelligence everywhere every time.
Our everyday environments such as homes, hotels, hospitals, etc. are being
stuffed with a wider variety of sensors, actuators, stickers, beacons, micro-controllers, single board computers, LEDs, RFID tags and readers, Wi-Figateways, connected consumer electronics, etc. These devices, embedded
with AI capabilities, are sensitive, perceptive, and reactive. Devices unam-
biguously understand people needs, produce appropriate services (context-sensitive) and deliver them to people in time. Not only information andtransaction services but also physical services will be supplied to people.Multidevice computing will become the new normal. With the astounding
success of containerization and the huge adoption of Kubernetes as the con-
tainer lifecycle management platform solution, composite services will berealized by leveraging sensors and devices collaboratively, cogently, correl-atively and corroboratively.
VR/AR in Retail : Some state-of-the-art stores are starting to leverage
cutting-edge technologies and tools in order to lure shoppers. Retail storesoffer the ability to try on clothes or makeup virtually by leveraging camerasand augmented reality(AR)-type applications to let potential customers
quickly decide the purchase. This capability shows how different color com-
binations, patterns, and fabrics might look on them via special monitors orsmart mirrors. Instead of trying every option physically, these new computervision-based and machine learning-driven services are enabling consumersto find what they want. Also stores can complete sales quickly than they have
in the past.
Fleet management : Logistics service providers generate massive amount of
multistructured IoT telematics data, which, when processed immediately178 Pethuru Raj et al.and intelligently, results in useful insights. This transition of data to knowl-
edge helps to realize smart fleet management operations. Vehicle-to-vehicle
(V2V) communication and vehicle-to-infrastructure (V2I) integrationenable drivers to take conscious and cognitive decisions in time in orderto take right decisions to steer the vehicle correctly to reach the destinationwithout any problem. Telematics data gets subjected to a variety of deeper
investigations by devices inside the vehicle as well as the devices on the road
to extract actionable insights in real-time.
Newer yet powerful IoT devices are continuously hitting the market to
cater to different personal, social and professional requirements.
Miniaturization technologies help to accommodate more inside devices.
Also containerized microservices are being installed in IoT devices.In-memory databases are also being deployed in devices’ memory module.IoT devices are being hooked into cloud servers in order to be remotely
monitored and managed. All kinds of patching and update happen over
the air. With an astounding advancements in AI chipsets domain, IoTdevices are individually as well as collectively made intelligent in order todesign and deliver context-aware services. With the participation of edge
devices, the software applications will be profoundly sophisticated.
Trend-setting people-centric services will be formulated and provided intime to meet up any spatial, temporal, physical, emotional requirementsof people. Enterprise and cloud applications will be considerably strength-ened to bring in decisive automation for businesses. There will be deeper
integration and complex orchestration in order to visualize integrated and
insightful applications. There will be a beneficial integration between edgeand traditional clouds in order to envisage next-generation applications thatfulfills the digital transformation goals.
The business world is heading toward intelligent and real-time enter-
prises. The contributions of edge AI technologies, platforms and acceleratorsare widely applauded.
18. Conclusion
Edge computing is a distributed computing paradigm which brings
computation and data storage closer to the location where it is needed.This is to improve response times, data security and save bandwidth.Real-time processing is one of the most robust features of edge AI. It allows
users to collate, process, and analyze data then implement solutions in the
fastest way possible, making devices highly useful for time-dependent179 The edge AI paradigmapplications. Edge AI has made notable contributions to some industries.
Though edge computing addresses connectivity, latency, scalability and
security challenges, the computational resource requirements for deep learn-
ing models at the edge devices are hard to fulfill in smaller devices.
Mission-critical applications such as factory automation, self-driving cars,
facial and image recognition require not only ultra-low latency but also high
reliability and fast, on-the-fly decision-making. Any centralized architec-
tures are not able to provide the new performance requirements mostly
due to congestion, high latency, low bandwidth and even connection avail-
ability. Furthermore, fast decision-making on the edge needs advanced
computing capabilities right on the spot, which can be provided only by
onboard computers or interconnected edge-computing local nodes working
together.
About the authors
Pethuru Raj is working as a chief architect at
Reliance Jio Platforms Ltd. (JPL), Bangalore.
Previously. worked in IBM global Cloud
center of Excellence (CoE), Wipro consult-
ing services (WCS), and Robert Bosch
Corporate Research (CR). In total, he has
gained more than 20 years of IT industry
experience and 8 years of research experience.
Finished the CSIR-sponsored PhD degree at
Anna University, Chennai and continued
with the UGC-sponsored postdoctoral
research in the Department of Computer
Science and Automation, Indian Institute of Science (IISc), Bangalore.
Thereafter, he was granted a couple of international research fellowships
(JSPS and JST) to work as a research scientist for 3.5 years in two leading
Japanese universities. Focuses on some of the emerging technologies such
as the Internet of Things (IoT), Optimization of Artificial Intelligence
(AI) Models, Big, fast and streaming Analytics, Blockchain, Digital
Twins, Cloud-native computing, Edge and Serverless computing,
Reliability engineering, Microservices architecture (MSA), Event-driven
architecture (EDA), 5G, etc. His personal web site is at https:/ /
sweetypeterdarren.wixsite.com/pethuru-raj-books/my-books .180 Pethuru Raj et al.Dr. J. Akilandeswari is working as
Dean—Academics and Professor of
Department Information Technology at
Sona College of Technology. She has
obtained her postgraduate and PhD degree
in Computer Science and Engineering from
National Institute of Technology,
Tiruchirappalli. She is a gold medalist and
University first ranker during her undergrad-
uate engineering degree. She has obtained
the Fulbright Fellowship during the year
2016. She had been selected as one among
10 administrators in India in Education industry to visit different
Universities in US and gain exposure. She also has travelled to Australia
as part of CII team and visited more than 15 universities. She has a teaching
experience of 23 years and research experience of 16 years. Her research
interests include Data Mining, Data Analytics, Web Services, Cloud
Computing and Computational Intelligence. She has published more than
60 research papers in both national and international journals and confer-
ences. She has filed three patents. She has membership in professional soci-
eties such as IEEE, CSI, IEI and ACM. She has written a book on Web
Technology published by Prentice Hall of India. She has executed projects
funded by Department of Science and Technology, Government of India.
One such project is to take up the training of usage of ICT tools to women
farmers.
M. Marimuthu is working as an Assistant
Professor in the Department of Computer
Science and Engineering at Sona College of
Technology, Salem. He is currently doing
his PhD at Anna University and his research
area of interest is cloud computing and
machine learning. He has 13 years of progres-
sive experience in teaching and industry. He
has published in more than 15 international
journals, attended more than 20 national
and international conferences, and partici-
pated in more than 50 faculty development
training programs (FDP, Workshop, and industry training programs). He
has completed 10+ NPTEL courses like Cloud Computing, Deep181 The edge AI paradigmLearning, Python programming, etc. and has also completed various online
courses like Udemy, Coursera, and IBM. He is a certified Apple Trainer in
IOS Application Development and EMC Academic Associate CloudInfrastructure and Services. He has membership in professional societiessuch as ISTE, IEI, ISSE, and ISRD. He has received various funds fromDST (SEED Division), AICTE (AQIS-PRERANA) and AICTE (ISTE
Quality Improvement Scheme) and has currently applied for various
funding schemes like AICTE ATAL, DST-NGP & DST—YoungScientist and Technologies.182 Pethuru Raj et al.CHAPTER SIX
Microservices architecture
for edge computing environments
Chellammal Surianarayanan
Government Arts and Science College, Srirangam (Affiliated to Bharathidasan University), Tiruchirappalli,
Tamilnadu, India
Contents
1.Introduction 184
2.Need for edge and fog computing 186
3.Nature and requirements in edge and fog computing environment 188
3.1 Evolving or changing needs of the IoT 188
3.2 Heterogeneous nature of the IoT layer 188
3.3 Mobility and low power 188
3.4 Distributed nature of the layer 189
3.5 Low computing/processing power 189
4.Why microservices architecture for edge/fog computing applications? 189
5.How the unique features of MSA fits as a natural choice for edge and fog layers? 190
6.Overview about elements of microservices 192
6.1 Design principle of microservices 194
6.2 Communication protocols of microservices 194
6.3 Application programing interface and microservices 195
6.4 Microservice registration and discovery 197
6.5 API gateway 198
6.6 Polyglot support for development 199
6.7 MSA and transaction support 200
6.8 MSA and design patterns 202
6.9 MSA and security 203
7.MSA for edge/fog computing 204
7.1 Fog computing environment 205
8.Challenges 206
9.Conclusion 207
References 207
About the author 207
Abstract
With the advancement in the field of the Internet of Things (IoT), the number of con-
nected devices keeps on increasing and in 2019 itself, it was about 6 billion devices allover the globe. This indicates the generation of enormous volume of data. As the IoT
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2021.12.001183devices typically have limited computing or processing power, more frequently the data
is being transferred to a centralized cloud infrastructure for further analysis. But this
poses another issue that the devices generate huge amount of data and if each andevery data needs to get transferred into cloud, obviously there would be lot of datatraffic between the devices and cloud. Another aspect to be considered is that thereare many situations, where the data needs to be analyzed in real time, that too nearto the data acquisition point itself. When data is analyzed near to the data acquisition,it enables the implementation of right, time critical tasks at right time. This initiates theevolution of fog computing. Having understood the arrival of fog computing, it shouldalso to be noted here is that though the fog devices are having comparatively richercomputing infrastructure, still they are of only moderate devices operated with lowpower. Hence, it becomes essential to find a right architecture for developing applica-
tions that run in fog layer. With this perspective, this chapter tries to show how unique
features and key elements of the evolving MicroServices Architecture (MSA) fulfillsthe requirements of fog computing environment and helps to achieve an optimizedcomputing solution for edge based applications.
1. Introduction
In conventional cloud computing model, data is being gathered by
different edge/the Internet of Things (IoT) devices over different edges
and transmitted to the centralized large scale data centers for processing
and analysis. The conventional cloud computing architecture has beengrown out of the fact that at that time, the edge devices had only limitedstorage and processing capabilities. So, they need to send the data to central-ized hyperscale servers, typically to cloud servers. The data is processed and
analyzed in the centralized servers. The conventional architecture is shown
inFig. 1 . There are some limitations with this architecture, namely, latency,
bandwidth,
cost, etc. In the conventional architecture, the edge devices send
the data to centralized servers for processing and this inherently involves cer-
tain latency associated with it. This is latency cannot be tolerated in certain
applications. Consider the example of an autonomous car. Assume that acow is in front of the car. Here, the car has to stop immediately as soonas it detects a cow in front of it while giving appropriate signals to other sur-
round vehicles. Consider another example where the locker area is moni-
tored using IoT based smart security system. Assume that the sensordetects the motion of some intruder during night time. Here also the controlaction has to be taken without any further latency or delay. Thus, there are
applications where processing and taking appropriate control actions in real
time become critical. At the time of conventional cloud architecture,184 Chellammal Surianarayananthe edge devices were produced with limited capabilities. But nowadays, the
edge devices are developed with reasonable and adequate processing and stor-
age capabilities. This makes the processing of data both at the place of datacollection and at the time of data collection rather than send it to centralizedservers.
As given in Ref. [1], the number of the IoT devices is around 30 billion in
20
20 (please refer Fig. 2 ). The growth of huge number of IoT devices tends
t
o use more bandwidth of the existing network to the cloud. In reality, many
enterprises are unable to provide the required bandwidth for the devices.Centralized
hyperscale
cloudservers
computa/g415on
andprocessingedge devices
with limited
hardwarecapabili/g415esedge devices
with limited
hardware
capabili/g415esedge devices
with limited
hardware
capabili/g415es
edge devices
with limited
hardware
capabili/g415esedge devices
with limited
hardware
capabili/g415esedge devices
with limited
hardware
capabili/g415esedge devices
with limited
hardware
capabili/g415esedge devices
with limited
hardware
capabili/g415esINTERNET
Fig. 1 Conventional cloud computing (edge devices of limited capabilities and central-
ized
processing).185 Microservices architecture for edge computing environmentsIn addition, the bandwidth requires additional cost. With this situation on one
side, on the other side, nowadays the devices are being developed with ade-
quate storage and processing capabilities. This lays the foundation for edge
computing. With the advent of hardware support, the edge devices are being
designed with its own computing and decision logic in edge layer itself. The
implementation of required processing at edge itself facilitates (i) performing
real time analysis on the data without latency (ii) reduces the frequent transfer
of data to cloud.
2. Need for edge and fog computing
In order to provide real time analysis and the corresponding control
actions in right, the edge devices are being equipped with somewhat reason-
able resources including processing power and storage so that an edge
becomes capable of performing the required and necessary real time analysis
on the acquired data. Also, the data is being sent to its next layer called fog
layer as shown in Fig. 3 .
The nodes of edge computing layer have limited resources and they tend
to carry out very important tasks related to real time analytics. For example,
consider an IoT based remote health monitoring system attached to a
patient. His vital parameters are being continuously monitored by different
sensors such as heart bean sensor, blood glucose sensor, temperature sensor,
pressure sensor, etc. It is mandatory that his vital parameters need to be
Fig. 2 Growth of IoT devices [1].186 Chellammal Surianarayanananalyzed of their values. If any abnormality is found, immediate it needs to
communicate to the concerned physician for timely treatment. Here anykind of tolerance may not be accepted. This kind of immediate analysis is
typically incorporated in the hardware itself. The edge nodes are supported
with required hardware mechanism to perform the critical actions.
Now the edge nodes communicate the acquired data and analyzed
knowledge to fog layer which is almost a centralized one. The two layersare typically established over a Local Area Network (LAN). The fog layer
is capable of performing rich analysis when compared to edge layer. The
fog layer sits as an intermediate layer. This layer decides which data needsto be sent to cloud for further processing. The computation, storage andprocessing are being performed along the IoT- to-Cloud path and not
simply in cloud [2]. Cloud computing provides resources with high avail-
ability
but also with higher power consumption whereas fog computing
fulfills the needs of edge layer by provide resources with comparatively
lower availability and lower power consumption. The key point is the avail-
ability of fog resources is very much adequate to meet the computing needs
of edges devices. Also, the fog layer is very near to the IoT layer. Moreimportant is that the fog layer takes the important decisions in regard tothe amount of data that is being transferred to cloud. This greatly reduces
the huge transfer of data from edge layer to cloud layer.Cloud compu/g415ng layer (Resource intensive)
fog compu/g415ng layer
Edge compu/g415ng layer (layer at data collec/g415on site itself)
Smart devices/sensors layerLA
NINTERNET
Fig. 3 Different layers of computing in edge and cloud computing environment.187 Microservices architecture for edge computing environments3. Nature and requirements in edge and fog computing
environment
3.1 Evolving or changing needs of the IoT
In the Internet of Things (IoT) layers, devices and sensors may get added
according to need arises. For example, consider a weather monitoring sta-tion. In general, certain number of wind speed sensors and temperatures may
be used to detect the temperature and speed of wind at different height such as
10m, 20m and 50m from the ground. But the situation may not be the sameduring all time. Assume that suddenly storm has set and it is required to startadditional wind speed sensors, temperature sensors and also rain gauge sensor
at many levels. In this scenario, more number of devices is getting added.
As far as monitoring and control actions, more functional modules need
to get incorporated. The key point is that there are changes in the function-ality of the application according to the needs that can arise with respect to
time. In most of applications, the software applications undergo frequent
changes in the requirements of the applications. Conventionally, softwareapplications were built for rigid requirements. But modern applications needto incorporate the changing requirements. Here, the applications are dynamicin nature. Also, when the applications grow in size, the deployment of larger
sized applications takes longer time. The mandates alternate architecture in
contrast to conventional architecture which is typically of three-tier in nature.
3.2 Heterogeneous nature of the IoT layer
The devices present in the IoT layer are of heterogeneous in nature. Theyoperate with different communication protocols such as Bluetooth, Zigbee,etc. Mostly the devices are equipped with short range wireless communica-tion. So, they are connected to the Internet via wireless protocols. Also, the
data formats among the devices are may vary. The devices are typically man-
ufactured by different hardware vendors. That is the technologies and pro-tocols used by the devices are of heterogeneous in nature. So, typically thedevices interact with one another with the help of the smart IoT gateway
which provides the right transformation required which interactions.
3.3 Mobility and low power
The devices are inherently having mobility. Also, the devices in the edge and
fog layer are of battery operated.188 Chellammal Surianarayanan3.4 Distributed nature of the layer
Various sensors present in the layer are of distributed in nature. Obviously,
some of decentralized control is sought for efficient handling of data.
3.5 Low computing/processing power
The nodes present in the IoT to fog layer of are battery operable and thoughhave adequate processing power, it is comparatively lower when compared
to that of centralized cloud-based servers which are of unlimited computingpower and having very high availability. Consequently cloud servers consumehuge power also.
4. Why microservices architecture for edge/fog
computing applications?
From the previous section, it is understood that an architectural style
of loosely connected independent and lightweight modules of applications
could be more conveniently deployed in fog nodes. MicroservicesArchitecture (MSA) is an architectural style for developing individual appli-cations that may undergo frequent changes and frequent deployment.
Basically, in MSA, an application is split into several microservices each ser-
vice is designed based on independent functionality with its own data sourceso that it can be independently deployed. Modern business applications tendto incorporate the customer needs and demands into the application nowand then the demands come. Also, when it is split into independent micro-
services managed by small team of people, the services may be deployed
while without disturbing the integration of overall business process. Thisfacilitates both incremental development and agile software development.In practice, MSA has evolved as a de facto standard for developing enterprise
e-commerce applications and cloud native and cloud-centric IoT applica-
tions. Microservices inherently support polyglot architecture where a busi-ness process in which the microservices can be developed using differentlanguages and different technologies. MSA architecture is language agnostic,
hardware agnostic and platform agnostic. This unique feature of MSA makes
it as an ideal architecture for edge computing where the IoT devices tend tohave heterogeneous communication protocols, hardware and data formats.MSA provides a flexible programming paradigm for edge applications. As far
as deployment is concerned, microservices can well be deployed both in
virtual and container-based environments. Compared to virtual machines,189 Microservices architecture for edge computing environmentsin containerized environment, the microservices will get deployed in con-
tainers and more than one container share the underlying operating system.The key point is that the container does not contain the full operating sys-tem. It contains only the microservice and its dependencies as in Fig. 4 . This
greatly
reduces the size and containers become lightweight which is again
very appropriate for edge computing where we are supposed to use the com-puting power only for necessary and crucial processing. Thus, MSA is anappropriate style for developing edge computing.
As mentioned in Ref. [3], the fog computing can benefit from three
unique
features of microservices, namely, independent deployable, indepen-
dently scalable and decentralized computing model.
5. How the unique features of MSA fits as a natural
choice for edge and fog layers?
MSA promote the development of functionally independent and
loosely coupled microservices which are generally designed around the prin-ciple of single purpose.
In MSA the application is realized in terms of several number of micro-
services which can be independently deployable and independently scalable.
Not only scalability, any quality of service parameter can be individually set
of a microservice according to the need of the application.
For example, in application, consider two microservices say A and B.
Imagine that A is interacting with users whereas B is interacting withMicroservice
and its
dependencies
containerMicroservice
and its
dependencies
containerMicroservice
and its
dependencies
containerMicroservice
and its
dependencies
container
container engine
Opera/g415ng System
Infrastructure
Fig. 4 Containerized environment for microservices deployment.190 Chellammal Surianarayananbackend database. Also, assume that the connection to database is done at
seldom. Here, the availability for A alone can be analyzed and can be con-
figured individually with required number of CPUs without affecting therest of the application. This means that, the instances of microservice Aalone can be increased whereas the number of instances of B can be keptat minimum to ensure the required connection to database.
Secondly MSA provides inherently a distributed nature of computing
model. Microservices are distributed in nature. This means that different por-tions of an application can be distributed and a decentralized computingmodel can be developed easily. The microservices can interact with one
another using a set of standard lightweight communication protocols such
as Representational State Transfer (REST) protocol. With the help of looselycoupled communication among different services, the entire application canbe composed using different means of composition namely service orchestra-
tion and service choreography. In service orchestration, a centralized coordi-
nator service will be involved with describes the workflow of the servicesinvolved in a process whereas in choreography every participating service isaware of the workflow and in this model there is no centralized coordinator
services. According to the situations exist, different compositions styles can be
realized for the IoT based applications.
Thirdly MSA supports the development of different portions of applica-
tion with different technologies and different programming languages.I.e. the architecture provides full flexibility to developers that they can
use any languages as they wish. They can use different languages such as
c, C++, Python, Java, etc. MSA supports polyglot nature of application.
Fourthly, the containerization of microservices and containers manage-
ment platforms ensures the provisioning of appropriate resources to different
microservices and ensures the adequate availability of microservices by the
automatic creation and management of required number of containerizedinstances of relevant modules. More importantly the containers enablethe scaling of individual services in a stand-along manner which leads to
the optimized usage of resources in fog environment which is one of the
critical needs of fog layer.
Fifthly, an IoT system should be resilient where the failure of any single
component should not bring the entire system to down. This is facilitated byMSA where the responsibility is decomposed into many services which
helps to increase the resilience of the application.
Ultimately, the elements of MSA including Not Only SQL (NoSQL)
databases, event-based communication and API gate concepts, service191 Microservices architecture for edge computing environmentscomposition can very well implemented in the fog and edge nodes with help
of evolving tools and platforms. In addition, the applicability of micro-services for IoT can be realized in different domains including smart cities,
smart commerce, smart farms, smart cars, smart factories, smart logistics and
cross-domain/general purpose applications, as shown in Fig. 5 [4] .
6. Overview about elements of microservices
The higher level schematic diagram of microservices application is
shown in Fig. 6 . From Fig. 6 , it is clear that like any other architecture,
the
MSA architecture also can be viewed as different layers. The bottom
most layer is the physical infrastructure. This layer may be virtualized foreffective sharing of physical resources. On the top of physical infrastructure,the microservices are deployed in the virtual or containerized environmentwhich is usually managed by the container management tools such as
Kubernetes. Each microservice exposes its functionality with well defined
Application Programming Interface. The microservices interact with oneanother with help of standard protocols such as REST API. On the topMicroservices
based
applica/g415onssmartci/g415essmartcommerce
smartfarms smartcars
smartfactoriessmartlogis/g415cs
Fig. 5 Different microservices based IoT platforms.192 Chellammal Surianarayananof individual services, there is services orchestration layer which is very
essential to integrate the individual services according to specific executionpattern so that a particular business process is fulfilled. Also, each and everyservices needs to register its API in a registry so that other consuming services
will come to know about its functionality.
Similarly, once a service is registered in a registry, it needs to be discov-
ered before its actual invocation. Also, when we talk about enterprise level,there may be several services involved in an overall business solutions. So,
there is a gateway mechanism which is having the knowledge of all APIs
of the individual services and existing load on different service instances.Thus on the top of core services, there is a management layer and on itstop there is gateway which will interact with service users. Also, the entiresituation is a distributed one, i.e., the services need not to be deployedCommunica/g415on protocols (such as REST)Microservice-1 …
Datastore-1Microservice-2
Datastore-2Microservice-3
Datastore-3Microservice-n
Datastore-n
Individualmicroservices/corefunc/g415onalservices
Physical InfrastructureAPI Gateway
service registry
and discoveryservice orchestra/g415on
and choreographyAPImanagement Load balancing and
network management
Virtualiza/g415on/Containeriza/g415onInternetService users
Fig. 6 Different layer of microservices architecture.193 Microservices architecture for edge computing environmentsin same server. The services are distributed and they can interact over the
Internet. Service users can consume the services over the Internet.
The key elements of MSA include
Design Principle of individual services
Communication protocols of services
API and microservices
Service integration techniques
service registration and discovery mechanisms
API gateway
Polyglot support for development
MSA and transaction support
MSA and design pattern
6.1 Design principle of microservices
The very basic idea of MSA is to provide as much as independence to eachmicroservice so that each microservice becomes individually deployable andscalable. To fulfill the basic idea of MSA, the microservices are designed
around single purpose. That is each microservice is designed to perform only
single purpose or single function. To maintain maximum independence,each microservice needs to be designed with its own data store.
The microservices should be cohesive. It means that only the function-
ality which are really are dependent one another should be kept together to
realize a single purpose or responsibility.
The service must be decentralized from other microservices so that a
microservice can be individually deployed and can be set for its other qualityattributes such as scalability, availability, security, etc. In addition micro-
services are distributed.
6.2 Communication protocols of microservices
As each microservice is designed with single purpose, individual micro-
services are required to interact with one another in order to completeany business process. Like in any other districted and decentralized comput-ing paradigm, the microservices can communicate in two different ways.They are (i) synchronous communication and (ii) asynchronous communi-
cation. In synchronous communication, the sender and receiver of the
message are online with each other. A typical example for synchronouscommunication is data exchange using HTTP request/response protocol194 Chellammal Surianarayananas shown in Fig. 7 . The sender sends some message to receiver and waits for
response
from receiver with a blocked state.
In asynchronous communication, the sender simply sends the message to
the receiver and continues to work without any blocked or wait state. The
asynchronous communication usually takes place between microserviceswith the help of a message broker as shown in Fig. 8 .
The 
core components of any message broker include
message—A message is an information packet
message producer—is an application that creates and sends messageto
broker
message consumer—is an application that consumes the message
message broker—enables the messages published by a message producerto
reach its consumers asynchronously. It has two key components,
namely, exchange and queue. Exchange component performs the routing
of the message and queue stores the message
6.3 Application programing interface and microservices
Each microservice has to expose its contract or interface to other services
so as to enables the other services to interact with that microservice.
microservice-A microservice-B
HTTP Request
HTTP Responseblocked state
Fig. 7 Synchronous data exchange between microservice-A and microservice-B.
Microservice-A
Microservice-BMessage broker
(like RabbitMQ)
Fig. 8 Asynchronous communication between microservices using message broker.195 Microservices architecture for edge computing environmentsThe interface gives the details of how to access a microservice. Each micro-
service tends to perform a particular functionality with its required input
arguments and gives back the processed results. So, in order to realize theinteractions among microservices, each microservice has to invoke anothermicroservice according to the calling semantics which basically includes thedata such as (i) which protocol to be used to access the service (ii) IP address
or DNS name of services (iii) service and method or function name
(iv) arguments and return data type etc.
It is the responsibility of a microservice to expose its functional contract
as an Application Programming Interface. A microservice can be accessed
only through its API as shown in Fig. 9 . The API never exposes the imple-
mentation
details of the service.
The API can be developed using different specification such as REST API
specification, Open API Specification (Swagger), SOAP, Apache Thrift spec-ification, etc. REST API is commonly used by many developers. A brief
overview about how REST API specification is given here for reader’sreference. According to the specification, the URL of a service contains allthe required details for accessing a service and services is considered as a
resource. Consider a service, say “ temperature_service ” will return the
temperature of a palce whose “pincode” or “zipcode” is given as input argu-
ment. The URL of this service is shown in Fig. 10 .
In
the above example, the arguments are passed as path parameters. The
architecture also supports other kinds of parameters such as Header parameter
which are included in the request header, Query string parameters which are
specified in as key-value pairs after query string (?), Request body parameters
which are included in the request body.
Applica/g415on Programming Interface
Microservice - AMicroservice -B
consumer/client
Fig. 9 Accessing a microservice through its API.196 Chellammal SurianarayananFrom the above example, it is clear that HTTP protocol is used to access
the service. So, it supports the standard HTTP methods, GET and POST.
Now it is clear that the microservice can be very flexibly access using GET as
GET www.weather.com/temperature_service/getT emperature/{620027}
6.4 Microservice registration and discovery
Along with API implementation, a microservice needs to register its API
into a registry as shown in Fig. 11, so that other microservices can find
the
API.
As each microservice tends to provide a single purpose or functionality,
as far as a business process is concerned, naturally many services will need tobe invoked in a particular order. So, the microservices need to interact withanother. As mentioned earlier, a microservice can interact with anotherMethod namehttp:// www/weather.com/temperature_service/getTemperature/{pincode} 
Argument
Service name HostH/g425p
protocol
Fig. 10 REST API example.
Service RegistryMicroservice Provider1. Service provider publishes the API
Microservice consumer2. Service consumer ﬁnds the API
Fig. 11 Service registration.197 Microservices architecture for edge computing environmentsmicroservice only through its API. So, each microservice has to register its
API in a registry so that other microservices can look into it and find them.
The process where a microservice registers its API in a registry is called ser-vice registration and the process where another microservice finds the APIfrom the registry is called service discovery. There are commercially avail-able tools such as Eureka which provides APIs for registering and discover-
ing microservices.
Service discovery is of two types, namely, client-side service discovery
and server-side service discovery. In client-side service discovery, the service
consumer or client is responsible for determining the location of a micro-
service. Here, the client makes a request to service registry which returns
the available service instances to client. Here determines the matched serviceand also decides which microservice to use, according to load balancingalgorithm. After identifying the right service, the client makes a request
to it through its API. The schematic of client-side service discovery is shown
inFig. 12.
In
server-side service discovery, client submits its request to API gateway
which in turn communicates with service registry to determine the relevant
microservice by matching its API against the request made by the client. The
schematic of server-side service discovery is shown in Fig. 13.
6.5 API gateway
Typically in microservices based application, since there are several distrib-uted microservices involved, all the APIs of the microservices are required to
Load
balancing
algorithmsService registryMicroservice-1
Microservice-2
Microservice-3Service registra/g415on
Client
Service
discoveryFinding matched
service
Service invoca/g415on
Fig. 12 Client-side service discovery.198 Chellammal Surianarayananbe managed effectively. This is done by API gateway. API gateway sits
between client and the microservices based application. API gateway is
the server-side entry point. Client makes its request to API gateway whichin turn needs to identify and locate all the participating services which arecapable of fulfilling the client’s request and invoke them according the
required execution pattern so that the expected business process would
be realized. This is shown in Fig. 14.
API
gateway provides the following key functions
It serves as single entry point to application
It hides the implementation details of individual microservices and pro-
vi
des customized APIs which are specifically designed for different variety
of service clients
It performs service composition
It performs service discovery and routing
It hides all the heterogeneity that arises in communication protocols, data
formats
etc.
6.6 Polyglot support for development
MSA architecture does not provide any mandatory conditions for devel-opers rather the developers can choose their own choice of developingthe microservices. This provides flexibility to developers. It enhances the
productivity of developers. They need to think about the languages or
development environment tools rather they can focus on the core businessLoad
balancing
algorithmsService
registryMicroservice-1
Microservice-2
Microservice-3Service registra/g415on APIgateway
Service
discovery
ClientFinding matched
service
Service invoca/g415on
Fig. 13 Server-side service discovery.199 Microservices architecture for edge computing environmentsprocess and goals. In addition, while designing API also, the architecture
does not put any constraint on the specification of API. Developers canchoose any kind of API styles such as REST API, Apache Thrift API,
Swagger specification for REST API or Open API specification etc.
Similarly as far as data formats for communication is concerned, developerscan choose any format such as XML, JSON format, YAML etc. Also, thearchitecture provides support for different varieties of communications
namely synchronous REST based communication, message broker based
asynchronous communication, event based communication. Here also, var-ious message brokers namely Java Message Broker, Rabbit MQ broker,Apache Kafka broker etc. can be used according to the needs of applications.
Thus the polyglot feature of MSA provides greater flexibility of developing
applications.
6.7 MSA and transaction support
Any business application involves persistence of data. That is there would bedata and information which are considered as assets for any business need tobe stored in a durable manner. Obviously there would be databases anddatastores which store different data of applications. As microservices are
likely to have their own datastores, they databases need to contain logically
valid data. Conventionally the requirements of any database transactionClientAPI gateway
Singleentry
pointto
applica/g415onMicroservice-1
Microservice-21
2
Microservicesbasedapplica/g415onInternet
Microservice-3
Microservice-43
4
Fig. 14 API gateway as single entry point to the microservices based application.200 Chellammal Surianarayananneeds to fulfill the fundamental Atomicity, Consistency, Integrity and
Durability (ACID) properties [5]. Atomicity specifies the need that a trans-
action
should occur in its entirety or it should not happen at all. Consistency
specifies that a transaction should follow serialization whenever simulta-
neous access of same data needs to be accessed. Isolation specifies that eachtransaction should occur in isolation. Durability refers to the persistence of
successful database changes and transactions.
Though conventional applications try to achieve the above ACID prop-
erties, modern e-commerce applications have slightly different requirements
such as availability. In addition, as mentioned earlier, MSA provides support
for polyglot. Here there is no constraint that the data needs to be only in
structured format, rather, it can be semi structured, or even unstructured.MSA actually support modern Not only (NOSQL) database servers suchas MongoDB, Cassandra, etc. More important aspect is that each micro-
service is having its own datastore as shown in Fig. 15.
As
shown in Fig. 15, in MSA data is associated among different micro-
services
and it is not at one place. This kind of data distribution among
different microservices leads difficulty in bringing consistency of data.When one service performs some update in its datastore, then it needs to
be updated in all microservices. Updating each and every change in datastoreof all involved microservices is a time consuming process. Also, NoSQLdatabases do not support Two Phase Commit protocol.
Microservice-1Cassandra
Microservice-2Oracle
Microservice-3MongoDB
Microservice-4XML database
Fig. 15 Distribution of data among different microservices.201 Microservices architecture for edge computing environmentsIn addition to the difficulties in achieving data consistency, modern
e-commerce applications are in need of implementing availability rather than
consistency at all times. So, modern applications and microservices or cloudnative applications are trying to implement what are called Basically Available,Soft state, Eventually consistent (BASE) properties in contrast to ACIDproperties.
According to BASE properties, an application needs to be always avail-
able. It may be in soft state. This means that the state of the application maychange over time. The system becomes eventually consistent. Though thestate of the application changes over time, it eventually becomes consistent.
So, the database becomes logical valid.
Basically, in microservices based applications, they use two kinds of data-
bases, one for read data and the other for writing data. So, the availability isbeing achieved using read database instance. The write database is containing
the updates in datastores with in turn initiate updates in related microservices
in order to bring eventual consistency.
6.8 MSA and design patterns
Any edge or fog computing application has its own typical application ordomain requirements and Quality of Service (QoS) attributes, namely, avail-ability, scalability, security, resiliency, response time, reliability, etc., MSAapplications are being designed and developed using standard design pattern.
Design patterns are developed from best practices and provide quality
solution for complex, recurring problems.
In order to achieve different architectural and domain requirements for
MSA applications, the following design patterns are predominantly used.

Decomposition patterns—Decomposition pattern such as Domain
Driven Design (DDD) and strangler pattern, are used while splittingan MSA application into several, single purpose microservice
Composition patterns—Composition patterns such as aggregator pat-
tern, proxy pattern, chained pattern, branch microservice pattern, API
gateway pattern and client-side UI composition pattern are commonlypreferred in MSA applications. Aggregator pattern is used to combinefunctions delivered by more than one microservices. Proxy pattern isused to prevent the direct exposure or access of a microservice by its cli-
ents. Chained pattern provides a variety of composition pattern where
functions of different microservices are combined sequentially as a chain.Branch microservice pattern is also a kind of composition pattern wherethe function of a microservice would be implicitly includes the function202 Chellammal Surianarayananof another microservice. API gateway pattern is used to provide single
entry point to an MSA application. Client-side UI composition pattern
is used which the GUI of an application is designed using severalmicroservices
Database patterns—Fundamentally each microservice has to be designedwith its own datastore. Commonly used database patterns include data-
base per service pattern, shared database per service pattern, Common
Query Responsibility Segregator (CQRS) pattern and saga pattern.
Observability patterns—To monitor the behavior of an applicationobservability pattern is used. Patterns such as centralized logging service
pattern, distributed tracing pattern can be used to monitor the function-
ing of an application.
Cross-cutting concern patterns. Whenever changes are performed inthe coding portion of microservices, cross-cutting concern patterns
can be implemented to avoid or reduce code modification. For example,
an application may involve many input parameters which are likely tochange. These parameters are brought out the programs and set as prop-erties in configuration files. The configuration files are external to pro-
grams. So, even when the values are changed later, it would not require
any changes in the code. This pattern is called external store configura-tion pattern.
Deployments patterns—A microservice is like to change and it may getredeployed. Whenever changes are performed in microservices, there
will be different version of microservices. The deployment of newer ver-
sions can be carried out using deployment pattern. For example, the bluegreen deployment pattern facilitates the deployment of newer version ofmicroservice by maintaining two identical production environment and
making one of them live.
6.9 MSA and security
Like any other application, MSA based applications also should be protected
against any unauthorized access and security of the application should beensured. There are various best practices for ensuring microservices security.Some of them include
API gateway - Implementing API gateway prevents the direct access of
microservices by the clients. In addition, the developers can implement
overall application level security mechanism in API gateway. They canimplement the overall authentication and access control in API gateway.In addition to application level security, security can be implemented203 Microservices architecture for edge computing environmentsfor each microservice. For example, OAuth 2.0, provides an industry-
standard protocol for authorizing users across distributed application.
Container Security—When microservices are deployed in cloud envi-
ronment,
containers are used for deployment and in such environments,
container security becomes important.
Regular scanning of source code repository—Typically application are
being
developed using different third party libraries and dependencies.
This makes an application vulnerable to attacks. So, the source code
of the application needs to be checked regularly
Use of HTTPS—Fundamentally HTTPS serves as the basic element in
bringing
security to any application as it prevents common attacks such as
phishing. HTTPS ensures privacy and data integrity by encrypting com-
munication over HTTP.
7. MSA for edge/fog computing
Basically, fog computing devices have limited computing power, stor-
age and processing capabilities. The power requirements are also low. Moreimportantly the devices are of heterogeneous. MSA fits as a natural choicefor development and deployment of fog applications due to the following
reasons.
1.Services support peer to peer communication.
2.MSA supports polyglot development of applications where differentmicroservices
can be developed using different technologies, program-
ming languages, protocols and data formats. This relieves the burden that
lies with developers that can freely choose their own development stack.
3.More important aspect is that MSA can run in containerized environ-
ment
which always optimizes the power requirements. It is one of the
desirable key points in fog computing environment where the available
infrastructure is minimal.
4.MSA provides a flexible and loose coupled way of developing applica-
tions
along with individual deployments.
With the above ideas, higher level architecture of MSA for fog computing
can
be realized as a layer architecture as shown in Fig. 16 .
Edge
devices—This layer is composed of various IoT devices, connected
sensors. These devices are of heterogeneous in nature in terms of commu-
nication protocols, data formats, power consumption needs, hardware ven-
dors, processors, computing power etc. Despite the heterogeneity, they are
of low powered devices have very limited computing infrastructure.204 Chellammal SurianarayananSo, they depend on the fog layer which is very near to edge layer and avail-
able in the same physical site or same campus geographically.
7.1 Fog computing environment
As far as fog computing layer is concerned, it is having comparatively highercomputing infrastructure and thus able to perform the necessary logic andEdgeDevicescommunica/g415on protocols (like HTTP) and Rou/g415ng protocolcontainerized
core microservice
in fog devicecontainerized
core microservice
in fog devicecontainerized
core microservice
in fog devicecontainerized
core microservice
in fog deviceService
orchestra/g415onresource
managementinstance and
placement of
servicesmanagementmonitoring
and
managementcontainer and
other
managementService
registra/g415on
and discoveryload balancing and other Quality of Service
(QoS) managementAPI
management
API gatewayFOGcompu/g415ng(MSABased)Resourceful Cloud Layer
Fig. 16 Typical MSA layers for fog computing environment.205 Microservices architecture for edge computing environmentsanalytics which are mandatorily needs to be done near the sensing point
itself. When the required analytics and processing is taken place near to
the sensing or edge layer, the unnecessary flow of data to cloud can beprevented which significantly reduces the network traffic and cost.
In the fog layer, the lower most layer refers to API gateway layer. This
layer serves as the entry point for the MSA based process deployed in fog
computing environment. On the top of this layer, one could find the com-
munication layer. Here, HTTP protocol is shown as it is a lightweight pro-tocol and more suitable for fog environment. The key point here is thatHTTP does not require any other additional infrastructure for communica-
tion and it is preferred in an optimized environment such as fog.
On the top of communication layer support, we have the layer of core
microservices, each of which performs a typical function of the application.These individual microservices are deployed in containerized runtime
which can be very well accommodated in fog devices.
On the top of core services, one can find the next higher level layer
where service orchestration, container management, management ofresources, services instance and monitoring of application. Still on the top
of this layer form the other functionalities of service registration, service dis-
covery, API management, load balancing and other QoS management.
All the components of different layers show only the schematic and try to
represent that the layers can be deployed in fog devices. At last, from thelayer, the processed and analyzed results are communicated to cloud via
the Internet. The cloud layer provides rich resourceful computing facility
to carry out further analysis and archival of useful knowledge.
8. Challenges
Despite MSA provides many desirable and unique features such as poly-
glot support, individual and independent deployment, implementation ofquality attributes such as scalability at individual service level, loose coupling
between services, MSA is inherently associated with the following challenges.

As the application is split into several services, the operational complexityincreased
Testing of the application becomes complex
Since microservices need to communicate with one another, therewould be latency issues
Implementation of transaction in databases becomes difficult206 Chellammal Surianarayanan9. Conclusion
This chapter discusses the need for edge/fog computing, critical
requirements of applications deployed in fog computing layer and how those
requirements are naturally fulfilled by the unique features of microservices
architecture. For better understanding, an overview of key elements of mic-
roservices is presented. Then a higher level conceptual layered architecture is
presented to show the applicability and feasibility of MSA for developing fog
applications. The challenges associated with microservices application are
highlighted.
References
[1] M. Capra, R. Peloso, G. Masera, M. RuoRoch, M. Martina, Edge computing: a survey
on the hardware requirements in the internet of things world, Future Internet
11 (4) (2019) 100, https:/ /doi.org/10.3390/fi11040100 .
[2] Ashkan Yousefpour, Caleb Fung, Tam Nguyen, Krishna Kadiyala, Fatemeh Jalali,
Amirreza Niakanlahiji, Jian Kong, Jason P. Jue, All one needs to know about fog com-
puting and related edge computing paradigms: a complete survey, J. Syst. Archit.,
Volume 98, September 2019, Pages 289 –330, https:/ /doi.org/10.1016/j.sysarc.2019.
02.009 .
[3] Samodha Pallewatta, Vassilis Kostakos, Rajkumar Buyya, “Microservices-based IoT
application placement within heterogeneous and resource constrained fog computing
environments”, UCC’19: Proceedings of the 12th IEEE/ACM International
Conference on Utility and Cloud Computing, Pages 71 –81. doi: https:/ /doi.org/
10.1145/3344341.3368800 .
[4]B. El Khalyly, A. Belangour, M. Banane, A. Erraissi, A comparative study of
microservices-based IoT platforms, Int. J. Adv. Comput. Sci. Appl. 11 (8) (2020) 389 –398.
[5]https:/ /www.geeksforgeeks.org/acid-properties-in-dbms/ .
About the author
Chellammal Surianarayanan is an
Assistant Professor of Computer Science in
Government Arts and Science College,
Tiruchirappalli, Tamil Nadu, India. She
earned a doctorate in Computer Science by
developing computat ional optimization
models for discovery and selection of semantic
services. She publishe d research papers in
Springer Service-Oriented Computing and
Applications, IEEE Tra nsactions on Services
Computing, Springer New Generation
Computing, International Journal of207 Microservices architecture for edge computing environmentsComputational Science, I nderscience, SCIT Journal of Symbiosis Centre for
Information Technology, etc. She p roduced books and book chapters with
Springer, IGI Global, CRC Press. She has been a life member in professionalbodies such as Computer Society of India, IAENG, etc.
Before coming to Academic service, Chellammal Surianarayanan served as
Scientific Officer in Indira Gandhi Centre for Atomic Research, Department
of Atomic Energy, Government of India, Kalpakkam, TamilNadu, India. She
was involved in the research and development of various need-based devel-opment of embedded systems and software applications. Her remarkable con-tributions include the development of an embedded system for lead shield
integrity assessment system, portable automatic air sampling equipment, the
embedded system of detection of lymphatic filariasis in its early stage anddevelopment of data logging software applications for atmospheric dispersionstudies. Totally she has 25 years of academic and industrial experience.208 Chellammal SurianarayananCHAPTER SEVEN
Edge data analytics technologies
and tools
N. Jayashreeaand B. Sathish Babub
aDepartment of Computer Science and Engineering, C Byregowda Institute of Technology, Kolar,
Karnataka, India
bDepartment of Computer Science and Engineering, R V College of Engineering, Bengaluru, Karnataka, India
Contents
1.Introduction to edge data analytics and benefits 210
1.1 Benefits of edge data analytics 211
2.Edge data analytics versus server-based data analytics 212
3.Architecture and methodology of edge data
analytics 213
4.Edge data analytics technologies and solutions 215
4.1 An extended AWS to the edge devices for function with data generatedwhile using the cloud for necessary resources is named AWS IoTGreengrass 215
4.2 CSA-A network assessment tool by Cisco named Cisco SmartAdvisor(Cisco discovery service) 216
4.3 An analytics software package named Dell Statistica 219
4.4 An edge system providing high performance, low latency data processingby Hewlett Packard enterprise named HPE edgeline 221
4.5 An edge analytics agent by IBM named IBM Watson IoT edge analytics 223
4.6 An IoT hub to apply analytics at edge devices named microsoft azure IoTedge 225
4.7 A solution by Oracle for event processing is named Oracle edge analytics(OEA) 226
4.8 A solution to handle complex analytical processes is named PTCThingWorx analytics 228
4.9 Components and use cases 230
4.10 A to-the-edge component to collect, process, and transmit data is named
streaming lite by SAP HANA 231
5.Working principles and feature comparisons 232
6.Some of the other use cases of edge analytics 233
References 234
About the authors 235
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.004209Abstract
Edge data analytics reduces the volume of data that needs to be sent to the cloud or
other available resources for processing. It facilitates with avoidance of an additionalprocessing state via autonomous behaviors of the machine, increased security, andminimized costs of data transmission. Data analytics happened to be of the highestimportance over the collected data to draw any meaningful insights (Donitha, 2017)
[1]. In this chapter, we discuss the tools and technologies of edge analytics which is
the
best choice compared to relying on descriptive analytics, diagnostic analytics,
predictive analytics, and prescriptive analytics.
Abbreviations
AWS Amazon Web Services
IoT Internet of Things
REST representational state transform
API application programming interface
CQL card query language
1. Introduction to edge data analytics and benefits
The devices generating data in IoT are sensors, ubiquitous computing
devices, cameras, etc., used in any IoT application. The streaming data are col-
lected at a centralized body for processing. As IoT technology emerges, theinformation being monotonously streamed by the IoT devices increases andis rapidly transmitted, and results in an accretion of unmanageable data, mostof which is never used. The data produced by various IoT devices need to be
stored, processed, and analyzed at the Edge for an effective digital engagement.
This is achieved by performing the d ata analysis at non-centralized
data-generating devices in a system. The term “at the edge” represents the databeing processed and analyzed nearer to the generating devices.
Edge Data Analytics: A data analysis procedure or tool implemented
at the Edge of the network either within or closer to the device thatgenerates data.
The edge data analytics minimizes the amount of data that need to be
transmitted to the cloud or other available resources for processing. It allowsthe application to avoid an additional processing state by enhancing themachine’s autonomous behaviors, increasing security, and minimizes datatransmission costs. Edge data analytics gains importance with faster decision
making irrespective of bandwidth and IoT technology for automation.
Edge data analytics is similar to any other analytics except that it is done at
the Edge on devices having storage capabilities and processing capabilities,210 N. Jayashree and B. Sathish Babualthough they have certain communication curbs. The edge data analytics is
not a substitution of centralized processing, whereas it can augment with it
ensuring the delivery of data discernment. The benefits of edge data analyticsare given in Table 1 [1][2].
1.1 Benefits of edge data analytics
SeeTable 1 .
Table 1 Benefits of edge data analytics.
i.Improved productivity by real-time analysis with faster and autonomous decision
making : As the analysis is done near the data, it is easy to be done in near
real-time and is challenging to be achieved if the data needs to be transmittedelsewhere like the cloud server or any data center.
ii.Time Efficient : Edge analytics filters unnecessary information before analysis.
This minimizes the processing time and data transmission times. As the data isanalyzed at the edge device, it is near real-time analytics.
iii.Averting latency : In many applications, it is not advisable to transmit the data to a
remote location and wait for the results as they need real-time analytics. Edgeanalytics reduces latency by real-time analysis and by identifying discernmentat the Edge.
iv.Furnished scalability over time : As every device analyzes its data, the computation
overhead is decentralized, resulting in improved scalability.
v.Reduced cost of central storage as most processing is done at the Edge and only necessarydata is transmitted : The usual way of data analysis at data centers involves
substantial costs. Also, the data storage, processing, and consumption ofbandwidth are tied with expenses. Some use IoT device resources to performthe analytics reducing the costs of back-end processing.
vi.Finer security and privacy by avoiding granular data from storage or transmission : The
analytics applied at the IoT devices avoid entire data across the network. Theactual data do not leave the device and hence is secure.
vii.Minimized bandwidth usage : Several connected IoT devices will be active
worldwide, and the bandwidth required by them is higher as the number ofdevices increases both to transmit and receive. Edge analytics provides theanalytic capabilities to the remote data analysis centers and reduces thebandwidth issue.
viii. Reduce Bottleneck : The data generated includes extensive data and causes
congestion in the network. The bottleneck is reduced as edge analyticsprocessed the raw data at the Edge rather than forwarding it.211 Edge data analytics technologies and tools2. Edge data analytics versus server-based data
analytics
Edge analytics analyzes data at the device, whereas server-based ana-
lytics requires the data to be transmitted to the remote server for analysis.
The comparison between the two approaches is given in Table 2 [3] .
Table 2 Comparison of the performance factors.
Factors Edge-based data analytics Server-based data analytics
Reliability High reliability with access to the
raw uncompressed data. The data
generated is treated by the
analytics at the device, and the
results are more promising.Less reliable since data arecompressed/processed andforwarded to the servers and
hence affects the quality of the
result.
Downtime There is no need to transmit data
to the cloud or data centers forprocessing, as the analysis is doneat the edge device. Downtime ofthe network does not affect the
analytics.Require data transmission to the
cloud or data centers for analysis,and the process is delayed duringdowntime.
Bandwidth Lower-bandwidth requirements as
only simple information like alarmsare transmitted to the servers anda n a l y t i c si sd o n ea tt h eE d g ea n dhence can be used in a bandwidth-
constrained environment.Requires high bandwidth for the
transmission of data to the serverand cannot be used in abandwidth constraintenvironment.
Latency Low - Data is analyzed at the
device.High—Data is analyzed at theserver.
TimeefficiencyAnalytics in real-time as the datagenerated by the device isprocessed at the Edge.Analytics has a delay as the dataneed to be transmitted across thenetwork to the cloud.
Connectivity It does not require network
connectivity to the server.It requires network connectivityto the server.
Cost Embedded analytics is set up low
cost in the devices.Focus more on power, reliabilitythan the cost.
Security There is the least risk of a security
attack as the raw data is nottransmitted to the server.There is more risk on data intransit.212 N. Jayashree and B. Sathish Babu3. Architecture and methodology of edge data
analytics
Edge analytics involves tools on or nearby IoT devices for collecting,
processing, and analyzing the data streamed from the devices instead of trans-
mitting them to the cloud or any other remote servers for analysis. Edge ana-lytics streamlines the data with real-time analysis and ensures the collectionof enough data from the device. At the Edge, the devices are designed to
have their analyzing capabilities.
The architecture [4]of edge data analytics is given in Fig. 1 . The edge
devices are
general-purpose devices with full-fledged operating systems and
battery power. The devices can run edge intelligence that runs commands
on the data with the help of an on-premise server and sends the signals to the
actuators. The device is connected to the cloud either directly or through aNetwork Switch . They act between the edge devices and the cloud for loca-
tion mapping services. The cloud sends the signals such as configurations, data
queries, or machine learning models. There is a pattern followed by any edge
analytics tool:
The collection of data by any device at the Edge, as the devices generatethe
data, they are streamed to the edge analytics tool.
Performing analysis of data at the Edge using in-built analytics features.
Data
analysis as the data is being streamed decreases the latency in
decision-making on the connected devices.
The results are used to take action, like responding immediately to the
received
data based on the status and respective measures.
There will be a transmission of the necessary summarized data from theEdge
to the cloud. This reduces the bandwidth requirement in case of
transmitting the raw data.
The protocols commonly used to transmit the data from edge device to thecloud
include HOOT/HTTPS (Hypertext Transfer Protocol/Secure),
MQTT (Message Queuing Telemetry Transport), RTSP (Real-TimeStreaming Protocol), WebRTC (A combination of standards, protocols,and JavaScript and HTML5 APIs), and Zigbee (uses packet-based radioprotocol).213 Edge data analytics technologies and toolsFig. 1 Edge data analytics architecture.4. Edge data analytics technologies and solutions
4.1 An extended AWS to the edge devices for function
with data generated while using the cloud for
necessary resources is named AWS IoT Greengrass [5]
(Fig. 2 )
It expands cloud capabilities to the local Edge through the accumulation and
analysis of the data from the data source, ensuring the autarchic responses to
the local events, and communicating securely with the native devices with-
out connecting to a cloud. The customers have the privilege to assemble IoT
devices and application logic. It also provides a message buffer for preserva-
tion in case of failures and also secures the user data. It consists of software
distributions, cloud services, and great features for data analytics.
4.1.1 Benefits
AWS IoT Greengrass helps build intelligent IoT devices faster. Here are the
benefits:
i.Pre-built components to add or remove and control device software
footprint
ii.Install and govern device software and configuration remotely and at
scale without firmware updates.
iii.Bring cloud processing and logic locally to edge devices and operate
even with the incomplete connection.
iv.Program devises to transmit only high-value data, making it easy to
deliver rich perceptions at a nether cost.
4.1.2 Working of AWS IoT Greengrass
AWS IoT Greengrass ( Fig. 3 ) is an open-source edge runtime and cloud ser-
vice for building, deploying, and managing device software. The client soft-
ware enables local processing, messaging, data managing, ML Inference and
offers pre-built components to hasten application development. Then the
Fig. 2 AWS IoT Greengrass architecture.215 Edge data analytics technologies and toolsAWS IoT Greengrass cloud service aids building, deployment, and
governing of device software over the squadron of devices.
IoT devices can vary in size. AWS IoT Greengrass Core components,
AWS IoT Device SDK-enabled components, and FreeRTOS components
are calibrated to interface with one another. If AWS IoT Greengrass Core
component loses affinity to the cloud, connected devices can continue to
communicate with each other over local network.
The pre-built components facilitate easy extending of edge device func-
tionality without writing code. AWS IoT Greengrass components enable
adding features and quickly connecting to AWS services or third-party
applications at the Edge.
4.1.3 Scenarios
i.Run at the Edge: AWS IoT Greengrass quickly gets intelligence to edge
devices, such as anomaly detection in precision agriculture or powering
autonomous devices.
ii.Manage apps: Deploy fresh or heritage apps across squadrons using any
language, packaging technology, or runtime.
iii.Control squadrons: Manage and operate device squadrons in the field
locally or remotely using MQTT or other protocols.
iv.Process locally: Assemble, amass, refine, and transmit data locally.
Manage and control what data goes to the cloud for optimized analytics
and storage.
4.2 CSA-A network assessment tool by Cisco named Cisco
SmartAdvisor (Cisco discovery service) (Fig. 4 )
It is a network assessment tool by Cisco for progressing customer networks
mainly used to organize network relocation and superlative network reforms
resulting in customer network evolution. It can summarize the data trans-
missions and manage the reports of the same. The data processing can be
applied to assess the network reforms, the services, and the vulnerabilities
Fig. 3 Working of AWS IoT Greengrass.216 N. Jayashree and B. Sathish Babuin the network. The client-side tool for CSA as accepted by Cisco is IP
Explorer [6]that elevates the network management activity. The objectives
of this service are:
–To evaluate the life cycle of the product in spot the outmoded software
and hardware components
–To discover issues in security
–To congregate commitment particulars, and
–To gauge the potential of components to launch new services or
technologies.
Finally, CSA acts as abet for Cisco associates to realize network concerns and
plan optimal network upgrades.
NETvisor’s IP Explorer for CSA helps perform Cisco Discovery Service
(CDS) to:
–automatize network revelation and documentation activity
–inspect the network’s robustness and end-of-life shape
–recognize network-related problems of the customer
–initiate Cisco product improvement strategies
4.2.1 Features and benefits
-Contemporary CDS information about components on network
revelation
-Handy interface to initiate and mark CDS transaction process
Fig. 4 Cisco SmartAdvisor dashboard.217 Edge data analytics technologies and tools-integrated CDS reports with extended network inventory data
-Requested CDS reports stored in a local database
4.2.2 Working of CDS (Fig. 5 )
IP explorer collects data from Cisco products through SMTP requests or
configuration files and forwards that to the CDS server for comprehensive
analysis. IP explorer web GUI has a CDS interface. A new transaction is
launched via the CDS interface to initialize a complete examination.
Customization is possible through the selection of transaction types which
in tracking the status of the activity via the web interface.
The complete analysis results in life cycle reports, security issues, product
deficiencies that are not related to security, and contract information. The
reports can be accessed through the CSA portal and CDS web interface of IP
Explorer and web GUI.
Fig. 5 Working of CSA.218 N. Jayashree and B. Sathish Babu4.2.3 Requirements for the application of CSA
i.User ID and partner level details in Cisco.com
ii.Internet for data transfer between IP Explorer server and CDS servers.
4.3 An analytics software package named Dell Statistica (Fig. 6 )
It was developed by StatSoft and acquired by Dell [7]. It facilitates data ana-
lytics
features combined with machine learning and visualization procedures
to identify outliers in the processed data. Statistica has been extended with
data preparation and new network analytics capabilities to enhance fraud
detection potentiality.
Dell’s top objective is to improve operational efficiencies. Dell Statistica
has facilitated the reduction of time spent on models by 50%. The develop-
ment process is much leaner and smoother compared to prior. Dell Statistica
was rated an overall leader in customer experience and vendor credibility
while scoring substantially above the overall sample based on 33 criteria.It also garnered best-in-class technical support product knowledge and con-sulting product knowledge. Empowering traditional and citizen data scien-
tists is a key differentiator that likely helped Dell earn a leadership spot.
Reusable Process Templates now empower all types of users to share and
distribute analytic workflows. Dell aims to create a foundation of openness
and flexibility to elevate Statistica’s customer experience. This strengthensDell’s credibility as a trusted, innovative analytics partner and support for
heterogeneous environments. This helps customers run any analytics on
any data, anywhere, to drive better decisions across their organizations.
Dell Statistica has built a firm basis of business knowledge by endorsing
top pharmaceutical, manufacturing, healthcare, financial services, and retail
organizations worldwide. For example, deep insights and analytics have hel-
ped Sanofi and other customers improve quality control, increase marketintelligence, reduce supply chain risk, elevate customer experiences. DellStatistica leaps out among the different solutions because of its persistent
spotlight on making analytics more accessible. The opportunity to modern-
ize analytics remains a guiding force and critical competitive discriminator inits approach, product development, and customer partnerships. The latestversion of Dell Statistica includes edge scoring for IoT analytics, native dis-tributed analytics architecture (NDAA), and collective intelligence.
The platform also offers network analytics to visualize entity relationships
and graphical associations and combine predictions with human expertise tounderstand relationships within networks better. Statistica’s collective219 Edge data analytics technologies and toolsFig. 6 Dell Statistica architecture.intelligence feature helps businesses embrace the app marketplace for
models. Users can construct models and data or import models written by
others. Hence, Statistica has good connectivity to increase its capability
based on user requirements. The NDAA serves the purpose of trying to
break down data repositories.
4.4 An edge system providing high performance, low latency
data processing by Hewlett Packard enterprise named HPE
edgeline (Fig. 7 )
When used, it is a facility that confederates the process of data collection,
control systems, and networks to enable new analytics abilities at the
Edge [8,9]. It exhibits high performance and minimizes latency using
edge-optimized services and remote system management procedures. It is
a converged operational technology (OT) like data acquisition, control sys-
tems, industrial networks, and enterprise-class IT in a single, consolidated
system that implements data center-level compute and management tech-
nology at the Edge.
4.4.1 Significance of converging OT and IT: Connecting workers with
data and insights at the edge
Connecting Edge works with remote experts to diagnose and resolve
production issues reduces downtime, making assembly lines more
productive.
Providing line workers with real-time step-by-step guided assembly
instructions and other operations-critical information critical through
Fig. 7 HPE edgeline.221 Edge data analytics technologies and toolsthe wearable or handheld device and overlaid digitally onto the product
and work surface improves product quality and throughput and reduces
rework.
Connecting technical support specialists to customer environments todiagnose and resolve product issues reduces service technicians’ numberof customer site visits.
As the amount, diversity, and pace of data continue to increase, it’s necessaryto collect data generated at the Edge, analyze the data in near real-time, anddeliver the insight directly to the edge workers and their devices. One ofsuch technology is Augmented Reality with capabilities like:
–
Two-way video streaming
–Audio for collaboration
–Content overlay
–Streaming data
–Enterprise system connections
–Quality and compliance standards in enterprise systems
–Access to training and other knowledge management systems
4.4.2 Benefits
i.Gains in productivity and worker efficiency
ii.Reduced impact from a retiring skilled workforce
iii.Improved production quality
iv.Increased asset uptime with guided service and maintenanceinstructions
v.Reduced time to diagnose and resolve production and product supportissues
vi.Reduced time and travel costs associated with service technician sitevisits
4.4.3 Use cases–
At a process manufacturer of hazardous chemicals, the time needed toidentify asset risks in its plant is reduced by 75%. Here, a single sparkcan result in an explosion. It is possible to digitally retrieve documenta-
tion and get 3D rotational views of assets and risk-based inspection
graphs. Workers are trained on production processes, using a connectedworker solution, on workflows and allow them to experience the envi-ronment prior to setting foot on the plant floor.
–Diagnosing and repairing IT devices by nontechnical workers in sizeableretail distribution centers and verifying in-store inventory levels through222 N. Jayashree and B. Sathish Babumobile devices and wearables to connect to HPE technical support. Here,
the number of site visits by HPE technical staff was reduced from 54 to 1
during a five-month pilot.
–A large-format digital printing press manufacturer can diagnose issuestwice
as fast and reduce resolution times by as much as 70% in its technical
support centers. Using HPE Visual Remote Guidance (VRG), the sup-
port technician remotely connects with the customer, who dons
AR-based wearables to allow the technician to see what the customersees, diagnose the issue, and visually guide the customer through fixingthe problem. This avoids dispatch of support engineer to spot.
–A large enterprise remodeling its location reduces travel time and dollars,according
to HPE, using HPE VRG for project status inspections.
Construction workers can don the wearables or use a tablet to walkremotely located project managers.
4.5 An edge analytics agent by IBM named IBM Watson IoT
edge analytics (Fig. 8 )
It is a fully managed, cloud-hosted service with device registration, connec-tivity, control, rapid visualization, and data storage capabilities [10,11] . It has
decisive 
intelligence to draw out valuable information from the streamed
data. It is usually applied to convert the voice signals to textual data foranalytics. It uses real-time analytics to monitor current conditions and respondsaccordingly. It leverages cognitive analytics with structured and unstructured
data to understand situations, reason through options, and learn about changed
conditions. Only the analytics results are stored at the edge gateways, repre-sented as numerical scores of varying emotions in a conversation.
Operations of Edge analytics agent in IBM Watson include: Filter and
reduce data sent to the cloud; Pre-process and transform raw data;
Identification of critical conditio ns to send to the cloud for additional
analytics; and Enable local actions. The capabilities of IBM Watson
Platform include: Complex analytics; A nalytic definition and distribution
to Edge; Longer-term trends; and Pattern detection and machine
learning;
Below are the use cases:
–Define and maintain cloud and edge analytics in a single view within the
IBM
Watson IoT platform on the cloud.
–View the results of Cloud and Edge analytics in a single dashboard withinthe
IBM Watson IoT platform on the cloud.223 Edge data analytics technologies and toolsFig. 8 IBM Watson IoT edge analytics architecture.–Multiple Edge Analytics Agents support many types of devices and can be
connected to a single Cloud instance.
–Edge Analytics capability has expanded from basic value comparisons to
Z-Scores, moving average and data smoothing
4.6 An IoT hub to apply analytics at edge devices named
microsoft azure IoT edge (Fig. 9 )
It transfers data analytics and logic to the edge devices to help the IoT appli-
cation focus on data discernments rather than data management [12,13] .I ti s
made up of three components: IoT Edge modules, IoT edge runtime, and a
cloud-based interface to enhance native data processing capabilities and also
manage edge devices.
Azure IoT Edge is a fully managed service built on Azure IoT Hub. On
deploying the cloud workloads, it helps them run on Internet of Things
(IoT) edge devices through standard containers. Moving few workloads
to the Edge of the network enables the device to spend less time commu-
nicating with the cloud, respond faster to local changes, and be reliable even
in extended offline periods. The feature includes:
i.Certified IoT Edge hardware: Works with either Windows or Linux
platforms with container engines
ii.Runtime: It is free and open-source and provides increased command
and code resilience.
iii.Modules: Provides Docker-compatible containers from Azure services
to the logic at the Edge.
Fig. 9 Microsoft azure IoT edge architecture.225 Edge data analytics technologies and toolsiv.Cloud Interface: Workloads can be managed and deployed remotely
from
the cloud via the Azure IoT hub.
The security features of Azure IoT Edge include:
i.Ensuring that the devices have the proper software and only authorized
edge
devices can communicate with one another.
ii.Integrating with Azure Defender for IoT to provide end-to-end threat
protection
and security posture management
iii.Support any hardware security module to provide strong authenticated
connections
for trustworthy computing.
4.6.1 Benefits
The
locally connected, Edge Compute resources built out with Azure IoT
Edge have several benefits for any IoT solution:
–Lower latency decisions —With domain logic and Azure services able
to run on-premises with an Azure IoT Edge Device, the solution can
make quicker decisions and take actions with lower latency.
–Offline capability —The IoT Edge Device can operate offline in
scenarios during temporary or even longer-term conditions.
–Data synchronization —With offline capabilities of Azure IoT Edge,
the IoT Edge Runtime will automatically save all IoT remote sensingevents on the local device storage and then transmit that data to Azure
IoT Hub when connectivity is restored.
–Lower bandwidth usage —Low
 data and IoT device remote sensing
must be sent to the cloud by utilizing Edge Compute for further
processing. Processing the data locally using Azure IoT Edge allows sum-
marizing the information being communicated to the Azure IoT Edgewhen all events are not necessary to be sent to or stored in the cloud.
4.7 A solution by Oracle for event processing is named Oracle
edge analytics (OEA) (Fig. 10 )
It helps to build applications to sieve, tally, and process real-time events byallowing a combination of CQL and Java codes [14]. It can handle fast and
streaming 
data with lower latencies exhibiting high performance, scalability,
and security. It includes an explorer facilitating user-friendly web toolingfeatures and a highly scalable, high-performance runtime platform. Someof the features include:
i.Programming model: embedded applications are developed as eventprocessing
networks (EPNs)226 N. Jayashree and B. Sathish Babuii.Oracle (CQL): Oracle CQL is supported, except for the functionality
provided by the data cartridges like JDBC drivers.
iii.Data source access: Although JDBC drivers are removed from the
embedded profile, using data sources to connect to the database
through JDBC is supported.
iv.Java data cartridge: The Java data cartridge enhances Oracle CQL to
invoke Java code from Oracle CQL code.
v.RESTful web services: implementation of RESTful web services
through the Jersey JAX-RS support included with Java
Embedded Suite.
vi.Local cache: Local caching service is included
vii. Security: Includes authentication and SSL and utilities such as
policygen.
viii. Configuration: Configuration Wizard silent mode to create and con-
figure domains.
ix.Deployer: You can deploy applications from the command line with
the Deployer tool.
x.Logging: Oracle Event Processing servers save information to log files
for viewing.
xi.Supports high throughput (hundreds of thousands of events per sec-
ond) and low latency processing.
xii. High-speed real-time data capture and analysis optimized for embed-
ded devices.
4.7.1 Benefits
–Real-time situational awareness, faster decisions, and immediate actions
locally ensure customer satisfaction and retention, driving higher
revenues.
Fig. 10 Oracle edge analytics architecture.227 Edge data analytics technologies and tools–Decreased costs and improved compliance with the real-time analysis of
data
patterns, identifying and immediately responding to critical events
and threats
–Cost savings in network bandwidth and processing power. Higher auton-
omy
and resilience in connection loss.
–Improve operational efficiency with on-time insight into the supply
chain,
integrated systems, and processes, facilitating dynamic resource
utilization optimization.
–Provide low cost of ownership and increase productivity with a complete
rapid
development and deployment platform for event-driven solutions
requiring complex event processing.
4.7.2 Use cases
Temperatur
e analysis: Within an Oracle Edge Analytics application
embedded in the thermostat devices, Oracle CQL queries aggregatethe event data and perform a threshold analysis before sending the eventsover the Internet. As a result, the events received have already been iden-tified as worth attention.
Server room monitoring: Separate Oracle Event Processing applica-
tions provide a two-tiered approach. Sensors at the very]\Edge of
the network represent event sources that send data to gateway devicesthat run Java Standard Edition Embedded and Oracle Edge Analytics.
Oracle Edge Analytics applications running on the devices use Oracle
Continuous Query Language (Oracle CQL) to query and filter eventsgenerated by the sensors. Only event data that meet the filtering criteriais sent to back-end servers in the data center. Oracle Event Processingapplications run to send alerts when needed, aggregate and correlate data
to identify consistency issues and produce data to be used in reports on
patterns.Grid modernization: Using Oracle Edge Analytics, deployed edge
devices establish a node-to-node communication, and a master edge
device sends voltage information, power quality, health status to a
centralized management system. This solution dramatically reducesoperational costs.
4.8 A solution to handle complex analytical processes is named
PTC ThingWorx analytics (Fig. 11 )
It is enlightened analytics that is user-friendly and easy-to-understand infor-mation with visualizations. It exhibits automated complex analytics228 N. Jayashree and B. Sathish Babuprocedures to enhance data management, insights, and predictions [15].
It provides tools for facilitating enhanced edge data analytics and arrange
the data collected from the devices. It can also represent the current, possible
future state of the events and actions to be taken if there is a need to change
the outcome. The functionalities include,
i.Automate analytical processes: experience in statistical analysis or
complex mathematics is not necessary since the AI and Machine
Learning technologies used in ThingWorx Analytics automate most
of the complex analytical processes of IIoT applications.
ii.Digital simulations: It simulates the behavior of physical products in
the digital world and incorporates simulation models into the solution.
It also utilizes the knowledge when the product is operating in the
real world.
iii.Predictive modeling and scoring: Using predictive analytic algo-
rithms and machine learning techniques, ThingWorx Analytics ana-
lyzes data from connected devices, finds the patterns in the data, and
generates a prediction model.
iv.Real-time anomaly detection: Helps to observe data from a device,
learn about the typical state, and monitor for data points that fall outside
the expected range. This helps trigger alerts and allows application users
to identify when to take action in IIoT applications.
v.Descriptive analytics: Provides services of standard statistical calcula-
tions and facilitates statistical monitoring versus historical data.
Fig. 11 ThingWorx analytics 8.1 architecture.229 Edge data analytics technologies and toolsFor example, finding Maximum, Mean, Median, Minimum, Mode,
Standard Deviation, Threshold Count, Range Count, and Trend Count.
vi.Transform microservices: The Property Transform microserver
provides on-demand transformation services to derive value fromstreaming data entering ThingWorx instead of Descriptive Analytics’historical limitation.
4.8.1 Benefits–
Operationalize insights, predictions, and recommendations across enter-prise functions with automated IoT data to enhance decision-making.
–Enable complex analytical capabilities for those who are not data expertswith user-friendly interfaces, tools, and applications.
–Enables analysis of historical data and forensic investigation of data after anincident through replay functionality.
–Production-ready deployment enables enterprises to get up and running
quickly—at the Edge, on-premise, or in the cloud.
4.9 Components and use cases
–Analytics server: Automates complex calculations and generates predic-
tions, simulations, and prescriptions. Using the data from the Things and
historical data as a learning source, Analytics Server uses machine learning
to build and validate predictive models without assistance. This dramat-ically reduces or eliminates the need for an expert team in modeling algo-rithms or technologies.
–Anomaly detection: ThingWatcher is a Java API that can be used to
build anomaly detection functionality into your IoT applications.
ThingWatcher can be used at any location, cloud, or Edge. By deployingThingWatcher at the Edge, you can monitor a stream of high-speed datathat would be difficult or impossible to process in the cloud.
–Predictive modeling and scoring: ThingPredictor provides the pre-
dictive scoring capabilities of ThingWorx Analytics. It uses predictionmodels generated by ThingWorx Analytics Server or equivalentPredictive Model Markup Language (PMML) compliant prediction
model generation tools to examine a dataset and predict results for each
record based on similarities to records analyzed during model training.ThingWorx Analytics predictive scoring can be accessed through aREST API Service, Analytics Builder, or Analytics Manager.
–Prescriptive scoring and optimization: ThingOptimizer provides
the prescriptive scoring and optimization capabilities of ThingWorxAnalytics and allows you to expand your analytical processes beyond230 N. Jayashree and B. Sathish Babupredicting outcomes to seeing how modifications might affect results by
automatically identifying influencing factors. ThingOptimizer uses pre-
diction models generated by ThingWorx Analytics Server or equivalent
PMML-compliant prediction model generation tool.
–Analytics builder: Provides a user interface for simple, intuitive inter-
action with data. It converts complex data readings into simple-
to-understand information and simplifies the advanced analytics process,
helping interpret information faster. It also supports data and metadata
uploading, predictive model creation (training and scoring), visualization
of data analytics (profiles, signals), and data filtering.
–Analytics manager: Allows to deploy and execute computational
models from external applications within the ThingWorx platform. It
leverages product-based analysis models developed using PTC and
third-party tools while building solutions on the ThingWorx platform.
4.10 A to-the-edge component to collect, process,
and transmit data is named streaming lite by SAP HANA
(Fig. 12 )
It is a lightweight version of the Smart Data Streaming (SDS) server designed
to run on IoT gateways to process data close to the source [16,17] .I ti sa
free-standing server used to situate streaming projects on remote gateway
devices. It is a to-the-edge component with the capability to collect, sieve,
aggregate, and transmit data. It is a self-contained, independent server that is
not a part of the SAP HANA streaming analytics cluster.
4.10.1 Use cases and benefits
–Equipment sensors can stream information about status and events back to
a central SAP HANA system. These sensors could be monitoring statistics
such as humidity level and temperature or machine status.
Fig. 12 Streaming lite component architecture.231 Edge data analytics technologies and tools–The devices can post messages to the SAP HANA system through an
adapter
or a customized interface such as the Streaming Web Service.
–The messages are consumed by streaming analytics, which applies
filters
to transform or normalize the data, thus capturing high-value
information in the SAP HANA database. For example, suppose youhave sensors that track humidity levels and temperature data, as data
from these sensors flows in, you can isolate the temperature and humi-
dity values and join them to a table of values from the SAP HANAdatabase.
–The streaming analytics model continuously computes a set of summary
information
streamed to a live operational dashboard. For example, look
at the current data (in the stream) and compare it with time-stamped
entries in the SAP HANA table to compare current values to values 5,10, or 15min old. You could use this data to identify trends: Is your
equipment heating up steadily or too quickly? What impact does humid-
ity play on machine performance?
–The streaming analytics data model also actively monitors the incomingdata
for conditions that warrant immediate action or attention and gen-
erates alerts and notifications (by sending an email or text message) whenthose scenarios are detected.
5. Working principles and feature comparisons
The application of edge analytics aims to adopt IoT by various indus-
tries as a trend in service provision [18]. Its ability to extract tangible and
measurable
metrics from the IoT devices results in faster data provided with
the help of analytics tools and the application of machine learning algorithmsfor real-time intelligent factors.
Edge analytics also provides the advantage of distributed data aggregation
and collects the data across all the industry sectors. They generate criticaldecentralized data at every instance and require analysis at the Edge withlow-bandwidth requirements on large-scale industry data. The edge analyt-
ics extends towards a variety of application like logistics, retail services,
manufacturing industries, healthcare industries which has a stream of datagenerated and needs immediate processing.
Analytics at the cloud can store and use all the previous data for the
streaming and analyze vast amounts of data from the devices. The industriesfelt the shortage of cloud analytics capabilities for supporting modern IoT232 N. Jayashree and B. Sathish Babuapplications. The analytics on the cloud, although is very fast, it is not an
instant response. There is a need to avoid network latency, cost inefficien-
cies, and connectivity.
Transferring most computation work to the Edge resulted in the best
communication costs and immediate responses. It was observed that themachines operated more effectively after activating the edge processing
mode and edge analytics became a critical factor in the strategy of
IoT-based industries. Instead of transferring data to the cloud for processing,it was now considered wise to apply processing tools at the Edge and shift theanalytics to the devices that generate the data. Currently, edge analytics is in
great demand in various IoT-based applications that depend on instant or
real-time responses.
6. Some of the other use cases of edge analytics [19,20]
Retail customer behavior analysis: Retailer data can be collected
from a range of sensors, including parking lot sensors, shopping cart tags,and store cameras. By applying analytics to these data, the retailers canprovide personalized solutions for everyone with the help of behavioral
targeting.
Remote monitoring and maintenance for various industries:
Industries
may need an immediate response in case of any machine fail-
ures and for maintenance purposes. Organizations can identify signs offailure faster when server-based analytics is avoided and take action
before any bottleneck can arise within the system. Edge analytics is also
used in improving efficiencies by adjusting the operating parameters.
Intelligent Surveillance: It
 is a real-time intruder detection edge
service for their security. With real-time images from the security
camera, edge analytics immediately detects and tracks all suspicious
activities.
Real-time human detection is a process similar to object detection
taking raw images from security cameras and putting them in the camera
buffer for processing in the detector & tracker. The identified human
figures from detector&trcker are sent to the streamer buffer.Therefore, the whole human detection process can be divided into threethreads: camera, detector&tracker, and streamer.
Smart cities: The
 basic building blocks are the IoT devices embedded
into the infrastructure. They help to monitor and collect information233 Edge data analytics technologies and toolsabout the behaviors of the components in the infrastructure. As real-time
decision-making is necessary for intelligent cities, edge analytics finds
application for fast actuators’ responses.
Healthcare: Hospitals deploy a wide range of devices that create a heavy
load on a cloud for processing and responding faster. Edge analytics isproven to be an optimal method that reduces the overload and is a
cost-effective solution considering storage security and minimal connec-
tivity issues.
References
[1]R. Donitha, Edge Data Analytics: What, Why, Who, When, Where, How, Digital
Transformation
Pro, 2017.
[2]B. Johnson, Analytics at the Edge: Three Benefits of Edge Analytics, Swim Continuum,
2017.
[3]R. Izadi, Video Analytics: Edge vs. Server-Based, DIDARC Trading Co. Ltd
Publications,
2017.
[4]E.A. Israel, O.F. Sammuel, C.M. Richard, J.J. Jason, Intelligent energy optimization for
advanced
IoT analytics edge computing on wireless sensor networks, International
Journal of Distributed Sensor Networks (2020).
[5]AWS IoT Greengrass, Developer Guide, Version 2, AWS Architecture Blog, 2021.
[6]Cisco SmartAdvisor using IP Explorer, IP Explorer-Network Discovery, Visualization,NetVisor,
2018.
[7]D. Sweenor, Dell Statistica Stands Out as Analytics Market Heats Up, DellTechnologies,
2016.
[8]HPE Edgeline Converged Edge Systems and OT Link, Hewlette Packard EnterpriseProducts,
2021.
[9]J. Lang, Connecting Workers with Data and Insights at the Edge, IDC TechnologySpotlight,
2020.
[10] An Introduction to IBM Watson IoT Edge Analytics, IBM Internet of Things, 2017.
[11] Watson IoT Platform: A Fully Managed, Cloud-Hosted Service With Capabilities for
Device
Registration, Connectivity, Control, Rapid Visualization and Data Storage,
IBM Technologies Products, 2017.
[12] Azure IoT Edge: Cloud Intelligence Deployed Locally on Edge Devices, Microsoft
Azure
Services, 2021.
[13] Azure IoT, Quickly Turn Your Vision Into Reality With Secure, Scalable, and OpenEdge-To-Cloud
Internet of things solutions, Microsoft Azure Services, 2021.
[14] Oracle Edge Analytics, Oracle Data Sheets, Java Embedded Products, 2015.
[15] Thingworx Analytics, Delivering Powerful, Operationalized Analytics to SolutionsBuilt
on the Thingworx Platform, ThingWorx Analytics Product Brief, 2017.
[16] R. Waywell, Learning How to Use Streaming Lite With HANA Smart Data Streaming,SAP
HANA Community Blogs, 2016.
[17] SAP HANA Streaming Analytics Use Cases: Master Guide, SAP HANA CommunityBlogs,
2016.
[18] B. Posey, Edge Analytics: Integrated Analytics Gives Users Operational IntelligenceEdge,
2020.
[19] D.E. Sweenor, Six IoT Use Cases for Edge Analytics: How to Acquire ManufacturingSuperpowers
in the Internet of Things, TIBC Software Inc, 2017.
[20] Data Analytics at the Edge, Pathfinder Report, SAS Dell Technologies, 2019.234 N. Jayashree and B. Sathish BabuAbout the authors
Jayashree N received her bachelors and mas-
ter’s degree in Computer Science and
Engineering from Visvesvaraya Technological
University. She had over 10 years of teaching
experience, and presently working as Assistant
Professor in the Department of Information
Science and Engineering at Cambridge
Institute of Technology, Bangalore. Her
research interests include Information and
Network Security, Artificial Intelligence and
Machine Learning, and privacy protection in
selected application domains. She has
Co-Authored in research papers based on security, privacy and queuing models
in IoT and cloud computing.
Dr. B. Sathish Babu received his bachelor’s
and master’s degree in Computer Science and
Engineering from Bangalore University. He
obtained his Ph.D. in Protocol Engineering
Technology Unit, Department of ECE at
Indian Institute of Science, Bangalore, in
2009. He has over 30 years of teaching expe-
rience and over 15 years of research experi-
ence. He is presently working as a Professor
and HOD of the Department of Artificial
Intelligence and Machine learning at RVCE
Bengaluru. His research interests include;
Information and Network Security, Cognitive computing applications
for Network controls, Soft computing solutions for Cloud Computing
Scheduling and Virtualization, Data Science, Opportunistic Networks rou-
ting and security, Building Machine learning and Deep learning models in
selected application domains, Quantum Computing, and others. He has
published over 100 international journal/conference papers in his area of
research, with many publications featured in Scopus-Quartile journals and
Web of science journals. Dr. Babu co-authored books on Mobile235 Edge data analytics technologies and toolsand Wireless Network Security published by Tata McGrawHill, and
Communication Protocol Engineering published by PHI-India. These books
are widely used as text and references in more than 15 Indian universities. Healso co-authored many chapters in the edited books published by the CRCPress and IGI Global International. He has worked as a co-investigator inthe joint research projects with the Indian Institute of Science, Imperial
college-London, and Florida International University-USA.236 N. Jayashree and B. Sathish BabuCHAPTER EIGHT
Edge platforms, frameworks
and applications
Kavita Sainiaand Pethuru Rajb,c
aSchool of Computing Science and Engineering (SCSE), Galgotias University, Delhi, Uttar Pradesh, India
bSite Reliability Engineering (SRE) Division, Reliance Jio Platforms Ltd. (JPL), Bangalore, India
cReliance Jio Cloud Services (JCS), Bangalore, India
Contents
1.Introduction to cloud computing 238
2.Cloud computing to edge computing 238
3.Edge computing: A brief overview 239
4.Essential of edge computing 240
5.Advantages of edge computing 240
5.1 Latency reduction 241
5.2 Safer data processing 241
5.3 Inexpensive scalability 241
5.4 Simple expansions to new markets 242
5.5 Consistent user experience 242
5.6 Speed 242
5.7 Edge computing technologies 242
5.8 Cloudlets: An overview 243
6.Significance of cloudlets 243
6.1 MEC benefits 246
6.2 FOG computing 247
6.3 Benefits 248
6.4 Edge computing applications 250
6.5 Edge computing and future 253
6.6 Shortcomings of edge computing 254
7.Conclusion 255
References 255
About the authors 257
Abstract
Edge computing, a rapidly growing technology, satisfies the business needs of most of
the smart business cultures today. Smart agriculture, smart cities, smart manufacturingor any other smart business can be benefited from Edge computing. The chapter dis-cuss how to about the cloud computing to edge computing. Various edge computingtechnologies and applications are also discussed in detail.
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0052375G Communications, Remote Monitoring, Healthcare and other such applications
require tremendous accuracy, high latency I less cost. Augmented and Virtual reality
applications or gaming are just few applications. There are many more to discuss about.The chapter discuss how edge computing is helpful for all such applications.
Unlike cloud computing, edge computing enables data analysis, its processing, and
transfer at the edge of the network. Basically data is analyzed locally where it is stored.The analysis is done in real time without latency and allows for quicker data processingand content delivery.
1. Introduction to cloud computing
Cloud computing by which remote servers hosted on the Internet
store and process data, rather than local servers or personal computers [1].
It
is ready to move to the next level, i.e., “ Edge Computing .” Icloud,
onedrive, Google are the few examples of cloud computing [2].
As
cloud computing is the “On-Demand” availability of the computer
system resources, especially data storage and computing power, without
direct active management by the user [3].
To
Leverage 5G wireless technology and artificial Intelligence to enable
faster response times, lower latency (ability to process very high volumes of
data with minimal delay), and simplified maintenance in computing. Cloud
provider are responsible to manage, restore and backup. In short data centers
are being managed by cloud service providers [4,5]. They are also providing
high-end
computing power, software and so on.
This is where Edge Computing comes in, which many see as an exten-
sion to the cloud computing, but which is in fact, different in several basicways [6].
2. Cloud computing to edge computing
The basic difference between cloud computing and edge computing lies in the
place where the data processing takes place . At the moment, the existing Internet
of Things (IoT) system performs all of their computations in the cloud
using data centers. Experts believe the true potential of edge computing will
become apparent when 5G networking go mainstream in a year from now.Edge offers an extra added scalability which will be needed for locallyapplicable responsibilities in the environment with a huge quantity of data
producers and customers [7,8]. Multifaceted, extensive and data determined
responsibilities
which aren’t time threating would be benefited greatly from238 Kavita Saini and Pethuru Rajthe richness of ascendable sources in clouds [9,10] . Edge can make the com-
munications
more consistent, in the manner that it could provide an option
if the network link to cloud breaks. It can be an interesting especially for
crisis management scene where edge provides optional substructure to pre-serve critical responsibilities thriving. Edge sources could generally be acces-sible within one hop from the wireless gateways which operators are linked
with. Preferably, edge system can identify and back handler flexibility, e.g.,
by moving its information and computing to the subsequent nearby posi-tions. User will be able to enjoy consistent connectivity without evenrealizing it [7].
3. Edge computing: A brief overview
Edge Computing enables data to be analyzed, processed and trans-
ferred at the edge of a network. The idea is to analyze data locally, closerto where it is stored, in real-time without latency, rather that send it far away
to a centralized data center [4,10] . Time-sensitive data is processed using
edge 
computing, while data that is not time-sensitive is processed using
cloud computing which is the main advantage of edge computing over
cloud computing. The prime aid edge offers consist lower latency, higher
bandwidth [11], device processing and data unburden as well as reliable
computations
and storing. Concludingly, 5G requires edge to drive need
for its facilities [5].
In
edge computing some content will be offload before using which will
help in accessing data without any delay. In short in Edge computing there is
a use of 5G wireless technology and AI. Edge could be considers as an addi-tional layer between the CC and users, applications and devices (such IoTdevices).
With the rapid advancement in IoT and Edge Computing, the tradi-
tional cloud computing was facing communication latency and networkbandwidth as a biggest challenge. A new driving technology coming intorole has now moved the functionality of centralized cloud computing tothe network edge node for addressing the difficulties in traditional system.
Several edge computing systems have emerged from various backgrounds in
order to reduce latency, increase computational capabilities and handle hugemachine connectivity [12]. This study provides an in-depth look at three
common
edge computing technologies: mobile edge computing (MEC),
cloudlets, and fog computing [6].239 Edge platforms, frameworks and applications4. Essential of edge computing
Pushes the intelligence, processing power, and Communication
Capabilities of an edge gateway or applicant directly into devices [13].T h e
i
dea is to analyze data locally, closer to where it is stored, in real-time without
latency, rather than send it far away to a centralized data center. So whether
you are streaming a video on Netflix or accessing a library of video games in
the cloud, edge computing allows for quicker data processing and content
delivery [12].
As
of now all the applications using IoT devices store and compute the
data on cloud data center. With edge computing it is possible to perform all
the analysis and computation at the edge on cloud and saves lots of time. Not
just saving time edge helps in computation in real time. Playing onlinegames, live video streaming are the examples where computation shouldbe finished in fraction on second. This is possible with edge computing
[14,15] . This is how edge computing different from cloud computing.
Only
important data is sent over the network instead of whole data. In
this way edge computing is reduces the amount of data traversal over thenetwork. Transferring data from cloud to devices and vice versa take time,also known as Internet latency. It can be achieved using AI, 5G wireless tech
and IoT devices. It will help in reducing amount of data stored over cloud as
some data some data could be stored only on edge [8].
Edge
Computing simplifier this C ommunication chain and reduces
Potential Print of Failure. In edge Co mputing, physical assets like pumps,
motor, and generators are again physica lly wired into a control system, but this
system in controlled by an Edge Programmable Industrial Controller, or EPIC.Edge Computing saves time and money by Streamline IoT Communication,reducing system and Network Archit ecture Complexity and decreasing
the number of potential failure in a n IoT application Reducing system
Architecture Complexity is key t o the Success of IoT applications [7,16] .
5. Advantages of edge computing
Edge computing has many important features unnoticed in preceding
network generations. These consist of huge data generation. Edge directs thecomputing information, application, and facilities aside from the Cloud
server to the edge of networks [10]. Content suppliers and app creators
can 
utilize an edge computational system by providing handlers the facilities
nearer to themselves. Edge computation is categorized as employing higher240 Kavita Saini and Pethuru Rajbandwidths, lower latencies, and real time admittance to the system data
which could be utilized by numerous appliances [13,17,18] . Edge is favored
to
supply the wireless communication needs of next-gen technologies, like
virtual reality & augmented reality, that are collaborating in behavior [14].
Edge
technology enables the computing to be executed at the networking
edges.
5.1 Latency reduction
Cloud computing can’t adequately support the volume of data beingprocessed every second. Having spoken about latency within the cloud
computing world, there is a lot that cloud computing does not provide tocloud-based applications [5]. Given the amount of stored data within the
cloud,
there are two problems that transpire during the processing
stage—latency in processing and high number of wasted resources. Theseissues exist especially in decentralized data centers, mobile edge nodes,and cloudlets [17].
By
reducing latency, edge computing enhances network performance.
The information does not go travel as far as it would in a traditional cloudarchitecture rather devices process data natively or at a local edge center. Ithas been noticed earlier if source is sending any mail at destination at sameworkplace also, there could be some delay in standard network. This delaydoes not exist if the procedure occurs at the edge and the company’s router
handling office emails [19].
5.2 Safer data processing
DDoS (Distributed Denial of Service) assaults and power outages are
common in cloud environments. Systems are less prone to interruptionand unavailability because edge computing spreads processing and storage.There is no single point of failure in the setup [15].
F
urthermore, because many procedures take place locally, cyber attackers
are unable to prevent data from being sent. Even if a data breach occurs on onemachine, the attacker can only access location data [20].
5.3 Inexpensive scalability
Edge computing empowers a company to increase its capacity with a com-bination of IoT devices and peripheral servers. Adding additional resourcesdoes not require investing in a more expensive private data center to build,
maintain, and expand [5]. Instead, the company can set up regional edge
servers
to expand the network quickly and inexpensively.241 Edge platforms, frameworks and applicationsEdge computing also reduces growth costs as each new device does not
add additional bandwidth requirements across the network.
5.4 Simple expansions to new markets
The company can partner with the local edge data center to quickly expand
and explore new markets. The expansion does not require expensive new
infrastructure. Instead, the company only sets up end-to-end devices and
starts serving customers without delay. If the market seems unwanted, theextraction process is quick and inexpensive.
This benefit is important for industries that need rapid expansion in areas
with limited connectivity.
5.5 Consistent user experience
As the edge servers work closer to end users, the problem of a remotenetwork is less likely to affect customers. Even if the local facility is discon-nected, the peripheral devices may continue to operate due to their ability to
handle important traditional tasks. The system can also extract the data route
in other ways to ensure that users retain access to the services [15].
5.6 Speed
Speed is absolutely vital to any company’s core business. Take the financial
sector’s reliance upon high -frequency trading algorithms, for example. A slow-
down of mere milliseconds in their trading algorithms can result in expensive
consequences. In the healthcare industry, where the stakes are much higher,losing a fraction of a second can be a matter of life or death [1,20] .
5.7 Edge computing technologies
There has been a substantial growth in connected smart devices andIoT nodes, which resulted into increased data generation at these nodes.Handling this massive amount of raw data is a crucial challenge becauseof limited computational and energy resources [2,4,9,19] . Due to the
requirement
of large processing and storage capacity, the existing cloud
computing platform can easily handle the tremendous heaps of data gener-ated by IoT devices. But this is not conceptual for dissipated IoT systems orthe On-time operation of deliciated latency IoT applications as they require
centralized manner of operation and are concerned with the associated delay
also. Edge computing can minimize end-to-end latency, save bandwidth inbacklog links, and mitigate the computational pressures on cloud-servers, by242 Kavita Saini and Pethuru Rajproviding cloud-like computing environment, storage, and communication
facilities at the network edge [ 21].
T
hepresenceof“Edge devices” reducesthecomputeburdenatdata centers
by handling some of the requests directed to the cloud locally, without the need
forcloudinvolvement.Asaresult,thedelayinresolvingrequestsisreduced,anda subset of requests can be handled in real time. Because of their widespread
availability and geographical distribution, edge devices also aid mobility.
There are various technologies that can be used to develop edge computing,which is based on the idea that it can expand the settings of IoT usage bycomplementing the cloud [ 22]. Let’s take a close look at some IoT based edge
co
mputing technologies, including cloudlets, mobile edge computing (MEC),
fog computing, and a novel idea called the Cloud of Things [11].
These
technologies are also predicted to be important in the develop-
ment of edge computing platforms.
5.8 Cloudlets: An overview
A cloudlet is basically a regional cloud that can bring far-flung cloud servicesnearer to the user. Cloudlets are small-scale, mobility-enhanced cloud datacenters that sit at the network’s edge. The cloudlet’s primary goal is to sup-port furious resource and interactive mobile applications by deliveringstrong computing resources to mobile devices with reduced latency. A wire-
less local area network with single hop at comparatively higher speed, allows
User Equipments (UEs) to connect to the computing resources in the neigh-boring cloudlet [15]. To ensure crisp reaction time, cloudlets constitute the
intermediate
tier in a 3-tier hierarchical architecture containing Edge device
layer, cloudlet layer, and cloud layer.
For security reasons, cloudlet is bounded in a tamper resistant box for
safeguarding safety in unregulated regions [9]. On the inside, cloudlets con-
sist
of a group of source rich multicore computer with high-speed internet
connection and higher bandwidth wireless LAN for the use by closer mobilegadgets ( Fig. 1 ).
The 
Cloudlet is an architecture model that enables cloud computing at
the mobile network’s edge. Low latency and high bandwidth characterize
this environment, forming a fresh ecosystem in which network service
provider can open their network edge for third party user, allowing themto quickly and flexibly install creative and innovative services.
6. Significance of cloudlets
The purpose of a cloudlet is to improve the response time of mobile
apps by employing low-latency, high-bandwidth wireless communication243 Edge platforms, frameworks and applicationsand physically bringing cloud computing resources, such as virtual com-
puters, closer to the mobile devices that access them [23].
Cloudlet vs Cloud: Cloudlets differ from Cloud in many cases:
1.Clouds are normally monitored by their service providers whereas
cloudlet is self-managed.
2.Provider locate the cloud in purpose specific areas at his premises, on the
other hand cloudlet are located in business premises in the form of a data
center in a box.
3.Cloud uses internet bandwidth/latency whereas cloudlet uses the
latency/ bandwidth of a local area network.
4.Since cloud has a centralized ownership, cloudlet is owned by the local
business/organization.
5.Cloud can accommodate hundreds and thousands of users at a time but
cloudlet can accommodate only few users.
Benefits : Cloudlet resides as a middle layer in its three tier view, offering
various benefits in the edge infrastructure
1.Simple to set up: The fact that cloudlet servers are exiled makes mainte-
nance easier; adding or replacing of a cloudlet just takes few minutes for
setup and simple steps of configuration.
2.Enhancement of security: The cloudlet’s proximity to mobile nodes makes
the architecture more resistant to DoS attacks (secondary variants). It can
also help to avoid data leakage from traffic analysis by limiting the range
of end-to-end connection, which prohibits snoopers from accessing
traffic data from afar.
3.Resilience: Even with shaky connectivity to a remote cloud provider, a
cloudlet collection can provide dependable cloud computing services.
Fig. 1 Three tier view of cloud data centers at edge network.244 Kavita Saini and Pethuru RajMobile Edge Computing (MEC): Multi-access Edge Computing, also known
as Mobile Edge Computing (MEC), is a network design that allows compu-
tational and storage resources to be placed within the Radio Access Network(RAN). The MEC aids in improving network efficiency and content deliveryto end-users. This device can do this by adapting over the load available on theradio link, resulting into increase in network efficiency and reducing the
requirement for long-distance backlogs. MEC offers mobile and cloud com-
putational abilities among the access system, and goals to unite the telco and ITat the mobile edge network [19]. As in near locations to users, MEC can pro-
vi
de a service environment with ultra-low latencies, high bandwidth, and
direct entree to real time system data [23,24] . Mobile Edge Computing is
t
he principal technology among the next gen system technologies.
As per European Telecommunications Standards Institute (ETSI), mobile
edge computing is defined as “Mobile Edge Computing offers an IT service
environment and cloud computing capabilities at the mobile network’s edge,
within the Radio Access Network (RAN), and close to mobile customers.”
Listed below are few characteristics of MEC:
Mobile edge compuƟng works in silos, which improves performance in a
machine-to-machine seƫng. MEC's ability to isolate itself from other
networksmakesitmoresecured.On-
Premises
Because mobile edge compuƟng is placed at a nearby area, it has an
advantage in analysing and materialising huge data. It's also useful for
gadgets that require a lot of processing power, such as AR (augmented
reality)andvideoanalyƟcs.
Mobile edge compuƟng services provides user devices in close vicinity,
separaƟng network data transfer from the core network. As a result, the
user experience is considered to be of excellent quality, with extremely
lowerlatencyandhigherbandwidth.Proximity
Reduced
latency
For informaƟon sharing, edge dispersed devices use low-level
communicaƟon. MEC gets data from edge devices in the local access
networkinordertolocatedevices.LocaƟon
awareness
ByincorporaƟngMECintotheirbusiness model,applicaƟonsthatprovidin g
network informaƟon and services of On-Ɵme network data can deﬁnitely
beneﬁt enterprises and events.TheseappscanesƟmate thecongesƟon of
theradionodesandnetworkbandwidthtypicallybasedonRANhavingreal-
Ɵme informaƟon, which will enable them make smart decisions for
improvedcustomerdeliveryinthefuture.Network
context
informaƟon245 Edge platforms, frameworks and applicationsMEC is a layer that sits between mobile devices and the cloud. As a result,
the infrastructure is organized into three layers: cloud layer, MEC layer, and
mobile device layer. Mobile edge computing, for the most part, works in
tandem with cloud computing for supporting and improving the perfor-
mance of end devices ( Fig. 2 ).
6.1 MEC benefits
Listed below are some advantages of Mobile Edge Computing (MEC) that
are proving to be beneficial to both Mobile Network Operators (MNOs)
and application service providers.
Mobile network operators might provide third-party suppliers with
real-time access, allowing them to deploy their applications and services
in a more flexible and agile manner. These services could make money
by charging for things like storage, bandwidth, and other IT resources.
MEC-enabled infrastructure-as-a-service (IaaS) platforms at the net-
work edge node could benefit application service providers by allowing
them to scale their services while maintaining higher bandwidth and
reduced latency. ASPs may also gain real-time access to radio pursuit that
is likely to develop.
Fig. 2 General architecture: three tier view of MEC.246 Kavita Saini and Pethuru RajEnd users may perform faster computation by offloading the MES
servers.
Driving business model evolution—The capacity to collect or process
data
from any facility’s base leads to the development of a wide range
of business models. Evolution of the Industry 4.0 is a best example forthis. In this case, mobile edge computing can act as an enabler for devel-
oping a proactive maintenance business model to extend the life of any
resources.
A new way to run your business more efficiently—Mobile edge com-puting
will allow SMEs to grow their marketing efforts, reach more
customers, and improve their services at considerably lower costs thanthe public cloud.
6.2 FOG computing
Fog computing is a framework hierarchical architecture defined by theOpenFog Consortium as a horizontal architecture that divides processing,
storage, control, and connectivity services and goods anywhere along
the spectrum from the fog to things [5,19] . Fog computing differs from edge
computing 
in that it includes tools for across networks and between edge
devices for spreading, coordinating, controlling, and protecting resources
and services [1].
It
is also known as Fog networking or fogging. It is a decentralized infra-
structure where application resides between data stores and the cloud. It is an
architecture which uses edge device for computation, storage and commu-nication [19]. This edge device is the device that controls the data flow at the
boundaries 
of any two networks, example: router, switch, IAD, gateways,
hub, multiplexer or bridge. It is actually a mediator between hardwareand remote server. Fog is a distributed network environment and is veryclose to cloud computing and IoT device. It has a high security network.
Instead of sending selected data to the cloud for processing, fog actually pro-
cess the data which saves the network bandwidth and reduces latencyrequirement which in turn helps in fast decision making capability ( Fig. 3 ).
Definition:
Some researchers have defined Fog computing as:
“Fog computing is a highly virtualized platform that provides compute, storage,
and networking services between IoT devices and traditional cloud computingdata centers, typically, but not exclusively located at the edge of network. ”
“Fog computing is a scenario where a huge number of heterogeneous (wirelessand sometimes autonomous) ubiquitous and decentralised devices communicateand potentially cooperate among them and with the network to perform storage247 Edge platforms, frameworks and applicationsand processing tasks without the intervention of third parties. These tasks can be
for supporting basic network functions or new services and applications that run in
a sandboxed environment. Users leasing part of their devices to host these services
get incentives for doing so. ”
“The term Fog computing or Edge Computing means that rather than hosting and
working from a centralized cloud, Fog systems operate on network ends. It is a term
for placing some processes and resources at the edge of the cloud, instead of esta-
blishing channels for cloud storage and utilization. ”
Considering these definitions we can define Fog computing as: “A distrib-
uted computing platform in which end or edge devices perform the majority
of the work. By existing in between users and the cloud, it is also associated
with the cloud for non-latency-aware processing and long-term storage of
important data.”
6.3 Benefits
6.3.1 Confidentiality
Fog computing can be used to keep the quantity of data shared to a mini-
mum. Instead of sending sensitive personal information to a centralized
Fig. 3 Three tier view of Fog infrastructure.248 Kavita Saini and Pethuru Rajcloud platform, any confidential material can be reviewed regionally. In this
way, the IT staff will be able to monitor and control the device. Any portion
of data that has to be analyzed can also be sent to the cloud [6].
6.3.2
Efficiency
Clients can employ fog procedures to make the machine function effectivelythey want it to. Publishers may easily design these fog applications with theright combination of tools. They can use it whenever they want once the jobgets completed [13].
6.3.3
Safety and security
Fog computing supports multiple devices to be connected to the same net-work. As a result, rather from being consolidated, processes in a complex
decentralized system take place at multiple end terminals. This makes it eas-
ier to identify potential threats before they have a large-scale impact on thenetwork [5].
6.3.4 
Bandwidth
The cost of bandwidth required for data transmission depends on the avail-ability of resources and it can be costly [7]. The throughput requirements are
greatly 
decreased because the chosen processing is done locally instead of
being transferred to the cloud. This bandwidth reduction will be particularlysignificant as the number of Internet—connected devices grows. When thenumber of IoT devices grows, this bandwidth savings will be especially use-ful. Fog computing allows multiple devices to share a common network. As
a result, instead of being centralized, processes in a complex distributed sys-
tem occur at multiple end points. This makes it easier to identify potentialthreats before they spread throughout the network.
6.3.5 Latency
Another advantage of processing data locally is the reduction in latency. Thedata can be analyzed or processed at the data source that is expected to begeographically nearer to the user. This can result in instantaneous responses,
which is highly valuable for services that require quick reactions [4].
S
everal edge computing systems have emerged from various backgrounds
in order to reduce latency, increase computational capabilities and handle
huge machine connectivity. This study provides an in-depth look at three
common edge computing technologies: mobile edge computing (MEC),
cloudlets, and fog computing.249 Edge platforms, frameworks and applications6.4 Edge computing applications
There are various applications of Edge computing, some are listed below
(Fig. 4 ):
6.4.1 Smart systems
It comprises of IoT devices used as home essentials like smart TVs, smart
phones, smart lights, CCTV cameras etc. Smart systems also include devices
used for monitoring air quality index, weather conditions, traffic manage-
ment, smart gardening and others which will help in making smart city.
All those systems used in healthcare like fitness tracker, apps for pandemic
diseases (Arogya setu) are also a part of smart systems [25].
6.4.2 Video streaming
Various reports have highlighted the statistics that video streaming on inter-
net will be capturing almost 83% of all internet traffic by 2022. This video
streaming requires good bandwidth resources and cache requirement which
Fig. 4 Edge computing applications.250 Kavita Saini and Pethuru Rajdirectly affect the cost parameter and the video quality. Edge computing is
undoubtedly providing a reasonable mechanism to cache the local video.
These devices are embedded with filtering capability, to figure out usefuldata[16].
6.4.3
Remote monitoring and predictive analysis
With the help of edge computing enabled IoT devices manufactures arenow able to remotely monitor their assets more carefully and at early stagebefore they create any disaster. Bringing the processing capabilities closer to
the device, has helped in predicting the real-time health status of the
machines [20]. This may beneficial in analyzing and detecting the changes
required
in production line before any failure occurs.
6.4.4 Gaming-as-a-serviceIt is a kind of online video game that runs on a cloud server and directlystream on the player’s device [26]. Xbox, a Microsoft soft product is a cloud
gaming
service, where game itself is hosted and processed in cloud data cen-
ters but feed directly to the gamers device through real time streaming. Edgeand 5G connectivity integrations provides the bandwidths required to aidhigh quality, multi-player gaming experiences. Edge is not only restrictedto the gaming but it is also the future of mobile applications. It significantly
contributes into the future of industries adopting hybrid multi-clouds &
edge structures as it plays a crucial role in the digital structures. Mobileexperience will be more real-time, more interactive, and rich in handling/operating just because it will be because of edge computing [23].
6.4.5
5G communications
For lower latency and higher throughput 5G communication is required.5G is considered as a next gen. Cellular network. With edge computing
it is possible to bring the cloud computing capabilities closer to the end user
or to the edge of network. Cloud computing where there is high latency,low throughput and less security, edge computing combat with all men-tioned problem and improves the user experience.
5G and edge computing technologies are capable to improve the appli-
cation’s performance significantly. Specially where data is to be processed inreal time, 5G increases speeds 10 times in comparison to 4G and edge com-puting reduces the latency by bringing the computational capabilities to the
edge of the network. This deadly combination of 5G and edge computing
improves the users real time application’s experience.251 Edge platforms, frameworks and applications6.4.6 5G smart health care
5G smart health care, on the other hand, has far more grave safety and seclu-
sion apprehension than traditional healthcare services. Traditional medicalservices are altered by 5G medical applications, which extend them fromthe healthcare center to an online examine mode that involves many users,data systems, and medical devices, is embedded by posing serious security,
massive medical data transmission, and privacy challenges. The quality of
medical services and the routine operations of medical facilities will beseverely harmed by security weaknesses in terminals, networks, and systems.
Science and technology in healthcare area is a considerable study part for
countless scholars. Similar to various manufacturing, health care subdivisioncan also be assisted from edge, for example heart patients suffering from heartattack [14]. Health care application are generally regarded time sensitive
applications
in Internet of Things. At first, cloud computing was utilized
for health care implementations but was not great success due to latenciesproblems. Introduction of edge solved these problems & made cloud accu-rate for health care IoT uses. Into the smart clinics edge is currently used innumerous ways, like in wireless health regulation the data received from the
affected ones directed to the physicians which eases them to tackle the emer-
gencies [1]. Meanwhile, there are no 5G security standards or 5G medical
engineering
safety standards in place, and it’s unclear to concern security
protection for 5G health check appliances.
6.4.7 Security monitoring
Various IoT devices like Intrusion detection systems (IDS) are deployed oneither ends of communicators for an efficient network monitoring. It helps
in security and privacy related issues while transmitting data either from one
network to another or from data centers to edge device [19].
6.4.8
AR and VR
Augmented Reality (AR) and Virtual Reality (VR) is one of the rapidlygrowing industry. Reason is both helps in reducing the cost and time to per-form any task, for example remotely managed any operation [2]. By remov-
ing
the geographical barriers any operation and be performed by the experts
sitting on same or different places.
But presently the biggest problem with deployment of such applications
is comprehensive deployment of mobile AR is that current devices. Still
there is a need to improve the computational power, data communication
and graphical performance.252 Kavita Saini and Pethuru RajWith the edge computing all computational task could be taken care by
the edge of network and will help in improved performance in all aspects.
Challenges that can now be overcome by 5G and edge computing by per-
forming computation at the network edge and not need to send and receive
data frequently to/from cloud and will support more and more real-life
applications [19](Fig. 5 ).
6.5 Edge computing and future
Edge computing is currently in its early stages and has a great capability to
pave the pathway for more effective dispersed computations [19]. The prime
goals of edge computing are to give real time interactions, local processing,
higher data rates, and higher availabilities. Edge advances network actions to
assist and locations varied scenes, like distant surgeries [17]. The flaws and
lacunas of cloud and fog are completed by the edge computing to a greater
amount which has consequence in low response time, lower latency, lower
bandwidth price, lesser energy usage and superior and higher data privacy.
The significance and need of edge computing is acceptable with the assis-
tance of few of the upcoming sectors and appliances that are extensively
utilized in recent times like smart city, smart houses, online marketing, etc.
Fig. 5 Applications of edge computing.253 Edge platforms, frameworks and applicationsSo, to offer an advanced and well-organized facility to IoT smart appli-
cations, the idea of edge computing is able to execute all the functions by
going beyond the cloud capabilities. Edge visions to get facilities and effica-cies of Clouds near to the handler for safeguarding fast processing ofdata-concentrated apps. Video analytics, online shopping, smart cities, smarthomes, digital health care, mutual edge and high privacy apps are few of the
extensively utilized famous areas & applications that uses the idea of edge.
Transferring data processes through the edge networks will be helpful
to companies so that they can take benefit of the increasing quantities of
IoT gadgets, advance network speeds, and increase customer experiences.
Scalable behavior of the edge also makes it an appropriate option for the
rapidly developing, sprightly industries, particularly to those who uses datacenters and utilizes cloud substructures [1]. Edge gives an unprecedented
benefit
of flexibility and reliability which will increase of each product com-
bined or integrated with it resulting into customer satisfaction. Edge com-puting provides varied benefits over conventional type of network structuresand will definitely play a significant part in near future. With the advance-ment in internet associated gadgets coming in markets everyday advanced
administrations have just scratched the uppermost part of what’s possible
with Edge.
6.6 Shortcomings of edge computing
So far the chapter talks about edge computing and benefits of its use over thecloud computing. Every technology be it cloud or edge computing havesome drawbacks and give scope of improvements in the technologies. It alsohelp in discovering the scope and hope for new technologies [4].
Like
other technologies, Edge computing also has some Shortcomings.
This section will elaborate the same. With Edge devices, (used for edgecomputing) open the door for the various attacks [10]. With different
attacks, 
an attacker can inject malicious activity. Edge devices can be infect
with these attacks which may lead to A software or even entire network maybe infected. These attacks are difficult to manage adequately due to distrib-uted environment.
Another problem is cost. Though with the edge computing we can
reduce the computing but at the same time its maintenance cost will
increase. The main reason for higher maintenance cost is typically used of
various edge devices, maintenance team should be knowledgeable enoughto manage this technology.254 Kavita Saini and Pethuru Raj7. Conclusion
This chapter briefly introduced cloud computing and given a more
comprehensive definition of edge computing. The chapter talk about var-
ious technologies around the edge computing. After a deep discussionabout the technology various benefits out of use of the same explainedwidely. The chapter also discuss the journey from cloud computing and
edge computing in detail. Various app lications after detailed explanation
also major part of the chapter. Though, there are noteworthy security
threats within IoT gadgets, which me ans edge security is most vital than
ever. Along with the threat resolvase techniques, edge also gives a chance
to improve the security by researchin g the related areas. The chapter con-
cludes that the edge computing has tremendous benefits over the cloud
computing and helpful to overcome the limitations of traditional and cloudcomputing. With 5G and edge technology there is very bright future of
many real time applications.
References
[1] K. Saini, V. Agarwal, A. Varshney, A. Gupta, E2EE for data security for hybrid
cloud services: a novel approach, in: IEEE International Conference on Advances in
Computing, Communication Control and Networking (IEEE ICACCCN 2018)Organized by Galgotias College of Engineering & Technology Greater Noida,
12–13 October, 2018, 2018, https:/ /doi.org/10.1109/ICACCCN.2018.8748782 .
[2]I. Goodfellow, et al., Generative adversarial nets, Proc. Adv. Neural Inf. Process. Syst.
(2014)
2672 –2680.
[3]H.T. Dinh, C. Lee, D. Niyato, P. Wang, A survey of mobile cloud computing:
architecture,
applications, and approaches, Wirel. Commun. Mob. Comput. (2013).
[4]P.J. Werbos, Backpropagation through time: what it does and how to do it, Proc. IEEE
78
(10) (1990) 1550 –1560.
[5] S.R. Jena, R. Shanmugam, R. Dhanaraj, K. Saini, Recent advances and future research
directions
in edge cloud framework, Int. J. Eng. Adv. Technol. 2249-8958, 9 (2) (2019),
https:/ /doi.org/10.35940/ijeat.B3090.129219 .
[6]V. Mnih, et al., Human-level control through deep reinforcement learning, Nature 518
(7540)
(2015) 529.
[7]S.R. Jena, R. Shanmugam, K. Saini, S. Kumar, Cloud computing tools: inside views
an
d analysis, in: International Conference on Smart Sustainable Intelligent Computing
and Applications under ICITETM2020, Elsevier, 2020, pp. 382 –391.
[8]F. Bonomi, R. Milito, J. Zhu, S. Addepalli, Fog computing and its role in the internet ofthings,
in: Workshop on Mobile Cloud Computing, ACM, 2012.
[9]C. Szegedy, et al., Going deeper with convolutions, in: Proceedings of the IEEEConference
on Computer Vision and Pattern Recognition, 2015, pp. 1 –9.
[10] K. Saini, P. Raj, Handbook of Research on Smarter and Secure Industrial ApplicationsUsing
AI, IoT, and Blockchain Technology, IGI Global, 2021. ISBN13: 9781799
883678, ISBN10: 1799883671, EISBN13: 9781799883685.255 Edge platforms, frameworks and applications[11] Z. Liu, Z. Dai, P. Yu, Q. Jin, H. Du, Z. Chu, D. Wu, Intelligent station area recog-
nition
technology based on NB-IoT and SVM, in: Proceedings of the IEEE 28th
International Symposium on Industrial Electronics, 2019, pp. 1827 –1832.
[12] J. Marescaux, J. Leroy, M. Gagner, et al., Transatlantic robot-assisted telesurgery,Nature
413 (2001) 379– 380.
[13] I. Stojmenovic, S. Wen, The fog computing paradigm: scenarios and security issues, in:
Federated
Conference on Computer Science and Information Systems (FedCSIS),
IEEE, 2014.
[14] M. Satyanarayanan, P. Bahl, R. Caceres, N. Davies, The case for VM-based cloudlets in
mobile
computing, IEEE Pervasive Comput. 4 (2009) 14 –23.
[15] C.-C. Hung, et al., VideoEdge: processing camera streams using hierarchical clusters, in:Proceedings
of the IEEE/ACM Symposium on Edge Computing (SEC), 2018,
pp. 115– 131.
[16] K. Hong, D. Lillethun, U. Ramachandran, B. Ottenwalder, B. Kold-ehofe,
Opportunistic
spatio-temporal event processing for mobile situation awareness, in:
Proceedings of the ACM International Conference on Distributed Event-Based
Systems, 2013.
[17] L.M. Vaquero, L. Rodero-Merino, Finding your way in the fog: towards a comprehen-sive
definition of fog computing, in: ACM SIGCOMM CCR, 2014.
[18] I. Stojmenovic, Fog computing: a cloud to the ground support for smart things andmachine-to-machine
networks, in: Telecommunication Networks and Applications
Conference (ATNAC), IEEE, 2014.
[19] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (8) (1997)1735 –1780.
[20] J. Zhu, et al., Improving web sites performance using edge servers in fog computing
architecture,
in: SOSE, IEEE, 2013. [12] H. Madsen, G. Albeanu, B. Burtschy, and
F. Popentiu-Vladicescu, “Reliability in the utility computing era: Towards reliable
fog computing,” in IEEE International Conference on Systems, Signals and Image
Processing (IWSSIP), 2013.
[21] S. Yi, Z. Qin, Q. Li, Security and privacy issues of fog computing: a survey,
in:
International Conference on Wireless Algorithms, Systems and Applications
(WASA), 2015.
[22] D.N. Le, R. Kumar, B.K. Mishra, J.M. Chatterjee, M. Khari, (Eds.),, Cyber Security inParallel
and Distributed Computing: Concepts, Techniques, Applications and Case
Studies, John Wiley & Sons, 2019.
[23] B. Chen, S. Qiao, J. Zhao, D. Liu, X. Shi, M. Lyu, H. Chen, H. Lu, Y. Zhai, A security
awareness
and protection system for 5G smart healthcare based on zero-trust architec-
ture, IEEE Internet Things J. (2020), https:/ /doi.org/10.1109/JIOT.2020.3041042 .
[24] B. Ottenwalder, B. Koldehofe, K. Rothermel, U. Ramachandran, Migcep: operator
migration
for mobility driven distributed complex event processing, in: Proceedings
of the ACM International Conference on Distributed Event-Based Systems, 2013.
[25] K. Hong, D. Lillethun, U. Ramachandran, B. Ottenwalder, B. Kold-ehofe, Mobile fog:
a
programming model for large-scale applications on the internet of things, in: ACM
SIGCOMM Workshop on Mobile Cloud Computing, 2013.
[26] S. Yi, C. Li, Q. Li, A survey of fog computing: concepts, applications and issues, in:Proceedings
of the 2015 Workshop on Mobile Big Data, ACM, 2015.256 Kavita Saini and Pethuru RajAbout the authors
Kavita Saini, is presently working as pro-
fessor, School of Computing Science and
Engineering, Galgotias University, Delhi
NCR, India. She received her PhD degree
from Banasthali Vidyapeeth, Banasthali. She
has 18 years of teaching and research experi-
ence supervising Masters and PhD scholars in
emerging technologies.
She has published more than 40 research
papers in national and international journals
and conferences. She has published 17 authored
books for UG and PG courses for a number
of universities including MD University, Rothak, and Punjab Technical
University, Jallandhar with National Publishers. Kavita Saini has edited
many books with International Publishers including IGI Global, CRC
Press, IET Publisher Elsevier and published 15 book chapters with
International Publishers. Under her guidance many MTech and PhD
scholars are carrying out research work.
She has also published various patents. She has also delivered technical
talks on Blockchain: An Emerging Technology, Web to Deep Web, and
other emerging areas and handled many special sessions in International
Conferences and Special Issues in International Journals. Her research inter-
ests include Web-Based Instructional Systems (WBIS), Blockchain
Technology, Industry 4.O, and Cloud Computing.257 Edge platforms, frameworks and applicationsPethuru Raj working as a chief architect at
Reliance Jio Platforms Ltd. (JPL) Bangalore.
Previously. worked in IBM global Cloud
center of Excellence (CoE), Wipro consult-
ing services (WCS), and Robert Bosch
Corporate Research (CR). In total, I have
gained more than 20 years of IT industry
experience and 8 years of research experi-
ence. Finished the CSIR-sponsored Ph.D.
degree at Anna University, Chennai and
continued with the UGC-sponsored post-
doctoral research in the Department of
Computer Science and Automation, Indian Institute of Science (IISc),
Bangalore. Thereafter, I was granted a couple of international research fel-
lowships (JSPS and JST) to work as a research scientist for 3.5 years in two
leading Japanese universities. Focuses on some of the emerging technologies
such as the Internet of Things (IoT), Optimization of Artificial Intelligence
(AI) Models, Big, fast and streaming Analytics, Blockchain, Digital Twins,
Cloud-native computing, Edge and Serverless computing, Reliability engi-
neering, Microservices architecture (MSA), Event-driven architecture
(EDA), 5G, etc. My personal web site is at https:/ /sweetypeterdarren.
wixsite.com/pethuru-raj-books/my-bo oks https:/ /scholar.google.co.in/
citations?user ¼yaDflpYAAAAJ&hl ¼en.258 Kavita Saini and Pethuru RajCHAPTER NINE
Edge computing challenges
and concerns
Kavita Sainia, Uttama Pandeya, and Pethuru Rajb
aSchool of Computing Science and Engineering (SCSE), Galgotias University, Delhi, Uttar Pradesh, India
bSite Reliability Engineering (SRE) Division, Reliance Jio Platforms Ltd. (JPL), Bangalore, India
Contents
1.Introduction 260
2.Cloud, fog and edge computing 261
2.1 Cloud computing 261
2.2 Fog computing 262
2.3 Edge computing 262
3.Implications and challenges in adopting edge computing 263
3.1 Accessibility 263
3.2 Control and management 264
3.3 Scalability 265
3.4 Privacy and security 265
3.5 Data storage 265
3.6 Latency 266
3.7 Performance 266
4.Concerns with edge computing 266
4.1 Cost 266
4.2 Data 267
4.3 Response time requirement 267
4.4 Security concerns 267
4.5 Security aspects 268
5.Security and privacy attacks on edge computing enabled devices 268
5.1 Physical attacks 268
5.2 Sniffing 268
5.3 Unauthorized access 268
5.4 Routing table attack 269
5.5 Distributed denial of service attack (DDoS) 269
5.6 Malicious hardware and software injections 269
5.7 Integrity attack 269
5.8 Privacy leakage 270
5.9 Logging attacks 270
5.10 Data storage and protection 270
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0062596.Countermeasures to security and privacy attacks in edge infrastructure 270
6.1 Solution to physical attack 270
6.2 Solution to sniffing 270
6.3 Solution to unauthorized access 271
6.4 Solution to routing table attack 271
6.5 Solution to distributed denial of service attack (DDoS) 271
6.6 Solution to malicious hardware and software injections 271
6.7 Solution to integrity attack 272
6.8 Solution to privacy leakage 272
6.9 Solution to logging attacks 272
6.10 Solution to data storage and protection 273
6.11 Embedding blockchain on edge infrastructure 273
7.Future of edge computing 273
8.Conclusion 275
References 275
Further reading 276
About the authors 277
Abstract
In the current trends of Internet of things and with the excessive use of end devices likelaptops, smartphones, sensors that are continuously generating large amount of data isencouraging the furtherance of Edge Computing. Since there is a demand for lowerlatency network, the edge computing has always supported the light weight enddevices to work in an efficient way toward complicated problems and provide desiredservices to the end user. The major issue in providing a better service to the customerlies in the delay and the network traffic which indirectly includes our concern for theissues like network bandwidth, cost efficiency, energy saving, data security on edgedevice. This chapter discusses about the several critical challenges that are faced in dif-ferent areas, also the trends in the area of edge computing.
1. Introduction
These days the amount of data generated, captured, consumed and
created is significantly increasing. Now the worry is to store and manage
the data to hard drive on your PC or any other device, will definitely
require the wide storage space which is highly cost effective and unmana-
geable [1,2]. So instead of saving data to the nearby hard drive, you can store
it
on a cloud based system which allow data to be collected from various
IoT devices and make it available anywhere in the world. It is expected
that more than 15 billion IoT devices will connect to the enterprise infra-
structure by 2029.260 Kavita Saini et al.The amount of data generated by these IoT devices are almost doubling
every year and it’s going to be approximately 79ZB by 2025. This large and
excessive amount of data produced at an IoT device is sometime unable to
be transferred from a remote network to another site [3,4]. The reason
being, that the various devices transmits the data at the same time and send-
ing this over sufficient quantity of data to the cloud or to the central repos-
itory for computation may create the latency and bandwidth issues. This
results in shifting to more efficient computing alternative called Edge
Computing. The need is to process and segregate the needful data generated
by the nodes in the network, close to the edge node. For this the edge device
should be made more capable to compute and pre-process the data on its
own. Instead of only generating the data and transferring it as it is to the cen-
tral repository for computation and storage, the device will be provided the
figuring capabilities to extract the meaningful data. Since irrelevant data is no
more transmitted to the data centers, this may lower the latency and save the
network cost [5].
In coming few years, industries are going to completely adopt cloud edge
technology along with the IoT applications for controlling and managing
their industrial services in more efficient and energy saving way. Various
authors have tried to define Edge computing in most natural and suitable
way[6].
2. Cloud, fog and edge computing
2.1 Cloud computing
Cloud computing works as an Internet store which provides a remote server
where you can process your data. It does not require any direct active man-
agement by the user, means you do not have to worry about the storage
management. Instead of saving data to the nearby hard drive on your PC
or any other device, you can store it on a cloud based system which allow
data to be collected from various IoT devices and make it available anywhere
in the world [7].261 Edge computing challenges and concernsIoT Device: These are the connected devices that continuously generate
large amount of data. If all the data is send to the cloud the cost will definitely
increase [8,9]. So, instead of sending the data directly to the cloud it is send to
the
devices near to the network to analyze and compute.
2.2 Fog computing
It is also known as Fog networking or fogging. It is a decentralized infra-structure and application resides between data stores and the cloud. It isan architecture which uses edge device for computation, storage and com-
munication [10,11] . This edge device is the device that controls the data
flow
at the boundaries of any two networks, example: router, switch,
IAD, gateways, hub, multiplexer or bridge. It is actually a mediator between
hardware and remote server [12].
Fog
is a distributed network environment and is very close to cloud com-
puting and IoT device [13]. It has a high security network. Instead of sending
selected
data to the cloud for processing, fog actually process the data which
saves the network bandwidth and reduces latency requirement which in turn
helps in fast decision making capability. Fog nodes can be planted on very
crucial places like under the sea, railway track, etc.
2.3 Edge computing
Edge computing and Fog computing, both are the extensions of cloud net-work. Edge computing technology saves time and resources in maintenanceof operations by collecting and analyzing data in real time [14]. As in cloud
computing
all the data generated by IoT device is stored on a cloud based
system and the maintenance and processing of data is done there, in edgecomputing data is not transferred anywhere [15,16] . The maintenance
and 
processing of data is done on the device that initially created it. The
computational power is given to the edges on the network. Edge computing
is more secure and it reduces the unnecessary traffic to the central repository,
as only the selected data after computation from edge is sent to the cloud.
“Edge computing is a new paradigm in which the resources of an edge
server are placed at the edge of the Internet, in close proximity to mobiledevices, sensors, end users, and the emerging IoT.” [1].
“Edge 
computing refers to the enabling technologies allowing compu-
tation to be performed at the edge of the network, on downstream data onbehalf of cloud services and upstream data on behalf of IoT services” [2].262 Kavita Saini et al.“Edge computing is part of a distributed computing topology where
information processing is located close to the edge, where things and people
produce or consume that information” [7].
Edge
computing can also be defined as a technology where the compu-
tation and control for optimized data is given to the node (an IoT device)
which itself is generating the data, in order to improve the performance of a
distributed network [7](Table 1 ).
3. Implications and challenges in adopting edge
computing
In vehicle automation, video surveillance, and other areas, edge com-
puting has the ability to provide superior intelligent services with a faster
reaction on programs that run in real time. But still, it suffers from various
challenges. Few are listed below:
3.1 Accessibility
Edge applications frequently create logistical challenges when it comes todeploying human IT resources to administer them, and they don’t allow
for high operator expenses. Companies cannot afford to hire a professionalTable 1 Comparison of cloud, edge and fog computing.
Parameter Cloud Edge Fog
Location
of DataProcessingData is processed on thecloud serverData is processedon the edge itself,may be IoTsensorsData is processed on theedge device connected tothe LAN hardware, saygateways
Capacity Does not provide any
reduction in data whilesending or transformingdataReduces the amount of data sent to cloud
Purpose Suitable for the
long-term, in-depthanalysis of data andstorageBoth are more suitable for the quick analysiswhich is required for real-time response
Latency High Low Very Low
Security Less security compared
to Edge and FogHigh security263 Edge computing challenges and concernsadministrator to oversee and maintain each and every Edge location. For
example, for every sensors located at the oil wells for keeping a check on
the status of wells or devices deployed at the agriculture farm land to con-tinuously monitoring the health of soil and plant, these operatorrestrictions—either due to distance, device volume, geographic accessibility,or other cost factors—need Edge applications that are not only small in terms
of computing footprint but also in terms of technical overhead, from instal-
lation to ongoing operations.
3.2 Control and management
For making the edge infrastructure more reliable Differentiation, Elasticity,Segregation (Isolation) and reliability should be considered as an importantfactor. With the rapid rise of IoT deployment, we expect different services,
such as Smart Home, to be provided at the network’s edge. These services
will be prioritized differently. For example IoT devices used in disaster man-agement must be processed earlier than other device.
Elasticity could be one of the major challenges at the network’s edge
because, the edge device must be extremely dynamic. Is it possible for
the owner to add a new item to the current service without any difficulty?Or, if anything wears out and needs to be replaced, can the prior servicequickly adopt a new node? These issues should be addressed with a flexibleand extensible approach.
Isolating the edge device from the network or from operating system and
device provides further protection against malware and other assaults. In caseof distributed networks topology, if a device or an application crash, thewhole system is required to e rebooted. Also, we have an option of token
management to solve the issues of locks. But in case of edge device comput-
ing where the data is processed closer to the device, the data may be sharedand accessed as a resource, among different applications. Say for example, insmart surveillance systems with multiple CCTV cameras in a shop, if one
device crashes or fails, the owner would still be able to control and monitor
mishaps without the failure of entire OS. The introduction of a deploymentmechanism could potentially fix this problem. If the OS detects a conflictbefore an application is installed, the user will be notified and the potentialaccess issue will be avoided.
From a reliability standpoint, it’s critical for the EdgeOS to keep track of
the entire system’s network topology, and each component must be able tocommunicate status/diagnosis data to the EdgeOS. It is sometime very dif-ficult to find out the potential reason of a service of a device failure at the host264 Kavita Saini et al.end. An IoT device sometimes fails to report the data constancy due to bat-
tery outage or bad connection condition under some unpredictable
situations. Some communication protocols must be used which providesdynamic connection with sensor nodes. Data sensing and data communica-tion between sensor and the application is also an important issue.
3.3 Scalability
IoT device collects data and transmit it to more powerful computing nodes,
where all of the original data is further processed and analyzed. Nonetheless,
individual edge node computational power is restricted, making scalabilityof computational capacity for edge computing a difficult task.
Though the number of edge devices are increasing with the increased
demand of cloud based services, the application on edge are expected to
work more consistently despite of increased load on the edge. In order tocomprehend the status of the network, Edge computing necessitates a largenumber of monitors, as well as servers and network devices. Finally, viewing
the status of the entire edge environment becomes challenging.
3.4 Privacy and security
Edge computing devices are more vulnerable to attack because if you are
putting your data on to the edge instead of cloud, it is exposed to the realworld. So we have to make it secure either by physical security whichmay or may not be possible, or by securing the machines by secure boot
which make sure that when the data is coming out, it must be encrypted.
Encryption will definitely require strong authentication to login to thedevice for a direct access remotely. Potential connectivity is also a challenge,as there is no guarantee that your device is always connected. It may shut-
down due to power failure or battery discharge. Edge devices that are very
small in size may suffer from physical isolation.
3.5 Data storage
There are large numbers of data generators and sensors in an IoT networkwhich are deployed to sense the data and report to the network gateway. Forexample in smart security surveillance system, the security cameras keep onsending the videos to the gateway where the data is just stored in the databaseand nobody is consuming the data. After certain period the video is flushed
with the latest videos. The edge node must have less human involvement,
and must be capable enough to process the data at the edge level resulting in265 Edge computing challenges and concernsthe optimized data after event detection and privacy protection. Choosing
the appropriate level of data abstraction might be tricky at times. Some appli-
cations or services may not be able to learn enough knowledge if too muchraw data is filtered out. However, there are several limitations if we want tostore a significant amount of raw data. Data storage would be a challenge.
3.6 Latency
Latency is defined as any lag in communication between a network and itsdevices. Edge computing, which is based on distributed network, can alle-
viate latency issues by ensuring that there is no disconnect in real-time infor-
mation processing and providing a more stable network. Application latencyand decision-making delay are lowered by placing computation capabilitycloser to the data at the edge. Faster request and response is expected fromback-and-forth movement from the edge to the core. Application data
travels the network in directions, sharing data and dealing with access per-
missions, with computing placed at both the core and the edge.
3.7 Performance
IoT devices are now used as building blocks in smart cities. These devices areintegrated into the city’s infrastructure, allowing for the monitoring ofdevice operation and the collection of periodic data on these assets. Edgecomputing sends critical technical data such as traffic, flood, safety, and infra-
structure monitoring. It allows for more on-device computation for
real-time decision-making. Monitoring the performance of these edgedevices from edge to cloud will be a challenge for the consumer.Technology which will provide end to end monitoring must be deployed
to keep an eye on edge to cloud transactions.
4. Concerns with edge computing
4.1 Cost
Edge computing devices with processing capabilities are expensive ( Fig. 1 ).
Increased
requirement of equipment at the site, requires deployment of new
devices along with the modifications to older versions that lack such
processing capabilities, resulting in additional expenditures. An edge com-
puting framework’s configuration, deployment, and maintenance are all
costly endeavors.266 Kavita Saini et al.4.2 Data
Since the edge computing provides on-device computation capability to the
edge node for extracting the meaningful data out of the raw data generatedby the IoT device. This subset of useful data is used for further processing
and lot of raw data is wasted. There could be a possibility of loss of some
important information which could provide additional insights in real timedecision, but is filtered now. Elimination of the critical data, for improvedefficiency is an important concern in different business applications running
over cloud.
4.3 Response time requirement
Any lag in communication between edge node and cloud should be reduced
in order to make an effective communication. The request and responsetime between user and cloud service provider termed as latency, is requiredto be as low as if user is working on a system with database on his device. For
example as mobile shopping becomes increasingly popular, it is critical to
improve the customer experience, particularly in terms of latency. Thelatency will be considerably decreased if the shopping cart updating isoffloaded from cloud servers to edge nodes in this situation.
4.4 Security concerns
Edge computing aims at protecting the processed data traveling from edge to
centralized data centers. Since edge computing is a decentralized system it is
important for the users to implement proactive threat detection technologiesCOST 
DATA 
RESPONSE TIME 
SECURITY CONCERNS 
SECURITY ASPECTS
Fig. 1 Concerns with edge computing.267 Edge computing challenges and concernsfor early detection of known and unknown vulnerabilities. There is a need
to secure the edge application beyond the network layer by firewalls,
encryption policies and access control mechanisms.
4.5 Security aspects
Due to the increased data generation at the edge node, Edge Computing willface new and unanticipated security and privacy concerns. Confidentiality,
Integrity, and Availability are the three primary requirements in attaining
security and privacy in a shared environment like Edge Computing. IoTfunctionality necessitates service migration between local and global scales,making the network more vulnerable to malicious activity. Furthermore,because users’ personally identifiable information would be exchanged
and/or kept on edge servers, security and privacy become critical concerns
in such a distributed framework. As a result, EC-assisted IoT networks aremore exposed to cyber threats and attacks. Malicious attacks can occur dur-ing any of the three core functions of Edge servers: communication, com-
putation, and storage.
5. Security and privacy attacks on edge computing
enabled devices
5.1 Physical attacks
It is basically the tampering of the valuable assets. If attackers have physical
access to the edge nodes/devices, they can launch this assault. In this situa-tion, valuable and sensitive cryptographic data can be recovered, the data can
be tampered, and the software/operating systems can be tweaked or
changed.
5.2 Sniffing
It is sometimes called as eaves dropping. Over communication channels,adversaries listen in on private discussions such as usernames, passwords,
and other sensitive information. Attackers can get vital information about
the network if sniffed packets contain access or control information ofthe Edge nodes, such as node configuration, node identification, and theshared network password.
5.3 Unauthorized access
To access or share the information, neighboring Edge nodes communicatewith one another. However, if an attacker gains access to one of the268 Kavita Saini et al.unsecured nodes, they can take control of the entire neighborhood and can
violate the network’s stated security policy.
5.4 Routing table attack
DoS, Packet mistreating, RTP (Routing Table Positioning), Hit and Run
are some of the common attacks to router. At the communication level,attackers can modify routing information by redirecting or discarding data
packets. The malicious edge nodes could: (1) Drain all network packets,
(2) Drain selective packets, (3) Record packets at one network locationbefore migrating them to another, or (4) Broadcasts “Hello” to all networknodes claiming to be their neighbor.
5.5 Distributed denial of service attack (DDoS)
DDoS assaults are carried out via networks of nodes that are linked to theInternet. These networks are made up of malware-infected IoT devices thatcan be manipulated remotely by an attacker. Individual devices are known as
bots (or zombies) and a botnet is a collection of bots. The attacker can direct
an attack once a botnet has been built by delivering remote instructions toeach bot. When a botnet targets a victim’s server or network, each bot sendsrequests to the target’s IP address, potentially overloading the server ornetwork and causing a denial-of-service to normal traffic. Outage, Sleep
deprivation and battery draining are considered to be famous DDos attack.
5.6 Malicious hardware and software injections
An unauthorized software and hardware components is injected into the
edge network by the attacker, resulting into a devastating impact on the
efficiency of existing edge servers and devices and even exploiting service
provider, in which entities that provide the software and hardware solutionsthat enable edge computing unwittingly begin executing hacking processeson the attacker’s behalf. Camouflaging and Node Replications are some
common frightening practices by attacker.
5.7 Integrity attack
In this attack, the attacker attempts to encrypt or corrupt the sensitive data
and may demand for some ransom to restore the data. In edge enabled com-puting infrastructure the attacker changes the teaching process of the learn-
ing model by injecting wrong datasets or may misuse the vulnerabilities of
the learning model without performing any changes to it.269 Edge computing challenges and concerns5.8 Privacy leakage
The functionality of Edge nodes may require the extraction of personal
information from data generated by user devices. Data owners must own
all information regardless of its importance like Personal activities, prefer-ences, and health state may be sensitive; and others, such as the air pollutionindex, public information, and social events, may not be. Unfortunately,
without the authorization of the information owners, they can be shared
with other users or network organizations, making them vulnerable toattackers during data transfer and sharing.
5.9 Logging attacks
This type of attack might cause damage to IoT systems in your Edge infra-structure, if log files are not secured. Infrastructure developers must keep
track of events like application faults and failed/successful authorization/
authentication attempts, and log these events timely.
5.10 Data storage and protection
Data gathered and processed at the edge does not have the same level ofphysical protection as data stored in more centralized locations. Vital infor-mation can be compromised and leaked easily by removing a disc drive from
an edge resource or copying data from a simple memory stick. It can also be
more challenging to provide reliable data backup due to restricted localresources.
6. Countermeasures to security and privacy attacks
in edge infrastructure
6.1 Solution to physical attack
Organizations should consider innovative approaches to improve the phys-
ical security of any edge nodes. Additional robust techniques may be used
during manufacturing of devices, or locking mechanisms and other physical
protections may be implemented in the field.
6.2 Solution to sniffing
Connect to a trusted network as the attacker as Public networks are usuallyput up and not monitored for any incursions or bugs. Attackers can eithersniff that network or construct their own network with similar names to
deceive users into joining it. An attacker sitting in a coffee shop can set270 Kavita Saini et al.up a Wi-Fi network called “Free coffee_Wi-Fi,” and adjacent users can join
to it, transmitting all data through the sniffer node of the attackers. Ensure
that all the data transmitted through the edge node must be encrypted, sothat even if your data is sniffed the attacker will not be able to sense thetraffic. Network administrators must check the network for any attemptsat infiltration or rogue devices set up in span mode to record traffic.
6.3 Solution to unauthorized access
Unauthorized access can be defended by authorization and authentication.Entities must mutually authenticate one another across different trusteddomains in the Edge computing environment. This comprises single-domain and cross-domain authentication, as well as handover authentica-
tion. Authorization prevents attackers or malicious Edge nodes from
receiving answers. It examines if a service provider or an Edge node/deviceor a router has access to, control over, alter, or share data.
6.4 Solution to routing table attack
Effective countermeasures against routing information assaults includedeveloping trustworthy routing protocols and installing a high-quality intru-
sion detection system (IDS) that monitors for hostile traffic and detects
policy breaches. Nodes that use reliable routing protocols can build atable of trusted nodes to share sensitive data.
6.5 Solution to distributed denial of service attack (DDoS)
Organize a DDos Response plan, if a DDoS attack is successful, prepare achecklist to ensure that your assets have advanced threat detection, as well
as any technical competencies and experience that would be required.
Establish an incident response team and inform the stakeholders. PreventDDoS attacks by securing your infrastructure and implementing preventionmanagement systems that combine firewalls, VPN, anti-spam, content fil-
tering and other security layers to monitor activities. Basically we need to
ensure that the network’s normal regulations are not breached. In a nutshell,regulate the behavior of devices in a network.
6.6 Solution to malicious hardware and software injections
To defend malicious hardware and software injections researchers have pro-posed three measures. First, side-channel signal studies, which use timing,
power, and spatial temperature analysis to detect hardware trojans. This271 Edge computing challenges and concernsapproach detects malicious firmware or software placed on edge nodes by
looking for anomalous system behaviors like increased execution time
and power consumption. Second, to detect and model harmful attacks,Trojan activation methods compare Trojan-afflicted integrated circuits withnon-Trojan-afflicted integrated circuits. Lastly, Circuit modification orreplacement to provide circuit-level protection and even allowing the node
to self-destruct in the case of an assault.
6.7 Solution to integrity attack
Encrypting your data is the most effective way to ensure its integrity. This
applies to data transmission as well as data in storage. Unauthorized partiescannot access encrypted data, thus even if your data silo is breached, yourdata will be totally protected. We now live in a time where password
authentication is no longer sufficient. Credential stuffing assaults are con-
stantly bombarding network security systems, and users continue to clickembedded URLs and email attachments that install key loggers. These con-stant attacks, along with our mobile environment, where remote logins are
the standard, create genuine dangers for any organization [5]. Emails with
enclosed
data attachments should not be forwarded, and what applications
interacting with data files should minimized. Microsoft’s Windows
Information Protection and Azure Rights Management are two examplesthat are providing data protection.
6.8 Solution to privacy leakage
Identifying the critical data and using Data Loss Prevention (DLP) software
to protect the sensitive information may help in defending against privacy
leakage. Organizations should implement a data protection plan, focusingon sensitive documents and their treatment, because DLP is strongly relianton correct classification of information. Closely monitor traffic on all net-
works. In order to detect unauthorized actions Data Activity Monitoring
(DAM) solution can provide another layer of protection for this. DataEncryption is also another way to keep your data secure [10].
6.9 Solution to logging attacks
Attackers frequently change the logs of the victim machine to escape detec-tion by system, network, and security administrators. The attacker will try todelete specific events from the logs related to gaining access, escalating priv-
ileges, and installing RootKits and back doors. The first step to ensuring the272 Kavita Saini et al.integrity and utility of your log files is straightforward: Logging should be
enabled on all of your sensitive systems. A policy or standard must be created
in the organization that states that logging is required. Setting adequate per-missions on log files is another common-sense defense for preserving essen-tial system logging and accounting [8]. Another step could be to put up a
separate
logging server. We can raise the bar for attackers by deploying a
separate logging machine. Encrypting log files is another effective methodof log protection. Without the encryption key, attackers will be unableto make any serious changes to the data.
6.10 Solution to data storage and protection
Because of the rapid advancements in IoT devices, some artificial intelli-gence (AI) functions can now be moved from the centralized cloud to
Edge devices/nodes. Security, privacy, and latency will all be improved
due to this [10]. In order to eliminate data redundancy and maximize band-
width 
in IoT networks, the replicate copies of data on intermediate nodes
must be removed. Unfortunately, intruders will have access to important
information as a result of this. Secure data deduplication is used to counteract
this problem, allowing intermediaries to access replicated data withoutobtaining any knowledge of it [5].
6.11 Embedding blockchain on edge infrastructure
Blockchain is a new technique that aims to create a trusted, dependable,and secure foundation for data regulation and information exchangesamong various operating network edge entities. It establishes rules that let
decentralized systems to make decisions regarding the execution of specific
transactions in concert, based on voting and consensus algorithms. This will:(1) provide safe audit-level tracking of EC-assisted IoT data transactions; and(2) remove the need for a central trusted intermediary between interacting
IoT edge devices.
7. Future of edge computing
1.Edge computing’s future will undoubtedly be open. Edge computingwill
converge with the utilization of data to transform knowledge into
actions that benefit businesses and their consumers, thanks to artificial273 Edge computing challenges and concernsintelligence and machine learning. It will eventually be regarded like any
other area where applications can be installed in a consistent and
uncompromised manner.
2.We will see boundaries between the edge, data centers—if they evenexist—and the cloud because there will be so much computing and stor-age available. Multiple ecosystems will start providing data from the edge
directly, posing challenges for data consolidation across the three realms.
At the edge, AI and machine learning will have progressed to the pointwhere more sophisticated autonomous use cases will be available. Datamanagement will continue to be important, but the problems will
be new.
3.Edge computing’s future will improve in tandem with sophisticated
networks such as 5G and satellite mesh, as well as artificial intelligence.It has suddenly opened up the globe to some potentially futuristic pos-
sibilities by having greater capacity and power, better access to fast and
widespread networks (5G, satellite), and smarter machines withincomputers (AI).
4.Shifting data processing to the network’s edge can help businesses cap-
italize on the expanding number of IoT edge devices, boost network
speeds, and improve customer experiences. Edge computing’s scalabilitymakes it a great alternative for fast-growing, agile businesses, particularlyif they already use colocation data centers and cloud infrastructure.
5.Companies can optimize their networks by leveraging the power of edge
computing to provide flexible and reliable service that strengthens their
brand and keeps customers pleased. Edge computing has various advan-tages over traditional network design and will undoubtedly continue toplay a significant role in the future for businesses. Innovative enterprises
have likely only scratched the surface of what’s possible with edge
computing as more internet-connected products hit the market.
6.Edge computing addresses a growing demand for decreased latency,processing of increasing amounts of data at the edge, and network
resilience.
7.“Edge computing” is broad enough to accommodate a variety of sub-
markets, but it will grow from thousands of custom patterns to a fewdozens, with cloud providers playing a key role all the way to the edge,or complementing edge solutions. Enterprises must emphasize a distrib-
uted cloud-based solution as the default and future-proof edge solutions
by relying on partnerships and ecosystems rather than a single-vendorstrategy, according to McArthur.274 Kavita Saini et al.8. Conclusion
Cloud and IoT are the two advancements in networking technologies
which has made Edge computing possible. It provided the storage capacity
and processing power near to the user’s infrastructure, resulting into reducedlatency and higher bandwidth. With the rapid proliferation of IoT devices,as well as the resulting massive data traffic generated at the network’s edge,
placed additional strains on the current state-of-the-art. Because of the band-
width and computational power available, a centralized cloud computingmodel has emerged to overcome with the shortage of resources. SinceEdge Computing technology has unique features and provided extended
QoS (Quality of Service), it involves huge risk in data security and privacy.
This paper provides the Security and privacy aspects of edge computing. Italso discusses the future aspects of this emerging trend.
References
[1]W. Shi, G. Pallis, X. Zhiwei, Edge computing, Proc. IEEE 107 (8) (2019) 1474 –1481.
[2]W. Shi, et al., Edge computing: vision and challenges, IEEE Internet Things J. 3
(2016)
637– 646.
[3] S.R. Jena, R. Shanmugam, R. Dhanaraj, K. Saini, Recent advances and future research
directions
in edge cloud framework, IJEAT 9 (2) (2019), https:/ /doi.org/10.35940/
ijeat.B3090.129219 .
ISSN: 2249 –8958.
[4] K. Saini, V. Agarwal, A. Varshney, A. Gupta, E2EE for data security for hybrid cloud
services:
a novel approach, in: IEEE International Conference on Advances in
Computing, Communication Control and Networking (IEEE ICACCCN 2018) orga-nized by Galgotias College of Engineering & Technology Greater Noida, 12 –13
October, 2018, https:/ /doi.org/10.1109/ICACCCN.2018.8748782 .
[5]M. Simsek, A. Aijaz, M. Dohler, J. Sachs, G. Fettweis, 5G-enabled tactile internet, IEEEJ.
Sel. Areas Commun. 34 (3) (2016) 460 –473.
[6]N. Hassan, S. Gillani, E. Ahmed, I. Yaqoob, M. Imran, The role of edge computing ininternet
of things, IEEE Commun. Mag. 56 (11) (2018) 110 –115.
[7]Y. He, F.R. Yu, N. Zhao, H. Yin, Secure social networks in 5G systems with mobileedge
computing, caching, and device-to-device communications, IEEE Wirel.
Commun. 25 (3) (2018) 103– 109.
[8]Y. He, J. Ren, G. Yu, Y. Cai, D2D communications meet Mobile edge computing forenhanced
computation capacity in cellular networks, IEEE Trans. Wirel. Commun.
18 (3) (2019) 1750 –1763.
[9]S. Choy, B. Wong, G. Simon, C. Rosenberg, The brewing storm in cloud gaming: a
measurement
study on cloud to end-user latency, in: 2012 11th Annual Workshop on
Network and Systems Support for Games (NetGames), IEEE, 2012.
[10] S. R. Jena, R. Shanmugam, K. Saini, S. Kumar, “Cloud computing tools: inside views
and
analysis”, International Conference on Smart Sustainable Intelligent Computing
and Applications under ICITETM2020 (ELSEVIER), pp. 382 –391.
[11] I. Sarrigiannis, E. Kartsakli, K. Ramantas, A. Antonopoulos, C. Verikoukis, Application
and
network VNF migration in a MEC-enabled 5G architecture, in: 23rd IEEE275 Edge computing challenges and concernsInternational Workshop on Computer Aided Modeling and Design of Communication
Links and Networks (CAMAD), 2018, pp. 1 –6.
[12] E. Ahmed, A. Ahmed, I. Yaqoob, J. Shuja, A. Gani, M. Imran, M. Shoaib, Bringingcomputation
closer toward the user network: is edge computing the solution? IEEE
Commun. Mag. 55 (11) (2017) 138– 144.
[13] Z. Zhao, K. Hwang, J. Villeta, Game cloud design with virtualized CPU/GPU servers
and
initial performance results, in: Proceedings of the 3rd workshop on Scientific Cloud
Computing Date, ScienceCloud ACM Press, 2012.
[14] W. Hu, Y. Gao, K. Ha, J. Wang, B. Amos, Z. Chen, P. Pillai, M. Satyanarayanan,
Quantifying
the impact of edge computing on mobile applications, in: Proceedings
of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems - APSys ’16, ACMPress, 2016.
[15] K. Saini, P. Raj, Handbook of Research on Smarter and Secure Industrial ApplicationsUsing
AI, IoT, and Blockchain Technology, IGI Global, 2021 (ISBN13:
9781799883678 jISBN10: 1799883671 jEISBN13: 9781799883685).
[16] J. Gonzalez, G. Nencioni, A. Kamisinski, B.E. Helvik, P.E. Heegaard, Dependability ofthe
NFV orchestrator: state of the art and research challenges, IEEE Commun. Surv.
Tutor. 20 (4) (2018) 3307 –3329.
Further reading
[17] J.A. Silva, R. Monteiro, H. Paulino, J.M. Lourenco, Ephemeral data storage fornetworks
of hand-held devices, in: 2016 IEEE Trustcom/BigDataSE/ISPA, IEEE,
2016.
[18] Y. Li, J. Xue, W.Z. Wang, T. Li, Edge-oriented computing paradigms, ACM Comput.Surv.
51 (2) (2018) 1 –34.
[19] C. Xu, K. Wang, P. Li, S. Guo, J. Luo, B. Ye, M. Guo, Making big data open in edges: a
resource-efficient
blockchain-based approach, IEEE Trans. Parallel Distrib. Syst.
30 (4) (2019) 870– 882.
[20] K. Ahmad, A. Kamal, K.A.B. Ahmad, M. Khari, R.G. Crespo, Fast hybrid-MixNet forsecurity
and privacy using NTRU algorithm, J. Inf. Secur. Appl. 60 (2021), 102872.
[21] D.N. Le, R. Kumar, B.K. Mishra, J.M. Chatterjee, M. Khari, (Eds.), Cyber Security in
Parallel
and Distributed Computing: Concepts, Techniques, Applications and Case
Studies, John Wiley & Sons, 2019.
[22] B. Chen, S. Qiao, J. Zhao, D. Liu, X. Shi, M. Lyu, H. Chen, H. Lu, Y. Zhai, A security
awareness
and protection system for 5G smart healthcare based on zero-trust architec-
ture, IEEE Internet Things J. (2020), https:/ /doi.org/10.1109/JIOT.2020.3041042 .
early
access, Nov. 30.
[23] S.S. Vedaei, A. Fotovvat, M.R. Mohebbian, G.M.E. Rahman, K.A. Wahid, P. Babyn,H.R.
Marateb, M. Mansourian, R. Sami, COVID-SAFE: an IoT-based system for
automated health monitoring and surveillance in post-pandemic life, IEEE Access. 8
(2020) 188538 –188551.
[24] Z. Liu, Z. Dai, P. Yu, Q. Jin, H. Du, Z. Chu, D. Wu, Intelligent station area
recognition
technology based on NB-IoT and SVM, in: Proc. IEEE 28th Int. Symp.
Ind. Electron. (ISIE), 2019, pp. 1827 –1832.
[25] J. Marescaux, J. Leroy, M. Gagner, et al., Transatlantic robot-assisted telesurgery,Nature
413 (2001) 379– 380.
[26] https:/ /iotbusinessnews.com/2020/08/10 .276 Kavita Saini et al.About the authors
Kavita Saini is presently working as Professor,
School of Computing Science and Engi-
neering, Galgotias University, Delhi NCR,
India. She received her Ph.D. degree from
Banasthali Vidyapeeth, Banasthali. She has
18 years of teaching and research experience
supervising Masters and Ph.D. scholars in
emerging technologies.
She has published more than 40 research
papers in national and international journals
and conferences. She has published 17
authored books for UG and PG courses for
a number of universities including MD University, Rothak, and Punjab
Technical University, Jallandhar with National Publishers. Kavita Saini
has edited many books with International Publishers including IGI
Global, CRC Press, IET Publisher Elsevier and published 15 book chapters
with International publishers. Under her guidance many M.Tech and Ph.D.
scholars are carrying out research work.
She has also published various patents. Kavita Saini has also delivered
technical talks on Blockchain: An Emerging Technology, Web to Deep
Web and other emerging Areas and Handled many Special Sessions in
International Conferences and Special Issues in International Journals.
Her research interests include Web-Based Instructional Systems (WBIS),
Blockchain Technology, Industry 4.O, and Cloud Computing.
Ms. Uttama Pandey born on 13 Oct 1984
at Faridabad, Haryana, she is currently work-
ing as an Assistant Professor in the
Department of Computer Science, DAV
Centenary College, Faridabad, Haryana.
She had completed her B.Sc. from Delhi
University, MCA from MD University and
M.Tech (CSE) from MD University.
Pursuing Ph.D. in Computer Applications
from Galgotias University, Greater Noida,
Uttar Pradesh. She has published papers
and attended various national and interna-
tional conferences. She has participated in faculty development programmes
and national level awareness programmes on “Research Methodology”,277 Edge computing challenges and concerns“Angular and Business Intelligence”, “Python 3.4.3” and various others.
Also participated in International E-Panel Discussion Programme organized
by ISM Patna.
Pethuru Raj - I have been working as the
chief architect and vice-president in the Site
Reliability Engineering (SRE) division of
Reliance Jio Infocomm Ltd. (RJIL),
Bangalore. My previous stints are in IBM
global Cloud center of Excellence (CoE),
Wipro consulting services (WCS), and
Robert Bosch Corporate Research (CR). In
total, I have gained more than 17 years of
IT industry experience and 8 years of research
experience.
Finished the CSIR-sponsored Ph.D.
degree at Anna University, Chennai and continued with the UGC-sponsored
postdoctoral research in the Department of Computer Science and
Automation, Indian Institute of Science (IISc), Bangalore. Thereafter, I was
granted a couple of international research fellowships (JSPS and JST) to work
as a research scientist for 3.5 years in two leading Japanese universities.
Published more than 30 research papers in peer-reviewed journals such as
IEEE, ACM, Springer-Verlag, Inderscience, etc. Have authored and edited
16 books thus far and contributed 35 book chapters thus far for various tech-
nology books edited by highly acclaimed and accomplished professors and
professionals.
Focusing on some of the emerging technologies such as IoT, data sci-
ence, Blockchain, Digital Twin, Containerized Clouds, machine and deep
learning algorithms, Microservices Architecture, fog/edge computing, etc.278 Kavita Saini et al.CHAPTER TEN
A smart framework through the
Internet of Things and machinelearning for precision agriculture
Veeramuthu Venkatesha, Pethuru Rajb, and R. Anushia Devia
aSchool of Computing, SASTRA Deemed University, Thanjavur, India
bReliance Jio Cloud Services (JCS), Bangalore, India
Contents
1.Introduction 281
2.Existing infrastructure in agriculture 282
2.1 Limitations in current agriculture infrastructure 282
2.2 Intelligent workforce infrastructure 284
3.IoT ecosystem —A complete view 285
3.1 IoT devices 285
3.2 Communication technology 286
3.3 Data storage and processing 286
3.4 Knowledge layer 287
3.5 Service layer 287
4.Agricultural monitoring system based on sensors 287
4.1 Field assessment 287
4.2 Cattle behavior control 288
4.3 Traditional agricultural monitoring 288
4.4 Intelligent IoT based irrigation system 289
5.Difficulties in sensor-based agribusiness observing frameworks 290
5.1 Cost 290
5.2 Reliability 290
5.3 Resources 290
6.Factors affecting climatic changes in savvy agribusiness 291
6.1 Climate-brilliant horticulture 292
6.2 Key highlights of atmosphere shrewd rural scenes 292
6.3 Climate-keen practices at field and homestead scale 292
6.4 Diversity of land use over the scene 293
7.AI in agriculture —An introduction 295
8.Machine learning techniques for smart agriculture 295
8.1 Wide division of machine learning algorithms 295
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0072799.Artificial neural network (ANN) 296
9.1 Artificial neural networks in agriculture 296
10. Automation and wireless system networks in agriculture 296
10.1 Smart agriculture 297
10.2 Smart farming (SF) 297
10.3 Smart IoT agricultural revolution 298
10.4 Smart system design methodologies 298
11. Hardware components in the smart agriculture system 299
11.1 Humidity sensor 299
11.2 CO2sensor 299
11.3 Moisture sensor 299
11.4 Rain drop sensor 300
11.5 Ultrasonic sensor 300
12. Use cases 301
12.1 Solar fertigation: Internet of Things architecture for smart agriculture 301
12.2 Wireless sensor based crop monitoring system for agriculture using Wi-Fi
network dissertation 302
12.3 Secure smart agriculture monitoring technique through isolation 302
12.4 Realizing social-media-based analytics for smart agriculture 302
13. Conclusion 303
References 303
About the authors 304
Abstract
Today, development in IoT, Information and Communication Technology (ICT) andWireless Sensor Networks (WSNs) have the potentiality to turn out few economic, envi-ronmental, and technical challenges along with opportunities in the ecosphere. It gen-erates a large amount of data with numerous modalities, temporal and spatial variations.It is mandatory to develop a higher knowledge level based on an intelligent system toanalyze this big data for accurate decision making, estimating, and dependable sensor
management. There are several applications based on IoT with machine learning tech-
niques that are globally available. This chapter focuses on advanced techniques used insmart agriculture systems based on IoT and machine learning algorithms. Several typesof research are completed on this system to offer smart services for real-time monitoringof any agricultural environment. The IoT based smart agriculture systems are the mostgifted approach to enhance the productivity of food items by reducing power andwater consumption.
Abbreviations
IoT Internet of Things
ML machine learning
SA smart agriculture
WSNs wireless sensor networks (WSNs)280 Veeramuthu Venkatesh et al.1. Introduction
The IoT paradigm has progressed to be one of the largest advance-
ments in the technology of the new age science in the past few years [1].
It
combines the advantages of several pre-existing technologies such as
the Wireless Sensor network, cloud computing, RF identification, mid-
dleware components and end-user applications. IoT enabled devices like
tablets, computers, and mobile phones can access information on the
surroundings and similar objects without any intervention by humans.Among the key features of IoT are RFID and WSN. A Wireless SensorNetwork is meant to monitor the environment it is deployed in and records
the physical conditions with relatively cheaper data acquisition techniques
and is composed of spatially dispersed nodes. Internet of Things (IoT) is arapidly developing technology all over India. But, the majority of the pop-ulation (around 70%) here depend on agriculture.
Around 60% of the land can be plowed and used to grow rice, potato,
wheat, onion, tomato, mangoes, sugar cane, bean, cotton, cereals, etc. Oneunfortunate fact would be that farmers are still predominantly dependent onthe traditional methods and techniques that have not evolved and werebeing used several years ago. This could be a major reason for the lower yield
of crops. Also, several contributors might lead to lower crop yield such as
seed rate, proper soil preparation, seed cultivar, lack of sufficient amountof moisture in the field, the difference in sowing time, salinity, waterlogging,not applying necessary fertilizers, protection of plants, not adopting modern
technologies and machinery, misleading marketing strategies and lack of
solid investment. Apart from these factors, farmers suffer huge financiallosses due to the incorrect usage methods for irrigation, pest and insect con-trol, and the prevention of plant diseases. Improper usage and miscalculation
in the quantities of insecticides and pesticides and wrong weather forecast
further contribute to their losses. To get a higher crop yield, monitoringwould be the key task for the farmers. Because of the existing constraintson agriculture, we need to take immediate measures to develop enhanced
and economically viable strategies to cultivate crops. This chapter proposes
an Internet of Things (IoT) based, user-centric architecture for helpingfarmers make use of the advancements in technology to improve their cropyield and increase profits in selling their products.
The IoT application in agriculture is meant to empower farmers with the
necessary tools to make better decisions and employ automation techniques281 A smart framework through the IOT and machine learningthat effortlessly combine knowledge products and several other services that
will help in generation productivity, profit, and quality [2]. Using sensors
parallelly
with intelligent algorithms can make smart recommendations
regarding field quality maintenance to yield high-quality crops.
2. Existing infrastructure in agriculture
The existing infrastructure in the agricultural domain can be catego-
rized based on the kind of work it performs, such as processing, production,
consumption, distribution and management of waste, excluding the cyber-infrastructure and the development of the workforce [3]. An instance could
be 
that the infrastructure related to food production includes farming equip-
ment like combine-harvesters, tractors, irrigation infrastructure and dusteraeroplanes. In the processing of food, infrastructure includes processingplants and food factories. The distribution of food infrastructure comprisestransportation systems such as trucks, railroads, barges, ships, etc., retail stores
and supply chains. Apart from the different infrastructures mentioned, agri-
culture also requires energy and water infrastructure. The water infrastruc-ture composes the pumping systems, natural and built structures for thestorage of water, networks monitoring water management and governance.
The infrastructure related to energy composes the electric grid, biofuel
production, and carbon-based energy production such as fossil fuels.
2.1 Limitations in current agriculture infrastructure
The current infrastructure mentioned has several challenges and limitationsthat need addressing to achieve intelligent agriculture infrastructure. Thelimitations are illustrated below in Fig. 1 .
2.1.1 
Social limitations
Some social challenges include the aging workforce, shortage of labor, andthe lack of urban community’s involvement. Presently, the various kinds ofcrop production demand high labor. Also, it has been noted that the average
Limitations
Social Environmental Technical Inherent Organizational
Fig. 1 Limitations of current agriculture.282 Veeramuthu Venkatesh et al.age of a farmer is around 60years. With these facts made available, it is evi-
dent that this domain will face labor shortages in the upcoming decade.
2.1.2 Environmental limitations
The pollination factors such as bees and butterflies declining rapidly is a sim-
ple example of an environmental challenge that a farmer faces in his daily life.Unpredictability in climatic conditions is also one huge factor that hindersthis domain.
2.1.3 Technical limitations
Technical Limitations include the inadequacy of cyberinfrastructure since
the internet bandwidth is limited in predominantly remote farming areas.The GPS services used for placing the farming equipment precisely are vul-nerable to spoofing and jamming. Apart from the problems already stated,there is inadequacy in the mechanisms that share agricultural data. This data
is sensitive and needs to be shared in a highly protected manner. The differ-
ential privacy, the best-of-the-breed protection methodology, is insufficientfor this data because of its spatiotemporal nature. Due to this, farmers arewidely reluctant to share their data. The remote sensing satellites provide
coarse resolution images, not frequently monitoring the crops or the farming
area, introducing a delay in the condition detection and preventing us fromtaking corrective measures early. Automation levels vary significantly in thefarming sectors. For instance, certain farms like pecan, corn and wheat have a
much higher automation level than the vegetable and fruit crops. Manual
interventions are also frequently needed since the fruits or vegetables donot become ripe together at a particular time. Hence, picking them upperiodically as it ripens becomes necessary.
2.1.4 Inherent limitation
Farmers manually predict when to apply fertilizers and how to manage land.
These predictions are often prone to errors since they are based on approx-
imations and assumptions.
2.1.5 Organizational limitation
The supply chain lacks visibility and transparency. This is one of the majororganizational limitations.
Having mentioned the limitations that exist in the various domains
within the agricultural sector, it is time that we build smart solutions for
we now have the power of technology and educated youths who can283 A smart framework through the IOT and machine learningcontribute wisely towards the growth of this sector. A good broadband and
internet connection to these areas could be among the starters.
2.1.5.1 We still get food and the necessary things to make our living. Does that
mean the agricultural sector is functioning well?
In Assam a majority of state’s population, almost 90% of an estimated 33.4million in 2001, live in rural areas where the mainstay of business is produc-
tion agriculture. In terms of the domestic production (SPD), the agriculturesector contributed over 40% of the state income in 2000 –01. Assam is far
behind in the use of modern agricultural technology to improve its agricul-
tural productivity compared to the rest of the country. For example, the
agricultural productivity index for Assam was 201 in 2010 –11 compared
to 179 in India.
2.1.5.2 What could be the cause?
There is still slavery in Assam and farmers wo constitute the majority of thepopulation are faced with poverty. Their lack of knowledge about modern
technology, illiteracy, lack of knowledge about market demandable agricul-
tural commodities, irrigation system, flood, drought etc. are the primaryreasons for why the productivity is still lower in that region. Let us nowsee what could be done to increase the productivity and make the farmer’s
life better.
2.2 Intelligent workforce infrastructure
To improve the agricultural industry and bring them at par with the othersectors, it is essential to: [4–6]
(1)Train farmers frequently and get them updated on the recent technol-
ogies
so that they are ready to embrace any advancements that will
contribute to the improvement in crop yield or crop quality
(2)Find methods to involve the next generation in this sector
(3)Find ways to involve employees from various engineering sectorssuch
as computer science and mechanical mining into the agricultural
sector and
(4)Go beyond the traditional methods to engage citizens.
To achieve this and to bring youngsters into this domain:(4a). We need to find ways to transform agriculture into a “cool” conceptin
the minds of the forthcoming generations. By introducing modern
technology like drones and robots and highlighting their application
in various stages of farming, we can gamify the entire process to attract
young minds.284 Veeramuthu Venkatesh et al.(4b). Small workshops and teaching events can be held to introduce the
farmers
to these new technological tools and give them hands-on
experience as to how to use the tools.
(4c). Augmented and Virtual reality can also be exploited when it comes to
the
agricultural domain. These platforms could be built in a
cost-effective and scalable way to make the training process fun
and realistic. Furthermore, associating the process of farming with
video games can help involve young minds. This could also be a fac-tor that triggers various ideas and solutions to existing problems in theagricultural domain.
(4d). we need to develop teleoperation facilities to engage more people inlabor-surplus
areas on the farm. This will help us bridge a gap in the
economy by bringing unemployed people from the labor shortageareas into the areas where intense labor is needed and places where
there is a workforce shortage.
3. IoT ecosystem —A complete view
This section is meant to provide an insight into the components that
make up an IoT ecosystem [7]. The five major components as illustrated in
Fig. 2 .
IoT devices
Communication technology
Data storage and processing
Knowledge Layer
Service Layer
3.1 IoT devices
IoT is a collection of sensors and actuators that interact through a wireless
medium. They are embedded devices programmed to carry out a particular task.
IoT devices
Connectivity
Storage
Knowledge
based system
Fig. 2 IoT ecosystem —components diagram.285 A smart framework through the IOT and machine learningThey consist of the FPGA or microprocessors and have input and output
interfaces to take in signals and send out signals. Communication modules
are responsible for handling the commutations within the network andsending the signals. The information that these sensors gather is not onlysensory information. They monitor the field on various factors like soilmoisture, rainfall, wetness in the leaves, speed of the wind, air temperature,
and other essential factors for the healthy growth of plants.
3.2 Communication technology
For a successful deployment of the IoT systems, Communication technol-ogy plays a key role. It is responsible for the data transmission between the
devices connected as a network. They can support either the IoT sensors orthe backhaul network. While IoT devices transmit lesser data over very shortdistances, the backhaul networks transmit data over long ranges and at a veryhigh speed with little power consumed. Some technologies even provide
bi-directional communication. This includes error correction techniques,
handshaking signals, data reliability, and encryption. The type of topologyof the network also influences the kind of communication technologydeployed within it. Some of the commonly used topologies are peer to peer,
star, and bus topology.
3.3 Data storage and processing
We know that an IoT ecosystem comprises the collection of the ever-
changing data and analyzing them to ensure that the environment in which
the crops grow is healthy and crops give their maximum yield. The data col-
lected is highly unstructured and can be of any form like audio, video,text—one prominent location for the storage of these data that are collectedis the cloud. The data are mostly analyzed in the data using fog or edge com-
puting. The gateways within the devices perform the necessary computa-
tions and analyze them to lessen the latency associated with the cloud andreduce the cost of computations. Users are also an important source of datathat we do not want to miss out on. The information gathered through their
circles, either professional or casual, are valuable resources to the domain.
Their circles can also be their place for information sharing and help inleveraging the delivery of services. Many different management informationsystems have aided in effectively managing this information. Some examples
are Far mobile, On-farm systems, Crops, Easy farm and so on. These provide
a platform for managing and storing farm data.286 Veeramuthu Venkatesh et al.3.4 Knowledge layer
Efficient farming mechanisms are the need of the hour. It requires methods
that consume minimal resources but yield maximum output. Essential
parameters need to be monitored to ensure an effective outcome. The datacollected needs to be stored, and then these relevant parameters need to beanalyzed. To send efficient warning messages, it is required that appropriate
parameters are chosen and monitored. Data respective to each kind of crop
have their set of conditions for relevant warning messages to be sent. Toensure that resources are spent appropriately, measures need to be takenconsidering the environmental conditions.
3.5 Service layer
This layer is associated with the end-users. Three major services are providedto the stakeholders. They comprise the fertilization schedule, irrigation
schedule, and an app to analyze the crops.
4. Agricultural monitoring system based on sensors
An approach to ensure that crops receive what is essential for their
healthy growth is called precision agriculture [8]. It has been devised to
enhance
crop productivity and improve yield. The goal is to ensure sustain-
ability, profitability, and environmental protection. Precision agriculture is
also known as satellite agriculture. Satellite is used to monitor the crop, andthis farming method is site-specific. Modern farmers have evolved and madeuse of smart farming tools. Researchers working in precision agriculture
have come up with concepts that engage modern technologies in everyday
farming practices. With the deployment of these tools and ideas, we caneffectively monitor the plants in terms of different parameters mentionedin the previous section. Prototype with maps of the crop fields accelerates
the planning and monitoring process. They also give the farmers and the
planners an idea of where to deploy the sensors and what topology toimplement.
4.1 Field assessment
As mentioned as an inherent limitation, various predictions and irrigationmethods are made by farmers manually based on assumptions and experi-
ence, if any. To overcome these limitations, low-resolution cameras and
sensor networks are deployed. Solar power nodes that monitor the moisture287 A smart framework through the IOT and machine learninglevel are used to collect data that are then analyzed. The captured data, such
as the greenness of the crops, are then sent to the base station. The base sta-
tion sets aside some time for the member node to transmit data to it. Cattleare also monitored using similar techniques.
4.2 Cattle behavior control
Monitoring and handling animals has always been a challenge. Their behav-iors are not easily predictable, and their mental or positional state is difficultto calibrate. Behaviors can be a consequence of various factors such as theclimate, temperature, kind of food, and similar factors. Animals are active
predominantly during the daytime than at night. Their activity images such
as sleeping and movement are sent for calibration. Inertial sensors collectinformation like the speed of movement of the animal, turning rate andso on. These help us identify their behavior.
4.3 Traditional agricultural monitoring
Tension meters and drip irrigation systems are employed to monitor the irri-gation in the farmlands. Fuzzy logic controllers are employed to monitorindependent crops [9]. The fuzzy logic controller has been implemented
for
an efficient irrigation system for fields with different crops. This increases
the accuracy of the collected values and accordingly aids in making betterdecisions. Modern agriculture that is based on greenhouse concepts needsto be accurately controlled to monitor temperature and humidity. Theatmospheric conditions of plants inside the greenhouse vary from place to
place, making it difficult to maintain uniformity at all places in the farmhouse
manually. So, GSM has been used to report the irrigation status to farmers’mobile handsets. The sensor-based irrigation monitoring system is dividedinto the bottom and upper layers, as shown in Figs. 3 and 4 . Hierarchical
sensor
networks are placed in the bottom layer, where nodes are placed
in widely separated clusters. These nodes send the data to the Base station(BS) connected by a Wireless LAN that holds the data logger software.The upper layer consists of five modules: the “acquisition module, network
management module, alarm/network status display module, decision mak-
ing module and business module.” Real and non-real-time data are collectedusing the data gathering modules from the sensor network stored in a data-base for decision/alert notifications. Alert notifications or displaying infor-
mation to the end-users are carried out by the Alarm/Network status display
module. The Alarm/Network status display module acts as an access pointbetween end-users and other modules/networks.288 Veeramuthu Venkatesh et al.The network management module performs the conditions of networks
such as localizations, collision, and network configuration. This developedsystem has introduced the concept of M2M communication, where waterand energy are conserved by using an intelligent sensor network and efficient
routing protocol.
4.4 Intelligent IoT based irrigation system
The existing Agricultural monitoring system has employed wireless sensors
for monitoring the soil condition for irrigation. Also, some of the systemshave employed mobile handsets for delivery. Intelligence does not existUser User User
Internet
Network
Management
ModuleModule for
Network Status
Display
Business ModuleDecision Making
Module
Data Base
Data Acquisition
Module
Fig. 3 IoT based smart agriculture monitoring system.
User User User
Internet Data
Store
Base Station Base Station
Sensor Network Sensor Network
Fig. 4 IoT based traditional agriculture monitoring system.289 A smart framework through the IOT and machine learningin any of these systems to analyze the real-time data based on experience for
irrigating the field. Most of the system captures the data from the field and
accordingly controls the sprinkler valve for watering the field.
5. Difficulties in sensor-based agribusiness observing
frameworks
This sensor-based farming observing framework should contain a
choice assert framework to expand creation, redesign resource use, and
decrease natural risks [10–12]. Furthermore, this framework would be
autonomous,
sensible (by poor farmers), precise and remotely controllable.
The time of this sensor-based farming observing framework must be as a base
one yield per season (4 - a half years). Anyway, the sensor-based farmingobserving framework had a high likelihood in traditional agribusiness in
rising nations.
5.1 Cost
Cost is the utmost significant factor on account of emerging nations. Cost
can be diminished by methods for sensor-based farming framework commu-nication networks that are not currently being used in agricultural monitor-ing systems. Moreover, this single framework of the sensor-based farming
observing system can be utilized in multiple applications and exactness hor-
ticulture, requiring thickly conveyed camera and sensor hubs.
5.2 Reliability
Reliability on sensor and camera hubs regardless of the atmospheric condi-tions in most creating nations is extraordinary and rare. Subsequently, sensorhubs must be covered to shield from open-air conditions, including heat andmoisture. One significant example is the accessibility and reliability of media
communications and the remote infrastructure in rustic territories of creat-
ing nations being extremely troublesome.
5.3 Resources
All over the world, the vast majority of poor farmers depend on downpourto cultivate for the creation of nourishment. In addition, conventional watersystem philosophy isn’t efficient in semiarid zones in creating nations. Thus,a sensor-based agriculture monitoring irrigation system with a decision sup-
port system is required to maximize irrigation and food production while
minimizing the intake of good water.290 Veeramuthu Venkatesh et al.6. Factors affecting climatic changes in savvy
agribusiness
Environmental change will impact crop dissemination, creation and
increment dangers related to cultivating. All-inclusive, environmental
change is relied upon to diminish grain creation by 1% to 7% by 2060. Inany event, 22% of the developed region under the world’s most significantyields is anticipated to encounter negative effects from environmental
change by 2050. Vulnerabilities in atmosphere systems could likewise
impact ranchers’ decisions and whether they will put resources into funda-mental sources of info and assets for their territory.
Let us currently take a gander at a little nation to know why we need
atmosphere brilliant farming.
The Republic of Bulgaria is arranged on the eastern Balkan Peninsula. The
fundamental climatic attributes of Bulgaria are: calmly mainland andsubtropical (in the south) atmosphere with four seasons and high varietyin the temperature, precipitation and moistness among the nation districts.Mountains spread 60% of the nation region as the streams are short,low-water and unevenly apportioned through the nation. Because of themainland atmosphere the mid-year in Bulgaria is hot and the winter—dryand cold. There are droughts in summers in July and August. The measure
of precipitation is commonly low with varieties among the districts.
West and upper east breezes rule and, in the winter, there are solid north and
upper east breezes. In light of the solid and consistent breezes, the snowspread is frequently overwhelmed from the level territories and the dirt getsfrozen. In end, Bulgarian agrarian creation is downpour taken care of, sig-nificantly relies upon precipitation systems and atmosphere changes are a
significant factor for rural advancement of Bulgaria.
You may think with such extraordinary conditions why intend to
develop crops? Be that as it may, Farming assumes a critical job for the econ-omy in Bulgaria. About 5% of GDP and 17.2% of complete fare of thenation in 2009 were given by agribusiness.
Presently it sure isn’t anything but difficult to simply stop farming
for them.
So what do the individuals in provincial territory accomplish for the liv-
ing when the atmosphere isn’t favorable to them?
The following segment gives them one arrangement.291 A smart framework through the IOT and machine learning6.1 Climate-brilliant horticulture
Research and strategy interfaces between environmental change and farm-
ing have propelled. “Brilliant atmosphere horticulture” has risen as a struc-
ture to catch the idea that rural frameworks can be created and actualizedalong with improving nourishment security and country employments,encouraging environmental change adjustment and giving moderation
benefits. Atmosphere brilliant horticulture incorporates huge numbers of
the field-based and ranch based economic agrarian methods that the boardrehearses in writing and puts it to wide use, for example, preservationculturing, agroforestry, building up the board, and others.
6.2 Key highlights of atmosphere shrewd rural scenes
Atmosphere keen horticultural scenes work on the standards of the incor-porated by the board while unequivocally coordinating adjustment andrelief into their administration destinations. An evaluation of environmentalchange elements identified with horticulture proposes three key highlights
that portray a keen atmosphere scene: shrewd atmosphere practices at the
field and ranch scale; a decent variety of land use over the scene; and theboard of land use communications at scene scale.
6.3 Climate-keen practices at field and homestead scale
Atmosphere brilliant scenes have contained an assortment of field and ranch
rehearsals, in various land and residency types, that help both adjustment and
relief destinations. These practices incorporate soil, water optimization andsupplement the board alongside agroforestry, domesticated animals, farm-ing, woodland, and field procedures. Progressively, effective water admin-
istration and assets undermined by environmental change is likewise basic for
arriving at the adjustment and occupation objectives of atmosphere smartfarming. Best practices for water systems, water-reaping innovation, and farmcultivating frameworks can improve water-use proficiency and preservation.
Especially in semi-dry and dry areas, where water assets are a worry, interest
in the water system builds creation, lessens inconstancy, and may spike extrainterest in horticulture. Improved plan, development procedures, and waterconveyance instruments can reduce the exceptionally high GHG emanations
related to customary water system frameworks. Utilizing coordinated supple-
ments, board standards like green composts, planting nitrogen-fixing crops,and consolidating animals’ fertilizers into the dirt, diminish the measure of292 Veeramuthu Venkatesh et al.nitrogen lost to overflow and outflows of nitrous oxide. Applying these
administration standards can serve adjustment needs by improving soil quality
while diminishing ranchers’ expenses and reliance on outside sources ofinfo. Domesticated animals are especially basic for atmosphere savvy horticul-ture. Improved field and meadow executions including rotational nibbling,recovering vegetation and reestablishing corrupted land, will be basic for the
versatility of environmental change. They additionally add to relief through
carbon sequestration in profound established vegetation and soils. For betterfertilizer, changing over excrement to biogas gives the additional advantagesof an elective vitality source with less negative wellbeing impacts from
cooking, warming, and lighting. Improved feed blends and healthful
enhancements can diminish methane outflows; be that as it may, this is pro-gressively practical at bigger activity sizes.
6.4 Diversity of land use over the scene
The second component of brilliant atmosphere scenes is an elevated level of
decent variety. This incorporates land spread, land use, species, and a varietal
of assorted plants and creatures. Assorted variety have fewer atmosphererelief and adjustment capacities: (1) to decrease dangers of creation and voca-tion misfortunes from whimsical and unforgiving climatic conditions; (2) touse zones of the scene deliberately as crisis nourishment, feed, fuel, and salary
stores; and (3) to continue to insignificantly upset natural surroundings inside
the scene mosaic that likewise fills in as carbon stocks.
6.4.1 Reduce hazard
A decent combination of land use and species can decrease natural dangers
related to a homogeneous yield spread, regarding vermin, illnesses and
powerlessness to sudden climate conditions. Improving hereditary assortedvariety on ranches by expanding the number of various harvests developedor the number of assortments of similar yields, additionally gives significant
atmosphere adjustment and decreases the hazards to the executive’s benefits.
Yield hereditary decent variety improves the odds that a few assortments willfit shifts in temperature, precipitation, and saltiness systems brought about byenvironmental change. Additionally, having an arrangement of assorted
nourishment and salary sources, from crops, domesticated animals, trees,
and non-developed terrains can pad family units and networks from climatic(and other) mishaps.293 A smart framework through the IOT and machine learning6.4.2 Provide key nourishment and feed saves
Occupation flexibility of family units and networks can likewise be
upgraded through access to various wellsprings of nourishment, feed andwork during scenes of unfavorable climatic conditions. Wild plant speciesin ranches, backwoods, savannahs and wetlands contribute to the eating reg-imens of huge numbers of the poor in creating nations and these nourish-
ment sources, especially the “starvation nourishments.”
6.4.3 Sustaining seasonal forest as a carbon resource
In today’s supreme cultivation system, we are mostly involved with annual
plant species. The topography which maintains other lands such as endless
grasslands, woodlands, forests, or wetlands enhances the environmental flex-ibility. In turn, it also improves the living surroundings for local people andbio diversification. To cut off the carbon emissions from the topographies,preserving land areas in these types of eternal systems is mostly required.
6.4.4 Effective functions of the ecosystem
To preserve the ecological connectivity between water and nutrients andimprove the living conditions of wild animals, plant groups, and microor-
ganisms favorable to humankind, natural living means like riverside areas,
woodlands and wetlands should be set and maintained. A fundamental trans-formation approach is required to have the relation between freshwaterresources and wildlife localities when the variations in climate increase.The methods used in achieving the cultivation productions should help
us maintain this relationship instead of obstructing it. For domestic purposes
like household and irrigation, rainwater can be preserved and used. Theconstructive methods of managing the crop waste and water are necessaryacross the river basins to control the ailments in humans and animals.
6.4.5 Improve the advantages of environmentally smart policies
on the ground
The preparatory measures of topographical elements boost farming produc-tivity. This farming productivity depends mainly on the lands used. The sur-rounding lands should have a nature to hold the insects and pest predators,
which are beneficial to the parts of the land nearer to the forest helping the
agricultural land in sustaining the pollination. To regulate the downstreamfisheries, the aliments and sediments from the farmlands can be used. Whilelivestock, forest production, and the upstream crop can be maintained to
enhance water regulation for downstream irrigation, livestock wastes can
be a replacement for nonrenewable fuel sources in fulfilling the local agroamenities.294 Veeramuthu Venkatesh et al.7. AI in agriculture —An introduction
AI has been practiced in almost all fields, from agriculture to medical
sciences. With machine learning it is possible to train machines to the extent
where they can predict the output without human intervention, and it is asubdivision of AI [13,14] . Many applications of AI can be used in the farming
industry, 
such as agricultural robots, which work faster than the human rate
of activity, crop & soil monitoring, and predictions on climatic changes.
The assistance of AI in the agriculture field results in healthier crops,
monitoring soil temperature and humidity, and many such tasks. We getlarge amounts of data from the fields every day; analyzing such data using
different machine learning algorithms helps make better decisions.
Implementation of drones in farms using IoT, wireless technology inte-grated with sensors as per the application requirement gives the data aboutcrop condition, spraying pesticides etc.
Decisions are made based on the data collected from sensors, and the
machine learning algorithms analyze that data. Integration of IoT and AImonitors every step of crop growth.
This makes the farmers predict the yield in advance based on the infor-
mation from the ML algorithm regarding climatic conditions and drastic
weather changes that may result in natural disasters helping us take certain
measures to save the crops and farms.
8. Machine learning techniques for smart agriculture
The ability of a machine to learn from the previous results is possible
only with Machine learning. Its algorithms use statistical methods to train
themselves from the dataset. The effectiveness of an algorithm depends
on its performance, i.e., how well it can adjust with the increasing numberof samples during training. The data which comes from the sensor containsnon-linearity in data, the usage of Machine learning techniques can takesuch problems to care. Thus, we can expect the right decision making with
less human interference. Estimation efficiency is influenced by data han-
dling, reference model presentation, and the correlation between sourceand reference variables.
8.1 Wide division of machine learning algorithms
This division includes supervised and unsupervised learning. Supervisedlearning uses a defined set of labeled data to train a model to predict the target295 A smart framework through the IOT and machine learningvariable for the sample data. The purpose of the unsupervised learning
approach is to find hidden patterns. It is mainly practiced in applications with
no particular purpose or where the information present in the data is unclear.This is also suitable as a method to reduce the dimensionality of data with avariety of applications. These wide divisions are mostly used in smart tech-nologies like IoT in a variety of domains. For example, WSN, IoT allows
smart farming where ML techniques evolve to measure and know the big
data in this field. The management deals with the estimation of yield, weedidentification along with livestock and phenotype classification.
9. Artificial neural network (ANN)
An artificial neural network is an information processing system with a
performance similar to a biological neural network. Chains of decision units
such as perceptron’s or radial basis functions are cascaded to construct
this learning algorithm mainly used to recognize non-linear and complexfunctions. The neural network is mainly distinguished based on (1) its archi-tecture (pattern of connections between the neurons), (2) its algorithm
(method of determining the weights on the connections), and (3) its func-
tion of activation. The architecture of the ANN algorithm is designed withinput units, single or multi-layer hidden units, and output units. ANN canalso be used to solve the problem of classification and regression. ANN
learning algorithms implementation include the radial basis function,
perception algorithms, back-propagation, and feed-forward propagation.
9.1 Artificial neural networks in agriculture
The agriculture sector is often incorporated with Artificial neural networksdue to its superiority over traditional systems. A neural network takes thebenefit of predicting and forecasting its result based on parallel reasoning.
Moreover, neural networks can be trained instead of thorough programming.
10. Automation and wireless system networks
in agriculture
Embedded intelligence (EI), the result of emerging research in the
automation field, made inventions and breakthroughs that made the agricul-ture sector adapt to it. Smart farming, smart crop management, smart296 Veeramuthu Venkatesh et al.irrigation and smart greenhouses are some of the outcomes of embedded
intelligence in the agriculture sector. For a growing nation, it is necessary
to include these growing technologies in the agriculture sector as manysectors are interdependent on agriculture.
10.1 Smart agriculture
The process of producing food and fiber to meet the fundamental need iscalled Agriculture. Agriculture is limited by disputes between nations andenvironmental limitations throughout history. Nowadays, an increase in
global warming leads to changes/imbalances in the climate conditions,
which in turn impacts agriculture; as a result, sustainability and the increaseof agricultural production is possible only with the collection of more dataand analyzing the use of them. Detection and transmission of necessary datacan be possible only with the efficient use of information technology. Even
though different productivity is obtained from different regions of farms due
to different soil structures, it aims to distribute the inputs such as fertilizersand chemicals used to grow crops based on farms’ needs. However, the con-cept “Sustainable Agricultural Production” aims to protect the environment
and the natural resources for the past 2 decades, which gives special impor-
tance to using these inputs in as little amount as possible and with as muchcare as possible. In this situation, variation in agricultural production needsto be measured, and the inputs need to be applied only after considering the
results. To be successful in this process called precision, agriculture reliability
and continuous data is needed.
10.2 Smart farming (SF)
Incorporating communication and information technologies into equip-ment, sensors and machinery for use in agricultural production systems,grant to generate a large volume of data and information with insertionof automation into the process, result in Smart farming. New and leading
technologies like the Internet of Things (IOT) and cloud computing are
expected to accelerate this development by introducing more artificial intel-ligence and robots into farming. Smart farming relies on making decisionsfrom analysis of various farm data transmitted and concentrated in a remote
storage system. SF originated with the combination of computer science
and software engineering and arrived with computing technologies andthe transmission of data from agriculture, with the overall computing297 A smart framework through the IOT and machine learningenvironment. These elements of computing are embedded in objects and
linked with each other and the internet. The SF field includes other terms
with similar meanings, such as smart agriculture. Farm management infor-mation systems in the agriculture field presented ideas for technologies likeprecision agriculture and management information systems, which helped inoverlapping technologies and interfaces accordingly for SF. The use of sen-
sors in agriculture made the tools of SF achievable. A sensor is an electro-
magnetic device that collects measurable quantities from the environmentand converts them to signals that an instrument can read. Sensors read mea-surements like temperature, humidity, light, pressure, noise levels, presence
or absence of certain objects, mechanical stress levels, speed, direction, and
object size.
10.3 Smart IoT agricultural revolution
Intelligent Agricultural Revolution refers to using and integrating the latesttechnology such as IOT more widely in agriculture to increase the quantity
and quality of harvests of domestic crops. Through this system, farmers can
monitor and control agricultural activities from anywhere using a smart-phone through a specially created application. Through IOT and smart agri-culture, we invite young farmers involved in agriculture. It does not involve
brute strength but is entirely driven by the technology at our fingertips. For
example, the drone can be used to spray insecticides, analyze and monitorthe results of soil cultivation of plants quickly and without using a work-force. In addition, sensors or sensor-based Internet of Things (IOT) can alsotransmit data or information related to the plant immediately (real-time) for
further action by the farmers.
10.4 Smart system design methodologies
This section gives the details about the design and implementation of the
Smart Agriculture System. People interested in agricultural developmentfor more production and faster results about the humidity of the soil, air
pressure, temperature, and location of sensors can use this system. Smart
Agriculture System uses the communication between Arduino andRaspberry Pi, which implements the Machine-to-Machine technology.Communication using devices like wireless sensors is preferred. With the298 Veeramuthu Venkatesh et al.help of the internet, data is transferred by sensors to Android-based mobile
devices. As a result, the Smart Agriculture System is a combination of
software and hardware components.
11. Hardware components in the smart agriculture
system
11.1 Humidity sensor
Humidity sensors sense the relative humidity of the environment in which it
is placed. They measure both moisture and temperature in the atmosphere
and express relative humidity as a percentage of the ratio of moisture andtemperature in the air to the maximum amount held in the air at the currenttemperature. As the hotness of air increases, the relative humidity changes
because it holds more moisture.
Most humidity sensors use capacitive measurements to determine the
amount of moisture in the air. This type of sensor reads measurements byplacing two electrical conductors and a non-conductive polymer with a filmbetween them to create an electric field between them. Moisture from air
assembles on the film and creates a voltage difference between those two
plates. This change is then converted into a digital measurement.
11.2 CO 2sensor
A carbon dioxide sensor or CO 2sensor is used to measure the quantity of
carbon dioxide gas in the air in which it is placed. The most common prin-ciple of CO
2sensors uses infrared gas sensors (NDIR) and chemical gas
sensors. The CO 2sensors measure CO 2levels by observing the number
of infrared radiations being observed by CO 2molecules. Similar to CO 2
sensors, a carbon monoxide sensor or a CO sensor detects the presence ofcarbon monoxide (CO) gas to prevent the environment from gettingpoisoned.
11.3 Moisture sensor
The Moisture sensors are used to detect the amount of moisture in soil orjudge if there is water around the sensor. As soil moisture measurement playsan important role in agriculture, it helps farmers manage their irrigation299 A smart framework through the IOT and machine learningsystem more efficiently. By knowing the exact condition of soil moisture in
their fields, farmers reduce water usage and increase the yield and quality of
the crop.
11.4 Rain drop sensor
The Raindrop sensors are used to detect rain. A raindrop sensor is a board
on which lines of nickel are coated. The simplicity of raindrop sensors
made it easy to detect rain. It can also measure the rainfall intensity andis also used as a switch when raindrops fall on the raining board. It usesthe principle of resistance. Raindro p sensors also measure the moisture
via analog output pin and provide a digital output when the moisture
exceeds the threshold. Rain sensors come under the classification of
weather sensors. It takes advantag e of detecting water beyond a humidity
sensor can detect.
11.5 Ultrasonic sensor
Ultrasonic sensors use sound waves to measure the amount of water leftinside a water tank. This helps farmers in making further arrangementsfor spare water tanks avoiding risk.
11.5.1 Implementation using raspberry PI 2 model B
A sensor network has tiny objects installed in different wireless sensor net-work monitoring areas to measure various physical data to finish a specifictask. Improvement of growth in various crops depends on environmental
parameters like light intensity, soil moisture, relative humidity, soil temper-
ature, usage of fertilizers and pH, etc. Any minor changes to these parameterscan cause problems like improper growth of crops and diseases in plants, etc.,resulting in less crop yield.
Fig. 5 shows the proposed smart agriculture, consisting of a transmitter
section
(i.e., mobile controlled robot) and a monitoring section. The trans-
mitter section consists of Raspberry Pi 2 Model B, various sensors such ashumidity sensors, pH sensors, Thermohygro sensors, CO
2sensors, Soil
moisture sensors, Obstacle sensors, Power supply section (i.e., solar plate),ZigBee transmitter, a Wi-Fi modem, Water sprinkler, Dc motor forspraying insecticides, an LM380 audio power amplifier, speaker, Liquidcrystal display (LED), and camera. At the same time, the monitoring section
consists of an Android smartphone, ZigBee receiver and a Laptop with the
application language. Raspberry Pi 2 Model B recommends Raspbian as its300 Veeramuthu Venkatesh et al.operating system, free OS based on Debian, optimized for Raspberry Pi
hardware. It consumes low power with a high-performance controller for
interfacing various sensors and performing the required task based on thewritten program.
12. Use cases
12.1 Solar fertigation: Internet of Things architecture
for smart agriculture
Using the industry 4.0 framework, Smart agriculture is a good Internet of
Things (IoT) application domain. Furthermore, change in climate affects
the natural resource exploitation policies becoming a serious matter in agri-
culture and food production. Therefore, monitoring environmental param-eters and agricultural processes continuously helps in resource optimizationand maximization of food production. This work proposes the conceptual
model and the design of Solar fertigation, an IoT system specifically designed
for smart agriculture. In particular, the envisioned solution can detect someof the most meaningful terrain parameters to feed a decision-making processthat drives automated fertilization and irrigation subsystems. In addition,
Solar fertigation is powered by a photovoltaic plant to attain energy self-
sustainability.Thermo Hygro Sensor
Soil Moisture Sensor
Humiditiy Sensor
UV Sensor
Obstacle Sensor
CO2 Sensor
pH Sensor
PIR Sensor CameraWi-Fi ModemLM 380 SpeakerSprayer DC MotorWater SprinklerMAX 232 Serial Raspberry
Pi 2
Model BZigBee TransmitterLCD (16 ×12 Lines)
BatterySolar
Panel
Fig. 5 Implementation of the proposed smart agricultural system.301 A smart framework through the IOT and machine learning12.2 Wireless sensor based crop monitoring system
for agriculture using Wi-Fi network dissertation
The process is coupling the sensor devices with wireless technologies
to monitor the important parameters of Indian Agriculture such as temper-
ature, humidity and moisture. Their idea is to have wireless sensors con-
nected through Wi-Fi to a Central Monitoring Station through GeneralPacket Radio Station. In addition to that, it also connects to the GlobalPositioning System (GPS) to send messages to the central monitoring
station. Further, they also has an external sensor such as soil moisture, pH
and leaf wetness. Depending on the values from the sensors, such as soilmoisture, it will turn the water sprinkler on or off. If the pH sensor sensesany information, it will be sent to the base station to inform the farmer using
a GSM modem to take further action.
12.3 Secure smart agriculture monitoring technique through
isolation
Using mobile and fixed sensors and mobile devices like smartphones and
tablets, the farmers can collect data in various formats regarding crops, soil,
and weather, allowing them to monitor their crops and access their data
effortlessly. The data collected are sent to the core cloud platform, wherethey are processed and analyzed using specific algorithms. The result gener-ated is sent back to farmers to improve their agriculture process and allowremote actuating of the irrigation system. The same devices, sensors, and
actuators are also shared by other stakeholders like disaster early warning
systems for efficient real-time management, and therefore they must be con-trolled with security.
12.4 Realizing social-media-based analytics for smart
agriculture
When a user uploads a query in social media where other users can post their
solutions, along with possible diseases names, images, and features of thesolution, like the time taken by disease to get cured and the cost of resolu-tion, the text used by the users can be unstructured; thus the features can be
mentioned anywhere in the text. In this case, NLP techniques are used to
extract those features to identify the disease based on the names suggestedby community members. It also considers the image posted by the user302 Veeramuthu Venkatesh et al.and evaluates the merits of the solutions posted by the members to select a list
of possible solutions. Later, we can extract the solution, and a ranking model
sorts the solution by considering trusted DB and relevant extracted featuresof the solutions.
13. Conclusion
The growth in the global population is compelling a shift towards
smart agriculture practices. This coupled with the diminishing naturalresources, limited availability of arable land and increase in unpredictable
weather conditions, makes food security a major concern for most countries.
As a result, the use of Internet of Things (IoT) and data analytics (DA) canenhance operational efficiency and productivity in the agriculture sector.As a result, there is a paradigm shift from using wireless sensor networks(WSNs) as a major driver of smart agriculture to use IoT and DA. The
IoT integrates several existing technologies like radio frequency identification,
WSN, and end-user applications. We presented the IoT ecosystem and howthe combination of IoT and DA is enabling smart agriculture. Furthermore,we provide future trends and opportunities categorized into technological
innovations, application scenarios, business, and marketability.
References
[1]J. Guth, U. Breitenb €ucher, M. Falkenthal, P. Fremantle, O. Kopp, F. Leymann,
L. Reinfurt (Eds.), A detailed analysis of IoT platform architectures: concepts, similar-
ities, and differences, in: Internet of Everything, Springer, Singapore, 2018, pp. 81 –101.
[2]J.-C. Zhao, J.-F. Zhang, F. Yu, J.-X. Guo, The study and application of the IOT tech-
nology
in agriculture, in: 2010 3rd International Conference on Computer Science and
Information Technology, vol. 2, IEEE, 2010, pp. 462 –465.
[3]X. Yu, W. Pute, W. Han, Z. Zhang, A survey on wireless sensor network infrastructure
for
agriculture, Comput. Stand. Inter. 35 (1) (2013) 59 –64.
[4]E. Peres, M.A. Fernandes, R. Morais, C.R. Cunha, J.A. Lo ´pe z, S.R. Matos, P.J.S.G.
Ferreira, M.J.C.S. Reis, An autonomous intelligent gateway infrastructure for in-fieldprocessing in precision viticulture, Comput. Electron. Agric. 78 (2) (2011) 176 –187.
[5]T. Kalaivani, A. Allirani, P. Priya, A survey on Zigbee based wireless sensor networks inagriculture,
in: 3rd International Conference on Trendz in Information Sciences &
Computing (TISC2011), IEEE, 2011, pp. 85 –89.
[6]K.R. Dabre, H.R. Lopes, S.S. D’monte, Intelligent decision support system for smart
agriculture,
in: 2018 International Conference on Smart City and Emerging
Technology (ICSCET), IEEE, 2018, pp. 1 –6.
[7]S. Bansal, D. Kumar, IoT ecosystem: a survey on devices, gateways, operating systems,
middleware
and communication, Int. J. Wirel. Inf. Netw. (2020) 1 –25.
[8]K. Lakshmisudha, S. Hegde, N. Kale, S. Iyer, Smart precision based agriculture usingsensors,
Int. J. Comput. Appl. 146 (11) (2016) 36 –38.303 A smart framework through the IOT and machine learning[9]S. Pooja, D.V. Uday, U.B. Nagesh, S.G. Talekar, Application of MQTT protocol for
real time weather monitoring and precision farming, in: 2017 International Conference
on Electrical, Electronics, Communication, Computer, and Optimization Techniques
(ICEECCOT), IEEE, 2017, pp. 1 –6.
[10] P. Nayak, K. Kavitha, C.M. Rao, IoT-enabled agricultural system applications, chal-
lenges and security issues, in: IoT and Analytics for Agriculture, Springer, Singapore,
2020, pp. 139 –163.
[11] A. Saidu, A.M. Clarkson, S.H. Adamu, M. Mohammed, I. Jibo, Application of ICT in
agriculture: opportunities and challenges in developing countries, Int. J. Comput. Sci.
Math. Theory 3 (1) (2017) 8 –18.
[12] S. Ghani, F. Bakochristou, E.M.A.A. ElBialy, S.M.A. Gamaledin, M.M.
Rashwan, A.M. Abdelhalim, S.M. Ismail, Design challenges of agricultural green-
houses in hot and arid environments –a review, Eng. Agric. Environ. Food
12 (1) (2019) 48 –70.
[13] R. Divya, R. Chinnaiyan, Reliable AI-based smart sensors for managing irrigation
resources in agriculture—a review, in: International Conference on Computer
Networks and Communication Technologies, Springer, Singapore, 2019,
pp. 263 –274.
[14] G. Bannerjee, U. Sarkar, S. Das, I. Ghosh, Artificial intelligence in agriculture: a liter-
ature survey, Int. J. Sci. Res. Comput. Sci. Appl. Manag. Stud. 7 (3) (2018) 1 –6.
About the authors
Veeramuthu Venkatesh received B.Tech
degree in Electronics and Communication
Engineering from NIT Trichy, M.E degree
in Embedded System, Anna University,
India in 2007 and completed Ph.D. from
SASTRA University Thanjavur, India in
2018 respectively. He joined SASTRA
University, Thanjavur, Tamil Nadu, India as
a Lecturer in the Department of Computer
Science Engineering since 2008 and is now
Assistant professor-III, His research interests
include Wireless sensor networks, Context
aware computing Information fusion and
IoT. So he has published 50+ Research articles in National & International
journals and 2 IEEE conference papers.304 Veeramuthu Venkatesh et al.Pethuru Raj have been working as the chief
architect and vice-president in the Site
Reliability Engineering (SRE) division of
Reliance Jio Infocomm Ltd. (RJIL),
Bangalore. My previous stints are in IBM
global Cloud center of Excellence (CoE),
Wipro consulting services (WCS), and
Robert Bosch Corporate Research (CR).
In total, I have gained more than 17 years
of IT industry experience and 8 years of
research experience.
Finished the CSIR-sponsored Ph.D.
degree at Anna University, Chennai and continued with the UGC-sponsored
postdoctoral research in the Department of Computer Science and
Automation, Indian Institute of Science (IISc), Bangalore. Thereafter, I was
granted a couple of international research fellowships (JSPS and JST) to work
as a research scientist for 3.5 years in two leading Japanese universities.
Published more than 30 research papers in peer-reviewed journals such as
IEEE, ACM, Springer-Verlag, Inderscience, etc. Have authored and edited
16 books thus far and contributed 35 book chapters thus far for various tech-
nology books edited by highly acclaimed and accomplished professors and
professionals.
Focusing on some of the emerging technologies such as IoT, data sci-
ence, Blockchain, Digital Twin, Containerized Clouds, machine and deep
learning algorithms, Microservices Architecture, fog/edge computing, etc.305 A smart framework through the IOT and machine learningR. Anushia Devi received B.E. degree in
Computer Science from Anna Univer-
sity, M.E degree in Computer Communi-
cation, Anna University, India and
completed Ph.D. from SASTRA Uni-
versity Thanjavur, India in 2020 respec-
tively. She joined SASTRA University,
Thanjavur, Tamil Nadu, India as a
Lecturer in the Department of Computer
Science Engineering since 2007 and is
now Assistant professor-III, her research
interests include Information security,
Data hiding, Information fusion and IoT.
So she has published 20+ Research ar ticles in National & International
journals and 2 IEEE conference papers.306 Veeramuthu Venkatesh et al.CHAPTER ELEVEN
5G Communication for edge
computing
D. Sumathia, S. Karthikeyana, P. Sivaprakasha,b, and Prabha Selvaraja
aVIT-AP University, Amaravati, Andhra Pradesh, India
bPPGIT, Coimbatore, India
Contents
1.Introduction 308
2.Architectures of edge computing 309
2.1 Edge computing reference frame 309
2.2 The architecture of edge computing 310
3.5G and edge computing 313
3.1 Importance of edge computing 314
3.2 Taxonomy of edge computing 314
4.5G and edge computing use cases 321
4.1 Industry 4.0 322
4.2 5G Communication technology in Industry 4.0 322
5.Challenges during the deployment of edge computing in 5G 325
6.Conclusion 327
References 327
About the authors 329
Abstract
Edge computing is a computational exemplar that facilitates the functionalities of the
cloud to the edge servers in the mini clouds so that the high computational tasks couldbe performed along with the storing of a huge amount of data in the near vicinity tothe user equipment. Nowadays, the progress in the development of IoT devicesincreases exponentially, and since these devices must be interconnected to transferthe information to the cloud and also it accesses the data from the cloud.Therefore, the objective of edge computing is to satisfy the necessity of informationtransfer. Due to the deployment of edge computing in domains, security has beenenhanced since there is no need for the data to travel all over the network. It has beenobserved that the data does not reside in one data center and it has been distributed.
5G and edge computing are found to be two intricate technologies that are deter-
mined to augment the performance of various applications considerably and theprocessing of a vast amount of data is done in real-time. Speed is enhanced throughthe innovation of 5G whereas the reduction in latency is done in edge computingthrough the computational capabilities in the network. Hence, several industries in
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.008307various domains expect to make use of 5G technology to provide new business
models, use cases, applications that make a tremendous shift in technology which
results in the development of business. The development of 5G wireless technologiesneeds to involve edge computing architecture. In this regard, this chapter provides thelimelight of the concepts of edge computing and the working methodology. Next, thechapter discusses in detail the importance of edge computing in 5G, the taxonomy ofedge computing in 5G, and the functional components of edge computing. A sectionwill provide the evolution of 5G. A brief explanation of the architecture of edge com-puting and 5G will also be done. Finally, an exploration of recent advancements inedge computing for 5G will be done.
1. Introduction
An exponential increase in the growth of IoT devices to transfer
information from and to the cloud leads to the development of edge com-
puting. The objective of edge computing is to transport various cloud
resources like storage, networking, and compute that are required by
devices, users, and applications. Edge computing is defined as an exemplartechnology that facilitates the extension of cloud potential at the edge of thenetwork by the edge server that is located in mini clouds. Through this
facility, computational-intensive tasks are done, and thereby a huge amount
of data is stored in close vicinity to User Equipment (UEs) [1–3].E d g e
c
omputing is chosen so that the communication requirements of the future
generation applications could be met out. The common characteristics ofedge computing, when compared with data center computing, are described
as given below:
Sharing of resources by multiple users and applications
Through virtualization and abstraction of the resource pool, morebenefits
are obtained
API is utilized in providing interoperability
Various advantages that are achieved through the deployment of edge com-puting
are adaptability, data throughput, scalability, robustness, and reliabil-
ity. Recently, it has been observed that the crucial requirements of the IT
industry in various areas like security and privacy, real-time business, opti-
mization of data, agile bonding, and to reach out to the needs of the highbandwidth and low latency in the network, edge computing has beendeployed to move the resources, storage abilities, and resources to the edge
of the network.308 D. Sumathi et al.2. Architectures of edge computing
Cloud services could be extended to the edge of the network with the
help of the introduction of edge devices among cloud computing and ter-
minal devices [4,5]. This is deployed with the edge computing architecture.
The
structure of edge computing could be segregated into three layers
namely the terminal layer, cloud computing layer, and edge layer.
Terminal layer: It comprises several devices such as cameras, smart cars,
and sensors, etc. These devices belong to both data consumers and as well asdata providers. The objective of the devices is to gather the raw data thatcomes from various places and it is transferred to the upper layer for further
computation process.
Edge layer: The primary layer in the architecture that plays an essential
role in accessing the data obtained from the terminal layer. Devices such as
routers, switches, gateways, and access points are installed in this layer.
Transmission of data is made feasible since it is located near to the user.
Real-time analysis and intelligent processing are done efficiently when com-pared with cloud computing.
Cloud layer: Servers and storage devices that perform well plays a crucial
role in analyzing the huge amount of data. Besides this, certain operations
like business decision support and consistent maintenance are also per-
formed. Data that come from the edge layer are stored permanently forfurther process. Based on the control policy, the cloud module is also usedto do modify the installation method dynamically.
2.1 Edge computing reference frame
In 2018, an edge computing white paper 3.0 has been released by ECCthat has been instigated by Huawei in association with various institutionslike Shenyang Institute of Automation, the Chinese Academy of Sciences,the China Academy of Information and Communications, and other
well-known enterprises [6]. The principle behind the reference frame is
the
model-driven engineering methodology. The four objectives that have
to be achieved over with this model are
Demonstration of the reusable knowledge model system and need to
complete
the interdisciplinary industry.
The decoupling of the software interface has been done in addition to
the
reduction in the system heterogeneity.309 5G Communication for edge computingAids in the life cycle of progress service, processing of data, and security.
Cooperation could be achieved in the digital and physical world.
Fig. 1 shows the architecture of the ECC edge computing reference. From
various inferences, the functionality of each layer could be demonstrated.
Services that are provided in this framework are security, lifecycle, and
management services. The entire framework has been linked with these ser-
vices. The objective of the data lifecycle services is to impart the management
that could handle various operations like preprocessing, investigation, dis-
semination, and accomplishment of data along with the storage and visual-
ization. Control of the architecture and bestowing the information lies with
the management services. Through the deployment of security services, the
whole life cycle of data has been brought under surveillance with the help of
the business orchestration layer. Also, the deployment and optimization of the
data service are done. Assurance and reliability of the whole architecture are
made feasible with the adaptation of the security services on the particular
architecture of edge computing. The edge layer consists of two entities namely
edge node and edge manager. The edge node transforms the business of edge
computing. All edge nodes are supervised by the edge manager. Edge nodes
could be categorized as edge sensors, edge gateways, edge controllers, and edge
gateways. Transferring of information in the layers from top to bottom and
vice-versa is made possible with the development of local edge resources.
Several service frameworks such as real-time computing, lightweight comput-
ing intelligent distributed and gateway systems have been provided.
2.2 The architecture of edge computing
The architecture of edge computing from the perspective of communication
service providers has been discussed in this section. The functional compo-
nents of edge computing could be identified as given below by referring Fig. 2 .
Fig. 1 Edge computing reference frame 3.0310 D. Sumathi et al.Fig. 2 Architecture of edge computing311 5G Communication for edge computingDistributed cloud infrastructure: Cloud data centers that are located
globally and in access locations are combined in the network and it is found
to have functioned with the help of an essential significant planning andadministration system. The selection of the corresponding infrastructuredepends on the corresponding applications and use cases.
Connectivity: As soon as the installation is completed, the configuration
of connectivity must be done. Applications that are to be executed neednecessities based on mobility, latency, throughput, and bandwidth so thatexecution must be done on several hardware setups. New innovative solu-tions are required for the routing of data that are required for applications.
Management of communication among the applications in the server and
employment of these mobile network solutions must be done.
Application runtime execution environment: Service provided by edge
computing is the runtime execution environment that could be able to func-
tion virtual network functions and non-telco workloads. Applications could
be hosted on the environment and they must synchronize with the requestsmade by the development communities. To execute these applications, sev-eral features and operational needs are deployed in addition to the diversified
platform mechanisms. Customization of the environment could be done by
the application developer based on the requirements.
Dynamic orchestration and management: A centralized planning and
administration facility must be enabled to be aware of the topology thathas been configured in the network and the availability of resources in
the distributed cloud environment. The objective of the orchestration layer
is to render the coordinated individual planning and administration over thenumerous organization functions. Managing the platforms for workloadsand virtual network functions depends on the service level agreements.
Service exposure: Server can disclose the core competencies that are
existing internally in the operator or to another partner. Several capabilitiessuch as optimization, security, connectivity, data, and analytics could beadded more significance to both internal and external users. Moreover, a
comparison of the edge computing reference architectures is given in
Table 1 .
It
has been observed from various inferences that there is tremendous
growth due to the progress in innovative technologies in terms of smart
devices, sensors, and various other gadgets. Hence, there is a huge expansion
of data which might be heterogeneous. Due to this, enterprises acquire solu-
tions that depend on several architectures. This in turn results in augmentingthe access speed and as well as visualization of data along with the other312 D. Sumathi et al.information from diversified systems irrespective of their interconnection.
As the development of edge computing continues, consequently the solu-tions are used to accumulate and accomplish the data. Recently, the growthin innovative technologies targets the business associated with IoT solutionslike Assisted Reality (AR) which results in substantial consequences in pro-
viding applications in various domains such as logistics, transport, and
manufacturing. A combination of 5G, mobile edge computing, and IoT willlead to the generation of more IoT devices recently.
3. 5G and edge computing
The performance of applications could be amplified with the help of
two indistinguishable technologies namely 5G and edge computing.Processing enormous data in real-time could be also enabled additionally.The increase in speed is done by 10 times in 5G when compared with
the 4G, but mobile edge computing decreases the latency through the com-
puting competencies into the network. The main features of 5G areØ
A huge volume of data generation due to the extraordinary growth ofmobile devices that results in ultradense networks.
ØQoS requirements are enforced to execute interactive applications thatneed high throughput and ultralow latency.
ØInteroperability of a varied range of user equipment, QoS necessities,type of networks, and so on.Table 1 Comparison of edge computing reference architectures.
ArchitecturesDistributed
cloudinfrastructure Connectivity SecurityData
encryption Blockchain Standards
FAR-Edge RA Yes Yes Yes No Yes No
Edge computing
RA 2.0Yes Yes Yes No No Yes
Industrial
Internet
Consortium RAYes Yes Yes No No Yes
Edge computing
reference
architecture 3.0.Yes Yes Yes Yes No No
EdgeX Foundry Yes Yes Yes Yes No Yes313 5G Communication for edge computing5G comprises three predominant technologies for the functioning of user
equipment in terms of high network capacity [7]. They are described as
given
below:
mmWave communication makes use of high-frequency bands that
ranges
from 30 to 300GHz [8]and also high bandwidth [9].
Transmission range and interference could be decreased by the utiliza-
tion
of mmWave. Cells that are small in size permit the user equipment
to perform the communication process.
A huge number of antennas are used by the base stations so that the trans-
mission
could be done directly which results in less interference. This
permits the neighboring nodes to connect concurrently.
3.1 Importance of edge computing
Applications that are enriched with high interaction are aided with certainfeatures such as high throughput and low latency [10]. During the real-time
packet 
delivery among driverless cars, the delay in data transmission is less
than 10ms [11]. During the accessibility from the cloud, the end-to-end
delay
might be more than 80ms [12,13] and it is considered to be unbear-
able.
The introduction of edge computing satisfies the submillisecond needs
of 5G applications and thereby the energy consumption is reduced by30–40% [14]that contributes to less energy consumption during the data
access 
from the cloud [15]. The next section provides the taxonomy of edge
computing.
3.2 Taxonomy of edge computing
It could be categorized into subsequent aspects namely objectives, accesstechnologies, computing devices, computing models, empowering technol-ogies, computational order, and applications. It is denoted in Fig. 3 .
Moreover,
the role of edge computing in 5G could be explained in terms
of the taxonomy. It could be determined as the goals, computational plat-forms, characteristics, utilization of 5G functions, performance metrics, andseveral roles of edge computing in 5G.
3.2.1 Goals
The purpose of edge computing in 5G is to satisfy various constraints such as
ØImprovement in data management: Due to the extensive growth of user
equipment,
the handling of a huge amount of data becomes a complex314 D. Sumathi et al.process since accessing data leads to high latency [16]. Hence, the data
could be managed locally by edge servers.
ØEnhancement in QoS services: This service is intended to provide deliv-
ery of content rich in multimedia that requires less latency and high
bandwidth [17]. Customization of QoS could be provided to develop
new and custom-made applications [18].
ØExamining network demand: Prediction of the essential resources to
accommodate the user demand in a local vicinity. The efficient alloca-
tion of resources could be done fairly based on the prediction of the
network demand.
ØHandling of location-awareness: Location-based service providers are
capable of providing services and data to the edge clouds.
ØAugmentation in resource organization: Optimization of network
resource usage has been done to enhance the networks since the resources
are inadequate. This is one of the challenging issues since it needs to serve
many applications that are diversified in nature and also demands and user
requirements that vary based on time.
Fig. 3 Taxonomy of edge computing315 5G Communication for edge computing3.2.2 Computational platforms
Deployment of the computational platforms could be done either in a single
mode or in combination mode. This kind of service depends on several net-work scenarios and the requirements of an application/service. The threemain computational platforms are described as given below:✓
Cloud computing: A massive amount of data is gathered from several
user equipment and transferred to the cloud. Subsequently, decisions
or data are also transferred from the cloud to the user equipment. Theprovision of real-time services is found to be a little complex due tothe distance between the cloud and the user equipment.
✓Edge computing: It has the facility of collecting, storing a huge amountof local data and information from the various user equipment in thatarea. It has been observed that edge computing connects easily withthe user equipment due to its proximity. There are three types of edge
computing. They are named local devices, localized data centers, and
regional data centers.
ØLocal devices: These are deployed for specific home applications.Cloud storage is joined with applications in such a way that there
is no need to move the applications into the cloud.
ØLocalized data centers: These are capable of providing considerable
processing and storage capabilities. Also, the deployment of data cen-ters is fast in present environments. These are made accessible as theyare preconfigured and assembled on site. 1 –10 Racks data center is
mainly used for broad applications that need less latency, high band-
width, security, and availability.
ØRegional data centers: Data centers with more than 10 racks could bepositioned near the user and data source. Hence, processing and stor-
age abilities could be enhanced due to the scalability.
✓Hybrid computing combines both edge and cloud computing. Real-
time data could be processed at the edge computing and nonreal-timedata are processed in the cloud computing. Throughput is increased with
edge computing.
3.2.3 Characteristics
Network context awareness : The whole network context information could be
obtained by enabling the edge servers. Certain information like the locationof the user, traffic load, and allotted bandwidth facilitates edge servers to
adjust and act according to the impulsive network constraints and user
equipment. Hence, the edge servers handle a huge amount of traffic so that316 D. Sumathi et al.the performance of the network could be enhanced. Microlevel information
also supports in providing precise services to traffic flows so that the user
needs are satisfied.
Location awareness : Through this feature, the edge server can collect the
data from the sources in its close vicinity.
Low latency and close vicinity : Through this, the response delay is decreased.
Three major factors that contribute to the response time are identified as thepropagation delay, communication delay, and computational delay.✓Propagation delay: It is based on the dissemination distance.
✓Communication delay: It depends on the data rate.
✓Computational delay: It is determined by the computational time.
Usually, in cloud computing, the end-to-end delay is more than 80ms [12].
3.2.4
5G Functions
Technologies that enable the edge computing could be described as givenbelow:
Network function virtualization (NFV) : Network functions are performed
in virtual machines. Managing a huge amount of data so that it results in mal-leable and scalability in networks [19,20] . Demands from the network could
be
managed in any of the two entities namely edge or at the cloud that could
protect the data and information in transferring to the cloud.
Access to new radio (NR) : 5G is a novel standard that connects several
devices so that scalability and low latency could be achieved so that the net-works could be extended shortly. Accessing radio access technologiesincludes access to several other technologies like NR [21].
Software-defined
network (SDN) : The network could be partitioned into
data and control planes for providing the networks actively and flexibly thataids in the deployment of new services and to simplify the network manage-ment [22]. The objective of the data plane is to forward the traffic based on
the
decisions done by the control plane and the control plane manages the
policy on the cloud. Edge servers operate and manage the real-time responsesthat are generated during the network functioning.
Multiple input/multiple output technology (MIMO) : 5G New radio encom-
passes the deployment of MIMO technology to cover the network andcapacity. The transmission gain and efficiency could be enhanced by thedeployment of the massive MIMO through the implantation of severalantenna components. This works based on the Shannon principle [23].I n
this,
the energy efficiency and the capacity of the network are boosted.317 5G Communication for edge computingEdge server takes the task of computation with massive MIMO that results in
less latency and energy consumption [24].
Communication
among the devices : Throughput of the system, efficiency of
the energy could be improved through the direct communication that
would be enabled among the devices without the necessity of the base sta-tions [25,26] . Edge servers carry out the computational tasks that the UEs are
authorized 
with the computational abilities. Through this direct communi-
cation facility, edge computing ensures the process to be a success [27,28] .
3.2.5
The functioning of edge computing in 5G
Storage: Through this technology, a huge amount of data is being offloadedto the edge clouds. The special capability of this is to provide the distributed
local storage for the substantial data. Storage is compatibly less when com-
pared with cloud computing. Data like metadata and computing strategiesare stored in the edge server with the deployment of several types of storagepolicies [29]. Temporary data storage is also used for holding the transient
information
about the group of devices that are connected [30,31] .
Computation
locally: Edge clouds do the computation process
irrespective of its complex nature. It is observed that the simple computationis done in the conventional cache and access technologies whereas the local
computation and data processing functionalities are done in the edge servers
intelligently and autonomously [32]. Through the performance of smart
tasks 
and responding locally results in the cost reduction and delay that
occurs due to the transmission of the needed data to the cloud.
Data analysis: To make a decision, an extensive analysis of data is
required. To decrease the latency for the transmission and receiving of dataand responses from and to the cloud, data analysis could be done locally. Todo this, raw data is collected from several applications and it is analyzed toproduce the necessary information needed for the decision-making process
[33,34] .
Decision-making
process: Decisions depend on the processed data [35].
The
advantages of the decision process in the edge cloud have resulted in the
diminishment of the requirement of components, data, and the exchange of
information or data frequently. Therefore, the availability of the system and
the bandwidth has been improved.
Local operation: Remote monitoring or controlling the devices is
enabled through this technology [36].
Progress
in the local security: One among the functionalities of the edge
server is that it can distribute the software patches, detection of malware,318 D. Sumathi et al.issuing of security credentials, and its management apart from communicat-
ing in a secured manner. Moreover, it can perform countermeasure attacks.
Due to the juxtaposition of edge computing, detection of malevolent objectsis done rapidly and real-time responses could be activated so that the impactof the attacks could be lessened. Therefore, the service disruption is mini-mized. The scalability and the edge computing abilities could expedite
the implementation of blockchain among the user equipment but, with
restriction in capabilities [37].
It
could be observed from various inferences that the performance of the
applications could be improved and thus it empowers the massive data trans-
mission in a real-time environment. When the speed of 5G is combined
with the processing capabilities of edge computing, then the applicationsthat need low latency acquired the focus. Hence, applications that makeuse of AR/VR, AI, and robotics need intensive decisions from computing
resources. However, several business applications have the potential of ava-
iling the benefits from both technologies.
3.2.6 Integration of 5G and edge computing
The working of IoT devices could be still made efficient by providing a highlevel of connection on the network edge. Revolution gets initiated when
the processing and storing of data is done in the devices that are located
at the edge that has been connected to the devices in the network. Datatransmission could be done uninterruptedly and effectively with the helpof an adequate network connection. For example, data that are collectedfrom the sensors that have been implanted in an autonomous car might
be transferred to the other vehicles for further process. To perform this trans-
mission rapidly, there needs a necessity for the 5G technology. Due to theavailability of the 5G infrastructure, data centers and other IoT devices thatare located in the edge can form a distinguished processing area where the
data could be constructed and collected. Furthermore, data analysis also
could be done locally with a marginal latency. Therefore, the network edgeacts as a loop of organized 5G networks that are interrelated. This device hasthe capability of handling the data and also it arranges the information that
has to be transmitted to the centralized servers based on the preferences.
Due to the increase in the speed and bandwidth of the network, the
backhaul across the network might jeopardize the innovation and thereby
the performance of the applications degrades. Therefore, hosting applica-
tions at the edge might perform well with high bandwidth and less latency
due to the provision of 5G. Several other advantages of edge computing are319 5G Communication for edge computingProgress in agility : CoSPs have been facilitated with the trial periods on the
new services and upgrade based on their performance. Upgradation could be
done without visiting the site. This is made possible with the replacement ofthe customer premises equipment with the edge computing platforms. Arapid recovery facility is also ensured during the process of the network fail-ure since the CoSP has the dynamic provision ability to choose the essential
service needed.
Context awareness : Hosting applications in the edge possess this facility.
Suppose, if an AR application is hosted in the edge, it deals with data andmany images associated with that corresponding location. Decisions could
be made in a real-time context since they might be delayed if it has to contact
the cloud for deciding.
Manage without connection : Edge computing enables the application to run
despite the feeble connection that exists between the cloud and edge.
Through the special feature of rapid data processing, performance could
be enhanced. Also, the cloud-based services could be made easier for theusers. Extensive growth of IoT devices and 5G implementation devicesare evolving to perform the computation process with the help of edge com-
puting technologies. The most important attribute is that the complexity
arises during the bandwidths balancing among the several networks sincethere is a need to interchange the data to data centers and other devices thatare located in the edge. Edge computing proves to show its performancethrough the deployment of the distributed computing technology for the
location and the capacity of the network. There are issues during the data
transfer. In this case, data gets transferred among the network through thegateways. Another issue is that the backup of the data. Organizations mustadhere to the policies of data management during the backup and accessing
of data.
3.2.7 Security and privacy
Traditional security principles and practices will not be substantial for thefunctioning of the 5G network. There is a necessity for cutting-edge securitymeasures that results in the innovation of the new network model. Apart
from that, several other protective and proactive security measures are in
need for the protection of the loads of data that is been transferred in mil-liseconds among the IoT devices. Due to the advancement in the endpointsnumber, the risks that might be encountered are also assumed to be high in
number. Also, data security is considered a challenging task and it will
increase vulnerabilities. Protection to the network could be devised in sev-eral ways. Methods such as strong authentication mechanisms at various320 D. Sumathi et al.levels and security for the gateways could be done. Additionally, the SDN
and NFV endpoints must be secured by following several security measures
properly. The integrity of networks could be done so that the security chal-
lenges that arise due to the evolvement of 5G could be reduced.
4. 5G and edge computing use cases
The main objective of 5G is to provide an extensive bandwidth. To
perform this process, there is a need for an enhancement in the infrastruc-
tures so that it could handle the issues such as crucial traffic prioritization.
This ensures that the throughput could be high and the latency could be less.
Cloud architectures that are used in massive data centers play a significant
role in providing more flexibility and efficient use of hardware resources
through the signaling functions. To avail the similar advantages for radio
applications, without creating an impact on the computational power and
the speed of the light, a transition is required to move to the edges of the
network. This empowers in providing the high bandwidth that is needed
for the radio equipment to communicate among the applications. A com-
parison of several architectures has been described in Fig. 4 to illustrate
the evolution of edge computing.
A two-tier architecture is described in the pre-edge computing in which
the clients get connected to the service providers and the network. To
decrease the investments done by the providers, the network access could
be revolutionized and thereby the providers turn into a transport service
provider belongs to another category of EaaS (Edge-as-a-Service).
Fig. 4 Evolution of edge computing321 5G Communication for edge computingThis section describes many use cases and characteristics that are aided
by 5G.
Smart city : Intelligent decisions could be made with the help of sensors
and actuators that could be deployed across the city so that the city becomesa smart city. Transmission of data is done at a high rate and low latency.
Smart grid network : Through this grid, changes could be detected, under-
stood intelligently, and planning of the behavior in response to the changes.
To do this process, a clear synchronization has to be done for keeping thegrid at a stable condition during the time of impulsive loads or demands.
Virtual reality, gaming, and augmenting reality : Information could be pro-
vided through live streaming so that it gives the facility of shrouding the sig-
nificant information. One could get completely engrossed in thevirtual world.
Remote surgery and examination : Through these contemporary technolo-
gies, the operator could be able to perceive and sense the physical world even
from a distant location. One such application is remote surgery in which thelatency communication is very low and the reliability is high which mightresult in life in a risky situation.
4.1 Industry 4.0
It is one among the use cases of 5G. To support Industry 4.0 connectionneeds, 5G could be deployed. A huge increase in production and severalinnovation strategies could be enabled through the investment in the dense
networks which focus on the manufacturing department. Moreover, the
manufacturing is strengthened and hence an extensive analysis of data resultsin handling the intricacy and instability of the global markets leads to highlycompetent capabilities. There is a possibility of data flow from the customers
to the producers through the products that act intelligently which enables
product development rapidly. Cyber-Physical Systems is a combinationof real and virtual systems that leads to the rearrangement of supply chainsand value creation. From various inferences, it could be denoted thatIndustry 4.0 is identical with the amalgamation of CPS in domains such
as production and delivery. Moreover, the usage of IoT and its services have
been found extensive usage in the industry.
4.2 5G Communication technology in Industry 4.0
To perform the wireless networking and to make the factory intelligent,Industry 4.0 does the process. Evolution progresses in the fourth revolutionin the world and it is denoted by Industry 4.0. Industrialized elements such as322 D. Sumathi et al.people, machines, and equipment are found to be in close association with
the help of 5G so that the manufacturing process could be done with efficacy
and coordination. Fig. 5 exhibits the deployment of 5G use cases. The
implantation of AI and Machine Learning results in making accurate deci-
sions in various fields like agriculture, healthcare, and supply chain logistics.
5G makes its footprints in the industry with the help of network slicing in
three different ways and they are described as given below.
1.eMBB (Enhanced Mobile Broadband)
2.URLLC (Ultra-Reliable Low Latency Communications)
3.Massive Machine type communication
This provides extensive coverage when it is compared with 4G and also
offers high data rates. Through the sustenance of the comprehensive range
of stakeholders that includes Communication Network Operators (CNO),
providers of communication and industrial technology, Tier 1 manufac-
turers, and system integrators, 5G lays its foundation with Industry 4.0.
The objective of using URLLC is to satisfy the end-to-end (E2E) latency
and reliability. This needs various components that include effective data
resource distribution, fast data turnaround, innovative channel coding
mechanisms, and uplink transmissions that are grant-free. The connection
among numerous IoT devices per square kilometer could encompass a wide
area and could penetrate more in indoor networks.
The implementation of 5G in the smart factory has been considered in this
work. The process of deployment of 5G in the smart factory is shown in Fig. 6 .
An example of the smart factory under the 5G communication is shown
inFig. 6 which the data collected is massive and wireless transmission is used
to transmit the data to various other departments. Sensors that are deployed
in various equipment transmit the data to the respective departments for fur-
ther process.
Fig. 5 5G use cases323 5G Communication for edge computingThe manufacturing process is found to be an intelligent task due to the
continuous connection of 5G network which has been implanted its foot-
print in various departments such as procurement, logistics, design, war-
ehousing, and production. Smart factory involves the process of decision
making in loading and unloading of materials, logistics, and storage mech-
anisms. Due to the availability of sensors at various places, the information is
being transmitted within a short period so that the decisions could be taken
rapidly in terms of any fault tolerance actions. Data thus collected from var-
ious parts results in a massive database. Further, the best solution could be
achieved by the deployment of industrial robots so that the supercomputing
abilities of cloud computing for self-governing learning are used. D2D tech-
nology under 5G is used for communication among objects which in turn
decreases the end-to-end delay in services and thereby the network load gets
by-passed. Moreover, the response is swift in these scenarios. The secret
behind the smart factory’s success is the utilization of Enhanced Mobile
Broadband (eMBB) technology for the collection of a massive amount of
Fig. 6 5G links smart factory324 D. Sumathi et al.data for analysis and to take the corresponding action based on the data.
Human-computer interaction is carried out in the system and AI plays a vital
role in the optimization of the processes and products. Various activities thatare carried out in the smart factories are
Cloud services: Huge data storage, management of data, and computing
are the activities that depend on the cloud computing architecture.
Ubiquitous computing: Data from various entities such as software,
hardware, and other personnel are collected for further process.
Accretion of knowledge: Data analysis abilities could be enhanced due to
the available mechanisms so that the information could be consolidated and
reused further.
Application improvement: Industrial APP development environment is
provided in which the functions and resources function.
A smart factory is known for its functionality of the application platform
for several intelligent technologies and is likely to the cooperation with the
other state-of-the-art technologies to exploit the resource utilization to themaximum, efficacy in production and economic benefits. Additionally, italso aids in the process of determining the variations and anomalies that
occurs during the manufacturing process. Due to this action, the supply
of energy, personnel, and production could be fine-tuned to assure the pro-duction normally which results in the efficacy of the energy. The deploy-ment of ERP leads to the automatic computation for procuring the rawmaterials and as well as supplier information. Production efficiency shall
show tremendous progress due to the state-of-the-art technology that has
been used to interconnect all the departments like sales, production, anddesign. Additionally, the optimization of resources is also obtained andthereby results in the improvement of the quality of the product.
5. Challenges during the deployment of edge
computing in 5G
Among the many issues, the most predominant is the security and pri-
vacy of data. Through this deployment, the data cannot travel across the net-work. Yet, two factors result in the vulnerability at the edge of the network.

The dynamic nature of the network results in the divergence of the data
and the network components.
Scalability is a significant factor due to the augmentation in the devicesthat communicates with each other.325 5G Communication for edge computingEnhancement in adopting the security and privacy policies has to be
strengthened due to the significance of the data. The possible solutions are
Applications that execute on the edge cloud must not be aware of thedata
that is being processed. Hence the information that is used over
here, must be encrypted.
Additionally, the raw information could be detached before reaching the
edge
cloud so that it assures privacy [38,39] .
Quality of Experience (QoE): This is one of the most significant parameters
used
in the evaluation of customer satisfaction with the service provider. The
main issue is to obtain a balanced deal between the high availability and the
QoE of the application. Availability must be high even when the user equip-
ment is not in the vision of the edge server and the QoE of the applicationcould be high when the UE is close to the edge server so that the jitter anddelay are reduced. The state of the network is maintained with the help of
edge computing. Due to this, the arrangement among the QoE and the
availability could be obtained. Aggregation of signaling messages could bedone to decrease the signaling overhead. Hence, the network congestionis reduced which results in the enhancement of the scalability and the high
throughput.
Protocol standardization: To deploy edge computing in 5G, there arises a
necessity of protocol standardization. For functioning properly, a group of
the systematic body is needed which takes the responsibility of forming therules and policies for the deployment. Yet, two main challenges must be
addressed.
Agreeing on a common standard is a complex task due to the flexibilityand
varied customization by several vendors.
Due to the tremendous increase in the assorted UEs, it needs diversified
interfaces
to perform the communication with the edge cloud.
Hence, Standards Institute (ETSI) [40]is identified to take care of the com-
munication
among the edge servers with the conglomerated UEs, several
layers, and computation patterns in a multivendor environment.
Heterogeneity in communication: Due to the dynamic nature of various
factors such as data rate, transmission range, it becomes a complex task to
determine a solution that might be compatible in a diversified environment.Solutions based on the software might be developed to carry out the work-loads in the edge nodes and at several hardware levels. Parallelism in task and
data level partitions the workload into self-governing and minor jobs that
could run parallel in several layers in edge clouds [41].326 D. Sumathi et al.6. Conclusion
The evolution of the next-generation cellular network results in a sig-
nificant enhancement in the quality of service. An emerging technology that
empowers the progress of 5G through the deployment of edge cloud capabil-ities so that many of the issues that arise in the conventional cloud could beavoided. The taxonomy denotes the several features of edge computing.
Through this deployment, various services like processing of data locally, pro-
gress in data rate, and availability could be achieved. A separate section hasbeen contributed to discuss the use cases that are supported by 5G. Finally,the challenges that could arise during the setting out of edge computing in 5G.
References
[1]W.Z. Khan, E. Ahmed, S. Hakak, I. Yaqoob, A. Ahmed, Edge computing: a survey,
Future
Gener. Comput. Syst. 97 (2019) 219 –235.
[2]N. Hassan, S. Gillani, E. Ahmed, I. Yaqoob, M. Imran, The role of edge computing in
internet
of things, IEEE Commun. Mag. 56 (11) (2018) 110 –115.
[3]E. Ahmed, A. Ahmed, I. Yaqoob, J. Shuja, A. Gani, M. Imran, M. Shoaib, Bringingcomputation
closer toward the user network: is edge computing the solution? IEEE
Commun. Mag. 55 (11) (2017) 138– 144.
[4]J. Ren, H. Guo, C. Xu, Y. Zhang, Serving at the edge: a scalable IoT architecture based
on
transparent computing, IEEE Netw. 31 (5) (2017) 96 –105.
[5]H. Bangui, S. Rakrak, S. Raghay, B. Buhnova, Moving to the edge-cloud-of-things:
recent
advances and future research directions, Electronics 7 (11) (2018) 309.
[6]S. Carlini, The drivers and benefits of edge computing, in: Schneider Electric Data
Center
Science Center, 2016.
[7]B. Yang, Z. Yu, J. Lan, R. Zhang, J. Zhou, W. Hong, Digital beamforming-based mas-
sive
MIMO transceiver for 5G millimeter-wave communications, IEEE Trans.
Microw. Theory Tech. 66 (7) (2018) 3403 –3418.
[8]N.C. Luong, P. Wang, D. Niyato, Y.-C. Liang, Z. Han, F. Hou, Applications of eco-nomic
and pricing models for resource management in 5G wireless networks: a survey,
IEEE Commun. Surv. Tutor. 21 (2018) 3298 –3339.
[9]Z. Pi, J. Choi, R. Heath, Millimeter-wave gigabit broadband evolution toward 5G:fixed
access and backhaul, IEEE Commun. Mag. 54 (4) (2016) 138 –144.
[10] A. Ateya, A. Muthanna, I. Gudkova, A. Abuarqoub, A. Vybornova, A. Koucheryavy,Development
of intelligent core network for tactile internet and future smart
systems, J. Sens. Actuator Netw. 7 (1) (2018).
[11] M. Simsek, A. Aijaz, M. Dohler, J. Sachs, G. Fettweis, 5G-enabled tactile internet, IEEEJ.
Sel. Areas Commun. 34 (3) (2016) 460– 473.
[12] S. Choy, B. Wong, G. Simon, C. Rosenberg, The brewing storm in cloud gaming: a
measurement
study on cloud to end-user latency, in: 2012 11th Annual Workshop on
Network and Systems Support for Games (NetGames), IEEE, 2012.
[13] Z. Zhao, K. Hwang, J. Villeta, Game cloud design with virtualized CPU/GPU serversand
initial performance results, in: ScienceCloud ’12: Proceedings of the 3rd workshop
on Scientific Cloud Computing, ACM Press, 2012.327 5G Communication for edge computing[14] W. Shi, S. Dustdar, The promise of edge computing, Computer 49 (5) (2016) 78 –81.
[15] W. Hu, Y. Gao, K. Ha, J. Wang, B. Amos, Z. Chen, P. Pillai, M. Satyanarayanan,
Quantifying
the impact of edge computing on mobile applications, in: Proceedings
of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems—APSys ’16, ACM
Press, 2016.
[16] X. Sun, N. Ansari, Green cloudlet network: a distributed green mobile cloud network,
IEEE
Netw. 31 (1) (2017) 64 –70.
[17] Z. Zhao, L. Guardalben, M. Karimzadeh, J. Silva, T. Braun, S. Sargento, Mobility
prediction-assisted
over-the-top edge prefetching for hierarchical VANETs, IEEE
J. Sel. Areas Commun. 36 (8) (2018) 1786 –1801.
[18] E. Chirivella-Perez, J. Guti /C19errez-Aguado, J.M. Alcaraz-Calero, Q. Wang, NFVMon:
enabling multioperator flow monitoring in 5G mobile edge computing, Wirel.Commun. Mob. Comput. 2018 (2018) 2860452.
[19] J. Gonzalez, G. Nencioni, A. Kamisinski, B.E. Helvik, P.E. Heegaard, Dependability of
the
NFV orchestrator: state of the art and research challenges, IEEE Commun. Surv.
Tutor. 20 (4) (2018) 3307 –3329.
[20] I. Sarrigiannis, E. Kartsakli, K. Ramantas, A. Antonopoulos, C. Verikoukis, Applicationand
network VNF migration in a MEC-enabled 5G architecture, in: 23rd IEEE
International Workshop on Computer Aided Modeling and Design of
Communication Links and Networks (CAMAD), 2018, pp. 1 –6.
[21] S. Parkvall, E. Dahlman, A. Furuskar, M. Frenne, NR: the new 5G radio access tech-
nology,
IEEE Commun. Stand. Mag. 1 (4) (2017) 24 –30.
[22] Y. Li, M. Chen, Software-defined network function virtualization: a survey, IEEE
Access
3 (2015) 2542 –2553.
[23] C.E. Shannon, A mathematical theory of communication, Bell Syst. Tech. J. 27
(3)
(1948) 379– 423.
[24] S. Wang, X. Zhang, Y. Zhang, L. Wang, J. Yang, W. Wang, A survey on mobile edge
networks:
convergence of computing, caching and communications, IEEE Access 5
(2017) 6757 –6779.
[25] Y. He, F.R. Yu, N. Zhao, H. Yin, Secure social networks in 5G systems with mobileedge
computing, caching, and device-to-device communications, IEEE Wirel.
Commun. 25 (3) (2018) 103 –109.
[26] H. Wang, J. Wang, G. Ding, L. Wang, T.A. Tsiftsis, P.K. Sharma, Resource allocationfor
energy harvesting-powered D2D communication underlying UAV-assisted net-
works, IEEE Trans. Green Commun. Netw. 2 (1) (2018) 14 –24.
[27] X. Chen, L. Pu, L. Gao, W. Wu, D. Wu, Exploiting massive d2d collaboration for
energy-efficient
mobile edge computing, IEEE Wirel. Commun. 24 (4) (2017) 64 –71.
[28] Y. He, J. Ren, G. Yu, Y. Cai, D2D communications meet mobile edge computing for
enhanced
computation capacity in cellular networks, IEEE Trans. Wirel. Commun.
18 (3) (2019) 1750 –1763.
[29] J. Zhang, W. Xia, F. Yan, L. Shen, Joint computation offloading and resource allocationoptimization
in heterogeneous networks with mobile edge computing, IEEE Access 6
(2018) 19324 –19337.
[30] N. Bessis, C. Dobre, Big Data and Internet of Things: A Roadmap for SmartEnvironments,
vol. 546, Springer, 2014.
[31] J.A. Silva, R. Monteiro, H. Paulino, J.M. Lourenco, Ephemeral data storage for net-works
of hand-held devices, in: 2016 IEEE Trustcom/BigDataSE/ISPA, IEEE, 2016.
[32] N.H. Ndikumana, T.M. Tran, Z. Ho, W. Han, D.N. Saad, C.S. Hong, Joint commu-nication,
computation, caching, and control in big data multi-access edge computing,
IEEE Trans. Mob. Comput. (2019) 1359 –1374.
[33] Y. Li, J. Xue, W.Z. Wang, T. Li, Edge-oriented computing paradigms, ACM Comput.
Surv.
51 (2) (2018) 1 –34.328 D. Sumathi et al.[34] E. Ahmed, I. Yaqoob, I.A.T. Hashem, I. Khan, A.I.A. Ahmed, M. Imran, A.V.
Vasilakos, The role of big data analytics in internet of things, Comput. Netw. 129
(2017) 459 –471.
[35] M.M. Hussain, M.S. Alam, M.M.S. Beg, Feasibility of fog computing in smart grid
architectures, in: Proceedings of 2nd International Conference on Communication,
Computing and Networking, Springer Singapore, 2018, pp. 999 –1010.
[36] C. Xu, K. Wang, P. Li, S. Guo, J. Luo, B. Ye, M. Guo, Making big data open in edges: a
resource-efficient blockchain-based approach, IEEE Trans. Parallel Distrib. Syst.
30 (4) (2019) 870 –882.
[37] R. Yang, F.R. Yu, P. Si, Z. Yang, Y. Zhang, Integrated blockchain and edge comput-
ing systems: a survey, some research issues and challenges, IEEE Commun. Surv. Tutor.
21 (2019) 1508 –1532.
[38] E. Ahmed, M.H. Rehmani, Mobile edge computing: opportunities, solutions, and chal-
lenges, Future Gener. Comput. Syst. 70 (2017) 59 –63.
[39] K. Kaur, S. Garg, G. Kaddoum, M. Guizani, D. Jayakody, A lightweight and
privacy-preserving authentication protocol for mobile edge computing, arXiv
(2019). preprint arXiv:1907.08896.
[40] K. Jain, S. Mohapatra, Taxonomy of edge computing: challenges, opportunities, and
data reduction methods, in: Edge Computing, Springer International Publishing,
2018, pp. 51 –69.
[41] T. Taleb, S. Dutta, A. Ksentini, M. Iqbal, H. Flinck, Mobile edge computing potential
in making cities smarter, IEEE Commun. Mag. 55 (3) (2017) 38 –43.
About the authors
Dr. D. Sumathi is presently working as an
Associate Professor Grade 2-SCOPE at
VIT-AP University, Andhra Pradesh. She
received the B.E Computer Science and
Engineering degree from Bharathiar
University in 1994 and M.E Computer
Science and Engineering degree from
Sathyabama University in 2006, Chennai.
She completed her doctorate degree in
Anna University, Chennai. She has an over-
all experience of 22 years out of which
6 years in the industry, 15 years in the teach-
ing field. She has guided many projects during her tenure. She is an active
m e m b e ro ft h eA I R - C e n t r eo fE x c e l l e n c ea tV I T - A P .S h ei sal i f em e m b e r
of ISTE. Her research interests include Algorithm Analysis, Cloud com-
puting, Network Security, Data Mining, Natural Language Processing,
Machine Learning, Deep Learning, and Theoretical Foundations of com-
puter science. She has published papers in international journals and con-
ferences. She has published 6 patents . She has developed various projects
during her tenure. She is guiding vari ous students to do research projects in329 5G Communication for edge computingNLP, ML, and DL. She has given various talks in educational institutions.
She has organized many international conferences and also acted as
Technical Chair and tutorial presenter. Further, she has published 5 pat-
ents. She has published book chapters in CRC Press, IGI Global, Springer,
IET, De Gruyter.
Dr. S. Karthikeyan is working as an
Associate Professor in the School of
Computer Science Engineering at VIT-AP
University, Amaravati, Andhra Pradesh-
India. He has 11 years of industry and acade-
mia experience. He is alumnus of PSGTech,
Coimbatore and VIT University. Currently
he is heading High Performance Computing
(HPC) Research Lab at VIT, AP Campus. In
addition to this, he is a NASSCOM
Associate Analyst certified Trainer and
handling various Data Analytics courses in
recent days. He has been teaching for 11+ about years and have guided
and motivated over 100+ students on independent research projects over
the last five years and personally supervised Undergraduate Research
interns in the last five years. Also, currently training students on the pro-
gramming, technology side IoT and AI and ML and helping them get indus-
try ready. My research interest includes Cloud and Big data Analytics and
AI & ML, Hadoop, Apache Spark. Also Dealing with IoT projects,
Drones, NoSQL (MongoDB, Cassandra, Neo4j), R Programming and
Full Stack developer. Also, he is interested in teaching, research article writ-
ing, developing IoT solutions and making as patent. He published several
articles, conference papers, journals, and authored book chapters. He also
organized and attended 70+ FDPs, 25+ workshops, 10+ refresher courses,
20+ conferences, and 21+ technical talks in his domain and gained exper-
tise. He received 10+ awards namely Best Researcher Award, Best
researcher with Patent Award, Academic Excellence awards,
Most Selfless service award, Above & Beyond Award, Best Paper
award, Best performer Award, best Team Leader Award from var-
ious prestigious organizations in India .A l s oh e holds 10+ Indian
design and Product patents in IoT and Machine learning areas
and also 4 more are under examination in IPR. Life member in interna-
tional professional bodies such as ISTE, IAENG, ISRD, IFERP. Also, he is330 D. Sumathi et al.Former Founder of Google Developer Club VIT AP and Founder of Rudra
Open Community Organization (NGO) and Microsoft Academy and
Learn Programme at VIT.
Mr. P Sivaprakash is presently a Research
Scholar cum Assistant Professor, Department
of Computer Science & Engineering,
PPG Institute of Technology, Coimbatore,
affiliated to Anna University-Chennai,
Tamilnadu, India. He received the B.E.,
Computer Science & Engineering and M.
Tech Information Technology degree from
Anna University, Chennai. His research inter-
ests include Cloud Security, Network secu-
rity, Big data Analytics, High performance
computing and cloud computing. He has an
abundant knowledge in networks and big data analytics. He has completed
a certified course in Cisco certified Network Associate.
Dr. Prabha Selvaraj is working as an
Associate Professor SG1, SCOPE, VIT-AP
University, Andhra Pradesh. She has com-
pleted her B.E CSE, M.E CSE and PhD in
Anna University, Chennai and has 20+ years
of experience in teaching and industry.
Her research interests include Data Mining,
Security, Wireless Sensor Network, IoT,
Information Retrieval System, Image proce-
ssing, machine learning and blockchain. She
has published 35+ papers in various national
and International journals and conferences
and seven book chapters in IGI, Wiley and IET in the area AR, Big data ana-
lytics, IoT, WBAN, Block chain and Image Processing. She has published
seven patents. She has received Top best faculty award in CSE by EET
CRS and Best Educators Awards 2018 by IARDO, Distinguished Leader
in Engineering Discipline 2019 by Higher Education Leadership Awards.
She is a reviewer in few national and international journals and conference.331 5G Communication for edge computingThis page intentionally left blankCHAPTER TWELVE
The future of edge computing
Swaroop S. Sononea, Kavita Sainib, Swapnali Jadhavc,
Mahipal Singh Sankhlad, and Varad Nagard
aDepartment of Forensic Science, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra,
India
bSchool of Computing Science and Engineering (SCSE), Galgotias University, Delhi, Uttar Pradesh, India
cGovernment Institute of Forensic Science, Aurangabad, Maharashtra, India
dDepartment of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
Contents
1.Introduction 334
2.Emergence of edge computing 335
3.Drawbacks of out-of-date cloud computing 335
4.Significance of edge computing 338
5.Edge computing technologies 339
5.1 Fog 340
5.2 Cloudlets 341
5.3 Mobile edge computing 342
6.Possible advancements in digitization using edge computing 342
6.1 Edge computing in network architecture 343
6.2 Remote monitoring 343
6.3 Healthcare 344
6.4 E-Commerce 344
6.5 Markets/business 345
6.6 Security 345
6.7 5G Communications 345
6.8 Smartphone advancements 346
7.Opportunities for edge in future 346
7.1 Multimedia and edge computing 347
7.2 Energy efficiency and edge 348
7.3 Smart living 349
7.4 Communication efficiency 350
7.5 Resource management 351
7.6 Environment monitoring 351
8.Conclusion 352
References 353
About the authors 355
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.009333Abstract
Edge computing is dispersed computational model that accompanies computing and
data storages closer to the sites wherever it is required, to increase response rates &preserve bandwidths. Edge computing is modifying form data is being managed,treated, & produced by billions of machines throughout the globe. The rapid emer-gence of Internet linked gadget with the Internet of Things (IoT) among with newer
technologies which need real-time computational powers, continue to drive edge com-
putational system. The future of edge computing holds the key assets as it can be mod-ified to advance any of the technology currently present. We will be able to transformthe computing technology by constructing newer network architectures. Drawbacks oftraditional cloud computing have given birth to edge computing. Edge computing cantransform the present technologies efficiently and productively. Edge includes elasticityto hold existing and upcoming AI (Artificial Intelligence) requests & the facts that com-putations at edge dodges network latency & enables quicker response. Soon, we cansee that edge computing is becoming the only one and most relied upon computingtechnique for advanced machines. The capability of edge computing is tremendous, it isjust needed to be understood and applied in day-to-day life.
1. Introduction
In the recent past, there has been an incredible expansion in the com-
munication sector especially wireless communications [1,2]. The data rates
have
also been grown speedily. Accompanying data rates, there arose
numerous novel wireless facilities, which need exhaustive computation in
a brief time [3–5]. Edge is the concept to store and use it and bring the com-
putational
sources nearer to the gadgets which produce and consumes the
data. The predominant counterpart to such an approach is cloud computa-tion, where the alleged sources are present at the data centers [6]. Edge
fetches 
computation and storing sources closer to the handlers & data
resources, consequently avoiding costly and slower links to detached cloud
computational substructures.
Edge is a computing model that allows edge-server in smaller (mini)
clouds or called edge clouds to extend the cloud abilities at the edge of
the networks to execute computationally rigorous errands and stock a huge
quantity of numbers at closer proximity to handlers. Edge is the start or end
of the objects which the network covers. It could be any computational ornetworking gadget. A smartphone could be the edge among the cloud & thebody. Edge establishes a novel idea into the computational landscapes. It
offers the services & facilities of the cloud near to the users and is recognized
by the quick processing & fast application response timings. The present334 Swaroop S. Sonone et al.advanced Internet abled gadgets like surveillances, virtual realities, & real-
time traffic tracking requires quick processing & fast responsive gadgets [5].
Handlers/users
generally run such applications on their source controlled
mobile-phone while the main services and processings are executed on the
cloud server. Leveraging the service of clouds by mobiles results in higherlatency & mobility problems [7]. Edge satisfies the above-mentioned appli-
cation 
demands by carrying the processing to the edge of the networks. Edge
has an above hand over the cloud in varied dimensions making it a possibleand strong contender to apply in all fields. Edge computing can be used foralmost every possible connected technology. It can sufficiently and power-
fully replace the existing technologies and may even contribute to the
upcoming automation. The emergence of edge over cloud computinghas opened the barricades to be utilized in all areas of human welfares.
2. Emergence of edge computing
Technical advancements in devices and gadgets computing are all-
owing an innovative current of real-time & universal implementations, likeintellectual aid, traffic monitoring, augmented reality, vehicle tracing, andcooperating filmed streamings [8]. Edge has lately earned incredible consid-
eration
in academics as well as industries. The novel model goals to locate
sources for storing and computing a bit nearer to the end operators, i.e., to
the edge of the networks. Edge is referred to as the type of tech which can
execute computing at the networked edge. The principal incentive to do sois the shortcoming of present-day cloud computational substructures whenprocesses larger volume information for latency dangerous application. Edge
is predicted as a next-gen communicating network to meet the demands of
next-gen systems. The instant emergence of edge computing has given birthto various fields where it can be successfully implemented and applied. It cantransform an existing technology by integration with the upcoming mech-
anization. Edge has converted itself into a potential option to go whenever a
new challenge puts out its head ( Fig. 1 ).
3. Drawbacks of out-of-date cloud computing
Cloud computation is a computing model that provides on-demand
facilities to the users by the connection of computational sources which
consists of storing facilities, computational sources, and many more.
Cloud computation further concentrates on the effective optimizations of335 The future of edge computingdistributed sources among various handlers [9]. Cloud caused a technical
change and standard turn into the ICT (Information & Communication
Technology) division in the past years. Cloud consummated a huge accep-
tance in nearly each area of social lifetime [10–13]. Conventional cloud
computations, that has a central computational model which offers incessant
admittance to extremely proficient information hubs, has been espoused to
permit UEs to divest computing and storing into the data hubs [14].I ti s
significant to keep in mind that edge does not aims to be a substitute for
the cloud, but to accompany it [15]. Cloud cannot offer any guarantees con-
cerning the communication part because data centers are positioned long
from the customer & characteristically, neither the cloud supplier nor the
operator has regulations over the transiting networks. Edge computation
resolves the issues in the order that it enhances scalability in the system
dimensions, i.e., a greater number of users could be aided with lower latency
link when attaching supplementary edge websites for example at wireless
gateways of the users. Though, due to the inadequate sources at distinct edge
locations, edge computation can’t provide the exact all flexibility like the
cloud [16]. Although cloud is retrieved by the wired backhaul networks,
one distinctive feature of the edge is that customers are characteristically
linked to the edge sources by the wireless passes. In addition to the wireless
accessible technology, it is also possible to differentiate the edge networks by
Fig. 1 Necessity of edge computing.336 Swaroop S. Sonone et al.its communicating outlines. Commonly, the present computational world
involves human and varied objects which are interconnected [17,18] .
There is no doubt that cloud is an efficiently organized computational model
but as the manufacturing development of technologies, it flops to familiarize
to the progressive interacting models. Servers of the clouds are incapable to
handle such a huge quantity of information with its conventional networked
structures. Additionally, fog computing experiences difficulties in the matter
[19]such as customizations of the apps of fog server as per the local require-
ments, accessibility of suitable fog sources to fulfill the need locally, the
amount and locations for deployment of fog servers to meet the require-
ments perfectly. Though fog is superior than cloud computing still it has
few safety restrictions. Fog node are vulnerable to invaders & hacks.
Therefore, because of the beforementioned threats of fog & cloud, the edge
ideology has come in reality [20]. Because of the problems of cloud, a dis-
persed computational model called fog computational model was intro-
duced where the computing of information is used to happen closer to
the network edges. But because of few restrictions of fog computational
model similar kind but a developed was introduced known as edge comput-
ing that was able to solve all of the prime problems and difficulties of cloud
and fog to a greater amount [21]. This cloud computational issue could be
solved by the three edge computing models which are further discussed in
Section 5 [22 –25](Fig. 2 ).
Fig. 2 Comparison of cloud computing & edge computing.337 The future of edge computing4. Significance of edge computing
Edge is an adjunct of cloud where the computational facilities are
transported nearer to last handler at the network edge. Edge idea has been
advanced to report the issues of higher latencies in delaying delicate facili-
ties & implementations which are not correctly used among the cloud com-
putational model. Such apps have the subsequent demands: (1) lower and
expectable latencies, (2) position consciousness, and (3) flexibility provision.
Though edge gives numerous benefits over the cloud, researches onto the
upcoming fields are still in its early stages. Edge is an independent compu-
tational paradigm which comprises of various dispersed varied gadgets which
interacts with the networks and execute computational task like the storing
and processes [26]. Edge is a progressed stage of cloud. Edge reduces the
weight of a cloud by offering the sources & facilities in edge networks.
Though, edge complement cloud by increasing the operator services for
the delay sensitive implementations [27]. Edge approves a distinct paradigm
which carries cloud abilities close to UEs to decrease the latency. Fig. 3
showcases the cloud computing and edge computing models.
Fig. 3 Cloud and edge computing models.338 Swaroop S. Sonone et al.Edge could either be used as single computational medium, or a coop-
erative medium with varied components, consisting the cloud [28]. Today
edge
is the essential as the conventional cloud computational platform is not
appropriate for decidedly communicating implementations which are
exhaustive in computation and has greater Quality of Service demands, hav-ing lower latency and higher outputs [5]. This is due to cloud which might
be 
far away from the end users, that expands the consumption of energy also.
To put it in another way, cloud server is stereotypically situated at a cen-
tral system, and edge server having mini clouds are situated at a network edge
[6,29] .
Additionally,
wireless gateway can offer an extra added related data to
the applications, to some degree which is not obtainable in cloud [4].
Edge
establishes a novel idea into the computational space. The presently
advanced Internet abled implementations like surveillances, simulated real-
ity, and real-time tracking or monitoring needs quick processes & quicker
response timings. Operators generally runs such application on their sourceinhibited mobile phones while the central facilities and processes are exe-cuted on the cloud server. Supporting service of the cloud by mobile gadgets
results into higher latency and mobility linked problems [7,30] . Edge com-
putational
tech completes the forementioned applications by providing the
processes into the network edges [29]. It will be an insightful scene to
observe
the integration of upcoming paradigms of Internet of Things
(IoTs), Blockchain technology, Artificial Intelligence with the edge com-puting. The field of research is open to work on these technologies with
the combination of EC.
5. Edge computing technologies
Edge-cloud associated techs are gaining consideration from all over
the world. Though, the idea & growth presently are in a significantly bud-ding stages and numerous difficulties are present to be diagnosed from everyfield especially from the industrial and academic perspectives. Many of thesepresent edge structures consists of devoted physical edge servers which
employs with the edge devices for computing and storing, or consist simple
stevedores which gives a restricted virtualization support at the edge node
[5]. Edge provides a chance to perform as a discretion allowing intermediary
among
the operator’s information and cloud dependent facilities, particu-
larly when handlers have admittance to edge structures which are in its trust339 The future of edge computingarea or those which are handled by the trusted suppliers [6]. Following are
the three basic and fundamental sides of edge computing ( Fig. 4 ).
5.1 Fog
Fog computational idea was given by CISCO, that permits the applications to
execute straight at a network edge by millions of connected smart gadgets.
Edge & fog application specifically focuses on the subcontracting computation
from static or mobile operator gadgets to compute nodes in closeness.
Though, these applications don’t completely exploit the assistances of edge
due to sources of adjoining operating systems which aren’t taken into
account as a probable computing node. Prominent progress in the arena
of edge computation emphasis on handler’s application capable to external-
ization computing on computational node at network edge known as fog
nodes [19,31] . Fog computing offers information, computational storing
and application service to operators. This enables novel diversity of use
and facilities. Fog computing is not at all likely to substitute cloud technol-
ogies, though its capability to decrease dormancy and rise safety and extra
conformation encounters.
The chief duty of fogging is to deliver data close to the users at edge
network. A significant advantage of edge computation is that it enhances
time required to act & decreases responsive time to millisecond, and preserv-
ing system sources. Fog is an important development in the computing tech-
nology. Emergence of fog has emphasized the dominance of a dispersed
system for computation which is more elastic and nimbler than the conven-
tional central model. Such dexterity and elasticity are essential with Big Data
Fig. 4 Edge computing models.340 Swaroop S. Sonone et al.implementations enchanting the forms of the Internet of Things and its
lower or no latency demands [29].
F
og has lesser latency, lesser likelihoods of outbreaks on data paths. It helps
real-time communication and flexibility that is improved than cloud compu-
tational technology. Fog is a newer model and experiences numerous diffi-culty which differs from the problems it receives through cloud. These
problems consist administration of varied gadgets, structural problems,
privacy, flexibility and security issues. Fog consists of various gadgets, withnumerous kinds of information gathered. Interoperability among variedobjects is a tiresome work. If the quantity of linked gadgets surpasses, this
may increase scalability problems for the fog computing. Additionally, for cor-
rect administration of sources and load balancing, an effective source handler isneeded. Building such source handler for varied gadgets and information is notan easy job. It is also perilous to execute proper regulation and administration
of devices, particularly those running real-time implementations.
Additionally, the regulation of road-traffic and billing mechanism is a
must needed. One of the prime difficulties in fog is to plan a fair-minded
billing structure for the facilities provided. Fog facilities are given through
different pricing plans and schemes, and the handlers expects higher QoS
with lowest prices [32]. The billing model must be fair minded & balanced
to
fascinate more readers and produce high revenue. Because of inaccessi-
bility of any ordinary billing model for fog, it is still an open study subject.Fog comprises arrangement of luxurious gadgets and system. It is significant
to execute predeployment testing of fog medium utilizing few simulation
tools. Though, there is no such standard simulation model/tool present atthe time for fog, that makes it an open study subject as well. Lastly, the safetyagainst malicious threats and security attacks are also a prime study challenge
for fog computing [33].
5.2 Cloudlets
Alike fog, cloudlets too represent the central tier of the three-tier structure:
Mobile device –Cloudlets –clouds. Cloudlets are seen as a data center in the
box with a goal to take cloud facilities near to the mobile handlers. Not justunderstanding to important technical development, mobile gadgets, likesmartphones are still source lacking when related to other immobile systems
like computer and server. This is mainly due to the smaller size, smaller
memory, and short battery life. On other hand, there is an important upsurgein advancement of varied mobile apps. Maximum of the developing341 The future of edge computingapplications, like virtual reality, responsive media, voice recognition, natural
language processing, need higher quantities of sources for processing with
least latencies [7].
To
match these requirements, cloudlets are manufactured with virtual
characteristics to particularly give computing sources to mobile operators.Mobile devices acting as small client, can offload computing work by a wire-
less network to the cloudlets, which are positioned one flight away.
Therefore, a cloudlet’s incidence in mobile device’s surrounding is essential,as the end-to-end response time with implementing applications should besmaller and expectable [8].
5.3 Mobile edge computing
Rise in the popularity of Smart Mobile gadgets like mobile phones and tabletendorses the growth of MCC or commonly known as mobile cloud comput-ing, and it improves the source-poor mobile hardware & decreases the growth
price of mobile applications by mixing the cloud computing into the mobile
environment [17]. The ETSI or European Telecommunications Standards
In
stitute has presented the idea of mobile edge computing where mobile oper-
ators can use the computational fac ilities from the base-stations [11,34] .M E C
offers
unified combination of various implementational service providers
and merchants toward mobile users, init iatives and other upright parts. It is
a vital part in the upcoming 5G structure which assists diversity of advancedapplications and facilities where ultra-low latency is essential [35].
M 
EC can resolve the significant calculations of content caching, task
scheduling, collaborative processing & other difficulties in huge scale systems;thus, mobile edge computing has concerned the attention of related res-earchers in numerous nations meanwhile it was planned [12]. Mobile edge
co
mputing tech is near to handler and can procedure information from varied
Internet of Things gadgets at the similar time to accelerate the advancement ofnew businesses. Thus, mobile edge computing tech has superior act in endors-ing smart cities and big data analysis, creating future cities smarter [16].
6. Possible advancements in digitization using edge
computing
Edge is the future. It can be applied to any sector without any doubt.
There is possibility of addition of edge computing in any field of technology.
Few of the applications of edge computing in the upcoming digital world are
as follows ( Fig. 5 ).342 Swaroop S. Sonone et al.6.1 Edge computing in network architecture
Edge solution is generally multifaceted dispersed structures surrounding &
balance between the work among the edge layer, the edge cloud or edge
network, & the enterprise layer. Additionally, along with the edge devices
there are edge local servers [21]. Edge is Internet of Things at large amount
due to the high increasements in gadgets & information being produced by
such gadgets, there will be blockages & latency problems with present struc-
tures. Edge computation discourses these problems, by shifting the
processing to the network edge. This shift to the edge of network, way from
the data centers & near to the users, cut down on time taken to transfer infor-
mation & take opinions, when related with conventional dispersed cloud
computing. Edge allows contracting out computing to compute node in
the near vicinity of the data sources, that centrals to lower latencies &
enhanced bandwidth usages [25]. Edge computing plays significant role in
IoT applications where instant processing of data is required [16].
6.2 Remote monitoring
New digital services have emerged that are much more proactive with faster
response to critical incidents and execution of maintenance activities.
Imagine a vendor service representative appearing at one of your sites to
Fig. 5 Applications of edge computing.343 The future of edge computingfix a problem you didn’t even know you had yet! New generation admin-
istration tool takes advantages of newly introduced technologies like cloud
computing, data analytics, AI (Artificial Intelligence) and mobile communi-cation. Compared to the traditional, license-based, install a third-partyserver on every site approach, next generation management tools offergreatly simplified deployment and maintenance of the tools as well as man-
agement of the physical infrastructure. This is especially valuable when you
are managing multiple edge computing installations with widely dispersedassets. These new digital remote monitoring services are built on top of thisnew management platform and share its benefits [28].
6.3 Healthcare
There have been countless software-enabled improvements in healthcare.Today, apps, medical devices, and data allow machines and doctors to mon-itor and treat patients near and far. As medical devices improve, they will be
able to sense more things about your body and respond appropriately in
real-time [6]. In order for these healthcare devices to work, they need edge
computing
and powerful AI, which isn’t there quite yet. This technology
can enable emergency calls and response before heart attacks, shows vital
signs monitoring and responses.
O n eo ft h ei m p o r t a n tu s e so fe d g ei nh e a l t h c a r ei sn o n i n v a s i v ec a n c e rc e l l
monitoring and responses. It can give a s mart and personalization health nudges
along with the electrolyte imbalance monitoring and notifications [29].
Edge
have been positively applied in current times & is now generated
utilized in various medicinal devices. Edge allows operators to regulate & act
to health associated information produced by different servers. Variousstructure which utilizes cloud, fog, and edge computing has been used to
gain the assistances of concerted computing models. The utilization of smart
sensors in Intensive Care Unit & closed loop networks is able of savingnumerous lives. Health consultants having admittance to the cloud serverscan directly analyze patient illness and help them instantly [5,7].
6.4 E-Commerce
E-Commerce can be profited extremely through edge. For lower band-width of wireless systems, there may be greater postponements for mobilephones. Today, online shopping is becoming prevalent, so to advance the
operator usage it has become an essentiality. At an edge node information
can be coordinated in the backend with the clouds. Edge thus can advance344 Swaroop S. Sonone et al.user experience & latency for time sensitive implementations. Users can get
the exact product from market through advancements in network struc-
tures. Real-time usages for wearables and accessories achieved throughthe edge computing [30].
6.5 Markets/business
Like the E-commerce sector, the markets have also great potential ofadvancing using the edge computing. There is great upsurging requirementinto the companies to exchange the information. For the consideration, the
social healthcare area is where administrative organizations, pharma indus-
tries, logistics and insurance companies are needed to cooperate amongthemselves for ideal efficiency and offering the on-time facilities. It is justhappening due to the assistance of edge. A shared platform is offered by edge
where all these companies exchange their information & get latest data in
real-time when the situation demands [35].
6.6 Security
Edge has introduced facial identification by the Internet of Things (IoT) sys-tem that has an effective implementation at airport for inspection at securitystations and in various platforms too. Different kinds of sensitive substitutesmake the security system and everyday living very secure. Truthfully, it is
greatly appropriate in cybersecurity mediums as it mainly concerns with
the information sources not by any platforms. Edge security is an implemen-tation of safety practices at network nodes which are outside the systemcore [16].
Edge
needs the similar fundamental safety characteristics like the net-
work cores; the complete network should be observable to managers.Automatic regulating apparatuses should be utilized by these handlers.Complete data is required to be encoded. There should be regulations on
admittance to operate information & network sources. Edge gadgets can
take the form of localized data centers, miniature data centers, or practicallyany tiny gadget with computational power closer to the handler/operator.IoT is going to progressively depend on & disposition by the edge network.
6.7 5G Communications
5G is next-gen mobile network which seeks to attain considerable enhance-ment on superiority of facilities like high output & lower latency. Edge is
developing tech which allows the growth to 5G by carrying cloud abilities345 The future of edge computingcloser to the handlers (or operator equipment) to tame the internal issues of
conventional clouds, like higher latencies & the absence of safety. MEC has
developed an essential portion in the rise of 5G tech in the cellular networksas edge executes computing near to the mobile users [36].
Edge 
advances system performance to assist & position dissimilar situa-
tions, like remote surgeries. Though the positioning of edge in 5G offers
many advantages, the integration of edge & 5G carries new problems which
should be solved in the upcoming times. Edge can be located at varied pre-mises like enterprises, in industrial structures, in houses & transporters, con-sisting rails, flights and automobiles. The edge substructure can be regulated
or presented by interactive facility providers or varied kind of service pro-
viders. Several other uses cases which need different implementations tolocations at various places. In these situations, a dispersed cloud is beneficialthat can be seen as an implementation setting for requests over numerous
locations, counting connectivity regulation as a solution [19]. Nowadays,
there
are only new market for the kinds of applications that enables 5G like
virtual reality, mass scale IoTs, AUVs/drones, robotics, etc. Edge can offer
developers a setting to make the 5G appliances which don’t exist today even
without “full 5G” being available yet [33].
6.8 Smartphone advancements
Smartphone advancements in the sectors of gaming, application develop-
ment & real-time data management and transfer are only possible due tothe edge computing. Business experts says that this is a developing drift
which will be a game changer in the mobile phone markets. As the reloca-
tion of the data centers near to the users, edge dramatically decreases latencythat results into significant data transfers at a faster and larger quantity.
In gaming sector, a small delay of a second is capable to completely dis-
turb the experience of user. Gaming is a real-time experience, & number of
people are playing tournaments from their gadgets every day. But when
thousands of these people are playing a graphic integrated game at a sametime, the greater the probability of latency is tremendously increased.Edge networks can offer a faster and seamless experience [11,30] .
7. Opportunities for edge in future
Edge computing depend on a diversity of technologies that have nur-
tured its growth. Edge is founded on the concept of locating smaller serversknown as edge servers or sourced networking gadgets in the close proximity346 Swaroop S. Sonone et al.of handlers. By this way, few of computing & data storing load is carried
from cloud to edge servers. Operator device generally consists of wireless
sensor systems, smartphones, handheld devices, & numerous IoT gadgetswhich needs real-time responses [3].
P 
ositioning computing & storing sources at network edge can allow a great
quantity of appliances which require real-time responses. Examples of these
applications consist road-traffic regulation and steering, which counts (1) traffic
reporting & computing of roads for a particular area close to the edge, (2) datasorting & segregation, which executes prefiltering of information & content atedge prior to sending it to clouds to decrease the data size, and (3) virtual real-
ity, real-time communicating videoplays, & health regulatory networks which
could create quick responses by utilizing edge nodes, thus advancing the userexperience for time bounded appliances [4].
In
spite of difficulties which rises when realizing edge computing, there
are various chances for institutional researches. Opportunities can anything,just it is necessary to see and utilize it. Benchmarks, standards, and mar-ketplace edge can be utilized in day-to-day practice & can be availableopenly if tasks, relations and dangers of all revelries involved are expressed.
Frameworks & languages are many options to execute applications in the
cloud paradigm. In light weighted libraries & algorithms, not like server’sedge nodes that won’t assist heavy weight software as the hardware restraints.Edge can benefit industries & academic collaborations.
Edge offers a special chance for institutions to focus again on its research
activities widely in applied distributed computing, particularly inside thecloud and mobile computations [31]. Leading academic organizations that
have
dependable industry & government relationships have yet produced
more expressive and impactful study. Researches in an edge space can be
drifted by an open association of industry associates like as mobile handlers
and user, device developers & cloud providers, along with interested aca-demic allies to share benefits of both [6]. Fewer edge computing applications
are 
discussed as follows.
7.1 Multimedia and edge computing
Multimedia, particularly, video is one of the prime clients of total Internetbandwidths. It has been stated that in the year 2015, video data was of 70% of
the total Internet traffic. These stats are fortuned to grow to 82% in the year
2020 [19]. In the upcoming Internet of Things situations, numerous mul-
timedia 
producing devices like closed-circuit TVs & visual sensor systems347 The future of edge computingwill produce huge volumes of multimedia data [31]. As the multimedia
needs
higher bandwidths, processing, and storing, so regulating these large
volumes in means of interactions, processing, & storing is a real problem.
Edge is intended to help in these conditions to decrease the completeend-to-end bandwidth utilization, supply, effective processing, and storingfor multimedia [29].
Today,
multimedia world is utilizing more newer techs by the
implementations of edge applications such as video analytics, smart speakers,smart TVs, etc. For example, the smart speaker has the inherent arrangementto track the voice teaching and play the melody. Edge computing has pro-
vided a solution for low quality of video-conference & it connects the
servers at the edges of the gadgets which has resolved all the annoying dif-ficulties of video conferencing. Edge intelligence powered by edge comput-ing has drifted all the entertaining mediums from a smaller screen of mobile
phones to huge screen of televisions by presenting smart TVs [33].
7.2 Energy efficiency and edge
At this stage, the world will require an advanced computational solution to
save our finite resources and prevent climate change. Farming in drought-stricken areas can be accomplished with drip-monitoring and measurementsystems. Before, it was prohibitively expensive to outfit a 1000-acre farmwith sensors and connect each to a cloud system. With edge computing, net-
work connectivity isn’t as big of an issue. These systems can make indepen-
dent decisions that balance ground moisture with available water resources
[1]. Energy efficacy is one of the compulsory & chief issue today due to envi-
ronmental
effects, energy requirements, and pricing.
The Information Communication Technology area is one of the prime
energy clients. ICT division is also regarded as a chief CFC (Chloro Fluoro
Carbon) contributor, a prime greenhouse gas, emitting about the 2% ofglobal CFC emissions [29]. The greenhouse gases emissions by cloud data
centers 
are calculated to be 1034 metric tons in 2020, which indistinctively
raises the climatic concerns and calls for accurate answers. In near past, var-ious proposals have been offered to employ edge computing for refiningenergy competence of cloud facilities and operating devices. Mobile edgecomputing allows the offloading of computation intensive & energy con-
suming applications from mobiles to servers, thus decreasing the energy
usages of operative gadgets. Lastly, edge has been studied as an incentivefor refining energy competence of cloud applications [3].348 Swaroop S. Sonone et al.7.3 Smart living
Edge computing has taken autonomous cars pretty far. However, this tech-
nology will be far more life-changing when it has the ability to connect to
other cars, buildings, and structures. These smart ecosystems will compile
the benefits of autonomous driving and turn cities into AI-powered
machines [29]. Communication & conversation among smart appliances,
like controllers, sensors, and actuators, is an essential and general spectacle
in all areas of smart living and universal surroundings [5]. Smart objects &
cloud interactive model utilized in varied smart solution, like cognitive gate-
ways, showcases varied restrictions & flaws in the cloud interactions, partic-
ularly impulsive postponement and jitter [5]. Edge technology provides a
solution for such issues which delay the ideas and actions of smart living solu-
tion. Development in the smart gadgets and detectors are focusing to fulfill
the smart lifestyle goals. EHOPES a common abbreviation of Smart Energy,
Health, Offices, Protection, Entertainment, and Surroundings presents the
basic parts of smart life. Fig. 6 shows the usage of edge in smart houses [ 5].
Smart homes consist of smart TVs, smart lights, robotically controlled
vacuum cleaners and various home appliances that run on the basis of IoTs.
Fig. 6 Smart home applications.349 The future of edge computingObjects are regarded to be ineffective when linked with the cloud by just
adding a wireless unit to such gadgets. The efficiencies of such smart homes
are increased drastically when segregated wireless detectors & regulators arelocated in the walls, floor, and pipe of such homes along with connectedappliances. The huge quantity of data produced by detectors; regulators &things must be treated nearby at edge of the smart homes in place of trans-
ferring it to clouds for effective utilization of such smart home [4]. This is
applied
by edge with an edge way gadget located in the house along with
the things & running a specific OS called edge operating system limited just
for edge. Smart houses furnished with a great amount of IoT gadgets belongs
to an upcoming application area of edge. IoT implementations intended for
the regulating & metering of smart houses will enable users to get automaticand accurate readings of various meters and allow invoices accordingly with-out any delay. Such IoT application are made for distant regulation &
metering of different efficacies like water, power, and fuel. The information
gathered from IoT gadgets can be transferred to the edge servers for the fur-ther processing rather than transferring it to the clouds, that can central to thereal-time data analytics [19]. Edge & IoTs can be utilized in smart power
administrations.
These appliances automatedly notices the usages and supply
patterns. Distributing nodes are utilized by the edge for real-time detection &processing. Cloud-computing is used as a cooperative tool to make theseappliances robust and dynamic for huge quantities of data in the positioningof wider area energy systems. Edge in IoTs can help in making smart towns.
Edge can be efficiently utilized in light regulation networks of roads &
streets, water and air superiority regulation, finding alternative ways inemergency situations of accidents or disasters, & automated watering gardensin towns. Edge in IoT setting assists traditional logistics & provides newer
captivating potentials which makes the flow administration of systems auto-
mated and relaxed [31].
7.4 Communication efficiency
5G and edge computing technologies has the capability to help organizationsdeliver a widespread range of exciting products and services. As IoT devicesand their algorithms become more powerful, they need to be equipped withlarger processors and storage, and consequently higher power requirements.
This is not always possible within the limitations posed by the form factor of
the device, and hence designers need to leverage the cloud for better350 Swaroop S. Sonone et al.compute and storage. With their distributed assets and unique network
resources, Communication Service Providers are in a unique position to
create value from the emerging distributed cloud and edge computing par-adigm. The convergence and availability of 5G and edge computing shouldalso usher in several new and interesting services that had not been possibleearlier due to inadequate bandwidth for devices on the move, as well as por-
table devices that required high computing resources but that could not
leverage cloud computing due to latency issues. Furthermore, network pro-viders and digital ecosystem enablers can also leverage new business oppor-tunities as they can fulfill the infrastructure requirements and provide better
connectivity solutions [5,37] .
7.5 Resource management
Edge computing solutions give the benefits of cost savings & simplifications
of edge endpoint administrations. By applying an autonomous life-cycleadministration method, one administrator can manage deployments tothousands of endpoints, with management tasks carried out based on intent
and with no intervention needed [3].
7.6 Environment monitoring
Environmental regulation, consisting data gathering and reporting of metricssuch as: outdoor temperature and humid ity, water quality, pH value and tem-
perature, dissolved oxygen, ammonia, nitrogen, and nitrite [30,35].T h em o r e
int 
elligent the edge becomes, the more i mportant it is to understand the per-
formance of the application at the edge and how it impacts the overall businessperformance. The growth of edge computi ng is being fuelled by the Internet of
Things. Vast amounts of data are being generated by sensing devices that cap-
ture information about the physical e nvironment—everyt hing from humidity
and light to chemicals and vibration. The collaborative use of edge and cloud
computing in IoT can enhance the quali ty of existing monitoring systems. An
automated system will collaborate with sensors and actuators. Applications
have been developed f or monitoring critical entities that exert a major effect
on the environment. These entities include monitoring of gas concentrationin air, water levels in lakes and undergrou nd, lighting conditions, soil humidity,
and changes in land position. Environment monitoring is crucial in many fields,such as agriculture, forestry, and food safety [2,38] (Fig. 7).351 The future of edge computing8. Conclusion
Edge computing is gradually developing and also paving the way for
more effectual dispersed computing. The main aim of edge computing is to
offload computations near the edge devices. It will also be necessary for
researchers to address the above challenges in order to mitigate them in
the future of edge computing. Many organizations are adopting the use
of edge computing since it is useful and also productive. More innovative
applications of edge computing are emerging to make human’s life safer
and more comfortable by supporting multiple devices such as secure smart
homes, automated vehicle insurance, safer remote surgeries, etc. Therefore,
edge is growing as an upcoming tech to satisfy the current requirements of
increased data production. Currently, edge computing has pleased all struc-
tures of transmissions, substation, energy productions, energy usages, energy
supply and transmittance involved in all network related fields. Lastly, we
can conclude that the hope of upcoming future is on the shoulders of edge
computing. Integration of varied and novel technologies will help into
progressing humanity into safe hands and efficient usage of resources.
Fig. 7 Future applications of edge computing.352 Swaroop S. Sonone et al.References
[1]F. Shi, J. Xia, Z. Na, X. Liu, Y. Ding, Z. Wang, Secure probabilistic caching in random
multi-user
multi-UAV relay networks, Phys. Commun. 32 (2019) 31 –40.
[2]L. Fan, N. Zhao, X. Lei, Q. Chen, N. Yang, G.K. Karagiannidis, Outage probability
and
optimal cache placement for multiple amplify-and-forward relay networks, IEEE
Trans. Veh. Technol. 67 (12) (2018) 12373 –12378.
[3]J. Zhao, Y. Liu, Y. Gong, C. Wang, L. Fan, A dual-link soft handover scheme for C/Uplane
split network in high-speed railway, IEEE Access 6 (2018) 12473 –12482.
[4]X. Liu, F. Li, Z. Na, Optimal resource allocation in simultaneous cooperative spectrumsensing
and energy harvesting for multichannel cognitive radio, IEEE Access 5 (2017)
3801 –3812.
[5] S. R. Jena, R. Shanmugam, K. Saini, S. Kumar, “ Cloud computing tools: inside views
and
analysis”, International Conference on Smart Sustainable Intelligent Computing
and Applications under ICITETM2020 Elsevier, Pages 382 –391.
[6]K. Saini, P. Raj, Handbook of Research on Smarter and Secure Industrial ApplicationsUsing
AI, IoT, and Blockchain Technology, IGI Global, 2021. ISBN13:
9781799883678, ISBN10: 1799883671, EISBN13: 9781799883685.
[7]J. Ren, Y. He, G. Huang, G. Yu, Y. Cai, Z. Zhang, An edge-computing basedarchitecture
for mobile augmented reality, IEEE Netw. 33 (4) (2019) 162 –169.
[8]M. Villari, M. Fazio, S. Dustdar, O. Rana, R. Ranjan, Osmotic computing: a newparadigm
for edge/cloud integration, IEEE Cloud Comput. 3 (6) (2016) 76 –83.
[9]I. Stojmenovic, S. Wen, The fog computing paradigm: scenarios and security issues, in:2014
Federated Conference on Computer Science and Information Systems, IEEE,
2014, pp. 1 –8.
[10] W. Shi, J. Cao, Q. Zhang, Y. Li, L. Xu, Edge computing: vision and challenges, IEEE
Internet
Things J. 3 (5) (2016) 637 –646.
[11] U. Shaukat, E. Ahmed, Z. Anwar, F. Xia, Cloudlet deployment in local wireless
networks:
motivation, architectures, applications, and open challenges, J. Netw.
Comput. Appl. 62 (2016) 18 –40.
[12] I. Yaqoob, E. Ahmed, A. Gani, S. Mokhtar, M. Imran, S. Guizani, Mobile ad hoc cloud:a
survey, Wirel. Commun. Mob. Comput. 16 (16) (2016) 2572 –2589.
[13] W. Bao, D. Yuan, Z. Yang, S. Wang, W. Li, B.B. Zhou, A.Y. Zomaya, Follow me fog:
toward
seamless handover timing schemes in a fog computing environment, IEEE
Commun. Mag. 55 (11) (2017) 72 –78.
[14] L.M. Vaquero, L. Rodero-Merino, Finding your way in the fog: towards a comprehen-
sive
definition of fog computing, ACM SIGCOMM Comput. Commun. Rev.
44 (5) (2014) 27 –32.
[15] W. Li, Z. Chen, X. Gao, W. Liu, J. Wang, Multimodel framework for indoorlocalization
under mobile edge computing environment, IEEE Internet Things J. 6
(3) (2018) 4844 –4853.
[16] Z. Ning, X. Kong, F. Xia, W. Hou, X. Wang, Green and sustainable cloud oft
hings: enabling collaborative edge computing, IEEE Commun. Mag. 57 (1) (2018)
72–78.
[17] J. Gedeon, F. Brandherm, R. Egert, T. Grube, M. M €uhlh€auser, What the fog? Edge
computing revisited: promises, applications and future challenges, IEEE Access 7 (2019)152847 –152878.
[18] D.N. Le, R. Kumar, B.K. Mishra, J.M. Chatterjee, M. Khari, (Eds.), Cyber Security in
Parallel
and Distributed Computing: Concepts, Techniques, Applications and Case
Studies, John Wiley & Sons, 2019.
[19] N. Hassan, S. Gillani, E. Ahmed, I. Yaqoob, M. Imran, The role of edge computing in
internet
of things, IEEE Commun. Mag. 56 (11) (2018) 110 –115.353 The future of edge computing[20] P. Pace, G. Aloi, R. Gravina, G. Caliciuri, G. Fortino, A. Liotta, An edge-based archi-
tecture
to support efficient applications for healthcare industry 4.0, IEEE Trans. Industr.
Inform. 15 (1) (2018) 481– 489.
[21] J. Pan, J. McElhannon, Future edge cloud and edge computing for internet of thingsapplications,
IEEE Internet Things J. 5 (1) (2017) 439 –449.
[22] H.T. Dinh, C. Lee, D. Niyato, P. Wang, A survey of mobile cloud computing: archi-
tecture,
applications, and approaches, Wirel. Commun. Mob. Comput. 13 (18) (2013)
1587 –1611.
[23] N. Abbas, Y. Zhang, A. Taherkordi, T. Skeie, Mobile edge computing: a survey, IEEE
Internet
Things J. 5 (1) (2017) 450– 465.
[24] E.M.E. Computing, I. Initiative, Mobile-Edge Computing: Introductory TechnicalWhite
Paper, ETSI, Sophia Antipolis, France, 2014, pp. 1 –36.
[25] H. Li, G. Shou, Y. Hu, Z. Guo, Mobile edge computing: progress and challenges, in:2016
4th IEEE International Conference on Mobile Cloud Computing, Services, and
Engineering (MobileCloud), IEEE, 2016, pp. 83 –84.
[26] Y. He, F.R. Yu, N. Zhao, V.C. Leung, H. Yin, Software-defined networks withmobile
edge computing and caching for smart cities: a big data deep reinforcement
learning approach, IEEE Commun. Mag. 55 (12) (2017) 31 –37.
[27] Y. Hao, P. Helo, A. Gunasekaran, Cloud platforms for remote monitoring system: acomparative
case study, Prod. Plann. Control 31 (2– 3) (2020) 186– 202.
[28] Y. Cao, S. Chen, P. Hou, D. Brown, FAST: a fog computing assisted distributed
analytics
system to monitor fall for stroke mitigation, in: 2015 IEEE International
Conference on Networking, Architecture and Storage (NAS), IEEE, 2015, pp. 2 –11.
[29] S. R. Jena, R. Shanmugam, R. Dhanaraj, K. Saini, “Recent advances and future research
d
irections in edge cloud framework”, Int. J. Eng. Adv. Technol. (IJEAT) ISSN:
2249-8958, Volume 9 Issue 2, December, 2019 DOI: 10.35940/ijeat.B3090.129219 .
[30] K. Saini, V. Agarwal, A. Varshney, A. Gupta, E2EE for data security for hybrid cloud
services:
a novel approach, in: IEEE International Conference on Advances in
Computing, Communication Control and Networking (IEEE ICACCCN 2018)
Organized by Galgotias College of Engineering & Technology Greater Noida,October 12 –13, 2018, 2018, https:/ /doi.org/10.1109/ICACCCN.2018.8748782 .
[31] M. Liu, F.R. Yu, Y. Teng, V.C. Leung, M. Song, Distributed resource allocation in
blockchain-based
video streaming systems with mobile edge computing, IEEE Trans.
Wirel. Commun. 18 (1) (2018) 695– 708.
[32] P. Wang, C. Yao, Z. Zheng, G. Sun, L. Song, Joint task assignment, transmission, and
computing
resource allocation in multilayer mobile edge computing systems, IEEE
Internet Things J. 6 (2) (2018) 2872 –2884.
[33] Z. Chen, L. Jiang, W. Hu, K. Ha, B. Amos, P. Pillai, M. Satyanarayanan, Early imple-m
entation experience with wearable cognitive assistance applications, in: Proceedings of
the 2015 Workshop on Wearable Systems and Applications, 2015, pp. 33 –38.
[34] J. Gedeon, C. Meurisch, D. Bhat, M. Stein, L. Wang, M. M €uhlh€auser, Router-based
brokering for surrogate discovery in edge computing, in: 2017 IEEE 37th InternationalConference on Distributed Computing Systems Workshops (ICDCSW), IEEE, 2017,
pp. 145– 150.
[35] M. Syamkumar, P. Barford, R. Durairajan, Deployment characteristics of “the edge” in
mobile
edge computing, in: Proceedings of the 2018 Workshop on Mobile Edge
Communications, 2018, pp. 43 –49.
[36] J. Zhang, B. Chen, Y. Zhao, X. Cheng, F. Hu, Data security and privacy-preserving inedge
computing paradigm: survey and open issues, IEEE Access 6 (2018) 18209 –18237.
[37] S. Mohril, M.S. Sankhla, S.S. Sonone, R. Kumar, Blockchain IoT concepts for smartgrids,
smart cities and smart homes, in: Blockchain and IoT Integration: Approaches and
Applications, 2021, p. 103.354 Swaroop S. Sonone et al.[38] Singh, A., Sonone, S. S., Sankhla, M. S., Parihar, K., & Saxena, M. Blockchain for IoT
edge devices and data security. In Handbook of Green Computing and Blockchain
Technologies (pp. 141 –169). CRC Press.
About the authors
Swaroop S. Sonone is currently working as
an Assistant Professor in Department of
Forensic Science, Dr. Babasaheb Ambedkar
Marathwada University, Aurangabad,
Maharashtra, India and he has completed
his Bachelor’s degree in Forensic Science
and Master’s degree in Forensic Science, spe-
cialized in Digital and Cyber Forensics from
Government Institute of Forensic Science,
Aurangabad, Maharashtra. His broad area of
interest includes digital and cyber forensics,
mobile forensics, computer forensics, and
multimedia forensics. He is currently working on digital transaction security,
cybercrime vulnerabilities, mobile forensics, cyber security, digital evidence,
and their legal aspects in courtroom. He has been participating and
presenting his work for more than 20 national and international conferences
and workshops. He has published more than 15 papers in Scopus Indexed
Journals and several papers/chapters are under progress. He has delivered
talks at various places including Commissioner of Police Office,
Aurangabad, Sessions and District Court, Aurangabad about Forensic
Science and its significance in society. He has given briefings about the
instrumentations and their workings to visitors at Government Institute
of Forensic Science, Aurangabad. He has also worked as Secretary at
Council for Research Applied to Forensic Technology and Sciences, the
research hub of Government Institute of Forensic Science, Aurangabad.355 The future of edge computingDr. Kavita Saini is presently working as
Professor, School of Computing Science
and Engineering, Galgotias University,
Delhi NCR, India. She received her
PhD degree from Banasthali Vidyapeeth,
Banasthali. She has 18 years of teaching and
research experience supervising Masters and
PhD scholars in emerging technologies.
She has published more than 40 research
papers in national and international journals
and conferences. She has published 17
authored books for UG and PG courses for
a number of universities including MD University, Rothak, and Punjab
Technical University, Jallandhar with National Publishers. Kavita Saini
has edited many books with International Publishers including IGI
Global, CRC Press, IET Publisher, Elsevier and published 15 book chapters
with international publishers. Under her guidance many MTech and PhD
scholars are carrying out research work.
She has also published various patents. Kavita Saini has also delivered
technical talks on Blockchain: An emerging technology, web to deep
web, and other emerging areas and handled many special sessions in inter-
national conferences and special issues in international journals. Her research
interests include Web-Based Instructional Systems (WBIS), Blockchain
Technology, Industry 4.O, and Cloud Computing.
Swapnali Jadhav born on April 10, 1998,
natively from Sangli District has completed
her Bachelor’s in Forensic Science and
Master’s degree in Forensic Science, special-
ized in Forensic Chemistry and Toxicology
from Government Institute of Forensic
Science, Aurangabad, Maharashtra. She has
been participating and presenting her work
for more than 15 national and international
conferences and workshops. She has publi-
shed more than 10 papers in Scopus Indexed
Journals and several papers/chapters are under progress. Her broad areas of
interest include forensic chemistry, nanotechnology, toxicology, forensic356 Swaroop S. Sonone et al.medicine, etc. She is engaged in the research of effects of various chemicals
on human body. She has hand on experience on variety of sophisticated
instruments like UV-Visible Spectrophotometer, IR Spectroscopy,
etc. She has completed her internships under Council of Scientific and
Industrial Research-North East Institute of Science and Technology
(CSIR-NEIST) and Council of Scientific and Industrial Research-Indian
Institute of Integrative Medicine (CSIR-IIIM) in year 2021 and
2022, respectively. She has delivered talks at various places including
Commissioner of Police Office, Aurangabad, Sessions and District Court,
Aurangabad about Forensic Science and its significance in society.
Mahipal Singh Sankhla is born on May
19, 1994, at Udaipur, Rajasthan. Currently,
he is working as an Assistant Professor
in the Department of Forensic Science,
Vivekananda Global University, Jaipur,
Rajasthan. Prior to this he has served as
Assistant Professor in the Department of
Forensic Science, Institute of Sciences,
SAGE University, Indore, M.P. He has com-
pleted BSc (Hons.) Forensic Science and
MSc Forensic Science. Currently he is pursu-
ing PhD in Forensic Science from Galgotias
University, Greater Noida, U.P. He has done training in Forensic Science
Laboratory (FSL) Lucknow, CBI (CFSL) New Delhi, Codon Institute of
Biotechnology Noida, Rajasthan State Mines & Minerals Limited (R&D
Division) Udaipur. He was awarded “Junior Research Fellowship-JRF,”
DST-Funded Project at “Malaviya National Institute of Technology—
MNIT,” Jaipur and “Young Scientists Award” for Best Research Paper
Presentation in Second National Conference on Forensic Science and
Criminalistics and “Excellence in Reviewing Award” in International
Journal for Innovative Research in Science & Technology (IJIRST). He
is edited of 4 books and published 10 book chapters in various national
and international publishers. He has published more than 120 research
and review papers in peer review international and national journals. He
has participating and presenting his research work for more than 25 national
and international conferences and workshops and organized more than
25 national and international conferences, workshops, and FDP.357 The future of edge computingVarad Nagar is born on March 17, 2002,
at Varanasi, Uttar Pradesh. Currently
pursuing BSc (H) Forensic Science from
Vivekananda Global University, Jaipur,
Rajasthan, India and Pursuing Foundation
Degree from IIT Madras in Data Science
and Programming. He has been participating
and presenting his work for more than
10 national and international conferences
and workshops. He has published more than
12 papers in Scopus Indexed Journals and
published 10 book chapters in various
national and international publishers and sev-
eral papers/chapters are under progress. He has hand on experience on vari-
ety of sophisticated instruments like UV-Visible Spectrophotometer, IR
Spectroscopy, SEM, etc.358 Swaroop S. Sonone et al.CHAPTER THIRTEEN
Edge computing security: Layered
classification of attacksand possible countermeasures
G. Nagarajana, Serin V. Simpsonb, and R.I. Minuc
aDepartment of Computer Science and Engineering, Sathyabama Institute of Science and Technology,
Chennai, Tamil Nadu, India
bDepartment of Computer Science and Engineering, Thejus Engineering College, Thrissur, Kerala, India
cDepartment of Computer Science and Engineering, School of Computing, SRM Institute of Science and
Technology, Kattankulathur, Tamil Nadu, India
Contents
1.Introduction 360
2.Four layer architecture of edge computing 361
3.Security attacks in edge computing: Layered classification and analysis 363
3.1 Physical layer 364
3.2 Data link layer 364
3.3 Network layer 365
3.4 Transport layer 365
3.5 Application layer 366
4.Edge based existing solutions for the security issues present in real world IoTapplications 368
4.1 Smart city 368
4.2 Industrial environment 369
4.3 Smart campus 369
4.4 Vehicular network 370
4.5 Healthcare system 370
5.Discussion 371
5.1 Analysis of existing defense mechanisms 372
5.2 Open research challenges 372
6.Conclusion and future works 374
References 375
About the authors 376
Abstract
Edge computing is a widely accepted approach in cloud based Internet of Things (IoT)environment to overcome the issues related to traditional cloud computing. Edge offersa fast computational response to IoT applications. The emerging IoT applications cannot
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.010359be served by a centralized server placed at cloud. The latency of traditional execution
pattern is not acceptable in some applications which perform the computations in
sequential pattern with high time constraints. The deployment of edge node at theedge replaces the computation process from the distant cloud server to the edge node.Placing of resources at the edge of a network can be done in different ways. Based onthe execution process at the edge, the computing scenarios are categorized into fogcomputing, mobile edge computing and mobile cloud computing. The end nodecan be placed as a one hop neighbor of the edge node and also as a member inMANET-IOT network which is connected to edge node. The edge computing also opensa wide range of chances to attackers to intrude the network. This chapter discussesthe possible attacks in the edge computing, classifies the attacks based on modifiedOSI model for cloud computing and analyzes the countermeasures present in edge
computing environment.
1. Introduction
Internet of Things (IoT) has a vital role in all emerging applications.
IoT made it possible to include the nonliving things in communication.
That removed the problems of inaccurate prediction of the state-of-object.An object became capable to claim the accurate state-of-object without anyinterference with the help of IoT. But, such claims demanded a huge work
to be done in background to infer the state-of-object by the self-assessment
process. It required enough resources and computation facilities for theassessment. The number of entities increased the demand of resources.Also, all the communications in the world have a sequential pattern. Suchsequences also have dependencies on previous communications. Thus, each
communication must happen during the intended time. The computational
limitations must not disturb the flow of communication. Having largeresource strength at each object was not practically possible [1].
At
this juncture, cloud computing offered a solution for the resource
constraints. All the computations can be executed by the resources placedat the cloud. The network is required to provide a communication channelbetween the cloud and end node. Cloud computing could stand as an opti-mal solution for this problem only till the emerging of applications that
requires quick responses. The applications like high speed autonomous driv-
ing vehicles and augmented reality cannot be served effectively by cloud dueto the high latency and jitter. Also newly emerging applications demand con-text aware routing. A large server placed at a distance for serving “n” number
of end nodes cannot offer a context aware routing for each end node.
Edge computing has been introduced at this juncture. This is a concept of360 G. Nagarajan et al.placing the resources at the edge of Internet rather than placing the same at a
distance [2].
Edge
computing is capable to offer the end nodes, the required service
with low latency and jitter. The edge nodes are usually placed one hop away
from the end nodes. Thus it can provide a context aware routing and mobil-ity support for all the end nodes. Edge also reduced the bandwidth utilization
by doing the required computations at the edge of the network. Also, the
implementation of edge nodes reduced the energy utilization at the centralcloud server. In short, the edge computing has removed all the shortcomingsof present network. The deployment of large number of edge nodes at the
edge of the network became an open door to the attackers in the IoT net-
work. Several attacks are possible in the edge based IoT networks. Thischapter classifies the attacks in the edge computing network and discussesthe existing countermeasures for defending against such attacks [3].
T
his chapter is organized in the following manner; the next section illus-
trates the four layer architecture of edge computing. Section 3 presents a
l
ayer-wise analysis of security threats present in the edge computing environ-
ment. Section 4 discusses about the edge based existing solutions for the secu-
ri
ty issues present in real world IoT applications. Section 5 gives an overview
of
open research challenges in edge computing. Section 6 summarizes the
ch
apter and also discusses the scope of future works in edge computing.
2. Four layer architecture of edge computing
The cloud computing has several limitations due to its centralized
architecture. IoT requires fast processing for real time applications. Also,it demands a context aware routing because the requirement is highlydependent on the environment. Traditional cloud computing cannot pro-vide such services efficiently. But, the basic contribution of cloud network
toward the low-resource computation cannot be neglected easily. Thus,
some paradigms have risen to make the cloud computation stronger. Themost effective way to accomplish the requirement is edge based computing.The computing facility has brought to the edge of the network. Thus the
data need not be taken up to the cloud server for the processing. The edge
becomes capable to offer all the services that were previously offered directlyfrom the cloud. The cloud server will be notified about a finished compu-tation whenever it is necessary.
The four layer architecture of edge computing is shown in Fig. 1 . The
initial
layer is connected to physical world. The duty of IoT layer is to sense361 Edge computing securityFig. 1 The four layer architecture of edge computing.the data and transfer the same to data aggregation system in layer 02. Layer
02 is an Internet gateway between the edge layer and IoT devices. Edge layer
is placed in between the cloud service and IoT applications at the edge of the
network. The edge layer is responsible to do the preprocessing, policy
scheduling, data analysis and data flow analysis.
3. Security attacks in edge computing: Layered
classification and analysis
International Organization of Standardization (ISO) has structured the
network activities into a seven layer abstraction, called Open System
Interconnection Model (OSI Model). Each layer in OSI Model is an abstrac-
tion of a set of activities and the participating components. Each layer con-
tributes to their neighboring layers. Based on this layered classification, ISO
could define the peer-to-peer interaction concept of network communica-
tion. Cloud technologies have been grown to the outer boundaries of OSI
model. Thus the conventional OSI model failed to address the overall cloud
functions within the designed boundary. A modified OSI model has been
raised based on the proposal by CISCO for cloud platform. Rather than lay-
ering the network into seven, the modified OSI model keeps a five layer
classification based on the cloud operations. The modified OSI model for
cloud computing is shown in Fig. 2 .
Fig. 2 Modified OSI model for cloud computing.363 Edge computing security3.1 Physical layer
This layer is analogous to the layer 01 of traditional OSI model. But, it is
more complex. The components in this layer are not visible to users. The
infrastructure is completely handled by cloud service providers. The physicaldevices present in the datacenters are the foundation of cloud platform.Cisco UCS, HP converged system, and VCE vBlock are the known examples
for the infrastructures used to build cloud environment. Actual data transmis-
sion is happening through this bottom layer. Binary data will be transmittedthrough this layer as analog/digital signals. Attackers can perform signal relatedattacks in this layer. They can try to retrieve the data from the signals. Todefend such cases, other layers jointly contributes security paradigms by var-
ious mechanisms like encryption, authentication, etc. In such cases, Signal
Jamming can be done to perform simple DoS attack. Through this jammingattack, attackers will not be able to get the data, but they can make theresources unavailable for a while [4].
3.2 Data link layer
Hypervisor is installed on top of physical layer. It provides virtualization.
The hypervisor present in data link layer allows the users to access the phys-
ical devices by creating Virtual Machines (VMs). It helps the user to use thenewly installed infrastructures without any delay. Such newly installed phys-ical devices can be made available by VMs. This layer is responsible for quick
response and resource rich execution. Hyper-V, KVM, Xen, NSX, ACI,
etc. are the examples for the hypervisors used in industries. Data link layerprovides Logical Link Control and Media Access Control. Media access con-trol can be done either in centralized or in distributed manner in edge com-puting. This layer is mainly affected by spoofing attacks to get access to the
communication. Logic Link Control includes error control, flow control
and acknowledgment services. Attackers can perform DoS attacks by simplymaking the bandwidth unavailable for communication. Inference Attack is apassive traffic analysis attack present in data link layer. Attacker observes the
data transmission through the edge nodes and tries to get knowledge about
the network and network entities. This attack does not have the intentionto get the messages from ongoing transmission. An attacker can do more withthe knowledge about the network than the communicating messages. The
most commonly used path and actively communicating entities can be iden-
tified by inference attack [5].364 G. Nagarajan et al.3.3 Network layer
The cloud accounts are created in this layer under software defined data cen-
ter (SDDC). Network layer handles the computing and networking process
by utilizing the resources under this layer. In SDDC, all services will be pro-vided to the user by infrastructure virtualization. The data center will beautomated by software. In SDDC, the resources and other infrastructures
are managed by intelligent software systems. vSphere, OpenStack, AWS,
Azure and GCP are used to manage the cloud infrastructure in this layer.In this layer, the data will be treated as tiny packets. The routing of datapackets are handled in this layer. Thus, this is the most attacked layer amongthe layers introduced in modified OSI model for cloud computing. A plenty
of attacks are present in edge environment which mainly focuses on packet
routing. The attack can happen from both inside and outside the network.Reputation based attacks/Smearing attacks are the most common attacks innetwork layer. A node always tries to increase the reputation among its
neighbors in order to participate in possible communications. The attackers
can falsify the reputation about a node either for making the targeted node toparticipate in a communication or for keeping that node away from thecommunication. DoS attacks (e.g., Flooding Attack) and Identity based
attacks (e.g., Sybil Attack) are also present in this layer [6,7].
3.4 Transport layer
The actual workload creation, data storage and process execution are hap-
pening in this layer. This is accomplished with the help of Virtual Private
Cloud (VPC). This layer can also be called as Native Service Layer or
Machine Instance Layer. The users in this layer will get the service evenif they do not have access to network layer. This layer functions as IaaS(Infrastructure as a Service). The VPC provides access to developer or
administrator through RDP, SSH or any other protocols. Transport layer
is responsible for flow control, error control and segmentation process. Acommunication can be established either as a connection oriented serviceor as a connection less service. This layer has a set of protocols for controlling
the flow of data through the network. An attacker can disturb/disrupt the
flow by exploiting the vulnerabilities present in transport layer protocols.Most commonly used transport layer protocols are transmission control pro-tocol and user datagram protocol. Replay attack is an active DoS attack pre-
sent in transport layer. Attacker captures all the packets coming from the
edge nodes and forward to the intended recipient edge node as the packets365 Edge computing securityoriginally come from the source node. By this attack, the attacker is able to
read or modify the message, if the message is not encrypted at source. A
properly encrypted message cannot be tampered by this attack. In such cases,this kind of attack can contribute only delay to the network. The replayattacker always tries to hide the presence of attack in the network. Thisattack can be performed as a combination attack of one or more attacks.
For example, replay attack can be done along with any cryptanalytic attack
to decrypt the messages. So, the replay attack can be extended to messagemodification attack with the help of any cryptanalytic attack [8,9].
3.5 Application layer
This is the end user layer. It provides measurable business values to a set ofconstituents. The user access right is defined in application layer. This layer is
analogous to the layer 07 of traditional OSI model. The front end of web
applications, interactions through mobile applications and connectivityamong IoT devices come under this layer. The software code running onthis layer will do the necessary to make the cloud resources available to
the end users. Blogs, Wikis, CRMs and HPCs are the examples of front
end applications used in cloud environment. It is the only one layer whichhas direct interaction with the end user. Thus the data handled in this layerwill always be in user understandable format. Protocols like, HTTP, SMTPand FTP are used in this layer. The attackers in this layer aims to retrieve the
actual data. It can be done either by using some intrusive software (e.g.,
Malware Injection Attack) or by directly collecting the packets which arebeing transmitted during the communication (e.g., Collusion Attack).This layer also determines the way the data needs to be transferred to the
network. The data can be sent either with compression or without compres-
sion. All such confidentiality aspects lie in this layer. Mostly the data will beencrypted for making it secure from the attacks. The attack in this layer alsotries to discover the encryption algorithm to retrieve the actual data from the
cipher text [10].
The
attacks present in the network can also be classified based on this
layering concept. Since each layer has peer-to-peer connectivity, each layer
holds set of protocols. Thus, the attacks are mostly affected within theboundary of a layer. The layered classification of attacks present in edge
computing is illustrated in Fig. 3 .
Both 
active and passive attacks are possible in every layer. Attackers aim
to intrude the network by analyzing the vulnerabilities present in each layer.366 G. Nagarajan et al.The layers are classified based on the duties which have to be jointly accom-
plished. All such duties are the work done required to fulfill a communica-
tion. Thus the attacks happening in each layer is dependent on layering
pattern. All the above described layers are interconnected and all layers
receive input from adjacent layers. Due to this relation, some attacks are pos-
sible in more than one layer. Such attacks are classified into multilayer
attacks. These attacks can be performed irrespective of layers. This includes
both active and passive attacks. Side Channel Attack is an active DoS attack.
This is a special kind of attack that focuses on the subsidiary information of
network. This attack does not consider the vulnerabilities of the system. This
will only consider the additional information about the channel communi-
cation, such as signal leakage, variation in response time, power consump-
tion and any other extra source of information. The attacker will find the
way to make use of this information to perform denial of service. For exam-
ple, response time will be small for highly sensitive information. An attacker
can find the time of occurrence of such communication, if that occurs reg-
ularly. Performing a jamming attack during the estimated time will affect the
next communication and that may cause enough damage to the IoT appli-
cation. The side channel attack can be performed in any layer of network.
The attacking strategies will be different based on the subsidiary information
Fig. 3 Layered classification of attacks present in edge computing.367 Edge computing securityobtaining from that layer. Man in the Middle attack is an active multilayer
attack. This can be performed in between any peer to peer communication.
The attacker will be present in between the edge servers and end devices.Both of them cannot identify the presence of attacker. The necessary com-munication will be carried out between the entities without knowing thepresence of attacker in middle. The attacker will get the data from the net-
work. Eavesdropping attack is a passive multilayer attack. It comes under
information leakage attack. This attack will not harm any ongoing commu-nication directly. But it could overhear the data transferred through the edgenodes. A good encryption scheme can successfully prevent this attack [11].
4. Edge based existing solutions for the security issues
present in real world IoT applications
The increased use of IoT devices also increased the security issues in
the cloud platform. The emergence of edge computing also opened a wide
door to the attackers to intrude the network. Since the IoT devices are mostly
implemented in real world applications, the vulnerabilities of the network areequally dependent on the physical infrastructure as well as real world reasons.This section discusses the issues present in smart city, industrial environment,
smart campus, vehicular network and healthcare system. This section also dis-
cusses the existing edge based solutions in cloud environment.
4.1 Smart city
A large number of edge computing units have been deployed for handlingthe communications/service requests in smart city. Almost all digital equip-ment are now connected to create a smart environment. The main goal ofnetwork engineer is to increase the performance of edge computing units
(ECU). But, a linear increase could not be achieved due to privacy and secu-
rity issues. Thus, the overall security maintenance has become challenging.In order to increase the overall performance of edge computing units, Xuet al. [12]proposed a Trust oriented IoT service placement (TSP) method.
They
have proposed this work for addressing the security issues in IoT based
smart city environment. The authors try to increase the performance ofstrength pareto evolutionary algorithm (SPEA2) to get an improved place-ment strategy.368 G. Nagarajan et al.4.2 Industrial environment
The combination of IoT network and traditional industry is called Industrial
Internet of Things (IIoT). IIoT receives wide acceptance from all industries
due to the smart services that could be exclusively offered by IIoT. IoT net-work has a great role in reducing the man power requirement at industry.That could make the work easier than the conventional execution pattern.
But the increased use of IoT devices also results in the reduction of Quality
of Service (QoS). This is happening because of the improper network man-agement and the security breaches. Thus industrial IoT demands a large scaleimprovement at defining security boundaries. Wang et al. [13]proposed an
edge
based unified trustworthy environment establishment for IIoT. The
main aim of the proposed work is to identify the malicious service providersand service consumers. It also eliminates malicious messages and choosesonly trusted service providers. Edge computing is incorporated with thecloud platform to perform trust evaluations based on the collected service
records. The security is mandatory in industrial IoT. The security breaches
in any level will badly affect the expected profit.
4.3 Smart campus
The most popular and widely used network paradigm in campus is mobilesocial networks. Thus mobile social network is used for the fast contentdelivery in smart campus. Conventional notice boards have been replaced
with this new paradigm. The interesting and significant feature of mobile
social network is the reliability. The message can be delivered both in“one to one” or “one to many” fashion. It is possible to ensure the deliveryin both cases. Also, one important feature is the response scheduling. The
response which we would expect to get can be scheduled. Sending reminder
and nested communications are other salient features of mobile social net-works. The wide acceptance of social network is mainly due to its incom-parable services offered for accomplishing the needs of campus. An
institution can use either an in-house platform or can utilize any globally
accepted social network. Both cases are vulnerable to security attacks.Also the quality of experience needs to be enhanced for utilizing the samefor institution purposes. Xu et al. [14]proposed an edge computing enabled
mobile
social network for smart campus. The main goal of proposed work is
proper resource allocation. In order to find the optimal edge node, a reverseauction game technique is used. The game analysis results in a Bayesian369 Edge computing securityequilibrium which is used to perform optimal edge node selection for each
mobile user. Smart campus is a place where the crowd density will be larger
than normal IoT environment. But, the crowd density will have a predict-able parallel line pattern with time duration. Thus a minimum edge nodeand maximum utilization strategy can be applied to smart campus.
4.4 Vehicular network
The vehicular network is the most advanced application of Internet ofThings. It requires dynamic processing. The traditional cloud platform isnot capable to handle vehicular network due to the increased latency.
The vehicular network can be implemented only with the fifth generation
(5G) network, since it requires fast processing and immediate response fromthe service providers. The vehicular network can be used to avoid conges-tion and accidents. It can also increase the traffic efficiency. The main reasonfor choosing 5G network is the capacity of 5G network to perform device to
device (D2D) communication. In D2D communication, the network
devices can exchange information without utilizing network infrastructure.The direct D2D communication avoids the risk involved with traditionalcloud network. Since the D2D communication does not demand a reliable
connection with nearby base station, the applications which use D2D com-
munication can also be used in 4G and 3G networks. Security attacks onvehicular network can risk the life of passengers. Thus such attacks haveto be identified and avoided initially before it hits badly. Such attacks can
be avoided only by improving the authentication system. The issues with
authentication system may lead to privacy leakage. Zhang et al. [15]pro-
posed
an edge based secure authentication system for vehicular network.
They have designed a protocol that could be expected to run over 5G net-
work. The system model includes edge computing vehicles (ECVs), normal
vehicles, road side units (RCUs) and cloud platform. Since the proposedscheme adopts D2D communication, the protocol can also be run over4G and 3G networks. The edge based execution removes the issues associ-ated with conventional execution.
4.5 Healthcare system
The emergence of wearable devices changed the concepts of conventionalhealthcare system. We have several sensor based wearable devices which can
monitor our heartbeat, blood pressure, temperature, etc. The most signifi-
cant achievement is that, an immediate medical care can be offered with the370 G. Nagarajan et al.help of these sensor based wearable devices. Self-assessment might not be
accurate, since public is still not having enough knowledge about human
body. So, the healthcare system is highly dependent on modern technolo-gies. Wearable devices can monitor our health and sensor nodes can generatethe data. A cloud based processing unit can find the health condition basedon the sensed data. All these processes are automated with the help of IoT.
The hospitals can offer emergency assistance to the patients even if they are at
home. Patients may be unaware about their serious internal health issues.The most modern assessments can predict the future health condition of apatient. Such findings are helpful to give medical assistance to the patient
before he/she fell into the worst condition. The data saved at cloud
datacenter will be huge. An accurate prediction and self-learning can beachieved only by considering such a big data. But, the data processingbecomes complex due to big data. Also privacy and security issues are pre-
sent in this field. The medical assessment of a person is completely private
and such records need to be handled properly. No one wishes to reveal theirhealth conditions to the public. Also, anyone can find the details of a humanbody from the sensed data. Illegal organ trade is the major concern which
may happen if the medical records reach at improper hands. Thus the pri-
vacy and security of cloud based healthcare system is really important. Liuet al. [16]proposed an edge based privacy preservation technique for wear-
able
devices. The system model includes patients, wearable devices and edge
computing units. The proposed technique includes identity authentication
and data access control. The method considers time aware and space aware
contexts. In time aware context, access control is maintained by encryptionalgorithm and bloom filter is applied to preserve the privacy. In space awarecontext, hash code based authentication is used to preserve the privacy. Both
contexts aim to preserve the privacy of sensitive data.
5. Discussion
This section discusses the open research challenges present in edge
computing. All the edge paradigms have been introduced to overcomethe shortcomings of cloud computing. The modern IoT applications could
run smoothly with the help of these edge paradigms. This chapter discussedthe vulnerabilities of edge paradigms in detail. The main reasons for all thediscussed security attacks are resource constraints and the storage limitationsof edge nodes.371 Edge computing security5.1 Analysis of existing defense mechanisms
Several mechanisms have been proposed in edge environment to avoid and
eliminate the security attacks. The application layer is mainly affected by
malware based attacks. Application layer is defended against malware attacksby architectural enhancements. Application layer also has the responsibilityto keep the confidentiality of network data. Thus the data will be encrypted.
All attackers aim to find the encryption algorithm based on the available infor-
mation to retrieve the original data. Research is still going on in this field todevelop more secure encryption schemes. All security paradigms present inapplication layer and transport layer try to increase the strength of authenti-cation schemes. Network layer in edge computing is affected by identity based
routing attacks. All existing works in this layer hold a robust trust based mech-
anism to identify the attack. Data link layer and physical layer are prone tosignal level attacks. Such attacks are identified and eliminated by employingsignal level mechanisms. Multilayer attacks are also discussed in this chapter.
It is not possible to define an exact solution to multilayer attacks, since it shows
different behavior in different layers. Thus, such attacks have to be analyzedand treated according to the characteristics of each layer.
5.2 Open research challenges
Edge computing is an emerging area required to run the advanced IoT appli-cations. Edge computing needs to be secured for eliminating the existingvulnerabilities. The existing solutions discussed in this chapter could over-come the vulnerabilities up to an extent. Still some more enhancements are
needed due to the following reasons.
5.2.1 Rapid increase in the number of network components
The introduction of 4G LTE network increased the speed (5 –12Mbps) and
capacity of network. But, 4G LTE network is not sufficient to run the IoTapplications. Thus an enhancement has been done on 4G LTE networkknown as LTE-Advanced, which has a speed of 300Mbps. Low level
IoT applications have been introduced during this phase. Such applications
could run in LTE-Advanced network with the help of cloud server. Themain difference between the traditional network and IoT network is theincreased connectivity among living and nonliving objects. For achievingthe connectivity, all the objects must be installed with an IoT device. In this
scenario, having highly efficient device at all communication end points is372 G. Nagarajan et al.not practically possible. Thus IoT uses low-weight end nodes with an access
to the cloud server. All the computations will be carried out at cloud server.
Upon increasing the number of components, the response time of the cloudserver increased rapidly. Thus the speed of the network expected to increasefor the smooth running of IoT applications. The emergence of 5G networkwith a speed of 20Gbps has solved the existing problems. But, the use of
remotely placed cloud server again continued with a threat of having
response delay due to the unexpected network issues. The number of com-ponents increased day by day and the speed of 5G network was sufficient tohandle all the end devices by cloud monitoring. The emergence of quick
response demanded services like vehicular network and augmented reality,
again pointed toward the known threat associated with remote cloudservers. The edge computing has been introduced in this stage to overcomethe threat. The edge servers are capable to provide quick response to the IoT
applications, since such servers are placed near the edge of the network. But
the deployment of such servers needed in every point to accommodate theincreased number of network components. Thus a highly equipped devicecannot be used due to the economic constraints. So the capacity of edge
server has been limited due to the economical reasons. That in turn increased
the security issues associated with edge computing. The IoT devices areincreasing day by day. Thus there is a need for some robust mechanismsto prevent the security attacks on edge nodes.
5.2.2 Heterogeneous nature
The most challenging task in IoT environment is the incorporation of het-erogeneous components. The range of components can be varied from a
simple sensor node to highly efficient edge device for advanced IoT appli-
cations. The edge server needs to accommodate all types of componentsirrespective of their capacity. Device specific security breaches must alsobe considered by edge devices. The security attacks associated with one type
of device may become stronger in another device due to the device specific
security breaches. Thus, handling security in heterogeneous environment isa crucial task. Also, the edge servers have limitations due to the low-resourcecapacity. The edge servers are equipped with the capacity to perform all net-
work related computations. But, it will not have enough capacity to incor-
porate security related advanced algorithms. Thus, it is a highly difficult taskto ensure the security in edge environment under these resource constraints.373 Edge computing security5.2.3 Possibilities of identity based attacks
Identity based attack is the most challenging attack in edge environment. It is
happening in more than one layer of network. In application layer, it is pre-sent in the form of authentication attacks. The attacker tries to find thematching credentials to intrude the network by using the identity of othernodes. The spoofing and impersonation attacks present in data link layer are
also identity based attacks. Such attacks are also present in network layer.
Sybil attack present in network layer can generate multiple identities byusing a single compromised node. All these identity based attacks are activeattacks. The main reason for such impersonation attack in edge environment
is the lack of centralized monitoring system among the edge servers. The
cloud server will not be updated regularly. All the computations can be per-formed at the edge of network. Only log information will be passed to cloudserver whenever the application demands a cloud server update. Thus, the
identity based attack became a major threat among edge nodes. New mech-
anisms have to be incorporated with edge computing to make it strangeragainst security breaches.
6. Conclusion and future works
The cloud computing has become efficient to serve all high level IoT
applications, only after the introduction of edge computing platforms. Thischapter discussed the security attacks and possible defense mechanisms pre-sent in edge environment. The security attacks on edge computing are hap-pening mainly due to the limitations of resource limited edge devices. Two
major classifications of attacks have been contributed in this chapter. A gen-
eral classification of attack has been done based on the attacking strategy anda detailed layer based classification has been done later, based on the mod-ified OSI model for cloud computing. The attacks happening in each layer
have relation with the functionalities of that layer. Thus, all those attacks are
occupying inside the virtual boundary of each layer. Multilayer attacks havealso been discussed in this chapter. Some common factors or strategies madesuch attacks possible in more than one layer. The defense mechanisms are
categorized in this chapter, based on the classification of attacks. The existing
mechanisms contribute toward one or two types of attacks expected to bepresent in the edge environment. A robust mechanism is needed in edgecomputing for eliminating all the vulnerabilities associated with edge nodes.
The future works in edge computing can be extended toward handling the
increasing heterogeneous network components and also toward the success-ful elimination of identity based attacks.374 G. Nagarajan et al.References
[1]M. De Donno, K. Tange, N. Dragoni, Foundations and evolution of modern comput-
ing
paradigms: cloud, IoT, edge, and fog, IEEE Access 7 (2019) 150936 –150948.
[2]S. Gong, A. El Azzaoui, J. Cha, J.H. Park, Secure secondary authentication framework
for
efficient mutual authentication on a 5G data network, Appl. Sci. 10 (2) (2020).
[3]R.I. Minu, G. Nagarajan, Bridging the IoT gap through edge computing, in: Edge
Computing
and Computational Intelligence Paradigms for the IoT, IGI Global,
2019, pp. 1 –9.
[4]B. Li, T. Chen, G.B. Giannakis, Secure mobile edge computing in IoT via collaborative
online
learning, IEEE Trans. Signal Process. 67 (23) (2019) 5922 –5935.
[5]T. Shanshan, M. Waqas, S.U. Rehman, M. Aamir, O.U. Rehman, Z. Jianbiao, C.-C.Chang,
Security in fog computing: a novel technique to tackle an impersonation attack,
IEEE Access 6 (2018) 74993 –75001.
[6]J. Yuan, X. Li, A reliable and lightweight trust computing mechanism for IoT edgedevices
based on multi-source feedback information fusion, IEEE Access 6
(2018) 23626 –23638.
[7]R. Smith, D. Palin, P.P. Ioulianou, V.G. Vassilakis, S.F. Shahandashti, Battery draining
attacks
against edge computing nodes in IoT networks, Cyber-Phys. Syst. 6 (2) (2020)
96–116.
[8]K. Fan, M. Liu, G. Dong, W. Shi, Enhancing cloud storage security against a new replay
attack
with an efficient public auditing scheme, J. Supercomput. 76 (2020) 4857 –4883.
[9]K. Mahmood, X. Li, S.A. Chaudhry, H. Naqvi, S. Kumari, A.K. Sangaiah, J.J.P.C.Rodrigues,
Pairing based anonymous and secure key agreement protocol for smart grid
edge computing infrastructure, Future Gener. Comput. Syst. 88 (2018) 491 –500.
[10] X. Li, S. Liu, F. Wu, S. Kumari, J.J.P.C. Rodrigues, Privacy preserving data aggregation
scheme
for mobile edge computing assisted IoT applications, IEEE Internet Things J. 6
(3) (2019) 4755 –4763.
[11] Z. Wang, A privacy-preserving and accountable authentication protocol for IoTend-devices
with weaker identity, Future Gener. Comput. Syst. 82 (2018) 342 –348.
[12] X. Xu, X. Liu, Z. Xu, F. Dai, X. Zhang, L. Qi, Trust-oriented IoT service placement
for
smart cities in edge computing, IEEE Internet Things J. 7 (5) (2020) 4084 –4091.
[13] T. Wang, P. Wang, S. Cai, Y. Ma, A. Liu, M. Xie, A unified trustworthy environment
establishment
based on edge computing in industrial IoT, IEEE Trans. Industr. Inform.
16 (9) (2020) 6083 –6091.
[14] Q. Xu, Z. Su, Y. Wang, M. Dai, A trustworthy content caching and bandwidth allo-
cation
scheme with edge computing for smart campus, IEEE Access 6
(2018) 63868 –63879.
[15] J. Zhang, H. Zhong, J. Cui, M. Tian, Y. Xu, L. Liu, Edge computing-basedprivacy-preserving
authentication framework and protocol for 5G-enabled vehicular
networks, IEEE Trans. Veh. Technol. 69 (7) (2020) 7940 –7954.
[16] H. Liu, X. Yao, T. Yang, H. Ning, Cooperative privacy preservation for wearabledevices
in hybrid computing-based smart health, IEEE Internet Things J. 6
(2) (2019) 1352 –1362.375 Edge computing securityAbout the authors
Dr G. Nagarajan received the B.E. degree
in Electrical and Electronics Engineering
from MS University and the M.E. degree in
Applied Electronics from Anna University,
in 2000 and 2005, respectively, and the
M.E. degree in Computer Science and
Engineering from Sathyabama University
and and the Ph.D. degree in Computer
Science and Engineering from Sathyabama
University, in 2007 and 2015. He is a
Faculty Member of the Department of
Computer Science and Engineering, School
of Computing, Sathyabama Institute of Science and Technology, Chennai,
India. His current research interests include Computer Vision, IoT, 5G,
Edge/Fog Computing, Artificial Intelligence, Machine Learning, and
Wireless Sensor Network. He has published more than 70 research papers
in peer-reviewed journals such as IEEE Conference, ACM, Springer-
Verlag, Inderscience, and Elsevier. He also has contributed 15 book chapters
thus far for various technology books. Finally, he has authored and edited 3
books thus far and is focusing on some of the emerging technologies such as
the IoT, Edge/Fog Computing, Artificial Intelligence (AI), Data Science,
Blockchain, Digital Twin, 5G, etc.
Mr. Serin V. Simpson has received his
B.Tech degree in Information Technology
from University of Calicut in 2012. He has
received his M.Tech degree in Computer
Science and Engineering from University
of Calicut in 2015. Currently, he is doing
Ph.D. programme in Computer Science
and Engineering in Sathyabama Institute of
Science and Technology. His research area
includes Edge Computing and Network
Security. He has 1 year of industrial experi-
ence and 6 years of teaching experience.376 G. Nagarajan et al.Now, he is working as Assistant Professor in the department of Computer
Science and Engineering at Thejus Engineering College, affiliated to APJ
Abdul Kalam Technological University. He has published papers in the
research areas of Edge/Fog Computing, Cloud Computing, Wireless
Sensor Networks, IoT, Mobile ad-hoc Networks, Network Security and
Cluster based communication.
Dr R.I. Minu received the B.E. degree in
Electronics and Commun ication Engineering
from Bharathidasan University in 2004 and
t h eM . E .d e g r e ei nC o m p u t e rS c i e n c ea n d
Engineering, and Ph.D. degree in Computer
Science and Engineering from Anna
University in 2007 and 2015, respectively.
She is a Faculty Member of the Department
of Computer Science and Engineering,
School of Computing, SRM Institute of
Science and Technology, Kattankulathur, India. Her current research interests
include Computer Vision, Machine Vis ion, IoT, 5G, Edge/Fog Computing,
Artificial Intelligence, Machine Learning, and Semantic Web. She has publi-
shed more than 40 research papers in pee r-reviewed journals such as Elsevier,
Springer-Verlag, Taylor & Fransics, a nd IEEE Conferences. She has authored
and edited 3 books thus far and is focusing on some of the emerging technol-
ogies such as the IoT, Edge/Fog Computin g, Artificial Intelligence (AI), Data
Science, Blockchain, Digital Twin, 5G, etc.377 Edge computing securityThis page intentionally left blankCHAPTER FOURTEEN
Blockchain technology for IoT
edge devices and data security
M.P. Anuradhaaand K. Lino Fathima Chinna Ranib
aDepartment of Computer Science, Bishop Heber College, Affiliated to Bharathidasan University,
Tiruchirappalli, Tamil Nadu, India
bDepartment of Computer Applications, Bishop Heber College, Affiliated to Bharathidasan University,
Tiruchirappalli, Tamil Nadu, India
Contents
1.Introduction 381
1.1 What is IoT? 381
1.2 Basics of edge devices 381
2.IoT layered architecture 382
2.1 The perception layer 383
2.2 The network layer 383
2.3 The application layer 383
2.4 The service support layer 383
3.IoT security threats and attacks 384
3.1 Classification of attacks based on IoT-architecture 385
3.2 Attacks—sensing (or) perception layer 385
3.3 Attacks—Network and service support layers 387
3.4 Attacks—Middle-ware layer (or) service support layer 388
3.5 Attacks—Application layer 389
4.IoT—Edge computing 391
4.1 Functions 391
4.2 Three-Tier edge computing model 392
4.3 Edge vs cloud 393
4.4 Attacks on edge nodes 393
5.Requirements for integration of blockchain and edge computing 395
5.1 Authentication 395
5.2 Adaptability 395
5.3 Data integrity 395
5.4 Verifiable computation 396
5.5 Low latency 396
5.6 Network security 396
6.Integration of blockchain and edge computing 396
6.1 Blockchain role in edge computing 396
6.2 Mixing—Blockchain and edge computing 397
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0113797.IoT framework: Secure edge computing with blockchain technology 400
7.1 Design overview 400
7.2 Blockchain framework layered architecture 400
7.3 Distributed-IoT device layer 401
7.4 Point-to-point edge servers network 402
7.5 Decentralized resources of cloud 402
8.Factors to be addressed in secure edge computing 403
8.1 Low latency 403
8.2 Longer battery life for IoT devices 403
8.3 More efficient data management 403
8.4 Access to data analytics and AI 403
8.5 Resilience 403
8.6 Scalability 404
9.Advantages —Integration of blockchain and edge computing 404
10. Use cases —Blockchain with edge computing 405
10.1 Smart city 405
10.2 Smart transportation 406
10.3 Industrial IoTs 406
10.4 Smart home 407
10.5 Smart grid 407
11. Further challenges and recommendations 408
11.1 Technical threats 408
11.2 Interoperability and standardization 408
11.3 Blockchain framework 408
11.4 Administration, authority, controlling and legal characteristics 409
11.5 Rapid field testing 409
12. Conclusion 409
References 410About the authors 411
Abstract
The Internet of Things (IoT) is expanding rapidly across various trade verticals in real timeapplications. In IoT-architecture, there is a need to focus offline devices with itsextensive-data due to its data processing restrictions and limited storage volume;but, the IoT-edge computing overcomes the above issues and offers a reliable architec-ture for managing the IoT devices with data. Due to the integration of computing andtransaction processing systems, Blockchain in IoT (BIoT) provides high end security, syn-chronization, intellectual property management, uniqueness, affordability, and data pri-vacy in real-time development. That basic initiative about BIoT-Edge computingtechnology including IoT layered architecture, IoT security threats and attacks andthe operational functions of Edge computing along with architecture models are the
foundations that are to be discussed in this proposed chapter. Future research direc-
tions of data integrity frameworks with edge computing platform in Blockchain ofthings are also focused.380 M.P. Anuradha and K. Lino Fathima Chinna Rani1. Introduction
1.1 What is IoT?
The Internet of Things [IoT] is defined as a system of unified computational
strategies. It is mechanically collaborated with digital technologies, things,
creatures or individuals that are provided with unique identifiers and the
capability for the transmission of information through network; but, notdemanding the intercommunication of man-to-man or man-to-system. A“Thing” in IoT can be any device with any type of built-in-sensors withthe capacity to accumulate and transfer data over a network without physical
involvement. The rooted technology in the Internet of objects which is used
in the progression of decision making supports them to interrelate withinternal states and the outside settings.
In a nutshell, the Internet of Thing is a perception that attaches all the
devices to the internet and lets them interconnect with each other overthe internet. IoT is a massive network of connecting devices, all of whichconnects and shares data about how they are implemented and the situationsin which they are executed. By doing so, each of the devices will be
observed from the involvement of other devices, as humans do. IoT is trying
to enlarge the interdependence of humans that is inter-reliant, subsidize andcooperate with things [1].
IoT
enabled network is to deliver rich understanding in the field of
health organizations, execution routines or monitoring of IoT-devices,and the execution settings of certain devices. The depth of data can mech-anize the variation in sensitive data and stipulate the execution performanceproficiently. When one or more IoT devices collected the data, the mech-anism behind the IoT assists in finding fault prediction, avoiding the oper-
ational cost, enlightening consumer security, diminishing the loss because
of manufacturing defects, and efficiently protecting the humans. Cloudcomputing technology is required for all the IoT technologies to revealthe competences of the IoT architecture. With quick scalability and huge
data maintainability, ML (Machine Learning) is used to distinguish risky
procedures in the information representation [2].
1.2 Basics of edge devices
An IoT network environment comprises web-enabled smart devices whichare used for embedded systems such as: sensors, processors and data channels381 Acceleration of blockchain technology in IoT for data securityused for accumulating data. They help to transfer and execute the procedures
in the collected information.
Fig. 1 explains that the devices of IoT can send their sensor data, then
gather those of which by linking to an IoT gateway or other edge devices
where data is either directed to the cloud for analysis or executed locally.
Occasionally, these devices interconnect with the other devices and perform
in the information which they get from one another [3].
The connectivity, networking and communication protocols employed
in these web-enabled devices basically depend on specific IoT applications.
The IoT-client and systems can get cloud proficiencies through the edge
devices. Therefore, The IoT- edge devices act like a bridge to resolve basic
issues related to the centralized cloud environment. Though the cloud envi-
ronment is authoritative, they make some sort of suspension for the IoT data
transmission. While incorporating cloud environment proficiencies with
IoT devices, edge computing executes the data quickly, avoids interruptions
and ensures safety measures besides some apprehensions [4]. IoT edge
devices solve the requirement of the larger IoT platform via removing
the round-trip period, which is used for cloud execution.
2. IoT layered architecture
Numerous IoT services fetch various layers from IoT architecture rep-
resentations. The basic four-layer architecture model consists of the percep-
tion layer, network layer, application layer and service support layer.
zigbeewave
Edge Devices and Sensors
Smart
GatewayMobility
AutomotiveIoT Platform
Consumer
AppliancesHealth Care
Fig. 1 IoT-edge devices.382 M.P. Anuradha and K. Lino Fathima Chinna Rani2.1 The perception layer
The bottom layer at the contractual IoT architecture environment is called
the perception layer. In this layer, the sensors and associated devices origi-
nate to perform as they gather various volumes of data based on the require-ment of the project. These can be edge devices, sensors, and actuators whichcooperate with their network environment. This layer either acts as an
in-charge to assemble information through IoT things or information taken
from the network settings like Wireless Sensor Networks [WSN], inter-connected IoT devices etc. It also computes the data collected from theabove resources. This is considered as the physical layer resides in the stan-dard IoT framework. The IoT devices such as the gateways, sensors, RFID
tags, etc., are located in this lowest layer. Therefore, it is termed as the device
layer or the recognition layer.
2.2 The network layer
The data which are collected by WSN, IoT, Interconnected IoT devices is
required to be carried and managed. That’s the work of the network layer. It
links these device policies to other smart objects, servers, and networkdevices. It also controls all the data transmission process in the network layerand also is accountable for executing the Perception Layer’s acknowledgedinformation. Moreover, it takes responsibility for communicating infor-
mation to the application layer via various network settings like wireless
as well as wired networks with Local Area Networks (LAN). The foremostchannels for information broadcast include FT TX, 3G (or) 4G, Wi-Fi,Bluetooth, Zigbee, UMB, infra-red, etc. Massive data will be processed
and there must be a need to adopt a wide-ranging middleware to accumulate
and execute this enormous data.
2.3 The application layer
This layer acts as the upper-most layer of the IoT Framework. This layerencompassing the application user interface takes accountability to organize
data and access to the services. It takes authority for transferring several
services to diverse customers. This layer offers a request of specific provisionto network users. The requirements that are carefully chosen is based on thedata which is collected from the IoT’s object sensors.
2.4 The service support layer
The service support layer is located in the subsequent layer from the upper-most layer. The standard architecture models contain an additional layer by383 Acceleration of blockchain technology in IoT for data securitythe name of support layer, which is placed in the midst of the application
layer and network layer. The ITU-T (International Telecommunications
Union—Telecommunication Standardization Sector) recommends a four-
IoT-Layered architecture which is described in Fig. 2 . The fourth layer is
intended to enhance the level of security in the standard IoT Framework.
If the gathered sensitive information is directly sent to the network layer, there
may be a chance of threat increases. An innovative layer is projected in order to
avoid errors in three-layer architecture. Here, the support layer performs two
tasks. First, it authorizes the quality of data received from the various authen-
ticated users and implements the protection mechanism against the attacks.
Here, several methods are to be followed to validate the authenticated clients
and their data. The common security mechanism is performed by authentica-
tion, which is executed by using secret keys and passwords. The second role of
the support layer is directing the processed data towards the network layer.
Here, the communication channel for transferring the data from support
layer to network layer may be wireless-network and wired network settings.
The security competences are classified into two types (i) generic (ii) specific.
These are dispersed among the four layers and shown in Fig. 2 .
3. IoT security threats and attacks
The attack against the protection of IoT is also abbreviated as
Interconnection of Threats (IoT). Certainly, IoT devices like sensors,
Smart CitySmart
EnergySmart
IndustrySmart
Health 
Application Layer 
Support Layer Security
Ecosystem
ServicesNetwork Layer Network
Technologies3G, UMTS, Bluetooth, IR, ZIGBEE
Perception
Layer Physical Objects Sensors & ActuatorsService
ManagementGateways Storage DatabaseAnalytics
Management
Control
Analytics
Management
Control
Analytics
Management
ControlGraphics Data RepresentationsSmart Applications &
Management
Fig. 2 IoT layered architecture.384 M.P. Anuradha and K. Lino Fathima Chinna Raniactuators, etc., are predominantly susceptible to physical threats, software
threats and hardware threats [5]. Existing IoT software and hardware
are
constructed with expertise’s solutions through an extensive number
of employees. Few of these IoT-software are a diverse combination of
mechanisms renewing through current results for implementing in well-constructed IoT platforms. They assure that these mechanisms are executed
in a protected technique.
Comprehensive risk and threat analysis mechanisms with management
tools for IoT platforms are required to prevent the attack. To develop the
qualification strategies for IoT attacks involves the study of various threat
types and the disciplinary measures executed if the attacks are taking place.
This chapter commences with the view of IoT attack classification and theanalysis of security attacks supporting to spot an actual assessment of the IoTenabled networks. In addition, it permits the authentic user to regulate the
qualification strategies.
3.1 Classification of attacks based on IoT-architecture
As has been elaborated in Section 1 , a concrete IoT architecture represen-
tation
is analyzed here as well. Generally, the IoT framework is composed of
four layers that are illustrated in Fig. 2 . There are some other vigorous attacks
that
may occur in all the fundamental layers such as: perception, network,
and service layers. Furthermost, the main protection of security correlationsin the IoT four-layered framework are briefly explained in Table 1 .
3.2 Attacks —sensing (or) perception layer
To entirely protect the IoT devices, there needs a plan as well as constructedmethods which must be hooked with IoT devices. It defines that IoT
Table 1 Threat categorization according to IoT architecture.
Security correlations Application 
and interface 
layer Service-
support 
layer Network 
layer Device 
layer 
Uncertain web edge * * *  
Inadequate authentication * * * * 
Insufficient network security services  * *  
Transport encryption deficiency  * *  
Confidentiality apprehensions  * * * 
Insufficient cloud security interface *    
Inadequate mobile security interface *  * * 
Apprehensive protection formation * * *  
Uncertain software and hardware *  *  
Low security on physical devices   *  385 Acceleration of blockchain technology in IoT for data securitydevices themselves are to verify the uniqueness of data, sustain legitimacy,
encrypt the information for maintaining honesty and restrict the quantity
of deposit information for assuring confidentiality. The device securitymodel needs to be firm enough for avoiding illegal access. Nevertheless,it must be adaptable for the protection of effective exchanges to the societyas well as diplomacy.
3.2.1 Node attack
The authentic information can be extracted by an attacker on the devices as asubstitute for demolishing the authorized data.
3.2.2 Sinkhole attack
If the sensors of the IoT-devices in the network are unnoticed for along-time, then these devices are turned to be vulnerable to sinkhole attack.
In this occurrence, the threatened system pulls out the authenticated data
through all the neighboring systems in the network.
3.2.3 Selective-forwarding attack
Vulnerable nodes may select data-packets and illegally access data. After the
obtaining of these data-packets, they can be released. Thus, SFA judiciouslycan screen specific data-fragments and permit the remaining packets. Thedropped data-packets may contain essential data for additional execution.
3.2.4 Witch attack
It can be arising after an unauthorized IoT system and gets hold over thefailure of an IoT-node. If the authentic node fails to work, then the accurateconnection to the next node will deviate through the vulnerable node for
further transmission which ultimately leads to data loss.
3.2.5 HELLO flood attacks
The vulnerable IoT-nodes originate a HELLO flood attack via transmitting
the HELLO texts to each and every adjacent node. They can be accessed
on a regular basis. Hence, there may be a chance to consider this node asa nearby IoT-node for each node in the network. Sequentially, thisunauthorized system can transmit a HELLO text to each of its neighboringIoT-nodes and obstruct the valid nodes. This attack acts as the basis of
non-accessibility of properties for authentic operators by dispensing a
massive junk message to obtain provision.386 M.P. Anuradha and K. Lino Fathima Chinna Rani3.2.6 Physical damage
Invaders can violate the procedural facts by their malicious activities. They
are implemented through demolishing the IoT-devices. Though IoT-device inclusions are frequently non-tamperproof, the IoT-devices maybe wide open and the physical parts can be examined through investigationsand pin-headers. Hardware protection involves outlining inviolability of
IoT-devices. Due to this, it is hard to tamper confidential information like
user data, passwords and encryption mechanism. The exterior access can bedominated in most of the IoT-devices. Consequently, an intruder couldreplicate the whole system and spoil the authentic code and information.
3.3 Attacks —Network and service support layers
These layers, combinedly, represent the IoT service management structure.
Moreover, it is accountable for IoT-devices which are used by the authenticusers, spreads on policies combined with rules, and merges digitization withthe IoT-devices. In this layer, Role based access control is used to find out
the authenticated users and devices by using security mechanisms. In order
to achieve authentication, there needs to maintain a sustainability for anaudit track variation made by all authenticated users as well as devices, sothat it will be intolerable for disproving the activities engaged in the IoT-
network. This data control can also be assisted to recognize the malicious
actions if it is detected. There are some harmful attacks may occur in thenetwork and service support layer. These are as follows:
3.3.1 Man-in-the-middle (MITM) attack
It is an illustration of snooping probably in the IoT environment. Thoughdevice confirmation contains the verification of exchangeable devices ofphysical features, self-identity stealing can occur in this attack.
3.3.2 Replay attack
In this attack, the information is gathered from either the personal unique-ness of data interchange or from the other IoT authorizations. These sensi-tive data can either be deceived and transformed or reiterated. This attack is
considered principally a dynamic method of man-in-the-middle attack.
3.3.3 Denial of service attack
The IoT-devices in IoT-network are considered to be resource controlled
and these are exposed to various resource attacks extensively. Intruders
could lead messages and information to an explicit system for the utilizationof resources specifically.387 Acceleration of blockchain technology in IoT for data security3.3.4 Sybil attack
A malicious node pretends to be an active IoT-node in the network by using
various uniqueness. Therefore, a network unknowingly permits a maliciousnode to implement and execute more than one time, in order to avoidredundancy. In wireless sensor networks (WSN), the attacker may use thismalicious system to diffuse data via a negotiated system. They bring down
the network system under their control.
3.3.5 Sinkhole attack
Intruders may outbreak incorporated system by attacking the informationstream through neighboring systems. The whole system may deceive as wellas simulate the information which was reached already in its terminal. In the
WSN, the attacker can use this vulnerable system to fascinate the network
traffic, and attack the sensor’s data.
3.3.6 Sniffing attack
Intruders can utilize the sniffer systems and services to reach the networkinformation and extract the sensitive data for further occurrences.
3.4 Attacks —Middle-ware layer (or) service support layer
This layer offers an interface as well as services to the application layer.
Intruders may break the facility to disturb the application layer. The threats
in database and system server can destroy the data as well as the operationalnode productivity. The cloud threats primarily intended on data virtualizationthat can produce an enormous danger for the user’s confidentiality. The goal
of middleware layer threat is to abolish the eminent applications as well as
user’s concealment.
3.4.1 Flooding attack in cloud
Cloud flooding attack is one of the methods of denial-of-service attacks. Inthis attack, intruders continuously transmit their requirements to a cloud ser-
vice which can weaken the cloud applications in such a way it attacks the
QoS (Quality-of-Service). It is a malicious attack on a network resourcewhich avoids authentic users to utilize the resources and naturally employsby starting an overwhelming number of fake requests for services.
3.4.2 Malware injection
The intruder can modify the information and access the control over the
resources as well as execute the vulnerable program via inserting illegal388 M.P. Anuradha and K. Lino Fathima Chinna Raniservice code or simulating mechanism that is hooked on the cloud. It defines
that, intruders replicate the data and upload a victim’s application code,
therefore, malicious code replies to the victim’s request if some applicationis needed. At last, an attacker may accomplish the sensitive data services.
3.4.3 Signature wrapping attack
Cloud network applies XML signature for confirming service honesty. The
intruder changes the snooped texts without signature authorization. Besides,an intruder may implement random guidelines and process like an authenticemployer.
3.4.4 Web browser attack
The browser engines are enabled to complete instructions and guidelines ona remote server, like authentication as well as authorization mechanism. But,the web-browser may not yield scrambled XML tokens. Intruders take
advantage over this weakness for improving admittance without verifica-
tion. The cloud-web services may create some meta-data that can includea massive cloud content and implementation code. When the attackers attainthis additional information, the intruders might have a chance to do cloud
attack.
3.4.5 SQL injection attack
By incorporating SQL queries with IoT-data, a weekly intended programma-
ble code can be vulnerable towards SQL Injection Attack. Intruders utilize
these SQL queries for accessing, inserting, and removing actions. This attackcannot individually obtain private data; besides, it aims to attack the wholenetwork’s database. After the Web applications attack via SQL injection, exis-ting webpage displays dissimilar results related to the original content.
3.5 Attacks —Application layer
The application layer threats primarily aim to take out user’s authentication
in an indirect way. Intruders characteristically reach the code and service sus-ceptibilities such as code injection, memory-buffer overflow, and malicious
access to outbreak the authenticity. Counterfeiting identity is one of the
techniques for an unofficial agent to attain identical sanction (or) approvallike an authenticated user. Apart from these threats, this layer is also endan-gered through system malware like Worms, Trojans and viruses. It can also
be demoralized by other vulnerable codes like cross-site scripting, spyware,
adware, etc.389 Acceleration of blockchain technology in IoT for data security3.5.1 Code injection
Inthis attack, the injection of vulnerable programs is fed into the system
through manipulating the uniqueness of the code. It can be applied to snipthe sensitive data, gain node access, and spread the viruses. The shell andHTML script injections are the most common vulnerabilities of theseattacks. This attack gains the system’s access control and the attackers take
advantage of the authenticated user’s confidentiality, and even make a whole
network completely shut down.
3.5.2 Buffer overflow
Buffer Overflow is one of the malicious threats that destroys the programlimit or memory buffer through abusing code susceptibilities. Most of thesystem programs executed in the primary memory, comprising code withfragments of data. This kind of attacker inscribes a lengthy program to a
predefined area which is already accessed by the programmer. The resultant
action may be the variation of other data, implementation of maliciouscode, and demolition of the program control flow. Additionally, this typeof attack gives the affirmation to an illegal agent by expanding the system-
administrator rights and implements the malicious programs.
3.5.3 Sensitive data permission and manipulation
The vulnerable control also destroys the user’s sensitive information and
their authenticity. This threat usually takes advantage over the designed
errors in the consensus model for regulating the IoT smart services. Theevents can be used to direct the sensitive information of an IoT-Deviceto SmartApp; these events can be used by the SmartApp to screen the
IoT-Device’s data. But, the lack of protection in the events can leak out
sensitive information and generate serious issues.
3.5.4 Phishing attack
There are more susceptibilities occurring in this attack. Here, the intruder
plays like an actual user or faithful organization in order to attain the user’sdata like login-passwords and pin number details. An E-mail is the mostcommon public channel attack. Here, the user’s complex data is manipu-
lated through the intruder while valid workers try to access the mail.
3.5.5 Authentication and authorization
Authentication and Authorization procedures act as a significant portion in
the guard of IoT sensitive information. The current verification methods are390 M.P. Anuradha and K. Lino Fathima Chinna Raninot sufficient to offer a full-fledged authentication. Additionally, the suscep-
tibilities also occur in the authorization network model. Here, the trick is
taking advantage of the user’s privacy that permits the intruder to retrievedata by ignoring mandatory details. Additionally, because of the lack ofperfect verification of algorithm in the IoT services, the intruder performsmalicious access.
4. IoT— Edge computing
Being an innovative architecture, IoT-Edge computing contains edge
server characteristics. They are placed on the web edge, nearer by theIoT-sensors, edge workers, and the mobile devices. They evolve IoT termssuch as “Cloudlets, micro data centers, and fog,” which are used in day-to-day life. They are meant by the minor types of edge-installed computationaltechniques. It can be characterizing the contrasts of merging with enormous
cloud computing data centers.
IoT-Edge computing denotes the empowering computational tech-
nique which permits the data to be executed at the network edge. The term
“downstream data” is to be used for cloud applications and “upstream data”
term can be used for IoT applications [6]. “Edge” is a well-defined term and
is
a continuation, by means of all computations emerging with network
properties on the pathway amid primary-data and massive cloud storage
[7]. An internet edge is an exclusive room which is positioned generally
prior
to the linked IoT-end devices. This technique suggested perfect loca-
tion for minimum latency offload structure towards the sustenance evolving
services like AR (Augmented-Reality), public-protection, transportation,industries, trade and health care.
When cloud computing diffuses with edge computing, innovative tasks
and openings are increasing.
4.1 Functions
The edge computing consists of Bi-directional computational streams: one isdata upstream in which the sensitive data moves from the IoT-devices to the
cloud. Another one is defined as data extracted from cloud to IoT-devices.
This is termed as data downstream which is described in Fig. 3 . IoT-end
devices 
in the edge computing model are considered as consumers as well
as producers of data.
The main goal of IoT is not only intended for cloud services, but also,
should they require accomplishing computational performance from the edge.391 Acceleration of blockchain technology in IoT for data securityEdge plays several roles like offload computing, massive data preservation, and
caching along with execution. Additionally, it can dispense the requirements
and distribute the applications through the cloud to the clients [8].B e s i d e s
these functionalities, the edge encounters the following prerequisites profi-
ciently like perseverance, privacy, and data authentication.
4.2 Three-Tier edge computing model
The edge computing properties are needed to be examined; Fig. 4 depicts an
intellectual three-tier edge computing model which comprises IoT-tier1,
edge-tier 2, and cloud-tire3. In the IoT-tire, there are more drones; sensors
are available for health appliances, plans and approaches which are executed
for smart homes, and provisions existing for web trading. In order to relate
IoT with edge tire, numerous data transmission procedures are organized.
Through 4G/LTE, assigning drones with cellular tower and by using home
gateway-Wi-Fi, smart home sensors can be linked together.
The cellular-tower, gateway, edge servers with its requests, and massive
loading with the execution of cloud capabilities will perform various tasks
[9]. There are procedures in edge and the cloud. They help to increase
and achieve high-throughput as well as the speed. The transmission proto-
cols between the edge and the cloud enhances the performance, such as
ethernet, optical fibers and 5G technology.
DATA 
 Get Data
 Offload Computation
 Data Caching
 Data Processing
 Add Additional Information
 Distribute to Consumers
 Authentication/Authorization
Data Up-Stream
Data Producer/ConsumerResulDataEdge
RequestCLOUD
Data Producer
Data Down-Stream
Fig. 3 IoT-edge computing.392 M.P. Anuradha and K. Lino Fathima Chinna Rani4.3 Edge vs cloud
Edge computing and cloud computing have reliable connections. The IoT
device’s pervasiveness and quick progression of current technology together
with cloud virtualization have diverted the attention to edge computing
[10]. Edge computing requires authoritative power-consumption and also
massive storage centers. The cloud data centers require the edge computa-
tional technique for progressing the massive secured information. Edge
computing significant advantages are classified as follows. Basically, a massive
amount of data is not fully sent to the cloud, but most of it is executed at the
network edge itself [11]. It significantly diminishes the network bandwidth
heaviness as well as resource utilization. Second, the data existing in edge are
parallelly executed and they are independent in nature. This feature signif-
icantly weakens the latency along with the progression of response compe-
tence. Finally, in edge-computing, the authenticated user’s sensitive data are
stored in the edge-device instead of uploading. The resultant of this feature
used to decrease the network hazard of data drip and shelter the data
efficiency.
4.4 Attacks on edge nodes
A better picture needed for meeting vulnerabilities so that one can suggest an
effective protection. There are two aspects to this: recognizing the ways that
an intruder can compromise the node, and understanding the hardness of
such action.
SMART HEALTH EDGE SERVER
EDGE GATEWAY
CLOUD SERVICESCELLULAR TOWER
SMART HOME
INDUSTRIAL DEVICESEthernetEthernet/
Optical FibreLTE/5G
RFIDNB-loTWI-Fi3G/4G/5G
Fig. 4 Three-tier edge computing model.393 Acceleration of blockchain technology in IoT for data securityAttack Modes : There are four ways to get into edge node:
4.4.1. Network attack
4.4.2. External ports attack
4.4.3. Side-channel attack
4.4.4. Breaking into the device
4.4.1 Network attack
While the network is the greatest protected port, it is only as good as thesafety in place. Entirely unprotected nodes can no longer continue by hop-
ing to remain under the radar. For e.g., Web tools can crawl the network by
recognizing every vulnerable node. The vulnerabilities may persist due tobugs in the poor random number usage in crypto algorithms, unobservedmalware and violent protocol attacks by determining professionals, and evenby weaknesses in the protocols themselves. Even with a significantly protec-
ted network, an intruder might compromise a poorly fortified edge node by
replicating a firmware update and substitute the genuine code with codewritten by the attacker.
4.4.2 Port attack
The network port may be the only linking available on a small, bare-bonesedge node. Sophisticated edge nodes, yet, may have segment ports for work-ing in dissimilar sensors, or they may have USB or other ports for fittings,
consumables, or assessment and debug equipment. Each of these ports
affords an opportunity to access the edge node. Either attack could originatethrough an unused port, or an attachment could be detached and substitutedby some other hardware intended to execute the attack.
4.4.3 Side-channel attack
Sophisticated attacks can also happen without producing any connection tothe edge node. By tapping the power line or measuring emissions or vibrations
on an insecure device, it is probable to extract information about the keys.
By manipulating undocumented behaviors or faults like issuing a power flow,it may be possible to put a device into an undocumented and leaky state.
4.4.4 Physical attack
Finally, a determined attacker may substantially disassemble the edge node inan effort to inquire the internal circuits either with and without power or even
eliminate and reprocess ICs to acquire the contents of embedded memories.
Comprehensive security must shield against all of these modes of attack.394 M.P. Anuradha and K. Lino Fathima Chinna Rani5. Requirements for integration of blockchain
and edge computing
There are some pre-conditions required to be encountered before the
incorporation of blockchain with Edge computing.
5.1 Authentication
As has been discussed earlier, edge computing settings include service pro-
viders, infrastructure set-up with innovative services; therefore, it is impor-tant to ensure entity verification, which is combined with consistentedge-interfaces for getting smart contracts. The properties and characteristicsof these entities are confirmed through blockchains in overall contract pro-
cess execution. Therefore, the blockchain authentication process is essential
to formulate secure transmission medium over edge-ecosystems eventhough they have different security measures [12].
5.2 Adaptability
In the current situation, IoT-edge devices, their exertion services, andpredominant restricted blockchain resources are highly increasing [13].
Consequently, 
the diffusion of blockchain with edge computing technology
must support the extensive number of users with their complicated tasks.In addition to that, the emerging technology must have the capability ofadapting fluctuated settings to let the system or thing in the network or to
leave from the settings easily.
5.3 Data integrity
It is defined as the preservation of data-honesty throughout its life cycle andgives the assurance of data precision and reliability. When the massive
decentralized computing resources are manipulated, the updated edge-server along with the distributed blockchain-secured framework, achievingthe authentication services are violated. As a result, they may lead to some
changes in the outsourced data and they may create some space for illegal
uploading. Hence, additional techniques are required for the authenticationprocess. They are mandatory for the Individual Service Provider andcustomers.395 Acceleration of blockchain technology in IoT for data security5.4 Verifiable computation
It allows some untrusted customers to offload their data for computing while
doing so, it preserves accurate outcomes [14]. Edge computing allows to
execute
outsourcing computation. This feature covers various kinds of com-
putations without using blockchain technology, whereas the inspirational
and self-sufficient Ethereum blockchain smart contract must guarantee
the efficient computation process and the perfect resultant service.
5.5 Low latency
Normally the application latency can be defined as the combination of
two terms: (1) Transmission latency and (2) Computational latency.Computational-latency specifies the time period taken together for data exe-cution and blockchain mining. It’s function is based on the system compu-
tational power. The emerging blockchain technology with edge computing
is supposed to regulate its planning among the computational types and itsperformance, because the volume to afford the high-speed computationalprocess increases from IoT-users to cloud servers which cause to raise the
transmission latency.
5.6 Network security
Heterogeneousness devices and threat susceptibility are the two serious
issues that are in the edge computing technology. The emergence of block-chain with edge computing features plays a vital role to substitute the sub-stantial significant management over some communication protocols. It
offers quick access from the preservation of large decentralized edge servers
and creates well-organized monitoring in order to avoid vulnerable activitiessuch as sniffing, and DDoS attack [15].
6. Integration of blockchain and edge computing
6.1 Blockchain role in edge computing
Blockchain relates to its capability for permitting the authenticated-user to
record their data in a distributed shared ledger in the well-organized net-
work settings. Additional consideration is rewarded for blockchain essentialslike consent protocol, ledger topology, credits and agreement. Fig. 5 depicts
that 
the integrated system extensions can be adopted into various kinds of
edge computing and in the already created appropriate groups. The signif-icant blockchain characteristics is the authentication and authorization396 M.P. Anuradha and K. Lino Fathima Chinna Raniwhich assures the data security enhancement and the necessity for improving
scalability measures [16]. Edge computing has the capacity to achieve net-
working with the IoT-device and to store the massive data and computa-
tional capability over the decentralized edge; The subsequent benefit is
followed with the application management.
The edge computing dynamic facts consist of attaining distributed scal-
ability. The essential absorption is to afford effective monitoring in a protec-
ted way [17]. Consequently, the united architecture and characteristics of
blockchain and edge computing- systems are projected at furnishing protec-
ted resources for attaining the client request by enchanting with network
examination, massive storage with a computational capability that shelters
the core blockchain framework and edge computing core competences
[18]. An integrated opportunity originates through both the identical dis-
tributed network architecture and the same storage volume with computa-
tional functionalities; whereas, integration stipulation lies between the
blockchain and edge computing disadvantages and it is needed to enhance
their corresponding functionalities.
6.2 Mixing —Blockchain and edge computing
6.2.1 Edge computing —Inadequate security
The edge computing, distributed architecture contains several advantages.
However, edge computing security remains an important issue. In the
future, edge computing is considered a complicated interlacing of several
Cloud
ServerMEC
ServersDecentralized
Cloud ResourcesPeer To Peer Edge
Resources
BlockchainComputation
Power
Data
OutsourcingIoT UsersOff-loading
Off-loading
Fig. 5 Integration of blockchain with edge computing model.397 Acceleration of blockchain technology in IoT for data securityand diverse technological developments such as Peer-to-Peer network,
wireless technology, virtualization, etc. The various IoT devices dealings
along with massive storage of edge computing servers and the global as wellas local message transmission generate the probable causes for vulnerableactivities. Most of the threats like jamming attacks, sniffer attacks, etc.,can be initiated to break down the entire connection via shut-off the whole
network, otherwise to screen the data flow. Thus, disciplinary actions taken
by the network managers take responsibility for the authentication process,which is needed to be tested repetitively because of the high-vitality as wellas the honesty of the edge computing technology.
It is a tedious progression to detach the data traffic from management
traffic while handling the diverse se t of edge devices in the network. It
gives a chance to the opponents to regulate the entire network easily.Furthermore, distributed authority at the internet edg e might be carrying
a heavy load to the system administration. In edge computing networks,
the data are detached into several fragments, and then, they are kept overdiverse data repositories. That makes it even easier for the intruders to dropdata fragments; it accumulates the da ta imperfectly. Therefore, edge com-
puting cannot assure the data authen ticity. Similarly, sensitive data drip
along with some other confidential ity problems that may happen when
the transmitted data from numerous IoT-edge nodes could be altered
(or) mistreated through illegal adversaries. One more resistance fordata-storage is confirming data cons istency; meanwhile, outdated mecha-
nism is used to rectify the data vulner abilities by applying error-removal
mechanisms and network security mechanisms. It produces larger storageover the edge computing structures.
Another significant security challenge in edge computing network is to
preserve the security and privacy in uploading computational responsibil-
ities to edge computation nodes. Some computation schemes exist in aplace where the computation is outsourced with the computation functionor the public key to one or more servers, which return the outcome of the
process as well as a proof to verify the execution. Thus, it can be perceived
that the security problems such as secu re control at the edge, data storage,
computation and the protection of network may require new thoughts toadapt to decentralization, synchroniz ation, heterogeneity and mobility of
edge computing. In particular, the grouping of scalability with security in
such huge joints but avoiding unn ecessary encryption outlays.398 M.P. Anuradha and K. Lino Fathima Chinna Rani6.2.2 Challenges and restrictions of blockchain
In spite of blockchain and its excessive performance, it comes across several
issues that could confine its extensive operation. The blockchain systems caninsist and can have two concerns of the distributed network, scalability andauthenticity [19]. Explicitly, decentralization allows the network to be per-
mitted
less and transparent. Security refers to the data reliability along with
the common attacks, and scalability whic h refers to the capability to progress
the communications, correspondingly. When it comes to current scalabilityissues, mostly the restrictions of low-throughput, high-latency, and resourcedraining inhibit the practical blockchain-based resolutions. Blockchain needs
to increase its storage space when the number of transactions-records increases.
In the blockchain size of 158 GB in September 2017, reboot time is
coarsely taking four days when a new node participates into the network
chain. Ethereum framework is hard to undergo similar progression of
requests, that is, upgradation of the whole blockchain record-history
requires maintenance at each node. However, even now, the internet isenormously large; it leads to a rise in the restrictions in the blockchain dis-tributed technologies. In addition, the blockchain size meets a centralization
challenge, if a minor body of broad business environments is proficient to
execute all systems which sometimes act like a forger whereas the light nodesdo not have any technique of noticing this malicious activity immediately.
Having been regulated by the exact maximum block size (Block
size¼1MB) and the processing block time period, both are the origins to
produce the next block. The public blockchains such as Bitcoin andEthereum framework could manage only an average of 7 to 20 transactionsfor each second, which is too low for a payment processor, like master creditcard, to deal with the average of two thousand transactions per second.
Moreover, the transaction fee could differ for the application services,
miners and the charge amount for the transaction record confirmation.The hardware charges and power consumption for the transactional miningprocess are not a negligible measure.
Towards increasing a blockchain’s throughput, a simple method of
various blockchain frameworks and huge block size have been disapproveddue to the security cost and the decentralization challenge. Currently,on-chain network scaling of “sharding” and off-chain network scaling of“state-channels,” both are certainly needed; Whereas all these mechanisms
are yet to be improved with the combination of other techniques.399 Acceleration of blockchain technology in IoT for data securityTherefore, the foremost research objective is to concentrate on growing
transactional throughput, reducing the system bandwidth, storage, and
processing consumption, in the meantime nominally losing its authenticity.
7. IoT framework: Secure edge computing with
blockchain technology
The secure framework is the concrete outline since blockchain is
incorporated with edge computing for IoT computational processing withstorage demands. Architecture is comprised of systemized layers for migrat-ing blockchain’s comprehensive processes to an isolated layer which can beexternal to the application layer, holding IoT-devices and possessing
restricted resources. The processes hooked on an individual layer of the
framework are explained as follows.
7.1 Design overview
The cloud layer, edge layer and the device layer are the three layers com-posed in the IoT Framework as demonstrated in the Fig. 5 below. This
framework 
consists of layers originating through the edge computing archi-
tecture since this framework is enhanced along with a Point-to-Point deviceconnected within the individual layer to offer more storage with computa-tional proficiencies. The device layer establishes Point-to-Point coupledwith IoT destination systems which initiate the data as well as they exploit
edge network resources. The edge layer contains various servers and storage
facilities that are associated with Point-to-Point fashion to make additionalstorage available. It confirms firmness also reduces the single points of failurehazards. The edge-layer takes responsibility towards the temporary data stor-
age, instantaneous data processing combined with data analysis and control-
ling data transactions swapped between the diverse set of systems. The cloudlayer is inclusive of much authoritative proficiency to afford enduring dataanalysis, processing with massive storage capability and trade level transac-
tions and reporting. The cloud resources can be systematized by blockchain,
which confirms the data authentication.
7.2 Blockchain framework layered architecture
The proposed framework contains a significant blockchain frameworklayer, specifically, distributed IoT-device layer, Peer-to-Peer network edgeservers and decentralized cloud properties which explain the issues and also
encounter specific goals.400 M.P. Anuradha and K. Lino Fathima Chinna Rani7.3 Distributed-IoT device layer
This layer contains IoT-devices to be located in Point-To-Point network set-
tings, which are incorporated with the blockchain system for interchanging
the sensitive information between the cloud resources and IoT-data. The dis-
tributed device-layer holds various IoT edge devices i.e., sensors and actuators
for assembling the data and forwarding to other systems in the peer network.
Otherwise, it will send the sensitive data to the higher layers. The smart
devices use the data transmission mechanisms for accessing either the central-
ized systems via edge-servers or distributed by a Point-to-Point networking
system. A private blockchain can be executed in the centralized message trans-
mission by means of communication between the nodes regulated through
the edge server. It is also responsible for appending new systems in the net-
work, block validation and elimination of an active system in the network.
The communication among the IoT-peers is simplified by a secret key sharing
between the IoT-nodes through the server. Contrarily, IoT-devices and
servers together could contribute within the public blockchain by Point-
to-Point transmission.
Due to determination of end device resources, their contribution in
blockchain is simplified through proficient servers located at the upper
layers, edge and cloud servers. The IoT-end devices can complete only
the easiest task like forwarding transaction reports with end nodes in the net-
work (or) hardware updating, while the servers complete the excessive pro-
cesses. The following Fig. 6 explains the process of distributed IoT-device
layer. At the request of IoT-devices with constrained properties which are
placed in both centralized and distributed network, edge servers, powerfully,
Edge
Computing
LayerDecentralized IoT Device LayerPublic Blockchain
Private Blockchain
Offloading to Edge Server
Offloading to more
capable peer
Data synchronization and
consensus disseminationComputation Power
Data Outsourcing
Fig. 6 Decentralized IoT device layer.401 Acceleration of blockchain technology in IoT for data securityoffer huge extended massive storage and high computational capabilities.
Besides, edge servers are closely located at IoT-end users which lead to quick
responses to their IoT-Clients.
The P2P connected IoT-devices can effortlessly offload all their data.
Here, the data refer to either stored data or computational data. The datagot loaded to an IoT-edge server (or) an adjacent peer for immediate response.
In the offloading process, unlike the entire thing only a part of main transac-
tions in the chain is accumulated by the IoT-devices. Here, the absorption isintensive computational management. Additionally, because of the lack ofvarious standards of smart devices among diverse sellers, blockchain permits
all these devices to contribute to the same blockchain network.
7.4 Point-to-point edge servers network
The edge layer maximizes the cloud scalability in order to carry the facilities
nearer to IoT-intended devices for enhancing the processing performance
and decrease the latency. The massive availability of resources to IoT-
devices, edge servers can interact themselves to create replicated datarepositories and synchronize data processing management. To gain this ben-efit, blockchain is positioned on the edge servers to launch a decentralized
framework which assures the authenticated transmission over the network.
During the P2P network interaction, small computational analytics isperformed by the edge nodes for peers in the network and for the sake ofthe network, it is to attain self-systemization by means of adding and remov-
ing the edge nodes. In addition, they also take responsibility for data
processing and forwarding the sensitive data report either to a decentralizedcloud for permanent storage or send back to IoT-end devices based on therequest.
7.5 Decentralized resources of cloud
The cloud layer is mainly intended for providing storage and computing ser-vices as per the requirement of the client while the blockchain considerseach system as an individual node on the blockchain network for profi-
ciently joining in the active mining progression. The cloud layer which
has massive data storage with computational capabilities requires a consensusmechanism in the blockchain framework to assure security, low-cost and theenhanced processing capabilities. Properties of nodes in the blockchain
architecture might be imposed once it is performed efficiently and they
are executed consequently when they move towards the united data402 M.P. Anuradha and K. Lino Fathima Chinna Raniauthenticity services. The nodes located at the cloud layer are autonomous
data and by the implementation of blockchain, record sustainability will be
maintained through the complete repetition process.
8. Factors to be addressed in secure edge computing
The addressing factors for securing edge computing execution that
have been recognized include:
8.1 Low latency
By its nature, the edge is nearer to the IoT device than the cloud. This meansa quicker round-trip for communications to attain local processing power,suggestively speeding up data communications and processing.
8.2 Longer battery life for IoT devices
Being able to expose communication channels for a short span of time due toimproved latency means that battery life of battery driven IoT devices could
be prolonged.
8.3 More efficient data management
Processing data at the edge makes simple data quality management such asclarifying and prioritization are more competent. Finishing this data man-
agement at the edge means cleaner data sets can be represented to cloud-based processing for further analytics.
8.4 Access to data analytics and AI
Edge processing power, information processing and data storage could beall combined processing to permit analytics and AI. They involve very fastresponse times or require the processing of large “real-time” data sets.
8.5 Resilience
The edge proposes more probable communication paths than a centralizedmodel. This distribution assures that flexibility of data communications ismore secure. If there is a failure at the edge, other properties are obtainable
to deliver continuous operation.403 Acceleration of blockchain technology in IoT for data security8.6 Scalability
In decentralized with the edge model, a lesser amount of load should even-
tually be positioned in the network. This means that scaling IoT devices
should have a fewer resources effect on the network, particularly if applica-tion and control planes are positioned at the edge together with the data.
9. Advantages —Integration of blockchain and edge
computing
Various features of edge computational process, data storage reposi-
tory, integrated network, required for both the blockchain and the edgecomputing are identically decentralized mechanisms. These technologiesdissimilar in compatible significances and are intended for their combina-tional working mechanism. There are several benefits that originated
through the diffusion of these two technologies. The blockchain and edge
computing combination increase the data authorization, confidentiality, andresource utilization.
 There is a possibility for constructing a distributed network. It contains
dozens of nodes over the blockchain architecture. Through mining pro-cesses and cryptographic methods, it achieves privacy, reliability and datavalidation with guidelines in a crystal-clear method [20]. Hence, it considers
being
an operating result in which larger diverse users are divided and
located on the edge (or) moving among detached physical edges.
In edge computing, privacy is a challenge. Here, the data is uploaded
either
locally or the entire data is divided into fragments between various
nodes in the network for exposing the form of synchronizing edgeprocessing. The blockchain technique uses sharing crypto keys among
the participants for accessing and regulating the data without a mediator.
So, it’s significant feature permits a bottom-up management approachwithout revealing the additional information like source node, destina-tion node and the sensitive data. Therefore, it is also termed as an
immutability network.
The resource interaction between the fog nodes is achieved through
active
synchronization [21]. Through dynamic execution of on-demand
resource allocation and algorithm, the Smart contract integrated withblockchain can utilize the resources based on the service demanded.
Here, the service level agreement for both the client and service provider
is authenticated by monitoring the resource utilization. Therefore, the404 M.P. Anuradha and K. Lino Fathima Chinna Raniintegration of these two technologies and edge computing resource
management achieves the consistent, self-regulation, powerful distrib-
uted network and effective executions suggestively minimize the func-tioning amount and massive edge storage.
The edge computing design framework starts through P2P followed bythe extension of the other edge devices in the network. It merges with
P2P cloud computing across the blockchain technology. The blockchain
scale measures are significantly implemented by using this hierarchy levelordered framework. It supports blockchain information and propagation.
The blockchain technology enhances the offloading authentication
quality by the verification of resource-restricted end users of the
blockchain network. Currently, Bitcoin is considered as the most immu-table technique, on the other hand, it could hardly execute on theIoT-edge devices like sensors, mobile devices, etc. It might be com-
prehended through implementing Proof-of-Work [PoW] mechanism
to the edge users. Similarly, more financial approaches can be executedby the edge with blockchain computational processes.
The essential requirement of public blockchain is a storage durability and
the private chain demands the authenticated and a self-regulating atmo-
sphere. All these provisions are efficiently afforded by an edge server.Blockchain offers only the restricted storage for multimedia applicationswhich requires off-loading storage and it applies some cutting-edge mul-timedia mechanisms to enhance the edge computing authenticity.
10. Use cases —Blockchain with edge computing
The edge computing technology amalgamated with the blockchain
technologies ensure security and data integrity in the following application
areas viz. Smart city, Smart transportation, Industrial-IoTs, Smart home andSmart grid.
10.1 Smart city
Deployment of IoT and blockchain technologies is used to build the Smart
city not only does it enhance the efficiency of urban management and
operation, but also promotes the leap forward development of the city bysensing information, securely communicating information and proficientlyprocessing information.
The blockchain-based decentralized cloud architecture with a Software
Defined Networking (SDN) which incorporates blockchain technologies405 Acceleration of blockchain technology in IoT for data securitycomprises three layers: IoT devices, edge servers and cloud servers. It
resolves the demerits of the outdated cloud computing. The inter-domain
of these two technologies such as blockchain and edge computing com-binedly create a framework which ensures the security and privacy of smartcities [22].
Approximatel
y, all the IoT-devices produce and execute sensitive user’s
data without authorization. A variant type of blockchain framework evolv-ing with IoT edge computing ensures the authorization of sense data. Amicro service which is founded on Blockchain decentralized architectureis employed in a hierarchical blockchain-based edge computing to shield
the data execution across various service providers. Such things are located
in the smart public safety system.
Blockchain technologies are used to enhance the consistency and dura-
bility of user’s data. A blockchain database resolves the security issues of the
Smart city connected with home devices and sensors.
10.2 Smart transportation
The smart transport system is a fusion of blockchain, whereas edge comput-
ing is vital in the field of transportation for future research. For instance,
an effective and protecting transportation network system assists various
things such as constrained authorization technique, one-to-many adjacencymatching, destination co-ordination and data controllability through merg-ing the vehicular fog computing with blockchain framework. Here, the fog
servers can match travellers’ details with transport drivers details in order to
create a private blockchain.
In the blockchain-based MaaS, the smart contracts employ the edge
servers, which can link travellers to providers in a more efficient way, and
attain many advantages including validation, authorization and development.
10.3 Industrial IoTs
In the progression of industrial IoT, the incorporation of edge computing
and blockchain technology offers a protected interface for the current cloud
computing technology, data processing and execution, and authorization
control, which speed up the delivering processes to the intended edgeservers.
The extensible and safe edge application management is achieved
through Edge intelligence- Industrial IoT architecture and blockchainsecurity. An inter-domain edge resource scheduling mechanism and a credit406 M.P. Anuradha and K. Lino Fathima Chinna Ranidifferential edge transaction approval mechanism are used to decrease the
edge services cost and expand facilities capabilities [23].
In
the Industrial IoT, key features of validation, transparency, authority
and privacy are achieved through lightweight key agreement protocol built
by securing public keys [24]. A novel Blockchain-based Internet-of-Edge
model
is constructed through the combination of edge computing,
blockchain and Industrial IoT. This framework generates a privacy defensemechanism for scalable and controllable IoT systems. For instance, IIoTBazar is a distributed business edge service market, which generates audit-ability for all participants through the blockchain secured framework. It is
used to deploy monitoring and tracking solutions mounted on IoT-edge
devices. Low computational power IoT-Edge devices fused with theIIoT Bazaar by fog computing technology. Augmented Reality (AR) con-nects the IoT-users with edge devices.
10.4 Smart home
An innovative secured cross domain framework ensures the honesty, privacyand accessibility through the integration of blockchain technology withsmart home. For example: A smart home emergency facility system can
be employed by the Ethereum blockchain framework. This architecture
is used to distribute the trusted IoT-smart applications, for illustration,and the Home Service Providers assures the authorized control to the smarthome IoT devices.
The Error Control and the data transmission quality are attained by an
innovative architecture that has been built on blockchain. They combinedwith edge computing technologies. Blockchain architecture permits thehealthcare organization for assembling the sensitive health data from the
home environment by the IoT-sensors as well as sending this protected data
with other peers. Certainly, the life concerned data accumulated by theinter-domain framework which are very important to the IoT-User’s treat-ment is approved by some sensors. These are used to monitor the biologicalas well as physical features and environmental quality. Therefore, they sup-
ply these data to the edge server for further processing.
10.5 Smart grid
A permissioned blockchain with edge model resolves two key problems viz.privacy and security in smart grid. Edge computing is integrated with the
blockchain by the common authentication mechanisms and the key407 Acceleration of blockchain technology in IoT for data securityagreement protocol for the secured smart grid. The main goal of this key
agreement protocol includes restricted anonymity and other shared key
management without other complex encryption conditions. Theblockchain can confirm secured energy transactions among the grids. Theutility functions authenticate the various transactions in the smart grid toapprove the node in the network.
11. Further challenges and recommendations
There are more favorable benefits and the bright foreseen future for
Blockchain with IoT edge computing. Yet, the substantial tasks in the devel-opment and distribution of current and prospective framework will requiremore analysis:
11.1 Technical threats
The features of blockchain with edge computing are such as efficient reli-
ability, security enhancement and the extension of scalability. But these
applications require some advanced technical measures for the improvementof the overall framework. Furthermore, blockchain technologies alwayshave some restrictions when it is incorporated with IoT-devices like trans-action volume, implementation in authentication mechanisms (or) some
smart contract execution [25]. Additionally, the decentralized mechanisms
must
be generated to resolve a flexibility issue.
11.2 Interoperability and standardization
When it comes to the participants in blockchain and IoT end users, both areplaying a vital role to attain the entire fulfilment of integrated authorities as
well as the effective implementation. Encryption standards along with theenhancement of data security and consensus among the users are require-ments for the adoption of collaborative technologies and reach the interna-tional standards.
11.3 Blockchain framework
There is a need to construct a complete authorized infrastructure whichmeets all the requirements of inter-domain execution. That can fulfil allthe necessities for the use of blockchain in IoT systems. Many advancedmechanisms need to be developed in order to face the issues which arise with
the integration of the blockchain with edge computing strategies.408 M.P. Anuradha and K. Lino Fathima Chinna Rani11.4 Administration, authority, controlling and legal
characteristics
Addition to technical issues, significant monitoring is another major prob-
lem for revealing the probability of Blockchain with IoT. There is a possi-
bility that some intruders counterfeit their blockchain attainment for
fascinating their customer through the predictable revenues.
11.5 Rapid field testing
In the future, there is a need for optimization for combining blockchain withseveral applications when a user needs blockchain security for the IoT appli-cations. The initial stage is to discover the adaptability (i.e.) the user mustfind out which blockchain framework fits to their IoT-requirement.
Consequently, there must be a concrete mechanism needed to examine
diverse blockchains. The two core steps of the above concept can beexplained as follows: (i) standardization (ii) testing. In standardization, itincludes deep consideration of blockchain applications like supply-chains,
marketplaces, goods, and security solutions. All services should be reviewed
and approved. In testing, more than one type of criteria must be estimatedwith respect to confidentiality, security, power consumption, throughput,latency and application of blockchain among the users.
12. Conclusion
This chapter mainly focuses on distributed applications and manage-
ment to meet privacy, execution performance, scalability and inter-domain
adaptivity for the future networks and systems. Eclipse attacks, selfish miningattacks and 51% of attacks are some of the attacks that still exist in theblockchain technology, though it provides various productive methods
for the edge computing. Here, the main challenges are authorization and
secure encryption algorithms. Resource utilization constraints in order toimprove the software quality after the incorporation of blockchain with edgecomputing. In addition, there is a need to concentrate on transactional
security and privacy in the IoT-devices.
This chapter explains the inter-domain of two eminent technologies
which comprises IoT-framework, possible threats and attacks, edge-devices,and the security measures of collaborative blockchain along with edge com-
puting technologies. At last, the opportunities and challenges are discussed.
In addition, this chapter gives an outline about evolution of the current409 Acceleration of blockchain technology in IoT for data securityapplications such as smart home, smart cities, smart grid, smart transportation
and smart industries over the blockchain technologies. Having given the
comprehensiveness of research areas, it is also determined that hard investi-gations are essential with better attention to be focused on transformingwell-established fog computing.
References
[1]O. Novo, Blockchain meets IoT: an architecture for scalable access management in IoT,
IEEE
Internet Things J. 5 (2) (2018) 1184 –1195.
[2]J. Pan, J. McElhannon, Future edge cloud and edge computing for internet of thingsapplications,
IEEE Internet Things J. 5 (1) (2017) 439 –449.
[3]W. Shi, J. Cao, Q. Zhang, Y. Li, L. Xu, Edge computing: vision and challenges, IEEE
Internet
Things J. 3 (5) (2016) 637– 646.
[4]S. Mostafavi, M.A. Dawlatnazar, F. Paydar, Edge computing for IoT: challenges and
solutions,
J. Commun. Technol. Electron. Comput. Sci. 25 (2019) 5 –8.
[5]K. Chen, S. Zhang, Z. Li, Y. Zhang, Q. Deng, S. Ray, Y. Jin, Internet-of-Things secu-
rity
and vulnerabilities: taxonomy, challenges, and practice, J. Hardware Syst. Sec. 2
(2) (2018) 97 –110.
[6]B. Varghese, N. Wang, S. Barbhuiya, P. Kilpatrick, D.S. Nikolopoulos, Challenges and
opportunities
in edge computing, in: 2016 IEEE International Conference on Smart
Cloud (SmartCloud), IEEE, 2016, pp. 20 –26.
[7]W. Shi, G. Pallis, Z. Xu, Edge computing [scanning the issue], Proc. IEEE 107(8)
(2019) 1474 –1481.
[8]K. Cao, Y. Liu, G. Meng, Q. Sun, An overview on edge computing research, IEEEAccess
8 (2020) 85714 –85728.
[9]M.A. Rahman, M.S. Hossain, G. Loukas, E. Hassanain, S.S. Rahman, M.F.Alhamid,
M. Guizani, Blockchain-based mobile edge computing framework for secure
therapy applications, IEEE Access 6 (2018) 72469 –72478.
[10] F.J. Ferra ´ndez-Pastor, H. Mora, A. Jimeno-Morenilla, B. Volckaert, Deployment of
IoT edge and fog computing technologies to develop smart building services,Sustainability 10 (11) (2018) 3832.
[11] H. Bangui, S. Rakrak, S. Raghay, B. Buhnova, Moving to the edge-cloud-of-things:recent
advances and future research directions, Electronics 7 (11) (2018) 309.
[12] J. Pan, J. Wang, A. Hester, I. Alqerm, Y. Liu, Y. Zhao, EdgeChain: an edge-IoT frame-work
and prototype based on blockchain and smart contracts, IEEE Internet Things J. 6
(3) (2018) 4719 –4732.
[13] T.M. Ferna ´ndez-Caram /C19es, P. Fraga-Lamas, A review on the use of blockchain for the
internet of things, IEEE Access 6 (2018) 32979 –33001.
[14] S. Naveen, M.R. Kounte, Key technologies and challenges in IoT edge computing, in:2019
Third International conference on I-SMAC (IoT in Social, Mobile, Analytics and
Cloud) (I-SMAC), IEEE, 2019, pp. 61 –65.
[15] S. Huh, S. Cho, S. Kim, Managing IoT devices using blockchain platform, in: 2017 19th
International
Conference on Advanced Communication Technology (ICACT), IEEE,
2017, pp. 464– 467.
[16] E.F. Jesus, V.R. Chicarino, C.V. de Albuquerque, A.A.D.A. Rocha, A survey of how
to
use blockchain to secure internet of things and the stalker attack, Sec. Commun.
Netw. 2018 (2018).
[17] Y. Ai, M. Peng, K. Zhang, Edge computing technologies for Internet of Things:a
primer, Digit. Commun. Netw. 4 (2) (2018) 77 –86.410 M.P. Anuradha and K. Lino Fathima Chinna Rani[18] P. Mendki, Blockchain enabled IoT edge computing, in: Proceedings of the 2019
International Conference on Blockchain Technology, 2019, pp. 66 –69.
[19] M.A. Khan, K. Salah, IoT security: review, blockchain solutions, and open challenges,
Fut. Gen. Comput. Syst. 82 (2018) 395 –411.
[20] H.N. Dai, Z. Zheng, Y. Zhang, Blockchain for Internet of Things: a survey, IEEE Int.
Things J. 6 (5) (2019) 8076 –8094.
[21] P.J. Escamilla-Ambrosio, A. Rodrı ´guez-Mota, E. Aguirre-Anaya, R. Acosta-Bermejo,
M. Salinas-Rosales, Distributing computing in the internet of things: cloud, fog and
edge computing overview, in: NEO 2016, Springer, 2018, pp. 87 –115.
[22] M.A. Rahman, M.M. Rashid, M.S. Hossain, E. Hassanain, M.F. Alhamid, M. Guizani,
Blockchain and IoT-based cognitive edge framework for sharing economy services in a
smart city, IEEE Access 7 (2019) 18611 –18621.
[23] I. Sitto ´n-Candanedo, R.S. Alonso, S. Rodrı ´guez-Gonza ´lez, J.A.G. Coria, F. De La
Prieta, Edge computing architectures in industry 4.0: a general survey and comparison,
in: International Workshop on Soft Computing Models in Industrial and
Environmental Applications, Springer, Cham, 2019, pp. 121 –131.
[24] C. Wang, G. Yang, G. Papanastasiou, H. Zhang, J. Rodrigues, V. Albuquerque,
Industrial cyber-physical systems-based cloud IoT edge for federated heterogeneous
distillation, IEEE Trans. Ind. Inform. (2020).
[25] R. Yang, F.R. Yu, P. Si, Z. Yang, Y. Zhang, Integrated blockchain and edge comput-
ing systems: A survey, some research issues and challenges, IEEE Commun. Surv. Tutor.
21 (2) (2019) 1508 –1532.
About the authors
Dr. M.P. Anuradha , Assistant Professor in
the Department of Computer Science,
Bishop Heber College, Tiruchirappalli,
Tamil Nadu, India, has been working in
the area of Wireless Sensor Networks and
Internet of Things, Block Chain, Secure
Supply chain management, Network
Security. She received her M.C.A., and
M.Phil, degrees in Computer Science from
Bharathidasan University, Tiruchirappalli,
India in 2002 and 2005 respectively. She
has obtained Ph.D. in “Establishment of
Reliable, Credible Routing Protocol with Maximum Connectivity for
Wireless Sensor Networks through Secured and Aggregated Data
Approach” from Bharathidasan University, Tiruchirappalli, India in 2014.
She is guiding number of research works and published significant number
of research papers in Scopus, Web of Science journals. She has presented
more than 10 papers in national, international conferences and also acted
as reviewer of the national and international peer reviewed journals.411 Acceleration of blockchain technology in IoT for data securityShe has a significant contribution in carrying out research projects on the
design and development of IoT based secure supply chain management.
K. Lino Fathima Chinna Rani is working
as an Assistant Professor in the Department of
Computer Applications, Bishop Heber
College, Tiruchirappalli, Tamil Nadu, India.
She post graduated in Master of Information
Technology from Bharathidasan University
in 2008. She had completed her Master
of Philosophy in Computer Science in
September 2009 at Bharathidasan University,
Tiruchirappalli. Currently, she is pursuing
doctorate of philosoph y in computer science
under the broad topic, “An Enhanced
Approach to Improve the Performance Efficiency of IoT with Block
Chain”. Her area of interest is Network Security; with a specific focus on
Block chain Decentralized Technology. She is passionate to explore on
implementation of Block chain and dis tributed technology to enhance the
quality in IoT sector. She has publish ed research papers in international
journals.412 M.P. Anuradha and K. Lino Fathima Chinna RaniCHAPTER FIFTEEN
EDGE/FOG computing paradigm:
Concept, platforms and toolchains
N. Krishnaraja, A. Danielb, Kavita Sainib, and Kiranmai Bellamc
aDepartment of Networking and Communications, School of Computing, SRM Institute of Science and
Technology, Kattankulathur, Tamil Nadu, India
bSchool of Computing Science and Engineering (SCSE), Galgotias University, Delhi, Uttar Pradesh, India
cDepartment of Computer Science, A & M University, Prairie View, TX, United States
Contents
1.Introduction 414
2.Machine learning (ML) in FC 416
3.Classes of service for fog applications 416
4.Clusters for lightweight edge clouds 418
4.1 FAP ’s machine learning algorithms 419
4.2 Machine learning for the protection of security and privacy 422
5.IoT Application with fog real time application 425
6.Safeguarding data consistency at the edge 426
6.1 Data embedding on computing device with IoT on fog environment 427
7.Cloud-fog-edge-IoT collaborative framework 428
8.Edge computing with machine learning 429
8.1 Resource management in fog computing 430
9.Security challenges in fog computing 432
10. Conclusion 433
Reference 433
About the authors 435
Abstract
At the moment, Web applications operating on smart phones create huge amounts of
data that may be processed on the Cloud. Yet, one of a Cloud ’s core limitations is its
connection to endpoint devices. Through the use of distributed compute, communica-tion, and storage services along the Cloud to Things (C2T) continuum, fog computingovercomes this constraint and empowers new application possibilities such as smartcities, augmented reality (AR), and virtual reality (VR) (VR). Furthermore, the use ofFog-based computing resources and its incorporation with the Cloud brings newresource management issues, necessitating the development of new techniques toensure application quality of service (QoS) compliance. In this setting, a critical challenge
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.012413is how to link application QoS needs to Fog and Cloud resources. One possibility is to
classify the applications that arrive at the Fog into Classes of Service (CoS). Thus, this
article provides a set of CoS for fog applications that incorporates the QoS criteria thatmost accurately describe these fog applications. Additionally, this article suggests theuse of a standard machine learning classification approach to differentiate Fog comput-ing applications based on their QoS needs. Additionally, this technique is demonstratedthrough the evaluation of classifiers’ efficiency, accuracy, and robustness to noise.
Adopting a technique for machine learning-based categorization is a first step in defin-ing methods for providing QoS in fog computing. Additionally, categorizing Fogcomputing applications can aid the Fog scheduler ’s decision-making process.
1. Introduction
Cloud computing provides widespread access to the available reser-
voirs of programmable resources and services through the Internet, which
may be deployed fast and with little administrative work. However, as
the Internet of Things (IoT), mobile, and multimedia applications gain trac-
tion, the transmission traffic jams between the Cloud and an end device haveindeed been deemed excessive and unsuitable for latency-sensitive applica-tions, posing the primary constraint on the Cloud’s use for latency-sensitive
and mobile applications. Cloud computing has enabled the provision of sev-
eral services, including platform, software, and infrastructure, as services,with the possibility of presenting anything as a service. Nevertheless, it isnot always practical to send sensor-generated large data to the cloud forprocessing and storage. Additionally, some IoT applications need quicker
processing, which only modern cloud infrastructure is capable of fulfilling
[1]. The problem is addressed through the use of FC, which utilizes the
computing
capacity of devices located near a user to assist with data storage
and processing. FC’s many objectives include increased efficiency and adecrease in the quantity of data that must be sent to the cloud for data
processing, analysis, and storage. Fig. 1 The primary reason for this is fre-
quently 
performance, although security and privacy can also be factors.
Recently, AI algorithms have been used to the processing of IoT data.
End-user devices at the network’s lowest layer have a variety of undesirable
characteristics, including insufficient memory, insufficient connectionbandwidth, limited computing power, and heterogeneous hardware thatdiffers from cloud infrastructure [2].
Throughout
the previous decade, computing technology have prog-
ressed in a variety of areas, including AI, GPU computing, cloud computing,and other hardware advancements. Machine learning is the most frequently414 N. Krishnaraj et al.used AI algorithm in a variety of disciplines. Several previous research have
utilized machine learning to solve networking challenges, including net-
work routing, security, traffic engineering, and resource allocation. ML is
critical in establishing a smart/intelligent environment capable of autono-
mous administration and operation. The importance of machine learning
is extended to IoT, as without it, IoT cannot fulfill functional, monitoring,
or preprocessing duties. Additionally, satisfying the varied QoS require-
ments of the IoT remains a difficult challenge due to the resource-
constrained nature of IoT devices. Thus, it is important to describe machine
learning in terms of fog, cloud, and edge computing for IoT implementa-
tion. Using existing technologies such as Cisco’s IOS XR, the ability to
examine data on network equipment such as routers and switches is simple
to deploy. Typically, machine learning research involves actuators, sensors,
and low-level fog nodes [3,4]. However, fog nodes may manage frame-
works such as Weka and Scikit-learn to create a variety of AI applications.
Machine learning is used to automate, optimize, allocate, and monitor
operational activities such as clustering, routing, duty-cycle management,
data aggregation, and medium access control [2,5–7].I ti sc h a l l e n g i n gt o
control the processes that occur in fog nodes due to their dynamic, com-
plex, and diverse nature.
Fig. 1 Role of machine learning in IoT devices.415 EDGE/FOG computing paradigm2. Machine learning (ML) in FC
Applying AI to fog and IoT enable the provision of services and appli-
cations, as well as the optimization of operation of the system and network
management. FC is about redistributing part of the cloud’s computationalcapacity to the network’s edge, which is often occupied by IoT devicesand human users. Thus, two strategies exist for incorporating intelligence
into FC [8]. The first is device-driven intelligence, which occurs when
IoT
devices and fog layers get smarter with the addition of sensors, increased
computing power, storage, and communication capabilities. Local server and
base stations, IoT gateways, and nodes for mobile data gathering carried
by a human user are all examples of devices with these characteristics. The
research intended to increase these devices’ intelligence capabilities, forexample, by adding intelligent data processing and networking servicesto an e-health gateway or a gateway capable of doing machine learning
[9]. Additionally, by monitoring wireless channel characteristics using a neural
ne
twork model, efficient coverage and connection may be achieved. With
these capabilities, edge devices may gather data with unprecedented granular-ity, enabling the network to be context-aware, make choices, and managelocal resources. The second method is intelligence that is driven by humans.
Human users are critical since they molded the IoT landscape’s architecture.
Humans are often the data sources in IoT networks. As a result, their behav-ioral patterns are critical in teaching the network to become smarter [5].
Nu
merous academic works are devoted to meeting human requirements,
such as the e-butler for managing household appliances and the cognitiveIoT-based smart home for enhancing living quality. The goal is to create asystem that uses user circumstances while serving people and learns how todo network-specific activities effectively and with an eye toward power usage.
In this way, user intelligence converts individual domain knowledge into ben-
eficial network domain decision-making [8]. Thus, device and human-driven
in
telligence can be regarded as a viable option when designing an FC system
that satisfies the QoS requirements of an IoT application.
3. Classes of service for fog applications
Fog computing allows innovative solutions, particularly those requir-
ing low latency and mobility. Such new applications can have diverse QoS416 N. Krishnaraj et al.needs, necessitating the use of fog management methods to deal with this
heterogeneity efficiently.
Therefore, resource management is extremely hard in fog computing,
necessitating the development of integrated systems capable of constantly
adjusting the resource allocation. The initial stage in managing resources
is to classify incoming requests into Classes of Service (CoS) based on their
Quality of Service (QoS) needs. Bandwidth. Certain applications require a
minimum guarantee of throughput, referred to as a Guaranteed Bit Rate
(GBR). Although some multimedia applications utilize adaptive coding
techniques to encode digitized speech or video at a rate that suits the pres-
ently offered bandwidth, multimedia applications are bandwidth-sensitive
Fig. 2 . Sensitivity to delay. Certain applications, particularly real-time appli-
cations, have a specified latency threshold below which latency must be
guaranteed. The loss sensitivity parameter specifies the percentage of packets
that do not reach their intended destination. Reliability refers to the capacity
of Fog components to perform the required operation in the face of a variety
of failure modes. Certain applications require the rapid re-establishment of
failed Fog components in order to complete activities within specified
latency constraints. Availability is a metric that indicates how frequently
the Fog’s resources are accessible to end users. Applications and services that
must run continuously, such as mission-critical applications, require high
availability [5].
Fig. 2 Services of Fog computing toward high availability devices.417 EDGE/FOG computing paradigm4. Clusters for lightweight edge clouds
Security is the process of developing and implementing authentication
and authorization mechanisms to safeguard personally identifiable and sen-
sitive information generated by end users. The data location specifies the
location of the application’s data. Locally, at the end device; remotely, at
a Fog node; or in a distant repository, in the Cloud. The data placement
requirements for an application are determined by a variety of variables,
including reaction time limitations, the computing capability of each Fog
layer, and the available capacity on network lines. Numerous edge devices
are intrinsically mobile. Continuity of supplied services should be guaranteed,
even for end customers who are very mobile. Continuous connection is
required to do the necessary processing Fig. 3 . Scalability refers to an appli-
cation’s capacity to run efficiently even when confronted with a rising number
of requests from end users. The number of users in a Fog might vary
depending on their movement and the activation of applications or sensors.
In large data processing, streams of data may need to be processed within a
certain time limit. Demand for Fog nodes is subject to fluctuation, and
resource flexibility must be given to meet these demands [10].
Because of the widespread deployment of artificial intelligence, we
delved deeper into the AI department [5]. The categorization in this part
is based on where applications or services are placed in Fog, task scheduling,
job offloading, and resource allocation. As a result, the authors chose the
Fig. 3 AI-based fog computing application placement strategies.418 N. Krishnaraj et al.algorithms stated in each category since they were their favorites, and they
did not include any other algorithms in this area.
4.1 FAP ’s machine learning algorithms
Classic learning algorithms and deep learning algorithms are two subsets ofmachine learning algorithms. supervised learning, unsupervised learning,
and reinforcement learning are three types of traditional learning algorithms(RL). Learning that is supervised. The supervised learning algorithm is a typeof machine learning (ML) algorithm that uses a labeled data set as input. The
basic goal of supervised learning is to build a model of input-output com-
munication and anticipate the intended outcomes. Regression and classifi-cation are two types of supervised learning. Decision Tree (DT), ANN,Random Decision Forest (RF), K-Nearest Neighbor (K-NN), Support
Vector Machine (SVM), Apriori, and Logistic Regression are among the
classifications (LR). Regression entails the use of a regression tree.
Regression is a statistical technique used to model the relationship
between dependent variables. Additionally, it is utilized to establish a link
between independent and dependent variables. The advantages of linear
regression include the following: it performs well regardless of the volumeof data and when the information is separated linearly; it provides informa-tion about the significance of features. Although linear regression is prone tooverfitting, this can be minimized by utilizing dimensionality reduction tech-
niques including such regularization and cross-validation. Linear regression’s
drawbacks include the assumption of linearity, susceptibility to noise andoverfitting, susceptibility to outliers, and susceptibility to multicollinearity.Polynomial regression has the followin g advantages: it is applicable to data
sets of any size and performs exceptionally well on non-linear issues.Disadvantages of polynomial regre ssion include the necessity to select
the appropriate polynomial degree fo r optimal bias, a variance trade-off,
and being overly sensitive to outlier s. The module placement strategy in
mobile fog computing is based on class ification and regression trees to
choose the optimal fog device. In comparison to the First Fit algorithm,the suggested approach consumed le ss power, responded faster, and
performed better [4].
Linear
regression is a type of mathematical modeling that falls under the
classification category and is a generalized form of linear regression. Theadvantages of logistic regression include the following: it performs wellwhen the dataset is linearly separable; it is less prone to overfitting than linear419 EDGE/FOG computing paradigmregression; nonetheless, it can overfit in high-dimensional datasets [11]. The
disadvantages of logistic regression include the following: In contrast to
real-world data, which is typically separable linearly, this approach assumes
a linear relationship between the dependent and independent variables.
W h e r e a si fn u m b e ro fo b s e r v a t i o n si sl o w e rt h a nt h en u m b e ro ff e a t u r e s ,
Logistic Regression should not be used. In this case, the likelihood of
overfitting increases. Only discret ef u n c t i o n sm a yb ep r e d i c t e du s i n g
Logistic Regression. Bashir et al. suggested a dynamic resource manage-
ment approach for fog computing and us ed logistic regression to determine
the load on each Fog node and its impact on subsequent decisions. The
proposed algorithm enhanced performance by 98.25% Fig. 4 .
KNN is among the most straightforward data mining and classification
techniques available. This algorithm performs simple categorization opera-
tions and gives accurate results in the form of predictions. The term "closest"
serves as the foundation for K’s classification process. Each new sample is
compared to all previous samples, and the closest previous samples are allo-
cated to those samples Fig. 5 . One of the benefits of this algorithm is that it
requires no training, which means that fresh data may be introduced effort-
lessly without affecting the algorithm’s accuracy. This approach is simple to
construct and has applications in both classification and regression. The
downsides of this technique include its inability to handle huge datasets,
its inability to handle high-dimensional data, its requirement for feature scal-
ing, and its sensitivity to noisy data, missing values, and outliers. Pastor et al.
recommended the use of IoT Edge and Fog computing technologies, as well
Knowledge
Representation
Natural
Language Processing
Prediction
Image
Recognition
Neural
Networks
Machine LearningMachine Reasoning
Analytics
Speech
Recognition
Deep Learning
Virtual Agents
Autonomics
Fig. 4 Machine learning framework for different data flow variables.420 N. Krishnaraj et al.as the integration of Edge and Fog paradigms, to construct smart building
services. KNN and decision tree algorithms are being utilized to regulate
energy use and generation from renewable sources [9,12] .
A support vector machine (SVM) is a non-linear machine learning algo-
rithm that is used for classification and regression. SVM’s goal is to generate a
hyperplane that separates the classes (classifies the data), such that this hyper-
plane is as far away from the samples in the classes as possible. The potential
benefits of the SVM algorithm also included the following: it is appropriate
for situations in which we have no clue about the data, it is best suited for
unorganized and semi-structured data, it is capable of solving the most com-
plex problems, SVM is not optimized for local optima, SVM models exhibit
generalization in practice, and the risk of overfitting is reduced with SVM.
The disadvantages of this technique are the difficulty of selecting the optimal
kernel and the lengthy training time for huge datasets. He et al. suggested a
multi-tier fog computing model for smart city applications that includes
large-scale data analytics services [10]. Two classification algorithms are used
to present the experiments: logistic regression and SVM.
Fig. 5 Different development tool sets for machine learning algorithm.421 EDGE/FOG computing paradigmANN is the abbreviation for artificial neural network. An ANN can
establish a link between the input and the desired output. The Multi
Layer Perceptron (MLP) is a prominent neural network model that resem-bles the human brain’s transfer function. The following are some criticalcharacteristics concerning neural networks: first, neural networks are verydependent on data sets. Second, while neural networks are slow during
the learning phase, they are lightning quick during execution. The advan-
tages of ANNs include the capacity to store data over the entire network, tobe fault tolerant, to have a distributed memory, to tolerate gradual corrup-tion, and to perform parallel processing. Limitations of ANNs include hard-
ware dependence, inexplicable network behavior, the inability to design an
ideal network structure, the network’s longevity is unpredictable, and neuralnetworks become stuck in the local optimal. Deep learning frameworks suchas TensorFlow and Keras, as well as Google Cloud Platform and Deep
Neural Networks on Fog servers could be used for object detection [13].
4.2 Machine learning for the protection of security and privacy
The studies in this part are focused on enhancing the security of FCs or net-
works in general. Additionally, some authors sought to safeguard the privacyof user data [5]. It identifies two primary security concerns: recognizing
zero-day 
attacks on IoT protocols and identifying cyberattacks launched
from IoT networks. Additionally, they stated that network-based intrusion
detection systems (NIDS) use storage for attack signatures and are unable to
detect fresh attacks in future network traffic. Additionally, host-based IDS(HIDS), such as anti-virus software, is incompatible with low-resourceIoT devices. They then recommended utilizing a Random Forest machine
learning algorithm implemented in the fog layer to detect dangers in IoT
systems. The system is designed around a master security fog node that mon-itors network traffic, detects intrusions, and sends cloud-based notifications.The RF model is trained using the UNSW-NB15 cyberattack data set.
Python was used for both implementation and training. The classification
accuracy was 99.34%, with a 0.02% false-positive rate. However, the con-fusion matrix revealed that the precision for attack detection is only 0.79%and the recall is 0.97%. They addressed how a centralized threat detectionsystem would be ineffective in IoT applications due to issues of scalability,
distribution, resource constraints, and latency.
Additionally, the majority of existing solutions rely on supervised
machine learning algorithms which needs a lot of labeled data for training.422 N. Krishnaraj et al.Additionally, accessible data sets (such as KDDCup99) are out of date,
resulting in less reliable conclusions. As a result, the authors provided an
intrusion detection and prevention framework that is based on the FC par-adigm and employs a semi-supervised Fuzzy C-means (ESFCM) algorithmthat is ELM-based. While connected to the IoT devices through a base sta-tion, the fog node detects the TCP/IP layers. We utilized SFCM and ELM
to train the model on labeled data in order to label the unlabeled data set.
This work makes use of the KDDTest-21 and KDDTest+ data sets. Thecentralized technique and other supervised algorithms were used as refer-ence points for evaluation (SVM, KNN, Logistic Regression, Random
Forest, and Bayesian Network). According to the results, the framework
required less detection time than the centralized framework and beat otherstandard classifiers for both data sets. Additionally, such encryption algo-rithms are not meant to work on low-power IoT devices. Thus, the authors
presented a technique for multifunctional data aggregation methods that
preserves privacy (such as additive and non-additive aggregation). The tech-nique utilizes a trained machine learning model in the fog layer to predicatethe results of the aggregation query, which are then delivered to the cloud
via the fog. In terms of privacy, the sensor IoT separates the original data and
sends it to distinct fog nodes individually. Three entities comprise the pro-posed system model: IoT sensors, fog nodes, fog centers, and the cloud. Inthis case, the cloud sends the aggregation functions (minimum, maximum,median, -percentile, mean, and summing) to the fog center, which generates
the appropriate queries for each sensor. After the fog nodes collect all of the
sensors’ reported data, a training data set is formed for training a basic linearregression algorithm. The regression model is then used to forecast the queryset supplied by the fog center, that can store sensor data in order to respond
to the fog center’s aggregate queries. The authors demonstrated rationally
that constructing the training data set satisfies the concept of differential pri-vacy, which is accomplished by introducing Laplace noise to the entiretraining set [14]. We simulated and trained the machine learning-based
Laplace
differential privacy technique (MLDP) using two real-world data
sets: the Mobile Health Data Set (MHEALTH) and the ReferenceEnergy Disaggregation Data Set (REDD). To determine the suggestedmodel’s accuracy, the mean absolute error (MAE) was determined by com-parison to the classic Laplace differential privacy approach (LapDP). The
results indicate that LapDP outperforms MLDP with a small enough query
set. Therefore, as the size of the query set expanded, MLDP outperformedLapDP Fig. 6 .423 EDGE/FOG computing paradigmMachine learning as a practical application The subsequent experiments
used machine learning with reinforcement learning to solve a problem and
create an application. Three modules comprise the system (FC, server com-
puting, and back-end interactive communication modules). We trained and
tested the Lie Group classifier model using photos of a mobile circuit board.
The photos were carefully labeled with 12 distinct fault categories. The tau-
ght machine learning model is distributed to local fog nodes, which receive
images from camera sensors suspended above assembly lines. The cloud
server stores the classification results. The experiments were carried out
using MATLAB simulations. The system’s performance was evaluated using
the Receiver Operating Characteristic (ROC) curve, which has been com-
pared to the contour detection method, the pixel-based method, and the
K-means algorithm. Additionally, efficient runs for a variety of items on
the assembly line eliminated the time and delay associated with fog
processing. In comparison to directly transmitting photos to the cloud
for fault identification, the solution increased running speed by 53%.
Additionally, it reduced time by 42% and boosted accuracy by 28% when
compared to previous categorization approaches [7].
To solve the proof-of-work (PoW) challenge, blockchain applications
need enormous amounts of computer power. Fuzzy services should be
incentivized to sell fog resources while still guaranteeing QoS. They devised
an optimum auction for fog resource allocation based on deep learning to
Copyright Mind CommerceSome Data
Stored
 Storage
 Queries
 AnalyticsCloud
Some Data
Processed
and Used
In
Real-timeDataIP NetworkCentralized
Control and MediationDistributed
IntelligenceFog Computing Architecture
IoT
Endpoints
Fig. 6 Fog computing architecture for storage of cloud data.424 N. Krishnaraj et al.mine blockchain data. There were two FFN networks utilized to address the
assignment’s optimization problem: one for payments, and the other for
assignment submissions using a single FFN network. When creating the firstFFN, the hidden layer employed the sigmoid activation function, and theoutput layer used the SoftMax function to generate the auction’s winners.Since the payments in the second FFN are not negative, the output layer
used the rectifier activation function. In objective unsupervised learning,
the loss function included weighted restrictions, and the AugmentedLagrangian approach was utilized. In order to identify weights which min-imizes the loss function, TensorFlow was used to execute deep learning and
then the model was uploaded in the cloud. There are 5000 application eval-
uation profiles for miners in the data sets for both FFN. Fast and smoothconvergence are the goals of the Adam optimizer during training. As a com-parison, we used the greedy algorithm, which selects the top bidders in order
to maximize income. Individualized rationality and incentive compatibility
(IC) violations were compared against multiple rounds of Lagrange multi-plier values in order to see which was worst. In comparison to the greedystrategy, the recommended auction quickly converged on a higher revenue
value [8].
5. IoT Application with fog real time application
There has been a noticeable improvement in the overall quality of life
as the Internet of Things (IoT) has progressed. To support real-time appli-cations, high-end processing and storage units are necessary. In order to
compute and store these enormous amounts of raw data, cloud computingis critical. However, the cloud-only set-up is not an energy-efficient anddelay-aware solution for handling such a vast volume of data. Fog and edge
computing have been invented to address this issue. On the other hand,
\seamless communication owing to the mobility of IoT devices is a criticalfeature to process the data in the faraway cloud servers. For time-criticalapplications such as health care, connectivity interruption and thereforethe increase in delay in providing the processed information, result in poor
Quality of Service (QoS). Delivering processed data/information becomes
difficult when the device becomes disconnected because to mobility. Thisneeds a hierarchical\ infrastructure, where each tier (IoT, edge, fog or cloud)either gathers, maintains and processes information for minimizing the
latency. That work proposes a collaborative Cloud-Fog-Edge system for425 EDGE/FOG computing paradigmprocessing IoT data and presenting the selection based on mobility analysis
to overcome the aforementioned issues. We have considered a hierarchical
mobility-based infrastructure made of four layers: IoT layer, edge layer, fog
layer, and cloud layer. Nowadays smart phone has become a popular
medium for ubiquitous Internet access and various user-specific IoT apps
are accessible through smart phones. It is possible that these smart phones
might move about a lot because they are used as edge devices [15]. The users
of our system utilize these time-critical IoT applications while commuting
throughout. The edge layer is connected to the fog layer via the IoT layer
and collects the raw data generated there. The cloud layer connects the fog
layer, that are used for high-end processing and mobility analysis Fig. 7 .
6. Safeguarding data consistency at the edge
Clients located near the network’s edge, commonly known as the
Internet of Things, can access a variety of services provided by several
cloud-deployed apps (IoT). Applications which require minimal latency,
such as augmented reality or online games, are frequently run using edge
devices. Much of these apps can only be used if they have a response time
of less than 6 –40ms. By using fog nodes to be doing portion of the compu-
tation that otherwise would have been performed on a cloud server, edge
Road side unit (RSU)Cloud for data storage and computation
Moving agents (User, Vehicle, Mobile-client app)Cloud server s
Fog device
Edge device
IoT device
Fig. 7 Hierarchical placement of IoT, edge, fog devices and cloud in Mobi-IoST
framework.426 N. Krishnaraj et al.computing saves network traffic while also producing fast results. Fog nodes
more anticipated to be administered by a number of different local providers
and to be located in more vulnerable physical locations. As a result, fog nodesare extremely vulnerable, and developers of edge computing applications andmiddleware must priorities security over all other considerations. Offeringsafe middleware elements, like as secure storage services, that can protect
the applications against fog node weaknesses is a viable way to make app
development easier in this environment [9,12] .
6.1 Data embedding on computing device with IoT on fog
environment
Connecting connected systems to an existing Internet infrastructure and cre-ating a computing environment is what is referred to it as the Internet ofThings (IoT). The cloud servers process the raw data that IoT devicesacquire. In contrast, storing and processing large amounts of raw data in a
remote cloud adds delay and consumes more energy. Fog computing
has been offered as a solution to this problem. To save time and energy,IoT devices’ raw data is then processed locally on the fog device ratherthan in the faraway cloud. Connection interruptions during data process-
ing become more difficult if the user is using a mobile device. For example,
there is an Internet of Multimedia Things (IoMT) and an Internet of HealthThings (IoHT) comment thread, as well as an Internet of Vehicles (IoV)sub-domain. IoST, or Internet of Geographical Things, is a new branch
of the Internet of Things that focuses on managing spatial data.
Geospatial interoperability standards across networks are commonly repre-sented as numerical values about physical objects that may be transmittedand received by ubiquitous and embedded computing devices, whichare referred to as IoST. Fog-based IoT uses switches, routers, and other
IoT devices as fog devices to analyze raw data more quickly. Edge devices,
such as smartphones and tablets, act as a bridge between the Internet ofThings (IoT) devices and the network. However, these mobile gadgets facesignificant resource constraints. As a result, mobile devices must save their
data on cloud servers. Additionally, mobile devices can offload computa-
tionally intensive tasks to the cloud to conserve battery life when resourcesare limited. Even for tiny amounts of work, using a distant cloud adds delayand increases the mobile device’s battery usage. Numerous current tech-
niques to offloading have already been targeted towards reducing energy
and latency [5,7].427 EDGE/FOG computing paradigm7. Cloud-fog-edge-IoT collaborative framework
Organizations and individuals are increasingly relying on smart gad-
gets and computers in the Digital Age. Data is generated by apps and sensors,
which use electronic devices. This means that a lot of businesses are respon-sible for storing a lot of data. Businesses need a dynamic IT infrastructurenow because of the shift to cloud computing, which offers benefits such
as scalability, access to resources on demand, and pay as you go pricing
models. Platform as a Service (PaaS), Software as a Service (SaaS), andInfrastructure as a Service (IaaS) are all moving toward "Anything" as aService thanks to cloud technology. Unfortunately, some of the large
amounts of data collected by sensors cannot be sent to the cloud for
processing. Many Internet of Things (IoT) applications which require fasterprocessing, however current cloud capacity will not be able to handle them.Fog computing is a way to solve this issue since it uses the processing capa-
bility of nearby devices (i.e., idle computer power) to help with storage,
networking, and processing [ 16]. Fog computing can be used to achieve
a
wide range of objectives, including improving efficiency and reducing
the amount of data that must be transmitted to the cloud for several reasons,including data processing, analysis, and storage. Most of the time, this is
done for reasons of efficiency, but it can also be done for security and reg-
ulatory compliance considerations. In IoT data analyzing techniques, AIalgorithms have already been deployed. In both industry and academics,the phrases "fog computing" and "edge computing" are interchangeable.
In this study, we make a distinction between the two words that have been
mentioned. Fog computing and edge computing have the same goal, whichis to reduce network congestion and end-to-end delay, but they differ inhow data are processed and managed, as well as where processing and cog-
nitive power is located. Fog computing, on the other hand, differs signif-
icantly from traditional computing in that it is decentralized (i.e. It does notinvolve centralized computing). Basically, data processing and storage areperformed between both the source as well as cloud infrastructure in a
decentralized computing architecture. With edge computing, a compute
facility is moved closer to data sources like mobile devices, sensors, andactuators by "pushing" the computation facility. Edge elements handle datalocally instead of transferring it to the cloud, and each one plays a differentrole. A fog node uses its resources to decide whether it should analyze data
from numerous sources or to transfer it to the cloud. This determination is428 N. Krishnaraj et al.taken by the fog node. In addition, edge computing doesn’t really support
several virtualized services such as SaaS, IaaS, PaaS, and others [8].
8. Edge computing with machine learning
There are sensors, hardware, and edge connections everywhere in IoT
systems. Many Internets of Things (IoT) applications must meet bandwidth,
latency, and security requirements. Despite this, cloud computing falls short
of meeting these standards. Edge computing is a currently available technol-
ogy that could meet these needs. In that other example, data commutation
by automobiles can be performed using edge networks, and vehicles here on
road move in a coordinated manner to improve consumer efficiency Fig. 8 .
For example: virtual reality and augmented reality apps that require high
bandwidth can obtain material from an edge network. Depicts a model of
the edge computing problem in IoT networks. The model makes it possible
Fig. 8 Connection of edge devices with machine learning based data analysis.429 EDGE/FOG computing paradigmto analyze information from sensors and traffic. Classification of data using
machine learning to identify patterns in the attributes gleaned from various
data sources. In intrusion detection, disease diagnosis, imaging recognition,and traffic engineering, the use of outcomes can be controlled [9].
8.1 Resource management in fog computing
Our algorithm’s main purpose is to estimate the amount of fog deviceresources that will be accessible. The software also has other capabilities,such as ensuring the accuracy of rendered results and determining how many
jobs should be repeated. The authors of this study, on the other hand, con-
centrated on proactive network association and open-loop wireless commu-nication allowed by ML [ 17]. There was also a study done to forecast the
overall
length of time required from processing to data transmission, as well
as link utilization for different pre—processing workloads through end toend delay Table 1 .The authors used the GENI infrastructure’s image pro-
cessing 
ensemble services to create a realistic fog computing environment.
This standard IoT network protocol, Wireless Application Protocol, was
proposed by the authors in order to overcome the energy and communica-
tion challenges connecting end resource—constrained devices. Four differ-ent ML algorithms were used to predict real sensor readings. Researchershave also shown that implementing several ML approaches in layers reducesthe amount of energy consumed. ML approaches used in ubiquitous com-
puting applications entail several phases, and these processes can be per-
formed at different levels of abstraction [2,8].
In
most cases, coordinating data processing there at network’s edge is a
difficult problem for applications around the world (fog). Despite the fact
that fog computing has solved many processing issues by offering
preprocessing and moving computation to the network’s edge rather thanrelying on centralized processing, fog computing still has a number of issuesthat need to be addressed. The use of machine learning (ML) has made a sig-
nificant impact on the way data processing is managed. Fog computing can
benefit greatly from the application of machine learning (ML) in the rightway. After that, we’ll talk about fog computing’s processing and computationmanagement concerns and unresolved topics. Fog design should really bescalable and adaptable enough to handle networked applications’ unpre-
dictable workloads. Increasing sensor and information quantity demanded
connectivity and energy limits, resulting in new issues requiring effectiveIoT cloud architecture. Existing DL systems are constrained by their430 N. Krishnaraj et al.Table 1 Comparison statement of various techniques and its application usage of
Machine learning Algorithms.
Problem Technique Data Application
Edge device
communicationSupervised (linear
regression)Real-world seismic
data traced from
Parkfield, CaliforniaSeismic imaging
Continuous
and real-timepatientmonitoringSupervised
(classification—SVM)Data provided in
Physiobank as “TheLong-term STDatabase” and ECG
with arrhythmia in
the middle of anormal ECG signalHealthcare –patient
monitoring
Automatically
generatesmusical score
from a huge
amount ofmusic data in anIoT networkUnsupervised
(clustering—hiddenMarkov model)Audio files or record
audio signals in realtimeMusic cognition
Large amount
of IoT sensordata adopted in
industrial
productionsDL A total of
10 categories, eachof which has 200
images for the
training process and50 test images fornetwork testingSmart industry
Centralized
data processingUnsupervised (density
estimation—CRBMs)One week of FCD
generated in
Barcelona CityTraffic modeling
Centralized
data processingUnsupervised
(clustering—PCA)MIRS dataset Smart dairy
farming
Energy
efficiency andlatencyrequirementsfor time-criticalIoTSupervised
(classification—SVM,decision tree, andGaussian naı ¨ve Bayes)Sensor data from
real human subjectsTime-critical IoT
applications
Accuracy and
adaptability ofdata analyticson the edge of anetworkSupervised
(classification—SVM)“Long-term ST
Database”Health monitoring
systems431 EDGE/FOG computing paradigmcomputational speed, therefore creates issues with transmission and
processing as the volume of in-the-wild data grows. When processing is done
at the sensor level, energy consumption at the edge and end-to-end delay are
minimized. To enhance information processing outcomes, a self-learning
algorithm can be used, and this can minimize the overall data processing/
communication volume needed in the complete IoT network. Existing
potential for the creation of new and creative [4].
9. Security challenges in fog computing
Devices for fog computing being typically used without being closely
monitored or protected, leaving them wide open to a wide range of security
risks and vulnerabilities. As a result, building trust in the fog is the most dif-
ficult challenge to overcome. A public key encryption is one method for
addressing some of the problems mentioned above Fig. 9 . Fog computing
is vulnerable to several hostile assaults, and as a result, a network’s capabilities
may be seriously weakened if no practical security measures are in place.
Malicious attacks, such as denial-of-service (DoS) attacks, are common.
DoS attacks are simple to initiate because most devices connected to the
network aren’t mutually authenticated. Sending bogus processing/storage
requests from several devices can also launch a Denial-of-Service attack.
Fog computing has an open network, making existing defensive
Fig. 9 Various security challenge factors in Fog computing.432 N. Krishnaraj et al.techniques for other kinds of networks ineffective. A large network poses a
significant problem. Fog/cloud services could be used by hundreds of
thousands of IoT nodes to solve computation and storage constraintsand improve performance [ 18].
Fog 
computing protection has a severe challenge with authentication
while front fog nodes provide services to large numbers of end users [ 19].
And
using fog network services, a device must first join the network by log-
ging in to the fog network using its own credentials [ 20]. These step is critical
in
preventing unauthorized computers from entering the network.
Furthermore, it’s a significant challenge due to the obvious numerous con-
straints placed on the equipment connected to the network [8,9].
10. Conclusion
The purpose of this article was to investigate the role of ML in the FC.
To narrow the study options, researchers used the Google Scholar search
engine. The initial selection of research was made using abstracts and key-words from the corresponding webpages as a second constraint. There’s a lot
of potential for machine learning to replace humans as the go-to technology
in many fields. As a result, it can be an effective analytical tool for FC. Anapplication solution, security and privacy preservation, and paradigmenhancement were the three FC perspectives offered in this paper. Thepapers examined present exemplary FC-environment-based intelligent
solutions. FC’s varied properties, lack of testbeds, and other challenges ham-
pered real-world evaluations. Statistics and supervised learning algorithmsseem to be the most common ML approaches, as seen in the tables in
Section 4 . Unsupervised learning ML methods in an SDN-enabled FC con-
text
should be included in future study. In addition, the IDS should be
implemented on actual hardware rather than just through simulation.Finally, the examined research addressed a wide range of concerns and obsta-cles, as well as verifying the pliability and efficacy of ML implementation in
the FC setting.
Reference
[1] L. Hernandez, H. Cao, M. Wachowicz, Implementing an edge-fog-cloud architecture
for stream data management, in: 2017 IEEE Fog World Congress, FWC, 2017, 2018,
pp. 1 –6,https:/ /doi.org/10.1109/FWC.2017.8368538 .
[2]S.R. Jena, R. Shanmugam, K. Saini, S. Kumar, Cloud computing tools: Inside views and
ana
lysis, in: International Conference on Smart Sustainable Intelligent Computing and
Applications under ICITETM2020, Elsevier, 2020.433 EDGE/FOG computing paradigm[3] D. Belli, et al., A capacity-aware user recruitment framework for fog-based mobile crowd-
sensing platforms, in: 2019 IEEE Symposium on Computers and Communications
(ISCC) IEEE, 2019. Available at: https:/ /ieeexplore.ieee.org/document/8969754/
(Accessed: 6 October 2021).
[4]
D. Borthakur, et al., Smart fog: fog computing framework for unsupervised clustering
analytics in wearable Internet of Things, in: 2017 IEEE Global Conference on Signal
and Information Processing (GlobalSIP). IEEE, 2017. Available at: https:/ /ieeexplore.
ieee.org/document/8308687/ (Accessed: 6 October 2021).
[5]
S.R. Jena, R. Shanmugam, R. Dhanaraj, K. Saini, Recent advances and future research
directions in edge cloud framework, Int. J. Eng. Adv. Technol. 9 (2) (2019) 2249 –8958.
https:/ /doi.org/10.35940/ijeat.B3090.129219 .
[6] B.B. Ma, S. Fong, R. Millham, Data stream mining in fog computing environment with
f
eature selection using ensemble of swarm search algorithms, in: 2018 Conference on
Information Communications Technology and Society (ICTAS). IEEE, 2018. Available
at:https:/ /ieeexplore.ieee.org/document/8368770/ (Accessed: 6 October 2021).
[7]
F. Mehdipour, B. Javadi, A. Mahanti, FOG-engine: towards big data analytics in the
fog, in: 2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure
Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf
on Big Data Intelligence and Computing and Cyber Science and TechnologyCongress(DASC/PiCom/DataCom/CyberSciTech). IEEE, 2016. Available at:
https:/ /ieeexplore.ieee.org/document/7588914/ (Accessed: 6 October 2021).
[8] 
J. Wang, et al., Maximum data-resolution efficiency for fog-computing supportedspatial big data processing in disaster sc enarios, IEEE Trans. Parallel Distrib.
Syst. 30 (8) (2019). Available at: https:/ /ieeexplore.ieee.org/document/8630038/
(Accessed: 6 October 2021).
[9]
K. Saini, V. Agarwal, A. Varshney, A. Gupta, E2EE for data security for hybrid cloudservices: a novel approach, in: IEEE International Conference on Advances inComputing, Communication Control and Networking (IEEE ICACCCN 2018) orga-
nized by Galgotias College of Engineering & Technology Greater Noida, 12 –13
October, 2018, https:/ /doi.org/10.1109/ICACCCN.2018.8748782 .
[10] L. Hernandez, H. Cao, M. Wachowicz, Implementing an edge-fog-cloud architecture
for
stream data management, in: 2017 IEEE Fog World Congress (FWC). IEEE, 2017.
Available at: https:/ /ieeexplore.ieee.org/document/8368538/ (Accessed: 6 October 2021).
[11]
R. Iqbal, et al., Context-aware data-driven intelligent framework for fog infrastructures
in internet of vehicles, IEEE Acc. 6 (2018). Available at: https:/ /ieeexplore.ieee.org/
document/8488344/ (Accessed: 6 October 2021).
[12] D.N. Le, R. Kumar, B.K. Mishra, J.M. Chatterjee, M. Khari, (Eds.), Cyber Security in
Parallel
and Distributed Computing: Concepts, Techniques, Applications and Case
Studies, John Wiley & Sons, 2019.
[13] A. Khochare, et al., Dynamic scaling of video analytics for wide-area tracking in urban
spaces,
in: 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing (CCGRID). IEEE, 2019. Available at: https:/ /ieeexplore.ieee.org/
document/8752708/ (Accessed: 6 October 2021).
[14]
R. Jaiswal, A. Chakravorty, C. Rong, Distributed fog computing architecture for real-time
anomaly detection in smart meter data, in: 2020 IEEE Sixth International Conference onBig Data Computing Service and Applications (BigDataService). IEEE, 2020. Available at:
https:/ /ieeexplore.ieee.org/document/9179551/ (Accessed: 6 October 2021).
[15] K. Ahmad, A. Kamal, K.A.B. Ahmad, M. Khari, R.G. Crespo, Fast hybrid-MixNet forsecurity
and privacy using NTRU algorithm, J. Inf. Sec. Appl. 60 (2021), 102872.
[16] S. Nguyen, et al., A low-cost two-tier fog computing testbed for streaming IoT-based
applications,
IEEE Int. Things J. 8 (8) (2021) Available at: https:/ /ieeexplore.ieee.org/
document/9249378/ (Accessed:
6 October 2021).
[17] S. Ghosh, et al., Mobi-IoST: Mobility-aware cloud-fog-edge-IoT collaborative frame-
work
for time-critical applications, IEEE Trans. Network Sci. Eng. 7 (4) (2020)
2271 –2285, https:/ /doi.org/10.1109/TNSE.2019.2941754 .434 N. Krishnaraj et al.[18] F. Faticanti, et al., Cutting throughput with the edge: app-aware placement in fog com-
puting, in: Proceedings—6th IEEE International Conference on Cyber Security and
Cloud Computing, CSCloud 2019 and 5th IEEE International Conference on Edge
Computing and Scalable Cloud, EdgeCom 2019, 2019, pp. 196 –203, https:/ /doi.
org/10.1109/CSCloud/EdgeCom.2019.00026 .
[19] K.H. Abdulkareem, et al., A review of fog computing and machine learning: concepts,
applications, challenges, and open issues, IEEE Acc 7 (2019) 153123 –153140, https:/ /
doi.org/10.1109/ACCESS.2019.2947542 .
[20] J. Clemente, et al., Fog computing middleware for distributed cooperative data analyt-
ics, 2017 IEEE Fog World Congress (FWC) IEEE, 2017 Available at: https:/ /ieeexplore.
ieee.org/document/8368520/ (Accessed: 6 October 2021).
About the authors
Dr. A. Daniel currently working as an
Associate Professor in School of Computing
Science and Engineering in Galgotias
University, Greater Noida, Uttar Pradesh.
He completed his B.E and M.E both in
Anna University. He has completed his Ph.
D. in Computer Science and Engineering at
Shri Venkateshwara University, Uttar
Pradesh. His research interests are Cloud
Computing, Networking etc. He has publi-
shed several article in reputed international
journals. He has membership in IEEE,
ACM, IFERP, IAENG and CSTA.
Dr. N. Krishnaraj is working as an
Associate Professor, School of Computing,
SRM Institute of Science and Technology,
Kattankulathur, Tamilnadu, India. He is hav-
ing 13 years of experience in teaching and
research, his research areas are Biometrics,
Wireless sensor networks, Internet of
Things, Medical image processing. He has
completed one funded research project
supported by DST, India. He is Cisco certi-
fied Routing and switching professional. He
has published more than 65 articles in
reputed international journals and 15 articles presented in international con-
ferences. He is being serving as an Editorial board member of MAT Journal,435 EDGE/FOG computing paradigmIRED and Allied Academic Sciences. He has delivered several special lec-
tures in workshops and seminars. He is a professional society member for
ISTE, IEI and IAENG.
Kavita Saini is presently working as
Professor, School of Computing Science and
Engineering, Galgotias University, Delhi
NCR, India. She received her Ph.D. degree
from Banasthali Vidyapeeth, Vanasthali. She
has 18 years of teaching and research experi-
ence supervising Masters and Ph.D. scholars
in emerging technologies.
She has published more than 40 research
papers in national and international journals
and conferences. She has published 17
authored books for UG and PG courses for
a number of universities including MD University, Rothak, and Punjab
Technical University, Jallandhar with National Publishers. Kavita Saini
has edited many books with International Publishers including IGI
Global, CRC Press, IET Publisher Elsevier and published 15 book chapters
with International publishers. Under her guidance many M.Tech and Ph.D.
scholars are carrying out research work.
She has also published various patents. Kavita Saini has also delivered
technical talks on Blockchain: An Emerging Technology, Web to Deep
Web and other emerging Areas and Handled many Special Sessions in
International Conferences and Special Issues in International Journals.
Her research interests include Web-Based Instructional Systems (WBIS),
Blockchain Technology, Industry 4.O, and Cloud Computing.
Kiranmai Bellam now at Professor in the
Computer Science Department at Prairie
View A&M University, she completed his
B.S from Madras University, Chennai and
M.S from New Mexico Tech, United
States. He has completed his Ph.D. in
Computer Science from Auburn University.
Her research interests are Cloud Computing,
Networking etc.436 N. Krishnaraj et al.CHAPTER SIXTEEN
Artificial intelligence in edge
devices
Anubhav Singha, Kavita Sainib, Varad Nagarc, Vinay Aseric,
Mahipal Singh Sankhlac, Pritam P. Panditc, and Rushikesh L. Chopadec
aSchool of Forensic Science and Risk Management, Rashtriya Raksha University, Lavad, India
bSchool of Computing Science and Engineering (SCSE), Galgotias University, Delhi, Uttar Pradesh, India
cDepartment of Forensic Science, Vivekananda Global University, Jaipur, Rajasthan, India
Contents
1.Introduction 438
2.Primer on artificial intelligence 440
2.1 Artificial intelligence 440
2.2 Deep learning and deep neural networks 441
2.3 From deep learning to model training and inference 442
2.4 Popular deep learning models 442
3.Edge intelligence 444
3.1 Motivation and benefits of edge intelligence 444
4.Edge intelligence model training 449
4.1 Architectures 449
4.2 Key performance indicators 449
4.3 Enabling technologies 451
4.4 Summary of the existing systems and frameworks 457
5.Edge intelligence model interface 458
5.1 Architectures 458
5.2 Key performance indicators (KPIs) 459
5.3 Enabling technologies 460
5.4 Summary of the existing systems and frameworks 469
6.Future research directions 469
6.1 Programming and software platforms 469
6.2 Resource-friendly edge AI model design 470
6.3 Computation-aware networking techniques 471
6.4 Trade-off design with various DNN performance metrics 472
6.5 Resource management and smart services 472
6.6 Security and privacy issues 473
6.7 The EI ecosystem is a large open collaboration that focuses on incentivesand business models 473
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0134377.Conclusions 474
References 474
Further reading 480
About the authors 481
Abstract
In the current era, advancements in deep learning have seen the services and uses of
artificial intelligence (AI) flourish. From individual support and commendation structures
to video/audio monitoring, there is something for every person. With the developmentof mobile computing and the Internet of Things (IoT), billions of mobile and IoT devicesare now connected to the Internet, generating tons of data at the edge of the network.As a result of this inclination, there is a demanding need to push AI beyond its limits tothe networks edge in order to fully understand the capability of big data. Edge com-puting, a developing model that encourages computing everyday jobs as well as net-work services, which are basic to the edge of the network, has long been touted as ahopeful explanation for meeting this demand. The resulting new interdisciplinary fieldknown as edge AI or edge intelligence (EI) has been attracting an incredible amount ofattention. Although EI experimentation is still in its early stages, both the computer sys-tem and AI societies would benefit from a committed forum for exchanging recent EI
accomplishments. At present, we are conducting a detailed survey of recent EI research
events. We go over the context and enthusiasm along with inspiration for AI runningfirst at the edge of the network.
1. Introduction
We live in the age of AI that has never before seen such rapid growth.
Deep learning has been propelled forward by recent advances in algorithms,
processing power, and large amounts of datasets [1]. Computer vision,
speech
recognition, and language processing, as well as chess (for example,
AlphaGo) and robotics, have all seen major advancements owing to AI’s
remarkable reach [2]. A plethora of intelligent applications, such as intelli-
gent
personal assistants, personalized purchasing recommendations, video
monitoring, and smart home appliances, have sprung up as a result of these
advancements. These intelligent applications are well known for greatlyenhancing people’s lifestyles, increasing human work rate, and improving
social productivity. Big data have lately seen a drastic change in the knowl-
edge source from giant-scale cloud data centers to more ubiquitous enddevices, such as mobile devices and Internet of Things (IoT) devices, as asignificant driver that boosts AI development. Big data, such as online shop-
ping records, social media-related items, and business information systems,438 Anubhav Singh et al.have traditionally been created and mainly stored in large data centers. Cisco
predicts that by 2021, people, machines, and things at the network edge will
have created nearly 850 ZB [3]. Global data center traffic, on the other hand,
will
only reach 20.6 ZB by 2021. However, due to concerns about effi-
ciency, cost, and privacy, driving the AI frontier to the sting ecology thatlives at the bottom of the web is not trivial [4]. When transmitting a large
amount 
of data across a wide area network (WAN), the pecuniary cost as
well as the transmission time may be excessively high, making privacy leaksa major concern [5]. Another option is on-device analytics, which uses AI
programs 
to process IoT data directly on the device, but it could suffer from
poor performance and energy efficiency [6]. Recently, moving cloud ser-
vices
from the networks core to its edge, i.e., closer to IoT devices and data
sources, has been proposed. A helper node can be any adjacent terminaldevices capable of communicating with each other via device-to-device
(D2D) communication [7], servers connected to access points (e.g.,
W 
LANs, routers, and base stations), network gateways, or even microdata
gateways that can be accessed by neighboring devices. While edge nodes
can vary in size from a credit card-sized computer to a microdata center with
multiple server racks, the most important feature highlighted by edge comput-
ing is physical closeness to the information-generating sources. In essence, thephysical proximity of information-processing and information-generatingsources promises many advantages over traditional cloud-based computingparadigms, including low latency, energy efficiency, data protection, low
bandwidth consumption, local awareness, and context [6,8]. In fact, the com-
bi
nation of edge computing and AI has generated a new field of study known
as edge intelligence (EI) or edge AI [9,10] . Rather than relying solely on the
cl
oud, EI leverages the best of widely available edge resources to provide AI
insights. EI has gained a lot of interest from both industry and academia. For
example, EI has been included in the well-known Gartner’s advertising cycleas a new technology that could reach productivity levels in the next 5 –10years
[11]. Pilot projects have been proposed by major corporations such as Google,
M
icrosoft, Intel, and IBM to illustrate the benefits of edge computing in
paving the way for AI. This initiative has enhanced various AI applications,from real-time video analytics to machine learning [12], reasoning support
[13]to correct farming, smart homes [14], and industrial Internet of Things
(I 
IoT) [15]. Notably, studies on and practice of this new interdisciplinary
f
ield of EI are still in their infancy stages. In both industry and academia, there
is a widespread lack of a platform dedicated to reviewing, debating, andsharing the current advancements in EI. To close this gap, we perform a439 Artificial intelligence in edge devicescomprehensive and detailed assessment of the current EI research efforts in this
study. First, we will go through the history of artificial intelligence. Next, the
rationale, definition, and grading of EI will be discussed. Following that,we will categorize and discuss the evolving computer architectures andsupporting technologies for EI model formation and inference. Finally, wewill discuss about some of the open research problems and possibilities for
EI. The following is a breakdown of this chapters structure.
(1)Section 2 provides
 an outline of AI’s core ideas, focusing on deep
learning—chosen AI’s field.
(2)The purpose, definition, and rating of EI are discussed in Section 3.
(3)The architectures, enabling methodologies, systems, and frameworksfor
training EI models are discussed in Section 4 .
(4)Section 5 discusses EI model inference structures, enabling methodol-
ogies, systems, and frameworks.
(5)EI’s future directions and difficulties are discussed in Section 6.W e
believe that by conducting this poll, we will be able to evoke moreinterest, spark constructive debates, and inspire new research ideason EI.
2. Primer on artificial intelligence
In this section, we go through AI ideas, reports, and approaches,
focusing on deep learning, which is one of the most prominent branches
of AI+.
2.1 Artificial intelligence
Although artificial intelligence (AI) has received a lot of press recently, it is
not a new idea; it was originally coined in 1956. Simply put, AI is a tech-nique used for building intelligent machines that can perform tasks as well as
humans. This is an extremely broad term, and it may apply to everything
from Apples Siri to Googles AlphaGo as well as to too strong technologiesthat are yet to be developed. In order to simulate human intelligence, AIsystems must exhibit at least a few of the following behaviors: planning,
learning, thinking, problem-solving, knowledge representation, perception,
motion, and manipulation, as well as social intelligence and creativity, to alesser extent. AI has risen, fallen, and risen again throughout the course of thelast 60years. Deep learning, a technology that has achieved human-level
precision in a variety of intriguing domains, has been the driving force
behind AI’s rapid growth after the 2010s.440 Anubhav Singh et al.2.2 Deep learning and deep neural networks
Machine learning (ML) is a strong tool for AI to achieve its objectives. Many
machine learning (ML) approaches have been developed to train machines
to categorize and make predictions using data from the 000 world, includingdecision trees, K-means clustering, and Bayesian networks. One of thecurrent machine learning techniques is deep learning, which uses artificial
neural networks (ANNs) [16]. To tell the truth, deep representation of
information
has produced excellent outcomes in a range of tasks, such as
image classification and face recognition. The model is dubbed as a deep
neural network because deep learning models often utilize an ANN withseveral layers (DNN). Each layer of a DNN is made up of neurons that
can produce nonlinear outputs based on the information provided by the
neurons input. The information is received by the input layer neurons,which then pass it on to the neurons in the central layer that is also knownas the hidden layer. The weighted sums of the computer file are created by
the middle layer neurons, which are then produced using certain activation
functions, and the outputs are then sent to the output layer. The final resultsare displayed on the output layer. Because DNNs contain more complexand abstract layers than does a traditional model, they are equipped with
high-level learning abilities that allow them to make highly accurate infer-
ences on tasks. There are three in-demand DNN architectures, namely,multilayer perceptrons (MLPs), convolutional neural networks (CNNs),and recurrent neural networks (RNNs) [17]. Convolutional layers in
CNN
models, unlike fully connected layers in MLPs, extract basic charac-
teristics from input by conducting convolutional operations. CNN models,which use several convolutional filters to capture the high-level representa-tion of a computer file, are the most popular choice for computer visionapplications like photo categorization (e.g., AlexNet [18], VGGNet [19],
ResNet [20], and MobileNet [21]) and artifact acknowledgment (e.g.,
Fast
R-CNN [22], YOLO [23], and SSD [24]). RNN models, which
employ
consecutive information nourishing, are another type of DNN.
The fundamental element of an RNN is called a cell, and each cell consists
of layers. A sequence of cells allows RNN models to be processed sequen-
tially. In the task of tongue processing, RNN models are extensively usedand when it comes to tongue processing, RNN models are commonly usede.g., dialectal modeling, computational phonology, enquiry responding, and
document organization. Deep learning is the most sophisticated AI approach
as well as a resource-intensive task that is best suited to edge computing.441 Artificial intelligence in edge devicesIn the rest of this chapter, we will focus on the interaction between deep
learning and edge computing for space reasons. We believe that the concepts
provided may be applied to a variety of AI models and processes. For exam-ple, stochastic gradient descent (SGD) is the most favored training techniquefor a variety of AI/ML systems (e.g., K-means, support vector machine, andlasso regression) [25]. As a result, the SGD training optimization methods
described
in this chapter may be carried out using different AI means and
methodologies as well as the training process of other AI models.
2.3 From deep learning to model training and inference
For each neuron in a DNN layer, there is a weighting vector that is propor-
tional to the size of the layer’s computer file. Of course, the weights of a deep
learning model must be refined through a training process. Weight values aregenerally set at random during the training phase of a deep learning model.A loss function is used to calculate the mean squared error of the error rate
between the results and the actual label, and the precision of the results is
determined by calculating the mean error of the error rate between theresults and the actual label. The neural network returns the error rate usingthe backpropagation method [26,27] .
The
weights are set to accommodate both the gradient and the learning
rate. The DNN model is deduced after training. In a photo classificationtask, for example, the DNN is trained to figure out how to recognize a pic-ture by providing it with an excessive number of coaching examples, and,
then, using real-world images as inputs, inference makes fast predictions/
classifications. The training technique utilizes both the feedforward andbackpropagation procedures. It is worth mentioning that the inference issolely dependent on the feedforward process, which means that the wholeneural network is aware of the input from the 000 world, and, therefore, the
model generates the prediction.
2.4 Popular deep learning models
For a more complete understanding of deep learning and its applications, this
section provides an outline of several typical deep learning models.
2.4.1 Convolutional neural networks
In 2012, AlexNet became the first CNN team to win the ImageNet
Challenge for image classification [18]. In this picture, there are five con-
volutional
layers and three totally connected layers. AlexNet requires442 Anubhav Singh et al.61 million weights and 724 million MAC to categorize a picture with a
resolution of 227 /C2227 pixels (multiply-add computation). In order to
achieve greater accuracy, VGG-16 [19]is trained on a deeper structure of
16
layers, comprising 13 convolutional layers and 3 completely connected
layers. To categorize an image with a size of 224 /C2224, it took 138 million
weights and 15.5G MAC [28]. GoogLeNet provides an inception module
consisting
of various sized filters to improve accuracy while decreasing
DNN inference computation. GoogLeNet outperforms VGG-16 in terms
of accuracy while using just 7 million weights and 1.43G MAC to analyze apicture of the same size. The “shortcut” structure is utilized to attain
human-level accuracy with a top-five error rate of less than 5% in the
state-of-the-art endeavor, ResNet [20], to increase accuracy while decreas-
ing 
expenses. The “shortcut” module is used to address the problem of dis-
appearing gradients during the training phase, allowing a DNN model with a
deeper structure to be coached. In computer vision, CNNs are often used.
The AI system learns to automatically extract the properties of these inputsto complete a job, such as image classification, face identification, or imagesemantic segmentation, given a set of photographs or videos from across
the world.
2.4.2 Recurrent neural networks
RNNs were designed to solve the problem of time series in sequential com-puter data. An RNN’s input is made up of this and the preceding samples. Inan RNN, each neuron has its own internal memory that holds the compu-tation data from previous samples. Backpropagation through time (BPTT) is
used to train RNNs [29]. RNNs with long short-form memory (LSTM)
[30]are a form of RNNs that is more sophisticated. In LSTM, the gate rep-
resents
the fundamental unit of a neuron. Each memory cell in LSTM
includes a multiplicative forgetting gate, an entry gate, and an exit gate.These gates are used to regulate access to memory cells and prevent them
from being confused with unwanted entries. Information can be added orremoved from the memory cell through the door. The information thatmay be stored on a memory cell is controlled by several neural networks
called gates. The forget gate may identify whether or not knowledge is
remembered throughout training. Since it is often used to handle data witha variable input length, an RNN is extensively used in natural languageprocessing. Language modeling, word embedding, and computational lin-
guistics are some of the AI approaches that might be utilized to build a system
that understands languages that people speak.443 Artificial intelligence in edge devices2.4.3 Generative adversarial networks
GANs (generative adversarial networks) [31]are made up of two main pro-
cesses,
the generator and discriminator networks. After cramming the infor-
mation dissemination of a training dataset with actual data, the generator
takes the responsibility of creating new data. The discriminator is responsiblefor distinguishing the 000 data from the bogus generator data. Imaging,
image transformation, image synthesis, image super-resolution, and other
applications often use GANs.
2.4.4 Deep reinforcement learning
Deep reinforcement learning (DRL) is made up of DNNs with RL. DRL’ssixth goal is to develop a genius agent that can utilize effective techniques to
maximize the advantages of long-term goals while taking up activities that
are under their power. DRL is frequently used to solve scheduling difficul-ties such as game selection concerns, video transmission rate selection, and soon. In the DRL method, the DNN is in charge of representing a large num-
ber of states and estimating the action values to evaluate the quality of the
action within the provided states. The RL is responsible for finding the bestpolicy of action toward states in the environment, whereas the DNN isresponsible for representing a large number of states and approximate action
values to assess the quality of the measures within these states. The reward
might be a function that indicates the difference between a predefinedrequirement and, as a consequence, action performance. The DRL model’sagent may be utilized for a variety of activities, including gaming [32], owing
to
continual learning.
3. Edge intelligence
EI is the result of the union of edge computing with AI. The rationale,
advantages, and definition of EI are discussed in this section.
3.1 Motivation and benefits of edge intelligence
Because AI and edge computing have a lot in common, it makes sense to
merge the two. Edge computing, in particular, aims to coordinate a large
number of cooperating edge devices and servers in order to analyze the pro-duced data in close proximity, whereas AI aims to imitate genius humanbehavior in devices/machines through data learning. Putting AI into the
mix benefits each other in the following ways, in addition to enjoying
the benefits of edge computing (low latency and reduced bandwidth use).444 Anubhav Singh et al.AI is necessary to fully realize the potential of data created at the networks edge :
As the number and type of mobile and IoT devices increase, significant
amounts of multimodal data (for example, audios, images, and videos) fromthe physical environment are constantly being recognized on the deviceside. Due to its ability to quickly evaluate large amounts of data and obtainhigh-quality decision-making information, AI is functionally essential in this
environment. As proven by population distribution, deep learning, the most
widely used AI technique, has the ability to automatically detect patterns andanomalies in the data captured by the edge device. Traffic flow, humidity,temperature, pressure, and air quality are factors that must be taken into
account. The insights acquired from sensed data are integrated into
real-time predictive decision-making (e.g., public transportation planning,control, and driver alert) in response to quickly changing surroundingsand increasing operational efficiency. Gartner [33] predicted that by
2022,
more than 80% of business IoT projects would use AI, up from
10% at the time of this study.
Edge computing is ready-to-prosper AI with more data and application scenarios :
It is well known that the four factors that have contributed to the current
surge in deep learning are algorithms, hardware, data, and application sce-
narios. Although the influence of algorithms and technologies on theadvancement of deep learning is obvious, the importance of data and appli-cation situations has been largely neglected. To improve the performance ofa deep learning system, the most commonly used method is to modify the
DNN by adding additional layers of neurons. As a result, we seek to learn
more about the parameters of the DNN, and the amount of data necessaryfor training will rise. This illustrates the importance of data in the case of AI.After recognizing the value of information, the next issue is determining
where the information comes from. Traditionally, data have been created
and stored mostly in large-scale data centers. Despite this, the tendency iscurrently reversing due to the rapid development of IoT. According toCisco’s research [3], significant IoT data will be created at the edge in the
near
future. If AI algorithms are used to analyze these data in the cloud data
center, it will require a lot of bandwidth and put a lot of strain on the clouddata center. To solve these issues, edge computing is offered as a way toaccomplish low-latency processing by sinking computing capacity fromthe cloud data center to the edge side, i.e., the data creation source, thus
allowing for high-performance AI processing. Although edge computing
and AI complement one another in terms of technology, their applicationand adoption are intertwined.445 Artificial intelligence in edge devicesAI democratization requires edge computing as a key infrastructure : Many digital
goods or services in our daily lives, such as online shopping, service sugges-
tions, video surveillance, smart home devices, etc., have achieved outstand-ing success owing to artificial intelligence technology. Self-driving cars,intelligent finance, disease diagnosis, and drug development are just a fewof the emerging creative areas where AI is the driving force. Beyond the
examples provided above, AI democratization or ubiquitous AI might be
utilized to enable a broader range of applications and push the limits of whatis possible [34]. AI has been declared as “for each person and each organi-
zation
everywhere” by major IT companies, with the objective of “creating
AI for each person and each organization everywhere.” For the time being,AI should come “closer” to people, data, and end devices. In this regard,edge computing is definitely superior than cloud computing. Because ofthese advantages, edge computing is a natural facilitator for ubiquitous AI.
Edge computing is frequently used to popularize AI applications : In the cloud
computing industry, throughout the early development of edge computing,there has always been an emphasis on which high-demand applicationscould take edge computing to the next level that cloud computing could
not and what killer applications for edge computing are. Microsoft has been
investigating what sort of data should be transferred from the cloud to thepremises since 2009 to eliminate any concerns [35]. Voice command recog-
nition,
augmented reality/virtual reality, and interactive cloud gaming are
just a few examples of [36]real-time video analytics. On the other hand,
real-time
video analytics is predicted to be a game changer for edge com-
puting [12,37,38] . Real-time video analytics is a new technique based on
computer
vision that captures high-definition footage from security cameras
in real time and analyzes it with high processing speed, high bandwidth, high
privacy, and low latency. The only viable option that can meet these strin-
gent requirements is state-of-the-art computing. In light of the aforemen-tioned rise of edge computing, new AI applications coming fromindustries such as IIoT, intelligent robotics, smart cities, and smart homes
will likely play a crucial role in popularizing edge computing. This could
be because many mobile and IoT-related AI applications fall into the cate-gory of practical applications that are energy- and computation-intensive,sensitive to data protection and lag, and thus are, of course, compatible withedge computing. Due to its attractiveness and need of running AI applica-
tions on the Internet, edge AI has recently garnered a lot of attention.
A report titled A Berkeley View on Systems Challenges for AI was published
in December 2017 [39]. According to a study published by the446 Anubhav Singh et al.University of California at Berkeley, the AI system at the edge of the cloud is
seen as a critical research path to achieve the goal of business-critical and
tailored artificial intelligence. In August 2018, the Gartner Hype Cycleincluded edge AI for the first time [40]. According to Gartner, edge AI is
still 
in its innovation activation phase and will reach a productivity plateau
in the next 5 –10years. Throughout the industry, many pilot programs aimed
toward edge AI have been distributed. Traditional cloud providers likeGoogle, Amazon, and Microsoft have built service platforms to deliver intel-ligence to the sting by allowing end devices to conduct machine learninginferences using pretrained models locally on the sting AI service platform.
Several high-end processors specialized for running ML models are com-
mercially available, as demonstrated by Googles Edge TPU, IntelsNervana NNP, and Huaweis Ascend 910 and Ascend 310. B. Scope andRating of Edge Intelligence. Although the term “edge AI” or “EI” is rela-
tively new, research and activity in this field have long existed. As mentioned
above, Microsoft built an edge-based prototype in 2009 to demonstrate thebenefits of edge computing by supporting mobile voice command recogni-tion, an artificial intelligence application. Despite the fact that this is the first
survey, there is currently no acknowledged description for EI. Most admin-
istrations [41,42] and journalists [43]consult with EI. At this point, the stan-
dard
has been set for operating AI algorithms directly on a device, generating
data (sensor data or signals) on the device. Although this is the most commonapproach to EI in the world today (e.g., using high-end AI processors), it
is important to note that this definition severely limits the scope of the EI
solution. Consecutive computationally intensive processes, such as DNNmodels, are resource-intensive and require the use of high-end processorsin the device. Such a strict requirement not only increases the cost of EI
but also renders legacy end devices with poor computing capabilities incom-
patible and unfriendly. We propose in this chapter that EI’s scope should notbe limited to AI models that operate just-on-edge servers or devices. In fact,a dozen recent studies have found that using edge –cloud synergy to execute
DNN models may reduce both end-to-end latency and power consumptioncompared to running locally. We believe that a collaborative hierarchy likethis should be integrated into the design of effective EI systems because ofthese benefits. Furthermore, the current EI concepts are mostly focused onthe inference phase (i.e., executing the AI model), assuming that the AI
model is learned in high-performance cloud data centers, where the training
phase uses significantly more resources than does the inference phase.However, because of concerns about data privacy, a large amount of training447 Artificial intelligence in edge devicesdata must be transferred from devices or edges to the cloud, resulting in pro-
hibitive communication costs. Instead, it is believed that EI would be the
paradigm that makes full use of the available data and resources in thehierarchy of end points, edge nodes, and cloud data centers to improvethe efficiency of training and inference of a DNN model. This suggests thatEI does not require that the DNN model be fully trained or derived at the
time but that data offload can serve as a mechanism to coordinate cloud edge
devices. Based on the number and length of information dumping pathways,we categorize EI into six types. The numerous EI levels and their meaningsare shown below.
1.
Cloud intelligence : The DNN model is entirely trained and inferred using
cloud computing.
2.Cloud –edge coinference and cloud training : The DNN model is trained but
inferred through edge –cloud collaboration. Data are partially offloaded
to the cloud as part of the edge –cloud cooperation.
3.Level 2: Cloud training and in-edge coinference: The DNN model is
trained in the cloud, but it is inferred on the edge. Model inference atthe network’s edge is referred to as “in-edge,” and can be done in whole
or in part by downloading data to nearby nodes or devices (via D2D
communication).
4.Level 3: Cloud training and on-device inference: The DNN model istrained in the cloud, but the DNN model on-device is inferred in ahighly localized manner. The phrase “on-device” refers to the fact that
no data will be sent off of the device.
5.Level 4: Cloud –edge cotraining and inference: The DNN model is
trained and inferred via the edge –cloud collaboration.
6.Level 5: All-in-edge: The DNN model is completely trained and
inferred on the in-edge.
7.Level 6: On-device training and inference: The DNN model is
completely trained and inferred on the device.
As EI rises, the number and duration of information dumping routes
decrease. As a result, information offloading transmission latency is reduced,
data privacy is enhanced, and WAN bandwidth costs are reduced. However,this leads to an increase in processing time and energy consumption. Thisparadox suggests that there is no universal “best-level” EI; rather, the“best-level” EI is not independent of application and should be determined
by taking into account a variety of variables such as latency, energy effi-
ciency, privacy, and WAN bandwidth cost. We will look at the currentempowerment methods and solutions for different levels of EI in the follow-ing sections.448 Anubhav Singh et al.4. Edge intelligence model training
As a result of the growth of mobile and IoT devices, data are being
created at the networks edge, which is important for AI model training.
The architectures, essential performance measurements, enabling method-ologies, and existing systems and frameworks for distributed DNN trainingat the sting are all covered in this section.
4.1 Architectures
For DNN training distributed at the edge, there are three popular designs:
centralized, decentralized, and hybrid (cloud –edge device). End devices,
which are data sources, are represented by cell phones, cars, and securitycameras, whereas the cloud refers to the central data center. We utilize basestations for the edge server because of the methodology.
(1)
Centralized: In a centralized DNN training, the DNN model is trained
within the cloud data center. Training data are generated and collectedfrom a variety of distributed end devices, including mobile phones, cars,and security cameras. When the data are received, they will be used bythe cloud data center to perform DNN training. The system that sup-
ports the centralized design may be classified as cloud intelligence level
1 or level 2 or a level depending on the particular inference techniqueused by the system.
(2)Decentralized: In the decentralized mode, each computer node trains
its own DNN model locally with local data, keeping private informa-
tion private. The nodes in the network will communicate with eachother to exchange local model updates to preserve the global DNNmodel by combining local training enhancements.
(3)Hybrid: This style combines the centralized and decentralizedapproaches. The sting servers, as the system’s hub, could train theDNN model either decentralized with each other or centralized withthe cloud data center. Therefore, the hybrid architecture covers levels
4 and 5. Hybrid architecture is also known as cloud edge device training
because of the responsibilities involved.
4.2 Key performance indicators
There are six important performance metrics to consider when evaluating adistributed training approach.449 Artificial intelligence in edge devices(1)Training loss: The DNN training technique fundamentally solves an
optimization problem by reducing training loss. Since it records the dif-
ference between the learned (for example, expected) value and thelabeled data, the training loss shows how well the trained DNN modelmatches the training data. As a result, the training loss is expected to bemodest. Drill forfeiture is mostly due to drill samples and drill methods
causing anguish.
(2)Convergence is the decentralized technique’s particular convergence
indicator. If the distributed training processes converge in a consensus,the result of the strategy formation, then this indicates that a
decentralized method seems to work. “Convergence” is a term that
explains whether a decentralized strategy converges to such a consensusand how quickly it does so. In the decentralized training mode, theconvergence value is defined by how the gradient is synchronized
and updated.
(3)When training a DNN model with data from a large number of end
devices, the information or intermediate data should be sent out ofthe top devices. In this circumstance, catering to privacy concerns is
unavoidable. To safeguard privacy, less privacy-sensitive data are likely
to be sent out of end devices. Whether data are sent to the sting affectswhether or not privacy is safeguarded.
(4)Communication costs: Training the DNN model is data-intensivebecause data or intermediate data must be sent between the nodes.
On the surface, this communication overhead looks to increase training
latency, energy, and bandwidth use. The transmission overhead is con-trolled by the size of the original input file, the mode of transmission,and the bandwidth available.
(5)Latency is one of the most significant performance variables for distrib-uted DNN model training since it influences when the trained model isavailable for use. The latency of a distributed training process is made upof computation delay and communication latency. The potential of sting
nodes has a significant impact on calculation time. Communication
latency may be affected by the size of the raw or intermediate data as wellas the network connection’s capacity.
(6)By training the DNN model in a decentralized manner, both comput-ing and communication activities consume a lot of energy. However,
most end devices are power-limited. Consequently, energy efficiency is
highly desirable in training DNN models. Energy efficiency is stronglyinfluenced by the size of the target training model and the device450 Anubhav Singh et al.resources used. It should be noted that training loss and convergence are
standard performance measures; thus, some DNN training literature
might not explicitly claim them.
4.3 Enabling technologies
When training the EI model, we consider the underlying technologies toimprove one or more of the abovementioned performance metrics.
Federated learning : Federated learning is dedicated to optimizing privacy
issues within the top performance metrics listed above. It is a promising new
technique for maintaining privacy while using input from multiple clients to
train the DNN model. In federated learning, rather than collecting andsending data to a centralized data center for training [44], clients (e.g., mobile
devices)
are left with the information while the server trains a shared model
by adding locally calculated updates. Optimization and communication arethe most difficult aspects of federated learning. The present task is toimprove the gradient of a shared model through distributed gradient updateson mobile devices for the optimization problem. SGD is used in federated
learning on this topic. SGD is a simple but widely used gradient descent
method that updates the gradient in extremely small subsets (mini batches)of the dataset. Shmatikov and Shokri [45]are developing an SGD (SSGD)
protocol
that allows clients to train their own datasets while selectively shar-
ing small sections of their model’s important parameters with the centralized
aggregator. Due to the ease with which SGD may be parallelized and run
asynchronously, SSGD addresses both privacy and training loss. When com-pared to training only on their own inputs, the loss of training will be min-imized by sharing the models across clients while maintaining client privacy.
One shortcoming [45]is that unbalanced and non-dependent identically dis-
tributed
data (non-IID) are not taken into account. As an extension,
McMahan et al. [44]advocate a decentralized approach known as federated
learning
and present a FedAvg method for associative learning with an aver-
age of iterative models supported by a DNN. Iterative version averaging
means that the customers replace the version regionally with the use of aone-step SGD, and the server then averages the consequent fashions theuse of weights. Because dispersed data might originate from a variety ofsources, the optimization in the study by McMahan et al. [44]highlights
the
features of unbalanced and non-IID. The issue of communication effi-
ciency is posed by the unstable and unpredictable network in the case of thecommunication problem. In federated learning, each client transmits to the451 Artificial intelligence in edge devicesserver a whole model or a full model update in a standard round. Because of
the unstable network connections, this phase will most likely be the bottle-
neck for large models. McMahan et al. [44]propose extending the calcula-
tion
of client-side local updates. It is, however, impracticable when the
customers’ computation resources are severely limited. In answer to thisproblem, Kone /C20cny ´ et al. [46]offer two novel update techniques, structured
update 
and sketched update, to reduce communication costs. In an
extremely structured update, the model learns an update directly from a lim-ited space that is parameterized with a reduced number of variables, such asB. Random or low-rank masks. When using a sketched update, the model
first learns a full model update and then compresses it with a mixture of
quantization, random rotations, and subsampling before sending it to theserver. Despite the fact that the federated learning method makes use of afresh new decentralized deep learning architecture, it relies on a central
server to aggregate local updates. Lalitha et al. consider the case of teaching
a DNN model across a totally decentralized network, that is, a networkwithout a central server [47]. They present a Bayesian-distributed method
in
which each device changes its belief by merging data from its neighbors
in a single hop to form a model best suited for network observations.Furthermore, using the newly developed blockchain technology, Kimet al. [48] offer blockchain federated learning (BlockFL), which uses
blockchain
to share and verify device model updates. BlockFL can also be
used in a completely decentralized network, where the machine learning
model can be trained without central coordination, although some devices
do not have their own training dataset.
Aggregation rate control : This approach focuses on reducing communica-
tion overhead while training a DNN model. A popular approach (for exam-
ple, federated learning) to train deep learning models in an edge computing
context is to first teach locally distributed models and then centrally addchanges. The frequency of update addition has a major impact on the com-munication overhead in this situation. As a result, the aggregation process,
including aggregation content and frequency, should be closely monitored.
Hsieh et al., based on the foregoing knowledge [49] for geo-distributed
DNN
model training, build the Gaia system as well as the approximate syn-
chronous parallel (ASP) model. Gaia’s core concept is to separate commu-nication within a data center from communication between data centers and
to allow alternative models of communication and consistency for each. To
do this, the ASP model was created to dynamically eliminate unimportantcommunication between data centers, with the aggregation frequency452 Anubhav Singh et al.governed by a significance threshold. Gaia, on the other hand, concentrates
on capacity-unlimited geo-distributed data centers, rendering it inapplicable
to edge computing nodes with restricted capacity Wang et al. [50]present a
sway
method that calculates the most effective trade-off between the local
update and aggregation of global parameters within a given resource budget,taking into account the capacity constraint of edge nodes. The technique is
based on distributed gradient descent convergence analysis and can be used
for federated learning in edge computing with proven convergence. Nishioand Yonetani [51]explore the problem of selecting a resource-constrained
client
to perform federated learning in a high-capacity, resource-constrained
computing environment. Particularly, FedCS is an update aggregation pro-tocol that allows the centralized server to collect as many client updates asfeasible in order to improve ML model performance.
Gradient compression : Another logical technique for compressing the
model update is gradient compression, which reduces the communicationcosts imposed by decentralized training (i.e., gradient information).Gradient quantification and gradient preservation have been proposed assolutions to this problem. Gradient quantization achieves, in particular, lossy
compression of gradient vectors by quantizing each of their components to a
low precision finite bit value. Sending only a portion of the gradient vectorssaves the gradient, thus minimizing communication costs [52] Gradient
compressibility
is demonstrated from the fact that 99.9% of the distributed
SGD (DSGD) swap is redundant. Lin et al. proposed deep gradient compres-
sion (DGC), which compresses gradients of 270 –600 for various CNNs and
RNNs. DGC uses four approaches to maintain accuracy during compres-
sion: momentum correction, local gradient cutting, momentum factor mas-king, and warm-up training. Motivated by Lin et al. [52], Tao and Li [53]
suggested the advantages of SGD (ESGD), a series of dispersed methods thatensure
both convergence and practical performance. ESGD comprises two
techniques to improve gradient-based first-order optimization of stochasticobjective functions in edge computing: (1) identifying which gradient coor-
dinates are relevant and communicating only these and (2) constructing
accumulated residual momentum to track obsolete residual gradient coordi-nates to avoid poor convergence rates caused by infrequent updates. Stichet al. [54]provided a brief convergence study of the scattered SGD, where
the
SGD is assessed using k-sparsification or compression (e.g., top-k or
random-k). When equipped with error correction, this technique convergesat the same rate as vanilla SGD according to the study (keeping track of accu-mulated errors in memory). To put it in another way, communication is453 Artificial intelligence in edge devicesdecreased by a factor of the problem’s complexity (often much more) while
staying convergent at the same pace and reducing the transmission band-
width by quantizing the gradients to values with low precision. Duringthis time, Tang et al. [55]created a framework for decentralized compression
training 
and introduced two alternative methods for compression by extrap-
olation and differential compression. Examination of the two methods
shows that they both converge to O (1/ nT), where nis the number of
customers and Tis the number of iterations. This is the convergence rate
for precision centralized training. Amiri and Gunduz [56]used a remote
parameter
server to implement DSGD at the wireless edge and construct
DSGD in both digital and analog formats. With each iteration of the
DSGD algorithm, the digital DSGD (DDSGD) assumes that customersare concerned about the limit of the multiple access channel (MAC)’s capac-ity range, and, to communicate its estimations of the gradient within the
permitted bits budget power, it employs gradient quantization and error
accumulation distribution. DSGD has an analog format (A-DSGD). The cli-ents first use error accumulation to sparsify their gradient estimations; then,the available channel bandwidth dictates that they be projected to a lower
dimensional space. Without using any digital coding, these projections
are sent straight across to the MAC.
DNN splitting: DNN splitting is used to protect privacy. By providing
partially processed data instead of raw data, DNN splitting protects the pri-vacy of users. It takes place between the top devices and the edge server to
enable edge-based data protection and preservation training of DNN
models. Mao et al. [57] used a differentially private technique to reduce
the
price of mobile devices by partitioning a DNN after the first con-
volutional layer. The proof in their study [57] ensures that training jobs
may
be outsourced to untrusted edge servers via a differentially private
method on activation. Wang et al. [58]looked at the topic from the perspec-
tive
of mobile devices and cloud data centers. To take advantage of the
computing power of cloud data centers and avoid privacy concerns,
Wang et al. [58] Arden (private inference framework that supports deep
neural
networks) is a framework that divides the DNN model using a light-
weight privacy approach. Arden achieves data protection through random
data cancellation and random noise addition. Taking into account the det-rimental impact of personal disturbance on the first data, Wang et al. [58]
enhanced the cloud-side network’s resilience to disturbed data by utilizinga
noisy training technique. With regard to the privacy problem, Osia et al.
[59]presented a hybrid user-cloud architecture that uses a private feature454 Anubhav Singh et al.extractor as a core component and breaks down large and complicated depth
models for collaborative analysis and privacy protection. The feature extrac-
tion module in this framework is built correctly to dump the private featurethat is restricted to retain the initial information while rejecting all othersensitive statistics. To make unexpected sensitive measurements, three dis-tinct approaches are used: dimensionality reduction, noise addition, and
Siamese fine-tuning. It is astonishing to observe how effectively this system
handles a DNN’s huge computation when employing DNN splitting forprivacy preservation. Taking advantage of the fact that edge computinggenerally entails handling DNN processing on a large number of devices,
parallelization methods are commonly used. Data parallelism and model par-
allelism are two types of parallelisms used in DNN training. Data parallelismmight result in significant communication overhead, whereas model paral-lelism frequently results in significant underutilization of computational
resources. To deal with these issues, Harlap et al. [60]Pipeline parallelism
is
a variation on model parallelism in which many micro-batches are injected
directly into the system to ensure efficient and concurrent computing
resource utilization. Pipedream is a technology developed by Harlap et al.
[60]that allows pipeline training and automatically identifies how a partic-
ular
models work is systematically divided among the available computa-
tional nodes based on pipe parallelism. Pipedream demonstrates thebenefits of minimizing communication overhead and effectively utilizingcomputational.
Knowledge transfer learning : The DNN splitting approach is strongly linked
to knowledge transfer learning or transfer learning for short. To reduce theenergy costs of training the DNN model on shaking devices, we first train abasic network (teacher network) on a basic dataset and then use the newly
learned functions, i.e., transfer learning. To be trained on a target dataset,
they should be moved to the second target network (student network).This procedure will attempt to determine whether the characteristics areuniversal (i.e., applicable to both base and target tasks) rather than task-
specific. An interaction from consensus to particularity is engaged with
the change. The transfer learning approach seems promising for erraticdevice learning due to extremely low resource requirements, but an inten-sive study of its efficacy is lacking. To fill this gap, Sharma et al. [61]and
Chen
et al. [62]carried out significant research on the performance of trans-
fer
learning (in terms of accuracy and speed of convergence), taking into
account the different topologies of student networks and the differentstrategies for transferring information from the teacher to the student.455 Artificial intelligence in edge devicesTechniques fail miserably, and a few even have a detrimental influence on
performance. The flat layers of a previously trained DNN in a dataset are
treated in the transfer learning approach as an overall component extractorthat can be applied to various objective assignments or informational collec-tions. Movement in learning is utilized in various exploration projects andinspires the improvement of different systems in light of this property. Osia
et al. [59]used this technique to figure out how to decide the level of over-
simplification
and distinction of an individual property, as described in
Section IVC4. Arden, as suggested in Wang et al. [58], shares a DNN
between
the mobile device and thus the cloud data center, with the flat sec-
tions of the DNN on the mobile device side transforming the information.
As mentioned in Wang et al. [58], the planning of the DNN division in
Arden
is inspired by transfer learning.
Gossip training : Gossip training, which is based on randomized gossip
algorithms and aims to reduce training latency, might be a new decentralizedtraining technique. Gossip averaging [63]is an early study on random gossip
algorithms 
that aims to quickly come to an agreement with hubs by sharing
the distributed data. Because they do not require centralized nodes or vari-
ables, gossip-distributed algorithms offer full asynchronization and ultimate
decentralization. Gossip SGD (GoSGD) [64] is a proposed method for
decentralized
and asynchronous training of DNN models. GoSGD is a sys-
tem that maintains a collection of autonomous hubs, every one of whichcontains a DNN model and is rehashed in two stages: angle update and
mix update. In the gradient update phase, each node internally changes
its hosted DNN model and, in the combined update step, shares its knowl-edge with another randomly selected node. The processes are repeated until allof the DNNs have reached an agreement. The goal of GoSGD is to solve the
problem of convolutional network training speeding up. Instead, gossiping
SGD [65], a gossip-based algorithm, is designed to preserve the benefits of
bo
th synchronous and asynchronous SGD techniques. Gossiping SGD
achieves asynchronous training by replacing the all-diminish aggregate activity
of coordinated preparing with a tattle total technique [64,65] When gossip
al
gorithms are applied to SGD updates, none of them experience a significant
performance degradation. Daily et al. [66] show that enormous-scope
ta
ttle-based paltry calculations lead to correspondence irregularity, helpless
combination, and high correspondence overhead when implemented in largesystems. To address these difficulties, Daily et al. [66] offer GossipGraD,
an 
SGD calculation dependent on the tattle correspondence convention,456 Anubhav Singh et al.which is valuable for scaling profound learning calculations for enormous-
scope organizations. GossipGraD diminishes the general correspondence
intricacy from (log (p)) to 0 (1) and takes diffusion into account so that com-puting nodes indirectly share their updates (gradients) after each log (p) step. Italso takes into account the rotation of communication partners to supportdirect gradient diffusion and asynchronous distributed sample shuffling during
the forward phase to avoid overfitting.
4.4 Summary of the existing systems and frameworks
The systems and methodologies of distributed EI model training on the
Internet are summarized in this section, including EI levels, aims, technol-ogy used, and efficacy of the aforementioned current systems and frame-works. The problem of data privacy is a major difficulty for distributed EI
model training in general organizations. For users, they are sensitive to their
own private data and do not allow any disclosure of private information.For businesses, they should consider data protection regulations to avoidsubpoenas and extrajudicial surveillance. Therefore, the emergence of
distributed training systems should carefully consider the protection of
privacy. Systems that address privacy issues in a decentralized architectureare of course data protection-friendly, so, systems based on a decentralizedarchitecture, such as BlockFL and GossipGraD, generally protect privacybetter. On the other hand, a centralized architecture incorporates a unified
information assortment activity, and, along these lines, the crossover design
requires an information move activity. FedAvg, BlockFL, GossipGraD, andother such systems all consider privacy issues. The centralized design, on theother hand, necessitates a centralized data collecting activity, whereas the
hybrid architecture necessitates a data transfer operation. As a result, systems
based on these two designs would go to great lengths to ensure privacy.Compared to DNN training in a cloud-based framework, DNN trainingin an edge-based framework is more about protecting user privacy and train-
ing an already accessible deep learning model. Furthermore, edge-based
training is highly desired in some circumstances, such as military and catas-trophe applications, when connection to the cloud center is difficult. On theother hand, a cloud data center can collect a larger amount of data and forman artificial intelligence model with stronger resources. Therefore, cloud
intelligence has the advantage of being able to create a much larger and more
accurate AI model.457 Artificial intelligence in edge devices5. Edge intelligence model interface
The rapid implementation of model inference at the edge will be
crucial to enable the delivery of high-quality EI services after distributed
training of deep learning models. This section covers the architectures,key performance indicators, enabling approaches, and current systems andframeworks for DNN model inference in sting.
5.1 Architectures
We describe different edge-centered inference architectures and classifythem into four DNN model inference modes, namely, edge-based,
device-based, edge –device, and edge –cloud. The following is how each
mode’s core process is described.
Edge-based mode : Device A is in the edge-based mode, which means that
the device will receive the input file and subsequently send it to the sting
server. The prediction results are sent to the device when the DNN model
inference is completed on the sting server. Because the DNN model resideson the sting server in this inference mode, it is simple to build the applicationon many mobile platforms. The biggest drawback is that the induction
depends on the capacity of the network between the gadget and, subse-
quently, the edge server.
Device-based mode : Device B is in the device-based mode. The DNN
model is downloaded from the sting server to the mobile device, which thenperforms model inference locally. The versatile gadget does not interface
with the sting server amid the deduction preparation. Hence, the deduction
is solid but requires a part of assets on the versatile gadget, e.g., CPU, GPU,and RAM.
Edge–device mode : Device C is in the edge –device mode. The device
divides the DNN model in the edge –device mode into different sections
according to the parameters of the current system environment, such as net-work bandwidth, device resources, and edge server workload. The gadget atthat point runs the DNN model up to a certain layer some time recently
sending the middle information to the edge server. The taking after layers
will be executed by the sting server, which can at that point provide theexpectation comes about to the gadget. The edge –device mode is more
dependable and versatile than are the edge-based and device-based modes.
Since the convolution layers at the starting of a DNN demonstrated are
computationally serious, it can too require a part of assets on the portabledevice.458 Anubhav Singh et al.Edge–cloud mode : Device D is in the edge –cloud mode. It is similar to the
edge–device mode and is appropriate when the device’s resources are lim-
ited. The device is responsible for computer file gathering in this mode, andthe DNN model is implemented via edge –cloud synergy. The quality of
the network connection has a significant impact on the model’s perfor-mance. It is worth noting that the four edge-centric inference modes listed
above can be used in the same system to perform complicated AI model
inference tasks (z, edge nodes, and clouds).
5.2 Key performance indicators (KPIs)
The following six metrics are used to describe the QoS of the EI modelinference.(1)
Latency: The time spent all through the derivation interaction, includ-
ing pre-handling, model surmising, information moves, and post-
handling, is called idleness. Some ongoing savvy versatile applications(like AR/VR portable games and keen mechanical technology) havesevere cutoff times, such as an inactivity of 100ms. The assets of anxious
gadgets, the technique for information move, and the gratitude to exe-
cute the DNN model are largely factors that impact inactivity.
(2)Accuracy: The proportion between the quantity and the completenumber of information tests, which procure the right forecasts fromdeduction, mirrors the exhibition of DNN models. Ultrahigh accuracy
on the DNN model deduction for a couple of portable applications that
require an undeniable degree of unwavering quality, like self-drivingvehicles and facial confirmation, is sought. Because of the restrictedassets of sting gadgets, certain information tests might be avoided in a
video investigation application with a high rate of care, influencing a
lower precision.
(3)In contrast to the edge waiter, and in this way the cloud server farm, thetip gadgets are for the most part battery-restricted with regard to exe-
cuting a DNN model. DNN model deduction burns through a ton of
energy due to the calculation and correspondence overheads. Energyeffectiveness is basic for an EI application, and it is affected by theDNN model’s measurements just as the assets of jittery gadgets.
(4)IoT and cell phones create a huge volume of information that can be
security-sensitive. During the induction period of an EI application
model, the protection and security of the information near the datasource should be ensured. The manner in which the first informationis handled decides the degree of security insurance.459 Artificial intelligence in edge devices(5)Aside from the gadget-based strategy, correspondence overhead essen-
tially
affects the induction of different modalities. In an EI application,
the overhead of the DNN model surmising should be decreased, par-
ticularly the exorbitant WAN data transfer capacity use for the cloud.The correspondence overhead here is for the most part controlled bythe DNN derivation mode just as the accessible data transmission.
(6)Memory footprint lessens the memory impression of a DNN modelsurmising
on cell phones. From one viewpoint, a high-accuracy
DNN model generally contains an enormous number of boundariesand requires a great deal of assets on cell phones. Then again, dissimilar
to elite discrete GPUs in cloud server farms, portable GPUs on cell
phones do not have committed high-transmission capacity memory[67]. Moreover, versatile computer chips and GPUs are often in a con-
test for shared and restricted memory transmission capacities. The
memory impression might be an unimportant sign for improving
DNN deduction on the sting side. The underlying DNN model’s size,just as the technique for stacking the huge DNN boundaries, leaves anenormous memory impression.
5.3 Enabling technologies
This part checks out the basic advances to work on at least one of the abovebasic presentation measurements for EI model surmising.
Model compression : To facilitate the strain between asset-hungry DNNs
and asset-helpless end gadgets, DNN pressure is generally used to limitmodel intricacy and asset necessities and to empower neighborhood surmis-ing on the gadget, thus lessening reaction inertness and security concerns.
The model pressure strategy, as such, advances the four markers referenced
above: idleness, energy, protection, and memory impression. Weight prun-ing, information quantization, and smaller compositional plan are among theDNN pressure approaches recommended. Weight pruning is, by a wide
margin, the most normally utilized model pressure approach. This method-
ology eliminates pointless loads (that is, joints between neurons) from a pre-pared DNN. To do this, it first scores the neurons in the DNN as per theircommitment and afterward eliminates the lower-positioned neurons todecrease the size of the model. Since eliminating neurons diminishes the
accuracy of the DNN, the fundamental trouble is discovering a procedure
to decrease the organization while retaining the accuracy. A pilot study inenormous-scope DNNs has been directed toward current huge-scope460 Anubhav Singh et al.DNNs. Han et al. [68]utilized an extent-based weight pruning method to
deal
with this issue. The essential idea driving this methodology is to elim-
inate smaller loads with sizes under a specific limit (e.g., 0.001) and afterward
refine the model to reestablish precision. This method can diminish thequantity of AlexNet and VGG-16 loads by 9 and 13, respectively, withoutlosing ImageNet accuracy. The subsequent works profound pressure [69],
The
pressure proportion is additionally moved to 35– 49 by consolidating
the advantages of pruning, weight sharing, and Huffman coding to packDNNs. Notwithstanding this, the earlier craft of size-based weight manag-ing may not be straightforwardly pertinent to control-restricted gadgets, as
the real information shows that lessening the quantity of loads does not gen-
erally bring about critical energy reserve funds [70]. This is on the grounds
that
the energy of the folding layers incorporates the whole energy cost in
the DNN like AlexNet; however, the sum inside the completely associated
layers makes up the vast majority of the absolute number of loads in the
DNN. This suggests that the measure of loads may not be a decent pointerof force and that the weight trim for the end gadgets should be unequivocallyenergy-cognizant. MIT has fostered a web-based DNN energy assessment
apparatus ( https:/ /energyestimation.mit.edu/ )
 to empower speedy and sim-
ple DNN energy assessment. The energy to move data from various layers of
the capacity chain of command, the quantity of Macintoshes, and the scar-city of information in the granularity of the DNN layers are all illustratedusing this fine-grained instrument. An energy-mindful pruning approach
known as EAP was utilized to help this energy-assessing apparatus [71].
Maybe of utilizing the 32-digit skimming direct arrangement toward depict
layer data sources, loads, or both, this methodology utilizes a more mini-mized construction. Information quantization expands generally speaking
registering and energy proficiency by encoding a number with less pieces,
which diminishes memory impression and velocities up handling. Most pastquantization techniques tuned the piece width just for a decent number sortimpromptu, which may bring about a helpless result. Late exertion was per-
formed to resolve this issue [72]. To decide the best piece width for the sanc-
tioned configuration dependent on the IEEE 754 norm, the analysts
analyzed the ideal number of portrayals for layer granularity. Because ofthe combinatorial multiplication of conceivable number structures, this issueis hard to address. Thus, the creators created a versatile programming inter-
face known as number dynamic information type (ADT). It permits clients
to characterize the data to be quantized in a layer as a number sort (e.g.,sources of information, loads, or both). ADT typifies the inward portrayal461 Artificial intelligence in edge devicesof variety along these lines, hence confining the significance of building an
effective DNN from the significance of expanding the measure of portrayal
at the touch level. While most current endeavors use a solitary pressureapproach, this will not be sufficient to meet the assorted necessities andlimitations enforced by different IoT gadgets as far as exactness, inactivity,stockpiling, and energy is concerned. Late exploration has shown how
numerous pressure approaches might be consolidated to pack DNN models
to their most extreme limit (e.g., precision, dormancy, and energy) and, inthis way, the distinctive accessibility of assets between stages (for instance,stockpiling and handling limit). To this end, the programmed enhancement
structure proposed by adadeep [73]efficiently details accuracy, inactivity,
stockpiling, and energy targets and imperatives in a solitary streamlining
issue and uses DRL to effectively find a fair mix of pressure calculations.
Model partition :
 As delineated, to ease the weight of the EI application on
end gadgets 13, one clear idea is that by apportioning the model andreappropriating the computationally exorbitant parts to the sting server ornearby cell phones, model derivation might be improved. Model divisionis for the most part disturbed by certain issues. Inactivity, energy, and pro-
tection are factors to largely consider. The model segment is divided into
two kinds: server-to-gadget segment and gadget-to-gadget segment.Neurosurgeon is a model partitioned between the server and the gadget.Kang et al. [74] represents an incredible exertion. The DNN model in
Neurosurgeon
is parted between the gadget and along these lines the waiter,
and the primary issue is tracking down a satisfactory parcel point that out-come in the best deduction execution of the model. The creators offer arelapse-based strategy to evaluate the dormancy of each layer inside theDNN to demonstrate and produce an ideal split point that will bring about
the model’s derivation, thus meeting the idleness or force necessities. Ko et al.
[75]present a model apportioning approach that joins lossy element encoding
wi
th edge-have parceling. That is, after the model is parceled, the middle
information is packed by lossy element encoding the preceding transmission.Moreover, through the mix of model dividing and lossy element coding,
the structure forms model parceling as an essential issue in applied science(ILP). Upgrading the model parcel to limit inactivity utilizing a coordinatednoncyclic diagram (DAG) rather than a series has been demonstrated to beNP-hard before. Subsequently, Hu et al. [76] present a most pessimistic
sce
nario execution ensured estimation strategy dependent on the chart
min-cut technique. Each of the structures mentioned above expect thatthe server has the EI application’s DNN model. For EI applications,462 Anubhav Singh et al.IONN [77]proposes a gradual offloading approach. IONN partitions the
DN
N layers and loads them into stages so that cell phones and edge servers
can cooperate on the surmising of the DNN model. IONN fundamentally
expands inquiry execution and force utilization when stacking DNN modelscontrasted with the technique that heaps the full model. The segmentbetween gadgets is one more kind of model segment. MoDNN [78]uses
Wi
-Fi Direct to deal with setting up a microscale processing group in the
WLAN with a few supported Wi-Fi-empowered cell phones for dividedDNN model surmising because of the spearheading work of model parcelacross gadgets. The gathering proprietor is the cell phone that plays out
the DNN work, whereas the others fill in as specialist hubs. In MoDNN,
two segment techniques are proposed to accelerate DNN layer execution.MoDNN speeds up DNN model deduction by 2.17 –4.28 for two to four
laborer hubs, as indicated by the test. In a subsequent study, MeDNN
[79], an eager 2D parcel, was introduced for versatile dividing of the
DN
N model on different cell phones and to deal with the pressure of the
DNN model utilizing an organized dispersion pruning approach. Withtwo to four specialist hubs, MeDNN expands the DNN model surmising
by 1.86 –2.44 and saves an extra of 26.5% calculation time and an extra of
14.2% correspondence time. In MoDNN and MeDNN, DNN layers are
parceled evenly, yet, in profound settings, DNN layers are apportionedupward [80]. To decrease the memory impression, it utilizes a combined tile
pa
rceling approach that partitions the DNN layers upward. DeepX [81]addi-
tio
nally attempts to isolate the DNN models; yet, it basically divides them
into submodels and disperses them to the nearby processors. DeepX proposestwo methodologies: RLC (Runtime Layer Pressure) and Father (ProfoundEngineering Decay) (Father). After pressure, the layer is executed by neigh-
borhood processors (computer chips, GPUs, and DSPs). Something else to
remember is that once we have many model parcel occupations, we wouldprefer to enhance the scheduler. LEO [82]is another detecting calculation
sch
eduler that allotments detecting calculation execution and conveys occu-
pations across the computer chip, co-processor, GPU, and cloud to upgradeexecution for a long-time versatile sensor application.
Models early exit : A high-accuracy DNN model generally comprises a
deep structure. Executing a DNN model on a tip device requires a signif-icant number of resources. The models early exit technique uses the output
data of the early layer to induce the classification result, implying that the
inference process is complete when utilizing the partial DNN model;the optimization objective of the models early departure is latency.463 Artificial intelligence in edge devicesBranchyNet [83]may be a programming framework that implements the
early
exit mechanism in the model. BranchyNet [83]modifies the quality
of
the DNN models structure. Exit branches can be added to specified layer
locations. Each exit branch is an exit point, and the normal DNN model
shares certain DNN layers with it. A CNN model with a score of 14.Three out of five points. At these many early exit points, the computer file
is categorized. A framework called DDNN [84]for distributed DNN span-
ning 
the cloud, edge, and devices has been suggested based on BranchyNet
[83]. The device layer, edge server layer, and cloud layer form the
three-layer
framework of DDNNs. Each level indicates an output from
the system BranchyNet [83]. Maximum pooling (MP), average pooling
(AP),
and concatenation (CC) are three suggested aggregation methods.
When multiple mobile devices stream intermediate data to a foothold server
or when multiple edge servers stream intermediate data to the cloud data
center, aggregation techniques work. By taking the maximum of each com-
ponent, MP adds the information vectors. By choosing the common of allcomponents, AP aggregates the information vectors. The information vec-tors are simply concatenated in concert vectors by CC. In addition, Edgent
was constructed on top of BranchyNet [10]. When using model early exit
and
model partition together, it is recommended to traverse the accuracy
latency trade-off. The core concept of Edgent is to use a regression-based
layer latency prediction model to maximize accuracy under a given latencyconstraint. There are a variety of techniques for implementing model early
departure in addition to BranchyNet, e.g., the cascading network [85]. The
MP 
layer and the fully bonded layer are simply added to the quality DNN
model, resulting in 20% acceleration. DeepIns [15]introduces an intelligent
industrial
manufacturing inspection system based on the DNNs early exit
model. Edge devices are in charge of data gathering, the sting server is
the first exit point, and the cloud data center is the second departure pointin DeepIns. Then, Lo et al. [86]propose that the basic BranchyNet model be
supplemented 
with an authentic operation (AO) unit. By setting different
threshold criteria of a confidence level for different output classes of theDNN model, the AO unit assesses whether an input needs to be sent tothe edge server or cloud data center for further execution [87]. By adding
regularization
to the DNN model’s evaluation latency, one can train a guide
to decide whether the current samples should move on to the next layer.
Edge caching : Edge reserving may be a novel, sensible methodology for
accelerating DNN model surmising, i.e., storing the DNN deduction resultsto lessen inertness. Edge reserving’s primary objective is to lessen EI464 Anubhav Singh et al.applications, questioning inertness by putting away and reusing position
results like picture characterization forecast at the organization edge. The
fundamental course of the semantic reserve procedure is that if a solicitationfrom a cell phone discovers the reserved outcomes put away on the edgeserver, the edge server returns the outcome; in any case, the solicitation isshipped off the cloud server farm for derivation utilizing the full accuracy
model. Impression [88] may very well be a creative endeavor to utilize
reserving
procedures for DNN deduction. Impression recommends that
for an item acknowledgment application, the obsolete acknowledgment
result is reused to perceive the article at the current edges. Impression
removes a portion of these reserved outcomes and computes the optical
progression of provisions between the handled edges and, furthermore,the current casing utilizing the consequences of the distinguished item fromthe out-of-date outlines put away on cell phones. The bouncing box will be
moved to an ideal spot in the current edge utilizing the optical stream reg-
istering discoveries. Impression speeds up at a pace of 1.6 –5.5. However, on
the grounds that storing discoveries locally does not scale past several pho-tographs, Cachier [89] has been proposed to acknowledge great many
item-recognizable
proofs. Cachier reserves the after effects of EI applications
on the edge server, saving the information highlights (e.g., picture) just as thework results. The least sum generally used (LFS) is then utilized as the reserverenewal method by Cachier. The sting server communicates the section tothe cloud server farm if the passage cannot be reserved. Cachier can expand
responsiveness by a factor of at least three. The Cachier augmentation is
Precog [90]. The reserved information in Precog is not just put away on
the
edge server yet in addition on the cell phone. Precog prefetch informa-
tion onto the cell phone utilizing Markov chain forecasts, bringing about a
5/C2speedup. Precog likewise suggests that the stored include extraction
model on the cell phone be progressively changed in light of the climate
data. Cachier has been upgraded again in Shadow Manikins. HazyReserve [91]is proposed to lessen repetitive estimations in the application
situation 
where a similar application runs on various gadgets in closeness and
the DNN model frequently parses comparative information records. HazyReserve has two difficulties: one is that the design of the information doc-ument is shaky, so the trouble is ordering the PC record with consistenthunt quality, and the other is communicating the comparability of the
PC document. To reconcile these two issues, versatile LSH (A-LSH) and
homogenized KNN ( H/C0KNN) techniques are proposed by Hazy store.
Hazy store cuts handling deferral and energy use by a factor of 3 –10.465 Artificial intelligence in edge devicesInput filtering : Information separation is a valuable methodology for accel-
erating DNN model surmising, especially in video examination. The main
idea behind information sifting is to avoid nontarget outlines from theinformation, eliminate repetitive computations from DNN model deduc-tion, and, accordingly, increase induction accuracy, diminish derivationinactivity, and decrease power costs. Noscope [92]has been proposed to
accelerate
video examination by skipping outlines with minor adjustments.
For this reason, Noscope contains a distinction finder that features the fleet-ing contrasts between outlines. For instance, the indicator watches edges tocheck whether vehicles show up in them, and just those edges containing
vehicles are handled in the DNN model derivation. Lightweight twofold
classifiers are utilized to recognize the distinction. On account of a multitudeof robots sending videos progressively, Wang et al. [93]advance for DNN
surmising’s
essential bounce remote transfer speed. To decrease transmission
by and large, four strategies are proposed: early disposing, just-in-time learn-ing (JITL), Reachback, and Setting Mindful FFS-VA [94]. It is conceivable
that 
a pipeline framework for multistage video examination might arise. The
FFS-VA sifting framework is developed in three stages. The essential iden-
tifier may be a stream-explicit distinction indicator (SDD), which is utilized
to dispose of edges with simply a background. A stream-specific organiza-tion model (SNM) for target object outline identification may be the sub-sequent choice. The third choice is to utilize a Tiny-YOLO-Voc (TYOLO)model to avoid outlines with focal points that are under a specific limit. For
video investigation, Canel et al. [95]offered a two-stage separating method.
It
removes the semantic measurements of the edges for sending out DNN’s
middle-of-the road information, after which yield capacities are amassed in
an incredible body cushion. The support is viewed as a DAG, and the sep-
arating calculation decides the top-k fascinating casings utilizing Euclidean
distance as a similitude metric. Derivation of the DNN model forcross-chamber investigation is sped up. Rex-cam channels video outlinesutilizing a learned spatiotemporal model. Rex-cam cuts computation time
by 4.6% and lifts DNN model surmising precision by 27%.
Model determination : A model choice methodology is introduced to work
on the idleness, accuracy, and force of DNN derivation. The essential
thought behind model choice is that we can prepare various DNN modelsfor an equivalent disconnected undertaking with various model sizes and
afterward pick the best model for online derivation. The principal contrast
is that the leave point imparts a portion of the DNN layers to the modelwith more branches, while the models are isolated inside the model466 Anubhav Singh et al.determination process. Park et al. [96]provide a structure to the choice of
huge/little
small DNN models. To put it in another way, a sensitive model is
utilized for arranging the PC record, and the enormous model is possibly
used when the minute model’s self-importance is not exactly a foreordainededge. Taylor et al. [97]show that for different pictures, distinctive DNN
models 
(e.g., versatile net, ResNet, and Origin) accomplish the least deri-
vation inactivity or best exactness on different assessment rules (top-1 ortop-5). They then, at that point, provide an approach to figure out whichDNN is best as far as dormancy and precision is concerned. A model selectoris prepared in this setting to choose the best DNN for different information
pictures. Similarly, for IF-CNN [98], the prevalence indicator (RP), a
model
selector, is additionally prepared to alter the model utilized in the
undertaking. RP may be a DNN model that performs various tasks, which
implies that it has many yields. The likelihood of every potential DNN
model’s best 1 name is addressed by the yield of RP. The image is the con-
tribution of RP, and if the yield of RP surpasses a foreordained limit, thencoordination with DNN model is picked. Beside the energy-saving improve-ment for DNN model induction delay, Stamoulis et al. [99]Considering the
ex
actness and correspondence restrictions of the gadgets, we can transform
the versatile DNN model choice issue into a hyperparameter improvementissue. Then, at that point, to resolve the issue, a Bayesian streamlining (BO) isutilized; this prompts an improvement of up to 6 as far as least energy perpicture dependent on accuracy prerequisites is concerned.
Multitenancy support : By and by, an end or edge gadget frequently runs a
few DNN applications simultaneously. For instance, in Web vehicles, thehigh-level driver-help framework (ADAS) executes DNN calculations forvehicle distinguishing proof, passer-by location, traffic sign acknowledg-
ment, and path line recognition simultaneously. Numerous DNN applica-
tions would vie for restricted assets in the present circumstances. Generally,effectiveness would be lost without satisfactory help for a long time, that is,both asset distribution and assignment booking for these simultaneous pro-
jects were seriously influenced. Multi-occupant support centers around
decreasing energy and capacity prerequisites. Considering the elements ofthe assets at runtime [100] , for each DNN model, home DNN has been pro-
posed
to provide customizable asset exactness compromises. Home DNN
utilizes a model substitution pruning and recuperation methodology to
transform the DNN model into a minimal variety model comprised of an
assortment of descendent models. Each relative model has a special assetexactness compromise. For every relative model that is simultaneously467 Artificial intelligence in edge devicesdynamic, home DNN fosters an asset-precision runtime scheduler to shape
the ideal compromise for each simultaneous descendent model by encoding
its exactness and idleness into a worth capacity. Essentially, Standard [101]
utilizes the famous exchange-learning DNN preparing system to show dif-ferent
DNN models with fluctuating levels of accuracy and an insatiable way
to deal with picking the best developer for the worth spending plan. On a
solitary gadget, a few DNN models might be executed Hivemind [102] . For
these
simultaneous errands, HiveMind has been prescribed to expand GPU
utilization. A HiveMind compiler and a runtime module are the two prin-
cipal parts. The compiler enhances information move, preprocessing, and
calculation among jobs, and the runtime motor believers the improved
models into a runtime DAG. This will be run on the GPU fully intenton getting as much simultaneousness as practical. At a point when granular-ity is better, profound eye [103] has been proposed to plan the executions of
heterogeneous
DNN layers to improve the performance of various task
derivations on cell phones. Profound eye divides all DNN layers into twoclassifications: convolutional layers and completely connected layers. Forconvolutional layers, a line-based first in (endash), first out (FIFO) execution
strategy is utilized. Expected for the totally related stores, profound eye to
capitalize on memory, it adopts an avaricious strategy to putting away theboundaries of the completely connected layers.
While the enhancement approaches referenced above are by and large
pertinent to EI applications, application-explicit streamlining can be utilized
to work on the exhibition of EI applications, like exactness, inertness, power
utilization, and memory space. For video-based applications, for instance,two handles, for example to decrease asset utilization, outline rate and assur-ance and are much of the time deftly altered. Be that as it may, on the
grounds that asset-sensitive handles debase derivation precision, they
unavoidably bring about an expense exactness compromise. When settingthe video outline rate and assurance, we wanted to track down a decentharmony between the expense of assets and the accuracy of the deduction.
While heading to the ethereal objective, Chameleon [104] changes the
controls
for every video investigation task by moving top-k’s best powerful
settings starting with one assignment and then onto the next. Video
errands in Chameleon are positioned by their spatial connection, and, then,at that point, the gathering chief looks for the best Top-k blends and
offers them with the devotees. Profound Choice [105] . The knob-tuning
issue 
is reformulated as a different decision multi-imperative rucksack
program that is addressed using a superior beast power scan approach.468 Anubhav Singh et al.Additionally, it is worth noting that equipment speed increase for proficient
DNN surmising has been a hot issue in the PC design field and has drawn a
ton of consideration. Endeavors to do explore. Peruses who are intriguedcan counsel the most recent monograph [106] for a more inside and out out-
line 
of current advancements in DNN handling equipment speed increase.
5.4 Summary of the existing systems and frameworks
Important frameworks and systems to show the use of the aboveempowering methods for IE model deduction, including viewpoints of tar-get applications, design and IE level, targets, and streamlining procedures.Utilized, and adequacy. Obviously, particular subsets of empowering tech-
niques have been embraced by current frameworks and structures, each
adjusted for exceptional EI applications and necessities. Far-reachingempowering approaches and different enhancement methodologies shouldcooperate for broad plan adaptability to boost the general exhibition of a
nonexclusive EI framework. In any case, we might confront a high-
dimensional setup challenge where an inordinate number of executionbasic arrangement boundaries are not set in stone continuously. High-dimensional arrangement boundaries for video investigation can incorpo-
rate, for instance, video outline rate, goal, model determination, and
expected model yield. The high-dimensional arrangement issue, becauseof its combinatorial nature, has an enormous quest space for boundaries,which makes it extremely hard to address.
6. Future research directions
We are presently recognizing the critical open inquiries and future
exploration ways for EI dependent on the phenomenal input above on cur-rent drives.
6.1 Programming and software platforms
Numerous associations across the world are presently zeroing in on
man-made intelligent distributed computing. Amazon’s Greengrass,
Microsoft’s Sky Blue IoT Edge, and Google’s Cloud IoT Edge are amongthe significant organizations dispatching programming/programming stagesto offer edge registering administrations. In any case, the greater part of those
frameworks presently serves essentially as transfers for connecting to incred-
ible cloud server farms. EI as a service platform (EiaaS) could turn into an469 Artificial intelligence in edge devicesinescapable worldview as increasingly more simulated intelligence-
controlled and registered, escalated, and portable IoT applications hit the
market and EI stages with powerful edge artificial intelligence highlightsare developed and carried out. This is often unmistakable from the ML asa help (MLAAs) presented by open mists. MLAA is a sort of cloud knowl-edge that focuses on picking the right server setup and AI structure to
cost-effectively show models in the cloud. Although EiaaS is essentially
focused on a strategy for model preparation and surmising in asset-compelledand protection-delicate edge processing settings, there is an unmistakabledistinction. There are various significant hindrances to tackle to completely
satisfy the guarantee of EI administrations. The EI stages ought to, above all
else, be heterogeneously viable. Since there will be various EI specialist orga-nizations/suppliers later on, a solitary open standard ought to be set up sothat purchasers can encounter consistent administrations on heterogeneous
EI stages. Second, there are a few simulated intelligence programming sys-
tems to browse (for instance, TensorFlow, Lite, and Caffe). The futureought to take into account the versatility of stings simulated intelligencemodels that were created by different programming systems across hetero-
geneously conveyed edge hubs. Third, a few programming systems (like
TensorFlow, Lite, Caffe2, Coral, and Maxent) are developed especiallyfor edge gadgets; in any case, experimental measures fail to [107] show that
there
is no champion that can outperform different structures in all measure-
ments; later on, we might expect a structure that performs well on a more
extensive scope of measures. To wrap things up, lightweight registering and
virtualization approaches, for example, compartment and capacityprocessing, should be additionally investigated to empower compellingposition and relocation of EI administrations in asset-obliged web-based
business conditions.
6.2 Resource-friendly edge AI model design
Numerous contemporary man-made intelligence models, like CNN and
LSTM, have been created with PC vision and discourse correspondencehandling as a main priority. Most profound learning-based artificial intelli-gence models are asset-concentrated, requiring vigorous figuring powerupheld by broad equipment assets (for example, GPU, FPGA, and TPU)
to work on the exhibition of these models. Man-made brainpower.
Therefore, as recently observed, a few researches have been conducted to470 Anubhav Singh et al.scale down simulated intelligence models utilizing model pressure draws
(e.g., weight pruning) to make them more asset-agreeable for edge sending.
We will showcase an interesting line, notwithstanding a particular line. Edgecomputer-based intelligence model plan that is asset mindful. Maybe ofdepending on asset serious simulated intelligence models that as of now exist,we will impact the Auto ML information [108] NAS (neural engineering
search)
strategies are likewise utilized [109] to plan asset effective edge arti-
ficial
intelligence models that oblige the equipment asset limitations of the
fundamental edge servers and gadgets. Techniques like RL, hereditary cal-culations, and BO can be utilized, for instance, to proficiently put require-
ments on execution measurements, like execution inertness and energy use,
through the KI model plan boundary space (d-work memory).
6.3 Computation-aware networking techniques
EI, by and large, runs computationally escalated, computer- andintelligence-based applications in a circulated edge-figuring climate.Hence, refined and PC-agreeable organization arrangements are incredibly
attractive, as computation results and information can be successfully traded
across numerous edge hubs. Ultra-Reliable Low-Latency Communication(URLLC) is intended for basic business applications that require low deferraland high dependability in the longest 5G organization of things to come.Subsequently, joining the 5G URLLC’s best-in-class figuring abilities to
provide incredibly solid Low Dormancy EI (URLLEI) administrations will
show guarantee. What is more, 5G will utilize complex advancements, forexample, programming-characterized systems administration and organiza-tion work virtualization. These methodologies take into account more
adaptable organization asset the board and empower on-request associations
across various edge hubs for handling concentrated simulated intelligenceapplications. Then again, the plan of an independent organization compo-nent is fundamental to the proficient arrangement of EI benefits in a
powerful heterogeneous organizational concurrence (e.g., LTE/5G/
Wi-Fi/lora), permitting recently acquainted edge hubs and gadgets withself-design in a fit-and-play style. What is more, PC helps specialized tech-niques, for example, angle coding [110] , to decrease the defer impact on
appropriated
learning and over-the-air calculation for DSGD [111] ,
which
could be worthwhile in accelerating model preparation. Edge
computer-based intelligence, gain in significance.471 Artificial intelligence in edge devices6.4 Trade-off design with various DNN performance metrics
There are, for the most part, various DNN model applicants fit for finishing
the work for an EI application with a specific target. Nonetheless, on the
grounds that quality exhibition measurements, for example, top-k precisionor mean exactness, do not address the runtime execution of DNN modelinduction on fringe gadgets, it is hard for programming engineers to pick
a model. DNN fitting for EI application. For a model, during the organiza-
tion period of an EI application, derivation speed and exactness are signif-icant contemplations. The utilization of assets is additionally a significantmeasurement. We need to dissect the compromises between these actionsand the variables that influence them. For item acknowledgment, Huang
et al. [112] explored the impact of different boundaries on the time and accu-
racy
of the derivation, like the quantity of clues, the size of the information
picture, and the choice of the element extractor. In light of the aftereffects of
their trials, they found that a new blend of these components outflanks the
best-in-class approach. Accordingly, it’s basic to examine the compromises
between different markers to work on the productivity of sending EIapplications.
6.5 Resource management and smart services
Edge hubs and gadgets that endow EI with usefulness are appropriated acrossnumerous geospatial areas and locales because of the disseminated idea ofedge registering. Distinctive computer-based intelligence models and con-centrated simulated intelligence errands can be run on various gadgets and
edge hubs. Way. Separating muddled edge computer-based intelligence
models into little suberrands and viably reappropriating these assignmentsto edge hubs and gadgets for synergistic executions are likewise importantto completely use the dispersive assets at edge hubs and gadgets. Since help
conditions in some EI application situations (e.g., shrewd urban areas) are
extremely powerful, it is hard to appropriately expect future events.Along these lines, outstanding Internet-based edge asset provisioning andarrangement capacities would be needed to reliably deal with enormous
EI responsibilities. The joint ongoing improvement of heterogeneous pro-
cess, correspondence and reserve asset assignments, just as the arrangementof high-dimensional framework boundaries (for instance, the determinationof suitable model preparing and induction draws near), which are custom-
ized to countless undertaking prerequisites, they are imperative. To adapt to
the intricacy of calculation plan, another space of exploration is utilizing472 Anubhav Singh et al.artificial intelligence methods, for example, DRL to change the ideal asset
designation approaches in a self-learning based way in information. To man-
age the intricacy of the calculation plan, another examination course is toutilize man-made brainpower strategies like DRL to tweak proficientasset designation strategy in an exceptionally information-driven and self-adapting manner.
6.6 Security and privacy issues
As a result of the open idea of edge processing, decentralized trust is impor-tant to guarantee that EI administrations provided by many organizations are
solid [113] . To provide client validation and access control, information and
model 
honesty, and cross-stage confirmation for EI, circulated and light-
weight security systems are significant. While considering the living together
of dependable edge hubs with malignant ones, look at new safe steering strat-
egies and trust network geographies for EI administration conveyance. Then
again, the principal clients and gadgets would produce a lot of information atthe edge of the organization that would be exceptionally dangerous to secu-rity, as it could incorporate client area information, well-being or movement
records, or assembling data, in addition to other factors. Straightforwardly
trading the principal informational indexes among various edge hubs mayhave the critical danger of security spillage when dependent upon securityinsurance necessities, like the EU’s Overall Information Assurance
Guideline (GDPR). Subsequently, combined learning is a practical world-
view for protection well-disposed disseminated information preparing whenunique informational indexes are put away on your delivered gadgets/hubsand Edge computer based intelligence model boundaries are shared. To fur-ther develop information protection, specialists are progressively going to
innovations like differential security and homomorphic encryption, and
secure multiparty figuring to make protection safeguarding man-madeintelligence model boundary trade plans [114] .
6.7 The EI ecosystem is a large open collaboration that focuses
on incentives and business models
Stage suppliers (for instance, Amazon), artificial intelligence programmingsuppliers (for instance, SenseTime), edge gadget suppliers (for instance,Hikvision), network administrators (for instance, AT&T), information gen-
erators (for example IoT and cell phone proprietors), and fix clients all have a
place with EI specialist organizations and clients (i.e., EI clients) [115] .473 Artificial intelligence in edge devicesFor instance, for embracing improved asset sharing and simple assistance
move, the profoundly proficient activity of EI administrations might require
close coordination and incorporation between numerous specialist organi-zations. Advanced, successful, and effective coordinated efforts betweenall individuals from the EI environment, a suitable motivation component,and a monetary model are needed. In this case, a stand-out savvy estimating
structure is important to represent the client’s administration utilization
just as the worth of their information input. Blockchain with a sensibleagreement may be executed into the EI administration by working ondecentralized edge servers as a way for decentralized participation. It is help-
ful to concentrate on the best way to effectively ascertain the cost and divide
the pay between the individuals from the EI environment as indicated bytheir work records. It would likewise be ideal to develop an asset-cordial,lightweight blockchain agreement framework for EI.
7. Conclusions
With artificial intelligence and IoT on the ascent, there is an earnest
need to move the computer-based intelligence boondocks from the cloud tothe edge of the web. Edge registering has been broadly perceived as a suitable
choice to help process serious artificial intelligence applications in asset-
obliged settings to meet this pattern. The association between state-of-the-artprocessing and simulated intelligence provides a new worldview of EI. In thispart, we have conducted an exhaustive survey of the most recent examinations
on EI. In particular, we initially analyzed the foundation and inspiration of
man-made intelligence at the edge of the network. We then, at that point,provided an outline of the arising key designs, structures, and advances forthe profound learning model for preparing and surmising at the edge of the
organization. Finally, we discussed EI’s open issues and potential exploration
pathways. We trust that this survey will produce more interest, cultivate usefulconversations, and create new exploration thoughts on EI.
References
[1]Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436.
[2]L. Deng, D. Yu, Deep learning: methods and applications, Found. Trends Signal
Process.
7 (3) (2014) 197– 387.
[3] Cisco Global Cloud Index, Forecast and Methodology [online], 2016 –2021, Available
https:/ /www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-
index-gci/white-paper-c11-738085.html .
[4]B. Heintz, A. Chandra, R.K. Sitaraman, Optimizing grouped aggregation in
geo-distributed
streaming analytics, in: Proc. ACM HPDC, 2015, pp. 133 –144.474 Anubhav Singh et al.[5]Q. Pu, et al., Low latency geo-distributed data analytics, in: Proc. ACM SIGCOMM,
2015,
pp. 421 –434.
[6]W. Shi, J. Cao, Q. Zhang, Y. Li, L. Xu, Edge computing: vision and challenges, IEEEInternet
Things J. 3 (5) (2016) 637– 646.
[7]X. Chen, L. Pu, L. Gao, W. Wu, D. Wu, Exploiting massive D2D collaboration forenergy-efficient
mobile edge computing, IEEE Wirel. Commun. 24 (4) (2017) 64 –71.
[8]Y. Mao, C. You, J. Zhang, K. Huang, K.B. Letaief, A survey on mobile edge com-puting:
the communication perspective, IEEE Commun. Surveys Tuts. 19 (4) (2017)
2322 –2358. 4th Quart.
[9] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, M. Chen, In-Edge AI:
Intelligentizing
Mobile Edge Computing Caching and Communication by
Federated Learning [online], 2018, arXiv:1809.07857. Available https:/ /arxiv.org/
abs/1809.07857 .
[10] E. Li, Z. Zhou, X. Chen, Edge intelligence: on-demand deep learning model
co-inference
with device-edge synergy, in: Proc. Workshop Mobile Edge
Commun. MECOMM, 2018, pp. 31 –36.
[11] Trends Emerge in the Gartner Hype Cycle for Emerging Technologies [online],
2018.
Available https:/ /www.gartner.com/smarterwithgartner/5-trends-emerge-in-
gartner-hype-cycle-for-emerging-technologies-2018/ .
[12] G. Ananthanarayanan, et al., Real-time video analytics: the killer app for edge com-
puting,
Computer 50 (10) (2017) 58 –67.
[13] K. Ha, Z. Chen, W. Hu, W. Richter, P. Pillai, M. Satyanarayanan, Towards wearablecognitive
assistance, in: Proc. ACM Mobisys, 2014, pp. 68 –81.
[14] C. Jie, L. Xu, R. Abdallah, W. Shi, EdgeOS_h: a home operating system for internet ofeverything,
in: Proc. IEEE ICDCS, 2017, pp. 1756 –1764.
[15] L. Li, K. Ota, M. Dong, Deep learning for smart industry: efficient manufactureinspection
system with fog computing, IEEE Trans. Ind. Informat. 14 (10) (2018)
4665 –4673.
[16] D. Svozil, V. Kvasnicka, J. Pospichal, Introduction to multi-layer feed-forward neuralnetworks,
Chemom. Intell. Lab. Syst. 39 (1) (1997) 43 –62.
[17] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, P. Kuksa, Naturallanguage
processing (almost) from scratch, J. Mach. Learn. Res. 12 (2011) 2493 –2537.
[18] A. Krizhevsky, I. Sutskever, G.E. Hinton, Imagenet classification with deep con-volutional
neural networks, in: Proc. NIPS, 2012, pp. 1097 –1105.
[19] K. Simonyan, A. Zisserman, Very Deep Convolutional Networks for Large-Scale
Image
Recognition [online], 2014, arXiv:1409.1556. Available https:/ /arxiv.org/
abs/1409.1556 .
[20] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:Proc.
IEEE CVPR, 2016, pp. 770– 778.
[21] A.G. Howard, et al., MobileNets: Efficient Convolutional Neural Networks for
Mobile
Vision Applications, 2017, arXiv:1704.04861 [online] Available: https:/ /
arxiv.org/abs/1704.04861 .
[22] H. Mao, S. Yao, T. Tang, B. Li, J. Yao, Y. Wang, Towards real-time object detection
on
embedded systems, IEEE Trans. Emerging Topics Comput. 6 (3) (2018) 417 –431.
[23] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: unified real-time
object
detection, in: Proc. IEEE Conf. Comput. Vis. Pattern Recognit, 2016,
pp. 779– 788.
[24] W. Liu, et al., SSD: single shot multibox detector, in: Proc. Eur. Conf. Comput. Vis,2016,
pp. 21 –37.
[25] L. Bottou, Large-scale machine learning with stochastic gradient descent, in: Proc.COMPSTAT,
2010, pp. 177 –186.
[26] D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning representations byback-propagating
errors, Nature 323 (6088) (1986) 533.475 Artificial intelligence in edge devices[27] Y. Chauvin, D.E. Rumelhart, Backpropagation: Theory Architectures and
Applications,
Psychology Press, 2013.
[28] C. Szegedy, et al., Going deeper with convolutions, in: Proc. IEEE Conf. Comput.Vis.
Pattern Recognit, 2015, pp. 1 –9.
[29] P.J. Werbos, Backpropagation through time: what it does and how to do it, Proc. IEEE78
(10) (1990) 1550 –1560.
[30] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9(8)
(1997) 1735 –1780.
[31] I. Goodfellow, et al., Generative adversarial nets, in: Proc. Adv. Neural Inf. Process.Syst,
2014, pp. 2672 –2680.
[32] V. Mnih, et al., Human-level control through deep reinforcement learning, Nature518
(7540) (2015) 529.
[33] AI Trends for Enterprise Computing, 2017. [online] Available https:/ /www.gartner.
com/smarterwithgartner/3-ai-trends-for-enterprise-computing/ .
[34] Democratizing AI, 2016. [online] Available https:/ /news.microsoft.com/features/
democratizing-ai/ .
[35] M. Satyanarayanan, P. Bahl, R. Caceres, N. Davies, The case for VM-based cloudlets
in
mobile computing, IEEE Pervasive Comput. 4 (2009) 14 –23.
[36] Microsoft Interactive Cloud Gaming, 2022. [online] Available https:/ /azure.microsoft.
com/en-us/solutions/gaming/ .
[37] H. Zhang, G. Ananthanarayanan, P. Bodik, M. Philipose, P. Bahl, M.J. Freedman,Live
video analytics at scale with approximation and delay-tolerance, in: Proc.
USENIX NSDI, 2017, pp. 377– 392.
[38] C.-C. Hung, et al., VideoEdge: Processing camera streams using hierarchical clusters,
in:
Proc. IEEE/ACM Symp. Edge Comput. (SEC), 2018, pp. 115 –131.
[39] I. Stoica, et al., A Berkeley View of Systems Challenges for AI, 2017,
arXiv:1712.05855
[online] Available https:/ /arxiv.org/abs/1712.05855 .
[40] Trends Emerge in the Gartner Hype Cycle for Emerging Technologies, 2018.
[online]
Available: https:/ /www.gartner.com/smarterwithgartner/5-trends-emerge-
in-gartner-hype-cycle-for-emerging-technologies-2018/ .
[41] IEC White Paper Edge Intelligence [online], 2017. Available https:/ /www.iec.ch/
whitepaper/edgeintelligence/ .
[42] Accelerating AI on the Intelligent Edge [online], 2022. Available: https:/ /azure.
m
icrosoft.com/en-us/blog/accelerating-ai-on-the-intelligent-edge-microsoft-and-
qualcomm-create-vision-ai-developer-kit/.
[43] Edge Intelligence for Industrial Internet of Things [online], 2021. Available https:/ /
www.co
msoc.org/publications/magazine s/ieee-network/cfp/edge-intelligence-
industrial-internet-things .
[44] H.B. McMahan, E. Moore, D. Ramage, S. Hampson, B.A.Y. Arcas,
Communication-Efficient
Learning of Deep Networks from Decentralized Data
[online], 2016, arXiv:1602.05629, Available https:/ /arxiv.org/abs/1602.05629 .
[45] R. Shokri, V. Shmatikov, Privacy-preserving deep learning, in: Proc. 22nd ACMSIGSAC
Conf. Comput. Commun. Secur, 2015, pp. 1310 –1321.
[46] J. Kone /C20cny´ , H.B. McMahan, F.X. Yu, P. Richta ´rik, A.T. Suresh, D. Bacon, Federated
Learning: Strategies for Improving Communication Efficiency, arXiv:1610.05492,2016.
[47] A. Lalitha, S. Shekhar, T. Javidi, F. Koushanfar, Peer-to-Peer Federated Learning
on 
Graphs [online], 2019, arXiv:1901.11173. Available https:/ /arxiv.org/abs/
1901.11173 .
[48] H. Kim, J. Park, M. Bennis, S.-L. Kim, On-Device Federated Learning Via
Blockchain
and Its Latency Analysis [online], 2018, arXiv:1808.03949. Available
https:/ /arxiv.org/abs/1808.03949 .476 Anubhav Singh et al.[49] K. Hsieh, A. Harlap, N. Vijaykumar, D. Konomis, G.R. Ganger, P.B. Gibbons, Gaia:
geo-distributed
machine learning approaching LAN speeds, in: Proc. NSDI, 2017,
pp. 629– 647.
[50] S. Wang, et al., Adaptive federated learning in resource constrained edge computingsystems,
IEEE J. Sel. Areas Commun. 37 (3) (2019) 1205 –1221.
[51] T. Nishio, R. Yonetani, Client Selection for Federated Learning with Heterogeneous
Resources
in Mobile Edge [online], 2018, arXiv:1804.08333. Available https:/ /arxiv.
org/abs/1804.08333 .
[52] Y. Lin, S. Han, H. Mao, Y. Wang, W.J. Dally, Deep Gradient Compression:
Reducing
the Communication Bandwidth for Distributed Training [online], 2017,
arXiv:1712.01887. Available https:/ /arxiv.org/abs/1712.01887 .
[53] Z. Tao, Q. Li, eSGD: communication efficient distributed deep learning on the edge,in:
Proc. USENIX Workshop Hot Topics Edge Comput. (HotEdge), 2018.
[54] S.U. Stich, J.-B. Cordonnier, M. Jaggi, Sparsified sgd with memory, in: Proc. Adv.
Neural
Inf. Process. Syst, 2018, pp. 4452 –4463.
[55] H. Tang, S. Gan, C. Zhang, T. Zhang, J. Liu, Communication compression
for
decentralized training, in: Proc. Adv. Neural Inf. Process. Syst, 2018,
pp. 7663 –7673.
[56] M.M. Amiri, D. Gunduz, Machine Learning at the Wireless Edge: Distributed
Stochastic
Gradient Descent over-the-Air [online], 2019, arXiv:1901.00844.
Available https:/ /arxiv.org/abs/1901.00844 .
[57] Y. Mao, S. Yi, Q. Li, J. Feng, F. Xu, S. Zhong, A privacy-preserving deep learningapproach
for face recognition with edge computing, in: Proc. USENIX, 2018.
[58] J. Wang, J. Zhang, W. Bao, X. Zhu, B. Cao, P.S. Yu, Not just privacy: improvingperformance
of private deep learning in mobile cloud, in: Proc. 24th ACM
SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2018, pp. 2407 –2416.
[59] S.A. Osia, et al., A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile
Analytics
[online], 2017, arXiv:1703.02952. Available https:/ /arxiv.org/abs/1703.
02952 .
[60] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, G.R. Ganger, P.B. Gibbons,
PipeDream:
Fast and Efficient Pipeline Parallel DNN Training [online], 2018,
arXiv:1806.03377. Available https:/ /arxiv.org/abs/1806.03377 .
[61] R. Sharma, S. Biookaghazadeh, B. Li, M. Zhao, Are existing knowledge transfer tech-niques
effective for deep learning with edge devices? in: Proc. IEEE Int. Conf. Edge
Comput. (EDGE), 2018, pp. 42 –49.
[62] Q. Chen, Z. Zheng, C. Hu, D. Wang, F. Liu, Data-driven task allocation for
multi-task
transfer learning on the edge, in: Proc. IEEE 39th Int. Conf. Distrib.
Comput. Syst. (ICDCS), 2019.
[63] S. Boyd, A. Ghosh, B. Prabhakar, D. Shah, Randomized gossip algorithms, IEEETrans.
Inf. Theory 52 (6) (2006) 2508 –2530.
[64] M. Blot, D. Picard, M. Cord, N. Thome, Gossip Training for Deep Learning [online],
2016,
arXiv:1611.09726. Available https:/ /arxiv.org/abs/1611.09726 .
[65] P.H. Jin, Q. Yuan, F. Iandola, K. Keutzer, How to Scale Distributed Deep Learning?
[online],
2016, arXiv:1611.04581. Available https:/ /arxiv.org/abs/1611.04581 .
[66] J. Daily, A. Vishnu, C. Siegel, T. Warfel, V. Amatya, GossipGraD: Scalable Deep
Learning
Using Gossip Communication Based Asynchronous Gradient Descent
[online], 2018, arXiv:1803.05880. Available: https:/ /arxiv.org/abs/1803.05880 .
[67] C.-J. Wu, et al., Machine learning at facebook: understanding inference at the
edge,
in: Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA), 2019,
pp. 331– 344.
[68] S. Han, J. Pool, J. Tran, W. Dally, Learning both weights and connections for efficient
neural
network, in: Proc. Adv. Neural Inf. Process. Syst, 2015, pp. 1135 –1143.477 Artificial intelligence in edge devices[69] S. Han, H. Mao, W.J. Dally, Deep Compression: Compressing Deep Neural
Networks with Pruning Trained Quantization and Huffman Coding [online],
2015, arXiv:1510.00149. Available https:/ /arxiv.org/abs/1510.00149 .
[70] Y.-H. Chen, J. Emer, V. Sze, Eyeriss: a spatial architecture for energy-efficient
dataflow
for convolutional neural networks, ACM SIGARCH Comput. Archit.
News 44 (3) (2016) 367– 379.
[71] T.-J. Yang, Y.-H. Chen, V. Sze, Designing energy-efficient convolutional neural net-works
using energy-aware pruning, in: Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. (CVPR), 2017, pp. 5687 –5695.
[72] Y.H. Oh, et al., A portable automatic data quantizer for deep neural networks, in:
Proc.
ACM PACT, 2018, p. 17.
[73] S. Liu, Y. Lin, Z. Zhou, K. Nan, H. Liu, J. Du, On-demand deep model compression
for
mobile devices: a usage-driven model selection framework, in: Proc. 16th Annu.
Int. Conf. Mobile Syst. Appl. Services, 2018, pp. 389 –400.
[74] Y. Kang, et al., Neurosurgeon: collaborative intelligence between the cloud andmobile
edge, ACM SIGPLAN Not. 52 (4) (2017) 615 –629.
[75] J.H. Ko, T. Na, M.F. Amir, S. Mukhopadhyay, Edge-Host Partitioning of Deep
Neural
Networks with Feature Space Encoding for Resource-Constrained
Internet-of-Things Platforms [online], 2018, arXiv:1802.03835. Available https:/ /
arxiv.org/abs/1802.03835 .
[76] C. Hu, W. Bao, D. Wang, F. Liu, Dynamic adaptive DNN surgery for inference accel-
eration
on the edge, in: Proc. IEEE INFOCOM, 2019.
[77] H.-J. Jeong, H.-J. Lee, C.H. Shin, S.-M. Moon, IONN: Incremental offloading of
neural
network computations from mobile devices to edge servers, in: Proc. ACM
Symp. Cloud Comput, 2018, pp. 401– 411.
[78] J. Mao, X. Chen, K.W. Nixon, C. Krieger, Y. Chen, MoDNN: local distributedmobile
computing system for deep neural network, in: Proc. Design Autom. Test
Eur. Conf. Exhib. (DATE), 2017, pp. 1396 –1401.
[79] J. Mao, et al., MeDNN: a distributed mobile system with enhanced partition anddeployment
for large-scale DNNs, in: Proc. 36th Int. Conf. Comput.-Aided
Design, 2017, pp. 751 –756.
[80] Z. Zhao, K.M. Barijough, A. Gerstlauer, DeepThings: distributed adaptive deep learn-
ing
inference on resource-constrained IoT edge clusters, IEEE Trans. Comput.-Aided
Design Integr. Circuits Syst. 37 (11) (2018) 2348 –2359.
[81] N.D. Lane, et al., DeepX: a software accelerator for low-power deep learning
inference
on mobile devices, in: Proc. 15th Int. Conf. Inf. Process. Sensor Netw,
2016, p. 23.
[82] P. Georgiev, N.D. Lane, K.K. Rachuri, C. Mascolo, Leo: scheduling sensor inferencealgorithms
across heterogeneous mobile processors and network resources, in: Proc.
22nd Annu. Int. Conf. Mobile Computing Netw, 2016, pp. 320 –333.
[83] S. Teerapittayanon, B. McDanel, H. Kung, BranchyNet: fast inference via earlyexiting
from deep neural networks, in: Proc. 23rd Int. Conf. Pattern Recognit.
(ICPR), 2016, pp. 2464 –2469.
[84] S. Teerapittayanon, B. McDanel, H.-T. Kung, Distributed deep neural networks overthe
cloud the edge and end devices, in: Proc. IEEE 37th Int. Conf. Distrib. Comput.
Syst. (ICDCS), 2017, pp. 328– 339.
[85] S. Leroux, et al., The cascading neural network: building the internet of smart things,
Knowl.
Inf. Syst. 52 (3) (2017) 791– 814.
[86] C. Lo, Y.-Y. Su, C.-Y. Lee, S.-C. Chang, A dynamic deep neural network design for
efficient
workload allocation in edge computing, in: Proc. IEEE Int. Conf. Comput.
Design (ICCD), 2017, pp. 273– 280.478 Anubhav Singh et al.[87] T. Bolukbasi, J. Wang, O. Dekel, V. Saligrama, Adaptive Neural Networks for
Efficient Inference [online], 2017, arXiv:1702.07811. Available https:/ /arxiv.org/
abs/1702.07811 .
[88] T.Y.-H. Chen, L. Ravindranath, S. Deng, P. Bahl, H. Balakrishnan, Glimpse:
continuous
real-time object recognition on mobile devices, in: Proc. ACM Sensys,
2015.
[89] U. Drolia, K. Guo, J. Tan, R. Gandhi, P. Narasimhan, Cachier: edge-caching forrecognition
applications, in: Proc. IEEE ICDCS, 2017, pp. 276 –286.
[90] U. Drolia, K. Guo, P. Narasimhan, Precog: prefetching for image recognition appli-cations
at the edge, in: Proc. ACM/IEEE Symp. Edge Comput. (SEC), 2017, p. 17.
[91] P. Guo, B. Hu, R. Li, W. Hu, FoggyCache: cross-device approximate computationreuse,
in: Proc. ACM Mobicom, 2018, pp. 19 –34.
[92] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, M. Zaharia, Noscope: optimizing neuralnetwork
queries over video at scale, Proc. VLDB Endowment 10 (11) (2017)
1586 –1597.
[93] J. Wang, et al., Bandwidth-efficient live video analytics for drones via edge computing,in:
Proc. IEEE/ACM Symp. Edge Comput. (SEC), 2018, pp. 159 –173.
[94] C. Zhang, Q. Cao, H. Jiang, W. Zhang, J. Li, J. Yao, in: FFS-VA: A Fast FilteringSystem
for Large-Scale Video Analytics, Proc. ACM ICPP, 2018, p. 85.
[95] C. Canel, et al., Picking interesting frames in streaming video, in: SysML’18, February15–16,
2018.
[96] E. Park, et al., Big/little deep neural network for ultra low power inference, in: Proc.10th
Int. Conf. Hardw./Softw. Codesign Syst. Synth, 2015, pp. 124 –132.
[97] B. Taylor, V.S. Marco, W. Wolff, Y. Elkhatib, Z. Wang, Adaptive deep learningmodel
selection on embedded systems, in: Proc. ACM LCTES, 2018, pp. 31 –43.
[98] G. Shu, W. Liu, X. Zheng, J. Li, IF-CNN: image-aware inference framework forCNN
with the collaboration of mobile devices and cloud, IEEE Access 6
(2018) 621 –633.
[99] D. Stamoulis, et al., Designing adaptive neural networks for energy-constrained imageclassification,
in: Proc. ACM ICCAD, 2018.
[100] B. Fang, X. Zeng, M. Zhang, NestDNN: resource-aware multi-tenant on-devicedeep
learning for continuous mobile vision, in: Proc. ACM Mobicom, 2018,
pp. 115– 127.
[101] A.H. Jiang, et al., Mainstream: dynamic stem-sharing for multi-tenant videoprocessing,
in: Proc. USENIX ATC, 2018, pp. 29 –42.
[102] D. Narayanan, K. Santhanam, A. Phanishayee, M. Zaharia, Accelerating deep learning
workloads
through efficient multi-model execution, in: Proc. NIPS Workshop Syst.
Mach. Learn, 2018.
[103] A. Mathur, N.D. Lane, S. Bhattacharya, A. Boran, C. Forlivesi, F. Kawsar, DeepEye:resource
efficient local execution of multiple deep vision models using wearable com-
modity hardware, in: Proc. ACM Mobisys, 2017, pp. 68 –81.
[104] J. Jiang, G. Ananthanarayanan, P. Bodik, S. Sen, I. Stoica, Chameleon: scalable adap-
tation
of video analytics, in: Proc. ACM SIGCOMM, 2018, pp. 253 –266.
[105] X. Ran, H. Chen, X. Zhu, Z. Liu, J. Chen, Deepdecision: a mobile deep learningframework
for edge video analytics, in: Proc. IEEE INFOCOM, 2018,
pp. 1421 –1429.
[106] V. Sze, Y.-H. Chen, T.-J. Yang, J.S. Emer, Efficient processing of deep neural net-
works:
a tutorial and survey, Proc. IEEE 105 (12) (2017) 2295 –2329.
[107] X. Zhang, Y. Wang, W. Shi, pCAMP: performance comparison of machine learning
packages
on the edges, in: Proc. USENIX Workshop Hot Topics Edge Comput.
(HotEdge), 2018.479 Artificial intelligence in edge devices[108] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, S. Han, AMC: Automl for model compression
and
acceleration on mobile devices, in: Proc. Eur. Conf. Comput. Vis, 2018,
pp. 815– 832.
[109] B. Zoph, Q.V. Le, Neural Architecture Search with Reinforcement Learning [online],
2016,
arXiv:1611.01578. Available https:/ /arxiv.org/abs/1611.01578 .
[110] R. Tandon, Q. Lei, A.G. Dimakis, N. Karampatziakis, Gradient coding: avoiding
stragglers
in distributed learning, in: Proc. Int. Conf. Mach. Learn, 2017,
pp. 3368 –3376.
[111] G. Zhu, Y. Wang, K. Huang, Low-Latency Broadband Analog Aggregation for
Federated
Edge Learning [online], 2018, arXiv:1812.11494. Available https:/ /arxiv.
org/abs/1812.11494 .
[112] J. Huang, et al., Speed/accuracy trade-offs for modern convolutional object detectors,
in:
Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2017, pp. 7310 –7311.
[113] D. Li, Z. Zhang, W. Liao, Z. Xu, KLRA: a kernel level resource auditing tool for IoT
operating
system security, in: Proc. IEEE/ACM Symp. Edge Comput. (SEC), 2018,
pp. 427– 432.
[114] M. Du, K. Wang, Y. Chen, X. Wang, Y. Sun, Big data privacy preserving inmulti-access
edge computing for heterogeneous internet of things, IEEE Commun.
Mag. 56 (8) (2018) 62 –67.
[115] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, J. Zhang, Edge intelligence: paving the last
mile
of artificial intelligence with edge computing, Proc. IEEE 107 (8) (2019)
1738 –1762.
Further reading
[116] J. Zhao, R. Mortier, J. Crowcroft, L. Wang, Privacy-preserving machine learningbased
data analytics on edge devices, in: Proc. AIES, 2018, pp. 341 –346.
[117] Y. Li, et al., A network-centric hardware/algorithm co-design to accelerate distributedtraining
of deep neural networks, in: Proc. 51st Annu. IEEE/ACM Int. Symp.
Microarchitecture (MICRO), 2018, pp. 175 –188.
[118] B. Reagen, et al., Minerva: enabling low-power highly-accurate deep neural networkaccelerators,
ACM SIGARCH Comput. Archit. News 44 (3) (2016) 267 –278.
[119] L. Zeng, E. Li, Z. Zhou, X. Chen, Boomerang: on-demand cooperative deep neuralnetwork
inference for edge intelligence on industrial internet of things, in: IEEE
Netw, 2019.
[120] H. Li, C. Hu, J. Jiang, Z. Wang, Y. Wen, W. Zhu, JALAD: Joint Accuracy-and
Latency-Aware
Deep Structure Decoupling for Edge-Cloud Execution [online],
2018, arXiv:1812.10027. Available https:/ /arxiv.org/abs/1812.10027 .
[121] S. Venugopal, M. Gazzetti, Y. Gkoufas, K. Katrinis, Shadow puppets: cloud-levelaccurate
AI inference at the speed and economy of edge, in: Proc. USENIX workshop
hot topics in edge Comput. (HotEdge), 2018.
[122] S. Jain, J. Jiang, Y. Shu, G. Ananthanarayanan, J. Gonzalez, ReXCam:
Resource-Efficient
Cross-Camera Video Analytics at Enterprise Scale [online],
2018, arXiv:1811.01268. Available https:/ /arxiv.org/abs/1811.01268 .
[123] Z. Fang, M. Luo, T. Yu, O.J. Mengshoel, M.B. Srivastava, R.K. Gupta, Mitigating
multi-tenant
interference in continuous mobile offloading, in: Proc. Int. Conf.
Cloud Comput, 2018, pp. 20 –36.480 Anubhav Singh et al.About the authors
Anubhav Singh , born on 1 January 2000 in
Ballia, Uttar Pradesh, is pursuing MSc
Forensic Science from Rashtriya Raksha
University and has completed BSc (Hons.)
Forensic Science from Galgotias University,
Greater Noida, UP. He has a Diploma in
Photography and PG Diploma in IT
Fundamentals for Cybersecurity at IBM. He
has completed several certificate courses. He
is a certified graphics editor and designer.
He has published more than book chapters
in national and international research and
review papers in peer-reviewed international journals. He has organized more
than 4 National and international conferences and has participated and pres-
ented his work at more than 12 national and international conferences and
workshops.
Kavita Saini is presently working as a profes-
sor at the School of Computing Science and
Engineering, Galgotias University, Delhi
NCR, India. She received her PhD degree
from Banasthali Vidyapeeth, Banasthali. She
has 18years of teaching and research experi-
ence supervising masters degree and PhD
students in emerging technologies. She has
published more than 40 research papers in
national and international journals and confer-
ences. She has published 17 authored books
for UG and PG courses for a number of uni-
versities including MD University, Rothak,
and Punjab Technical University, Jallandhar, with national publishers.
Kavita Saini has edited many books with international publishers, including
IGI Global, CRC Press, IET Publisher, and Elsevier, and has published
15 book chapters with international publishers. Under her guidance, many
MTech and PhD students are carrying out research work. She has also481 Artificial intelligence in edge devicespublished various patents. Kavita Saini has also delivered technical talks on
“Blockchain: An Emerging Technology,” “Web to Deep Web,” and other
emerging areas and has handled many special sessions in international confer-
ences and special issues in international journals. Her research interests include
web-based instructional systems (WBIS), blockchain technology, Industry
4.0, and cloud computing.
Varad Nagar , born on 17 March 2002, in
Varanasi, Uttar Pradesh, is currently pursuing
BSc (Hons.) Forensic Science from
Vivekananda Global University, Jaipur,
Rajasthan, India, and is also pursuing founda-
tion degree from IIT Madras in data science
and programming. He has participated and
presented his work at more than 10 national
and international conferences and work-
shops. He has published more than 12 papers
in Scopus indexed journals and 10 book
chapters in various national and international
publications, and several papers/chapters are under progress. He has
hands-on experience on a variety of sophisticated instruments like
UV-visible spectrophotometers, IR spectroscopy, SEM, etc.
Vinay Aseri , born on 28 December 2001, at Churu,
Rajasthan, is currently pursuing BSc (Hons.) Forensic
Science from Vivekananda Global University, Jaipur,
Rajasthan, India. He has participated and presented
his work at more than 15 national and international
conferences and workshops. He has published more
than 10 papers in Scopus indexed journals and
13 book chapters in various national and international
publications, and several papers/chapters are under
progress. He has hands-on experience on a variety of sophisticated instruments
like UV-visible spectrophotometer, IR spectroscopy, SEM microscopy, etc.482 Anubhav Singh et al.Mahipal Singh Sankhla , born on 19 May
1994, in Udaipur, Rajasthan, is currently
working as an assistant professor in the
Department of Forensic Science,
Vivekananda Global University, Jaipur,
Rajasthan. Prior to this, he has served as an
assistant professor in the Department of
Forensic Science, Institute of Sciences,
SAGE University, Indore, MP. He has com-
pleted BSc (Hons.) Forensic Science and
MSc Forensic Science. Currently, he is pursuing PhD in Forensic Science
from Galgotias University, Greater Noida, UP. He has undergone training
at the Forensic Science Laboratory (FSL) Lucknow, CBI (CFSL) New
Delhi, Codon Institute of Biotechnology, Noida, and Rajasthan State
Mines & Minerals Limited (R&D Division), Udaipur. He was awarded
Junior Research Fellowship-JRF, a DST-funded project at Malaviya
National Institute of Technology—MNIT, Jaipur, the Young Scientists
Award for Best Research Paper Presentation at the 2nd National
Conference on Forensic Science and Criminalistics, and Excellence in
Reviewing Award in the International Journal for Innovative Research in
Science & Technology (IJIRST ). He has edited 4 books and published 10 book
chapters in various national and international publications. He has published
more than 120 research and review papers in peer-reviewed international and
national journals. He has participated and presented his research work at more
than 25 national and international conferences and workshops and organized
more than 25 national and international conferences, workshops, and FDPs.
Pritam P. Pandit is currently pursuing his
masters degree in forensic science from
Vivekananda Global University, Jaipur,
Rajasthan. He has completed Post-Graduate
Diploma in Forensic Science and Related
Laws from the Government Institute of
Forensic Science, Aurangabad, with distinc-
tion and has graduated from Rajarshi
Chhatrapati Shahu College, Kolhapur,
Maharashtra, with first class. He has also com-
pleted Maharashtra State Certificate in
Information Technology (MSCIT) with 90%. He has published more than483 Artificial intelligence in edge devices10 papers in Scopus indexed journals and more than 11 book chapters in
various national and international publications, and several papers/chapters
are under progress.
Rushikesh L. Chopade is currently pursu-
ing his masters degree in forensic science
from Vivekananda Global University,
Jaipur, Rajasthan, India. He has completed
Post-Graduation Diploma in Forensic
Science and Related Laws from the
Government Institute of Forensic Science,
Aurangabad, and BSc Chemistry, Botany,
Zoology (CBZ), from GSG College,
Umarkhed, with a first class. He has publi-
shed more than 6 papers in Scopus indexed journals and more than 10 book
chapters in various national and international publishers, and several papers/
chapters are under progress.484 Anubhav Singh et al.CHAPTER SEVENTEEN
5G— Communication
in HealthCare applications
R. Satheeshkumara, Kavita Sainib, A. Danielb, and Manju Kharic
aDepartment of Electronics and Communication Engineering, Galgotias College of Engineering and
Technology, Noida, India
bSchool of Computing Science and Engineering (SCSE), Galgotias University, Delhi, Uttar Pradesh, India
cSchool of Computer and System Sciences, Jawaharlal Nehru University, New Delhi, India
Contents
1.Introduction 486
2.5G—IOT for E-healthcare 486
3.5G—Industrial Internet of Thongs (IIoT) 492
4.5G—Network requirements for healthcare 494
5.5G—Virtual HealthCare 496
6.TeleHealth vs. virtual health 497
7.5G—Remote HealthCare monitoring 498
8.5G—Remote surgery 498
9.5G—Futures and robotics in healthcare 499
10. 5G—Impact on HealthCare 500
11. Conclusion 501
References 502
About the authors 503
Abstract
Every industry will benefit greatly from edge computing enabled by 5G. It moves com-
puting and data storage closer to the point where data is generated, allowing for
improved data control, lower costs, continuous operations, and faster insights and
actions. It is a distributed system in which data is handled as close as feasible to theoriginal data source. This architecture requires the efficient use of resources like as cellphones, tablets, sensors and laptops that are not always linked to a network.
Data transmission is prohibitively expensive. Latency is minimized and end users
enjoy a better experience by bringing computation closer to the source of data. TheInternet of Things (IoT) and Augmented Reality (AR) or Virtual Reality (VR) are twoemerging Edge Computing application cases. The rush that consumers felt whenplaying an Augmented Reality-based Pokemon game, for example, would not havebeen conceivable if the game did not include “real-timeliness. ”Because the smart
phone, rather than the central servers, was doing the AR, it was conceivable.
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.0144851. Introduction
The fifth-generation communication is referred to as 5G. After 1G,
2G, 3G, and 4G mobile communication networks, it is a new global wireless
communication standard. 5G-allows for the creation of a new type of wire-less network that connects nearly everyone and everything, includingmachines, objects, and gadgets.
The 5G of mobile infrastructures expected to built soon for commercial
purpose, supporting to video familiarity via mobile internet, perk up theexcellence of voice and a many of application state of affairs,. The healthcarebusiness is transitioning to an epoch of information and quickness, and more
than a few time-critical applications, such as telemedicine, require 5G
connections at the same time [1].
Healthcare
will be radically different in 10 or 20years. Hospital opera-
tions will be more efficient, clinicians will have access to real-time data to
aid in clinical decision-making, and patient outcomes will improve.
Patients will be able to make better health decisions for themselves, resultingin fewer difficulties later in life.
The 5G network will be exemplify by towering speed of internet and
smart networks. Presently, to download a full film over 4G gets roughly
8min; with 5G, it will gets less than 5s.
The video gaming, high definition and three dimensional (3-D) video,
television interaction, virtual reality, driverless vehicles, robotics, and
manufacturing advancement, will be possible thanks to the speed of internet.
Because there will be billions of devices online, not all data will need to be
transferred at the same time. Some applications necessitate real-time connec-tivity, while some can send data during off-peak hours.
Intelligent networks, which will manage travel and systems design in
ways that promote well-organized direction-finding and decision making,are a major part of defining 5G. However, 5G will introduce intelligenceacross the network, and information analytics will be an integral elementof the service. Real-time aggregation and analysis will be possible with
5G, allowing users to make sense of data and enhance and modify the
capabilities of each application.
2. 5G—IOT for E-healthcare
The recent advancements in big data-oriented wireless technologies,
such as forthcoming 5G, internet of things (IoT) connected devices,486 R. Satheeshkumar et al.information analytics, and edge computing, as well as methodologies,
has allowed the interconnected healthcare services at an advantage and
healthier life.
There is an unprecedented need for linked healthcare, thanks to the rapid
rise of interconnected devices, Internet of Things (IoT) services, apps andwireless technologies. In terms of size, rate, multiplicity, connected
healthcare, and pace, together with linked strategy and sensors, produces
a tremendous number of datasets [ 2]. The massiveness, complexity, and
multidimensionality 
of the wireless big data generated by linked healthcare,
handling it is a tough problem. With a very the highest accuracy, quick reply
time, improved resource effectiveness, and next-version of big data cogni-
zant wireless technology, such as the forthcoming 5G paired with IoT,recommend an important potential to improve the complexity. Althoughthe development of 5G-aware big data analytics, the Internet of Things,
cognitive sensors, and has permitted the factual vision of linked healthcare
to be understand, human emotion recognition is serious for hold emotionalcare to create an enhanced life. The conventional methods of emotional carecommuniqu /C19e may not offer sufficient sensitive get in touch with. It’s diffi-
cult to produce a structure that can gather, recognize, excavation, examine,and method successful information at the side of healthcare big data in anappropriate and precise manner outstanding to the intricacy, dynamicity,and multidimensionality of healthcare services [ 3].
A
cognitive healthcare based on 5G grow to be a practicality with the
increase of IoT and smart homes [ 4]. The healthcare industry is booming
and
has the potential to generate billions of dollars in income. It is as well
supply to the improvement of human life by giving the most up-to-datehealthcare amenities at any time and from anywhere. The growing linked
healthcare has seen enormous growth thanks to the wireless network, edge
computing, and cloud computing technology.
Many countries have constructed smart cities to provide people with
cutting-edge technology. Embedding an emotion recognition module
within the framework is a current trend in healthcare. Some components
in the framework of cognitive are devoted to be familiar with the emotionof user or mind state; after be familiar with the emotion, a service suppliercan distribute an suitable service to the user, or use the emotional reaction toperk up the quality of service in the future [ 1,5]. The framework also
includes
inside and outside placing method for locating the user as necessary.
Consider the case of a client who is over the age of 65. Assume the client
be alive only in a smart home with numerous IoTs. The customer subside tothe floor and is unable to move. The IoTs are constantly gathering signals,487 5G—Communication in HealthCare applicationsand a cloud server uses these data to detect a fall. Caregivers must be dis-
patched to the client by the service provider. A global positioning system
(GPS) guides caretakers to the smart home where the customer lives;though, an accurate inner positioning system is necessary to pinpointprecisely where the customer collapsed. Once the exact site has been deter-mined, the caregivers may hurry there devoid of invading other people’s
privacy.
E-health arose from a demand for better documenting and patient health
tracking and treatments carry out on them, notably for insurance compen-
sation principles. Usually, health care giver keep document report of their
patient’s medical histories and current conditions. Increasing healthcare
expenses and technology advancements, on the other hand, prompted thecreation of electronic following systems [ 6]. The e-health expertise devel-
opment,
a new area called telemedicine arose telecommunication expertise
are used to deliver fitness treatment tenuously.
Consumers benefit from improvements in e-health by being able to
obtain prescriptions over the Internet and have them delivered directly totheir homes. Health care centers and acute care facilities maintain network
pages detailing their proficiency and services for patients. The e-health have
supported community-dwelling people with disabilities by allowing patientcommunication via text, audio, or video conferencing to track home-baseddevelopment.
The new technologies (5G) are expected to explore the difficulties raised
above, as well as performance criteria such as high speed, connection andcapacity density, enhanced reliability, peak throughput per connection,low latency, low power consumption and system spectral efficiency [ 7,8].
5G
wireless technology is designed to provide multi-gigabit per second
hit the highest point information speeds, improved reliability, enormousnetwork capacity, ultra-low latency, increased availability, and a more con-sistent user experience to a larger number of users. Higher performance andefficiency allow for new user experiences and industry connections.
Real-time multicasting, data throughput, ad hoc peer-to-peer, latency,
and data encryption are just a small amount of the benefits of 5G aboveexisting wireless network technologies. Healthcare, with its current IoMTinfrastructure, is a vital area for possibly revolutionary 5G technologies.
The probable of 5G in the sanatorium workflow is already being dem-
onstrated by current research trends. After the COVID-19 epidemic, Zhouet al. [ 9] documented the creation of small house hospitals’ in China, and are
now
constructing a 5G network solution that fix each small house network488 R. Satheeshkumar et al.area to the host via a VPN tunnel erected on pinnacle of a 5G network.
Zhou emphasizes the success of the 5G hospital solution, the benefits that
can only be obtained all the way through this network, and the prospectof joining numerous sanatorium.
Furthermore, Li and Wang [ 6], Stefano and Kream [ 10], and Anwar and
Prasad [11], explain and show the emerging 5G infrastructures for telemed-
icine, 
which have a significant impact on both the speed of growth and the
total reimbursement for healthcare. Furthermore, Alani and Saeed [ 5], and
Mavrogiorgou
et al. [12]highlight the BigData advantages associated with
low-latency,
resilient, ultra-fast, and protected data transmission throughout
healthcare communications using a 5G network. Finally, Mattos andGondim [13]discuss how a correctly developed 5G infrastructure can pro-
vide 
significant prospects for direct machine-to-machine (M2M) connec-
tions in healthcare.
A sequence of alters introduced by “Internet+” and “Smart+” assist the
“intellectualization” of the social order, and this tendency is reproduced inthe healthcare commerce [ 2]. innovative service-oriented system architec-
ture, 
space interfaces, and end-to-end system slicing characterize 5G tech-
nology [ 4,14,15 ], which can become accustomed to the network supplies of
a
variety of applications. In addition, 5G offers powerful technological hold
up for the expansion of 5G healthcare, and smart health checkup applicationsis one of the most important upright industry for 5G technology. In themedical industry, 5G can hold up enormous medical image information
transport and giving out, real-time distant manage of smart devices, and
ultrahigh-definition video communication [ 7,16], and thus can meet the
network
needs of multidisciplinary consultation, remote surgery, intelligent
diagnosis of medical images, and other medical applications [17,18] . The
medical
manufacturing, on the other hand, is grappling with issues such
as centralized medical resources, opaque people, multifaceted informationsystems, and a wide range of diverse medical apparatus. Because many med-ical appliance scenarios need the integration of applications, several medical
devices, and services, 5G confronts numerous hurdles when used in the
medical field.
One of the prospect orientations of the service model of healthcare is 5G
smart healthcare, which slot in several knowledge such as 5G, the Internet ofThings (IoT), fog computing, edge computing, and artificial intelligence.
Innovations in 5G smart healthcare, such as new healthcare service models
and resolution, are based on the idea of mobile healthcare, Internethealthcare and telemedicine. The administrator balance of the R16 edition489 5G—Communication in HealthCare applicationsof 5G will be exposed by the 3rd Generation Partnership Project (3GPP) in
July 2020, representing that the technological answer for 5G smart
healthcare will be particular further. Once the principles and safety suppliesare met, all types of 5G smart healthcare manufacture will be hastened, andmany of them will be more available to the general people. According toIHS Markit, the 5G-enabled physical condition and robustness sector will
be worth more than a trillion dollars.
The medical manufacturing is being improve at a rapid pace, and 5G is
pull towards you a lot of notice from all in excess of the globe. A lot of nation
have leap into the use and extension of 5G smart healthcare, responsibility a
slew of research and progressive carry out in the aim and completion of
5G healthcare system and apps [19]. Verizon, for instance, said in the second
half
of 2018 that it would roll out a 5G profitable wireless system and 5G
core network in the United States. In October 2018, China Mobile and
Zhengzhou University’s First allied hospital exposed that doctors could
do inaccessible ultrasound surgeries and real-time analysis via the 5G system.In February 2019, Vodafone and hospital linked on 5G-based distant sur-gical procedure examination mean to create Spain’s first 5G smart hospital.
In June 2019, the University hospital of Birmingham in the United
Kingdom demonstrate the feasibility of 5G ambulance and secluded ultra-sonography process in collaboration with British Telecom and WestMidlands 5G (WM5G).
In October 2019, Huawei, China Mobile, and Zhengzhou University’s
first affiliated hospital issued a white paper on a 5G healthcare system basedon flexible network segment. A COVID-19 immunity detection and anal-ysis system based on 5G cloud computing in April 2020, released by SichuanUniversity and China Mobile where a 5G distant CT scan was employed in
COVID-19 preventive system.
With 5G smart healthcare, a range of new or better medical appliance
situation have emerged. Virtual surgery, remote consultation, emergency
rescue, remote teaching, and monitoring, for example, have all been fast
developed using 5G equipments [15,17,20] . While these application situa-
tion
get better the quality of the patient experience and health services, they
also introduce new security risks. The security element of 5G healthcare will
cooperate key roles in national, and information security [21], because med-
ical
information contains a lot of user seclusion and some medical appliance
situation damage patient protection.
5G technologies is a new network that be different from 4G and 3G and
it enables mobile Internet of things by combining considerable processing490 R. Satheeshkumar et al.capabilities with a virtual computing environment (mIoT). The severe sup-
plies in healthcare appliances to offer flexible, cost-effective, and reliable ser-
vices have been the subject of ongoing study. 5G promises to boost thehealthcare industry by extending service options to a variety of healthcaredomains, including healthcare professionals, caregivers, and patients.
According to analysts, the healthcare business is booming because it
accounts for a considerable portion of national GDP, with figures rangingfrom 6% in China to 10% in Europe and 18% in the United States.E-Health curve on a common hype since the early 2000s, with high aspira-
tions of rapid improvement in experience quality, service quality, and afford-
able savings in health care [22]. In its most current study on e-Health, the
W
orld Health Organization (WHO) believes that “there is a need for stronger
political commitment for e-Health, underpinned by sustainable finance, and
for effective policy implementation” [16].
This
can be performed by employing millimeter wave, tiny cell,
beamforming, full-duplex, and other techniques [23]. Unlike earlier net-
work
technologies, 5G uses three distinct spectrum bands: lower-range fre-
quency band, middle range frequency band, and high range frequency band.
This not only enhances the network’s capacity but also allows even the tiny
devices to execute high calculation and attach fast to the system’s dispersedhanding out control [24].
The 
number of linked devices it maintain, exceptionally low latency, and
network speed are the features that set 5G apart from its predecessors.
With the help of new technology, IoT can give high-quality solutions.
In the sphere of medicine, it becomes a new reality of an original idea thatprovides the best service to COVID-19 patients and executes precise surger-ies. Complicated problems are easily managed and digitally controlled dur-
ing the current epidemic [22]. In the field of medicine, the Internet of
Things
tackles new challenges in developing effective support systems for
physicians, patients and surgeons. For effective IoT deployment, the various
process phases are carefully identified.
Remote surveillance became possible in the health sector thanks to
IoT-enabled technology, which opened up the possibility of healthy pre-serve patients, empowering clinicians to give superior care and security. Italso enlarged patient contribution and approval by making interaction withclinicians more convenient and efficient. Furthermore, remote health mon-
itoring helps to cut down on healthcare wait and re-admissions. In
healthcare, IoT has a substantial impact on cost reduction and enhancedtherapeutic benefits [16]. Without a doubt, the Internet of Things (IoT)491 5G—Communication in HealthCare applicationsalters the healthcare industry by expanding the reach of devices and people’s
connections to solutions. IoT recommend healthcare appliances that benefit
families, doctors, patients, insurance businesses, and hospitals.
Robots that are connected to the internet of things are flattering further
general. It’s used to clean medical cent er, disinfect healthcare equipment,
distribute medications, giving doctors and nurses more time to focus on
their patients. China is the first country for example, to use UVD robots
from a Danish business to clean its medical services during the emer-gency. These IoT-enabled robots [25] help disinfect treatment area and
clean 
rooms.
The all knot of wires tied to a patient is not convenient for the patient,
then results in limited movable, and greater than before nervousness but it isalso difficult to handle for the staff. Purposeful disconnections of sensors byexhausted patients, failures to correctly reattach sensors, and as patients are
encouraged around in a hospital are fairly common. Wireless sensing gear
that is less apparent and has a constant network connection to back-endmedical record systems helps to eliminate knots of wires with patientconcern while also reduces the errors [25].
3. 5G—Industrial Internet of Thongs (IIoT)
To move beyond conventional networks of mobile, safe system access
point must be disseminated, intelligence and allowing smooth the tiniestlinked sensors to be empowered. With a mixture of applications assembledabout a variety of types of “smart” devices with sensors, and machine learn-
ing models, the IIoT has the probable to make over omnipresent computing.
The smart devices make bigger our understanding and awareness of theglobe as an agitated-connected environment, posturing fresh challenges inpreparing the IIoT for extensive deployments. By combining 5G networks
for connectivity with external devices, data conversion, digital processing,
and spectrum detection. 5G is projected to be more than just a latest mobilecommunications production. Instead, it is previously being referred to at thesame time as the “unifying fabric” will join billions of procedure in a few of
the most efficient, speedy ways possible, and dependable. Allowing technol-
ogy is expected to have an innovatory influence.
The novel communication is intended to change industries and revolu-
tionize the global of linked sensors. Of course, such a revolution would
necessitate research and development to ensure sensor cohabitation
and device interoperability with 5G networks. Integrated IIoT is a smart492 R. Satheeshkumar et al.communication network that can learn about its users and their surround-
ings and utilize that information to assist them to attain their targets in a
context-based way.
This significantly get better quality of user life while also assisting in the
optimization and control of rapidly growing resource expenditure rates in
IIoT framework. Services, or systems, people, devices that are involved with
smart enabling technologies such as radio frequency identifications (RFIDs),
sensors, 5G Smartphones, and other applications in our daily lives are con-
sidered inhabitants (users) of IIoT settings.
E-Healthcare is a significant application for 5G-based IIoT, as it attempts
to keep a medical information patient in electronic environments such as the
Cloud using cutting-edge communications paradigms. Wireless medicinal
sensor networks (WMSNs) have become a popular technology for wireless
sensor networks (WSNs) in healthcare application system [16].
For early diagnosis, hospitals have created a variety of therapeutic
healthcare sensors that are used to sense the patient body and record fitness
data such as temperature, heart rate, and blood pressure [25]. These
healthcare data are also disseminated via professional handy devices, such
as a personal data analyzer (PDA) and a Smartphone, for additional analysis.
WMSNs have been used in patient health monitoring by several medical
research communities [1,22 –24]. As a result, a user authentication strategy
is required to safeguard the medical information system from unauthorized
access (Fig. 1) .
5GEducationVirtual Reality /
Augmented 
RealityTele HealthCare
E- Government Smart city Retail and 
supply chain
Media IOT
Fig. 1 Applications of 5G.493 5G—Communication in HealthCare applicationsTechnological advancements are transforming the globe about us, but
they are also propelling the wireless sector to increase the next age band
of communication network technology. Mobile technologies such as 1G,
2G, 3G, and 4G have concentrated on improving the rate and competence
of wireless networks during the last 25years, but there are still a few appli-
cation areas anywhere recent wireless networks move violently to supply.
Utility applications, wireless healthcare services, V2X communication
(especially Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I)
communication), and industrial automation, consumer, augmented reality
and virtual reality services, and primary broadband access services, among
others, are among these areas [ 26]. 5G applications are currently being used
in a variety of industries, including energy [23–25], smart vehicle parking
system [ 5], intellectual station area detection technology [21], medicare
[15,25] , and so on.
4. 5G—Network requirements for healthcare
Healthcare scenarios appear to have larger and more essential network
requirements than scenarios such as highway traffic supervision, smart cul-
tivation, and so on. The network’s role in the healthcare environment is
depicted in Fig. 2 using a conceptual diagram. The patient’s ability to contact
his caretaker, or any other emergency agency would necessitate continuous
Fig. 2 Network ’s role in the healthcare environment.494 R. Satheeshkumar et al.connection and doctor. Doctors or caregivers on the other hand, would be
required to respond to the appeal of client instantly, which would necessitate
recover information from network servers.
The instance it takes to regain information and respond to the long-
suffering is crucial. Monitoring in Mass-Casualty Disasters: While emer-gency medical triage techniques exist [20,21] their effectiveness might
quickly 
deteriorate as the number of victims grows. Furthermore, during
mass-casualty events, it is necessary to improve the assessment of firstresponders’ health.
Wireless sensing systems’ greater portability, scalability, and quick
deployment capabilities can be utilized to more effectively report the triagelevels of multiple victims and track the health status of first responders at thecrisis scene [11].
There
are two types of healthcare scenarios: “the one who is cared for”
and “the one who is cared for by.” The user or patient using the systemwould go under the “Cared for” group, while medical care professionalssuch as insurance providers, doctors and nurses, and so on would fall underthe “Cared by” category [ 6].
Since
it permits health information to be supplely accumulated and com-
mon among many entities, electronic healthcare (e-health) has gotten a lot ofattention [ 7].E-health
systems that use 5G networks may handle a wide
range of applications while maintaining high information security, reliabil-ity, and seamless medical information transfer. Remote monitoring for tele-
medicine in real-time and the transfer of massive health information files are
two prominent use cases. Healthcare practitioners may watch patients ten-uously and meet real-time data without worrying about disconnections, net-work outages, or lag time, thanks to 5G’s ultra-low latency [ 8,15]. It makes
preventative
care and other custom-tailored healthcare more accessible.
Huge information files in the systems can be professionally transmittedbetween users and service providers for well-organized patient treatmentsthanks to the elevated crest information rates provided by 5G network.
Huge picture files generated by Computerized Tomography (CT) or
Magnetic Resonance Imaging (MRI) scans, for example, can be swiftly deliv-ered to specialists for review [ 4,22]. Furthermore, the combination of 5G
an
d e-health systems would allow for the effective construction of smart
healthcare and the Internet of Medical Things (IoMT) [18,19,27] . Many
m
embers of a family unit can use general healthcare services, such as genetic
testing [11], in addition to accessing personage services. They will be able to
ke
ep track of the health state of their family members in this manner.495 5G—Communication in HealthCare applicationsThe services will considerably improve the long-term benefits of medical
treatments because families play an essential role in promoting health and
minimizing the risk of illness [ 6]. Healthcare panel at sanatoriums, healthcare
so
ciety, and crisis care centers can also provide group health services. The
entire treatment would be improved as a result of the healthcare teams’ jointefforts [ 8,9].
Patients, 
physicians, pharmacists, medical researchers, caretakers, and
others are among the e-health users in a 5G-permited healthcare location[9,27]. Healthcare suppliers (e.g., hospitals), data center administrators or
medical 
experts who provide services for specific consumers should be
the servers [ 8]. Because users and servers communicate over the open
Internet,
their sensitive information may be vulnerable to a variety of
assaults. As a result, the security and privacy of e-health systems are critical
[1]. It necessitates a comprehensive process capable of preventing potential
security
issues. Many studies have proposed protected password based or
two-part (password plus smart tag) authentication solutions to solve securityconcerns [ 3,11]. These aren’t, however, long-term solutions. For example,
with 
two-factor authentication schemes, if the adversary knows password of
the user, it can easily carry out assaults with a stolen smart card.
The entities must accumulate large qualifications (e.g., unique id, pass-
words) for receiving a rising numeral of medical care services with asingle-server architecture defined in several works [28].
5. 5G—Virtual HealthCare
The “virtual visits” that get place between patients and healthcare
expert via communications technology the Audio and video connectivitythat permits “virtual” gathering to take position in real time, as of nearlyevery position are referred to as virtual healthcare.
A videoconference between a doctor and a patient at home can be con-
sidered a virtual visit. It might imply that instead of flying to another city, apatient can connect with an offshore health check expert passing through ahigh quality conference hookup at his or her local hospital. It may also make
it easier for patients to locate experienced second observation online.
Virtual healthcare has thus far primarily been utilized for consultations,
status reporting, check-ins, meeting and rather than in-depth diagnosis or
treatment. Even Nevertheless, as expertise advances, extra severe disorderslike diabetes are becoming more accessible through virtual healthcare.496 R. Satheeshkumar et al.Virtual healthcare also makes it easier for specialists to keep an eye on
problems or operations from afar. Patient monitoring at home has also been
found to be effective in the treatment of patients with continual sicknessessuch as cardiac problem, hypertension, and diabetes where rehospitalizationsare all too common due to a lack of communication or transparency regard-ing the patient’s health.
6. TeleHealth vs. virtual health
Virtual healthcare, like remote patient management, is commonly
confused with telemedicine, or telehealth although they are not the similarobsession [21]. Telehealth is a larger word that encompasses any distant and/or
t 
echnology-driven healthcare. Virtual healthcare is a subset of telehealth.
Telehealth technology might take the form of a phone, videoconferenc-
ing capability, or an interactive voice response system.
Telemedicine is a term that refers to the management of various
healthcare problem with no having to observe the patient in person. Toaddress a concerns of patient and assess their state remotely, healthcare pro-viders can use telehealth stages such as live vision, live audio, or instant mes-saging [23]. Getting health advices, guide patients via at-home workout, or
referring
them to a nearby professional or ability are all examples of this.
Even more thrilling is the rise of telemedicine mobile apps, which allowpatients to receive care directly from smartphones or tablets [28].
VirtualHealth 
has continuously been acknowledged as the leading pro-
vider of medical management and related technology on a national level
[29]. We have strong expertise in value-based care and count the majority
of
the country’s largest health plans among our clients, which matches
exceptionally well with our objective to make healthcare more proactive.
Virtual healthcare is a wide expression that refers to all of how healthcare
suppliers communicate with their patients over the internet. In adding up to
care for patients via telemedicine, doctors can communicate with themremotely using live video, audio, and instant messaging.
Telemedicine has reached a critical juncture (also known as telehealth).
Doctor-patient contacts have largely been face-to-face in health check work-place, houses, hospitals, and care facilities for hundreds of years. Medical tech-nology, as well as speedier networks such as 5G, are transforming the featuresof healthcare.
Telemedicine has been progressing at a steady pace. The emergence of
wearable medical check devices to observe essential signs has been497 5G—Communication in HealthCare applicationsparticularly striking [12]. Although the concept of doctors visiting patients
through
the Internet is enticing, it has yet to be widely implemented.
The COVID-19 epidemic, however, has changed that. Health institu-
tions are progressively more implementation tele medical care technology
to help patients to keep patients and medical workers safe [1]. As a result,
the
telemedicine sector is expanding rapidly over the world. Medical equip-
ment manufacturers are scrambling to keep up with demand. Furthermore,the rise of telehealth and other healthcare activities is being fueled like neverbefore by today’s 4G LTE and upcoming 5G wireless technology [19].
Extensive
cellular system availability, easiness and pace of deployment,
and greatly better bandwidth are all central technical portion.
7. 5G—Remote HealthCare monitoring
In the last few years, medical electronics have advanced significantly.
A silicon platform is available from Maxim Integrated for device makers thatare developing products to monitor common vital signs. Stress, Pulse/heart
rate, electrocardiogram (EKG/ECG), blood oxygen (SpO2), body tempera-
ture, and UV light are all examples of these (skin exposure) [27]. With built-in
GP 
S and cellular, Apple’s new 6 Series smartwatch can measure blood oxygen
and ECG. Fitbit has unveiled its Sense wristwatch, which uses a Wi-Fi con-nection to measure skin temperature, blood oxygen, ECG, and stress.
To save money on healthcare, sanatorium are attempting to release
patients as soon as their condition has stabilized, such as those suffering fromheart attacks. Hospitals will use wireless technologies to tenuously verifyhealth of patients health problems after they have been discharged.
Smartwatches are valuable for monitoring the health of wearer regularly.
8. 5G—Remote surgery
While the majority hospitals have experienced surgeons on staff, they
rarely have specialists in every surgical area of expertise [18]. Frequently,
they
will seek the advice of doctors with specific knowledge. Video and
audio have been used by experts in specialized medical specialties to assist
the surgical team throughout the procedure.
As medical technology progresses, many and more surgeries are reliant
on it. A little surgical microscope, for example, can be put into the body
and, with the help of a high-definition 4K monitor like the Olympus
Visera, vividly depict the specifics of the area to be operated on.498 R. Satheeshkumar et al.With a 5G network connection, the same data that the surgical team sees
may be live-streamed in real-time to the behind specialist team, and in the
future, in even greater 8K quality. The experts will be intelligent to advise
the team in real-time as the surgery takes place thanks to this real-time
connection.
The first 5G remote brain surgery has been completed by doctors. The
patient had Parkinson’s disease and was over 1500km distant. During a
three-hour procedure, the patient received a cavernous brain stimulation
embed. Dr. Ling Zhipei performed the pioneering surgery using a computer
powered by China Mobile and Huawei’s 5G network to manipulate instru-
ments in Beijing from his position in Sanya City.
9. 5G—Futures and robotics in healthcare
We have merely scratched the surface of what 5G can accomplish for
telehealth and healthcare. It will enhance patient remote monitoring, home
medical care, and surgical procedures. 5G will improve the way emergencies
are handled in smart cities. The dangerous information will be available in near
real-time, allowing team to react much faster and provide better treatment.
Many of these choices and functions are likely to be undertaken by AI in
the future. To increase precision, machine learning, artificial intelligence,
and robotics can play a larger part in procedures [12].
In addition, carers and emergency teams will have more time and will be
better prepared to deal with unforeseen emergencies such as sickness, acci-
dents, and natural catastrophes [17].Fig. 3 shows 5G enabled robotic
healthcare environment.
Fig. 3 5G enabled robotic healthcare environment.499 5G—Communication in HealthCare applicationsThe concept of remote surgery is based on the exchange of medical data.
Medical data is digitized and transmitted via cable or wireless telecommu-
nication networks, including images, audio, and video. Surgeons can usethe networks to control the surgical robot and execute surgeries from afar
[11,22] . The main barriers to real-time remote surgery have been system
slowness 
and network instability. The latest revolution of the 5th generation
wireless infrastructure (5G) has made remote surgery a reality. The 5G net-work provides exceptional high-speed, low-latency, and high-bandwidthcapabilities [ 30].
Many
experts believe that 5G will transform healthcare by giving firms
more connection power and higher broadband speeds. Remote robotic-assisted surgery is one area where 5G could be very revolutionary.
According to a paper by Fitch Solutions Macro Research, a unit of
Fitch Group, doctors might potentially undertake remote, global treat-
ments that are currently impossible. Robot-assisted surgery is still a popular
alternative to traditional surgery because it offers more precision andaccuracy [ 2].
10. 5G—Impact on HealthCare
Furthermore, the utilization of 5G technology has the potential to
protect quality while also lowering overall medical expenses.1.The use of various medical related sensors and remote patient healthmonitoring
equipment to aid patients living in rural places have access
to top medical support are just a few instances. Using video conferencing
or telemedicine to bridge the geographic distance and provide high-
quality care to underserved populations can help bridge the gap.
2.Point-of-care testing (POCT), a relatively new technology, can savemoney
by keep away from the costly hospital visits. Patients can use
mhealth technology, digital platforms, or remote monitoring equipmentinstead of visiting a huge medical center. By 2018, the POCT market isexpected to be worth $27.5 billion. 36 These devices improve patient
accessibility by bringing technology to the patient’s bedside or home.
3.Health therapies from home are a technique of providing larger qualityhealth
care to patients without requiring them to travel long remoteness
to healthcare of hospitals facilities. It can send medical informationelectronically and have health center offer diagnosis and treatment
suggestions from afar.
4.In many areas, diabetes is a big issue. The condition of Mississippi
exposed
that 13% of its people have diabetes, with 54% of those populace500 R. Satheeshkumar et al.living in rural areas with inadequate access to appropriate care. Health
check authorities, on the other hand, saw cost savings of $339,184 for
around 100 patients participating in the program and predictedMedicaid savings of $189 million per year after establishing a Diabetes
Telehealth Network with remote healthcare services.
5.Health IoT can keep expenses down and save money while maintaining
high-quality treatment by keeping people out of hospitals. Not every
medical issue necessitates a trip to the hospital. The routine concernscan be diagnosed remotely, providing patients with more options thantraditional treatment.
6.Using voice recognition software to automate administrative tasks cansave time and money. According to a study of technology in sanato-riums, it allowed doctors to “provide care without being stopped by datainflowing and querying tasks.” 41 People may record medical informa-
tion without stop and enter data thanks to the program.
In a nutshell, 5G technologies connect devices so that smarter and faster
decisions can be made. These data enable caretakers and policymakers tohave real-time awareness of people, diseases, and symptoms, allowing
them to create new insights. Interoperable gadgets operate in tandem with
intelligent notification systems to guarantee that each patient receives thebest possible care.
11. Conclusion
Cost savings alone can be a motivator for many businesses to imple-
ment edge computing. Companies that used the cloud for many of their apps
may have discovered that bandwidth expenses were greater than anticipatedand are looking for a less expensive alternative. Edge computing could be agood option.
Carriers all around the world are introducing 5G wireless technologies,
which promise tremendous bandwidth and low latency for apps, allowingbusinesses to scale their data capacity from a garden hose to a firehose.Rather than simply providing faster bandwidth and instructing businesses
to process data in the cloud, many carriers are including edge-computing
technologies into their 5G installations to provide speedier real-timeprocessing.
While the original purpose of edge computing was to lower bandwidth
costs for IoT devices across long distances, it’s apparent that the emergence
of real-time applications that demand local processing and storage will con-tinue to propel the technology ahead in the future years.501 5G—Communication in HealthCare applicationsReferences
[1] K. Saini, P. Raj, “Handbook of Research on Smarter and Secure Industrial Applications
Using AI, IoT, and Blockchain Technology”, IGI Global, 2021, ISBN13:
9781799883678, ISBN10: 1799883671, EISBN13: 9781799883685.
[2] K. Saini, V. Agarwal, A. Varshney, A. Gupta, E2EE for data security for hybrid cloud
services: a novel approach, in: IEEE International Conference on Advances in
Computing, Communication Control and Networking (IEEE ICACCCN 2018)
Organized by Galgotias College of Engineering & Technology, Greater Noida,2018, pp. 12 –13,https:/ /doi.org/10.1109/ICACCCN.2018.8748782 .
[3]D.N. Le, R. Kumar, B.K. Mishra, J.M. Chatterjee, M. Khari, (Eds.), Cyber Security in
Parallel
and Distributed Computing: Concepts, Techniques, Applications and Case
Studies, John Wiley & Sons, 2019.
[4]M. Chen, J. Yang, Y. Hao, S. Mao, K. Hwang, A 5G cognitive system for healthcare,Big
Data Cogn. Comput. 1 (1) (2017) 2.
[5]M.M. Alani, H. Tawfik, M. Saeed, O. Anya, Applications of Big Data Analytics:
Trends,
Issues, and Challenges, vol. 219, Springer, 2018.
[6] E.-L. Li, W.-J. Wang, 5G will drive the development of health care, Chin. Med. J. 132
(23)
(2019) 2895 –2896, https:/ /doi.org/10.1097/CM9.0000000000000534 .
[7]S.A.A. Shah, E. Ahmed, M. Imran, S. Zeadally, 5G for vehicular communications,IEEE
Commun. Mag. 56 (1) (2018) 111– 117.
[8] S. Ullah et al., “UAV-enabled healthcare architecture: issues and challenges,” Futur.
Gener.
Comput. Syst., vol. 97, pp. 425– 432, 2019. [Online]. Available: http:/ /www.
sciencedirect.com/science/article/pii/S0167739X18318247 .
[9] B. Zhou, Q. Wu, X. Zhao, W. Zhang, W. Wu, Z. Guo, Construction of 5G
all-wireless
network and information system for cabin hospitals, J. Am. Med. Inform.
Assoc. 27 (6) (2020) 934– 938, https:/ /doi.org/10.1093/jamia/ocaa045 .
[10] G.B. Stefano, R.M. Kream, The micro-hospital: 5G telemedicine-based care, Med. Sci.Monit.
Basic Res. 24 (2018) 103.
[11] S. Anwar, R. Prasad, Framework for future telemedicine planning and infrastructure
using
5G technology, Wirel. Pers. Commun. 100 (1) (2018) 193 –208, https:/ /doi.
org/10.1007/s11277-018-5622-8 .
[12] A. Mavrogiorgou, A. Kiourtis, M. Touloupou, E. Kapassa, D. Kyriazis,
M.
Themistocleous, The road to the future of healthcare: Transmitting interoperable
healthcare data through a 5G based communication platform, in: Proc. Eur., Medit.,Middle Eastern Conf. Inf. Syst., in Lecture Notes in Computer Science, 341, 2019,pp. 383– 401, https:/ /doi.org/10.1007/978-3-030-11395-7_30 .
[13] W.D. de Mattos, P.R.L. Gondim, M-health solutions using 5G networks and M2M
communications,
IT Prof. 18 (3) (2016) 24 –29,https:/ /doi.org/10.1109/MITP.
2016.52 .
[14] K. Ahmad, A. Kamal, K.A.B. Ahmad, M. Khari, R.G. Crespo, Fast hybrid-MixNet forsecurity
and privacy using NTRU algorithm, J. Inform. Security Appl. 60 (2021),
102872.
[15] A. Ksentini, P.A. Frangoudis, Toward slicing-enabled multi-access edge computing in
5G,
IEEE Netw. 34 (2) (2020) 99 –105.
[16] C. Qingping, H. Yan, C. Zhang, Z. Pang, L. Xu, A reconfigurable smart sensor inter-
face
for industrial WSN in IoT environment, IEEE Trans. Ind. Informat. 10 (2) (2014)
1417 –1425.
[17] J. Ordonez-Lucena, P. Ameigeiras, D. Lopez, J.J. Ramos-Munoz, J. Lorca, J. Folgueira,Network
slicing for 5G with SDN/NFV: concepts, architectures, and challenges, IEEE
Commun. Mag. 55 (5) (2017) 80 –87.
[18] A. Vergutz, G. Noubir, M. Nogueira, Reliability for smart healthcare: a network slicing
perspective,
IEEE Netw. 34 (4) (2020) 91 –97.502 R. Satheeshkumar et al.[19] R. Khan, P. Kumar, D.N.K. Jayakody, M. Liyanage, A survey on security and privacy of
5G technologies: potential solutions, recent advancements, and future directions, IEEE
Commun. Surveys Tuts. 22 (1) (2020) 196 –248. 1st Quart.
[20] D. Rupprecht, A. Dabrowski, T. Holz, E. Weippl, C. P €opper, On security research
towards future mobile network generations, IEEE Commun. Surveys Tuts.
20 (3) (2018) 2518 –2542. 3rd Quart.
[21] A. Akhunzada, S.U. Islam, S. Zeadally, Securing cyberspace of future smart cities with
5G technologies, IEEE Netw. 34 (4) (2020) 336 –342.
[22] B. Karschnia, “Industrial Internet of Things (IIoT) benefits, examples control
engineering,” Control Engineering 2017. Accessed on Aug. 31, 2017. [Online].
Available: http:/ /www.controleng.com/single-article/industrial-internet-of-things-
iiot-benefits-examples/a2fdb5aced1d779991d91ec3066cff
40.html .
[23] S.R. Jena, R. Shanmugam, R. Dhanaraj, K. Saini, Recent advances and future research
directions in edge cloud framework, Int. J. Eng. Adv. Technol. (IJEAT) 2249-8958, 9
(2) (2019), https:/ /doi.org/10.35940/ijeat.B3090.129219 .
[24] W. Chung, C. Yau, K. Shin, A cell phone based health monitoring system with
self-analysis processing using wireless sensor network technology, in: Proc. Int. Conf.
IEEE Eng. Med. Biol. Soc., 2007, pp. 3705 –3708.
[25] R. Satheeshkumar, R. Arivoli, Real time virtual human hand for diagnostic robot
(DiagBot) arm using IOT, J. Adv. Res. Dyn. Control Syst. 12 (Special Issue 1) (2020).
[26] A. Keller, The Road to 5G: Drivers, Applications, Requirements and Technical
Development, Ericsson, Huawei, and Qualcomm, Global Mobile Suppliers Assoc.,
Washington, DC, USA, 2015.
[27] R. Gupta, S. Tanwar, S. Tyagi, N. Kumar, Tactile-internet-based telesurgery system for
healthcare 4.0: an architecture, research challenges, and future directions, IEEE Netw.
33 (6) (2019) 22 –29.
[28] E. Gilman, D. Barth, Zero Trust Networks: Building Secure Systems in Untrusted
Networks, O’Reilly Media, Sebastopol, CA, USA, 2017.
[29] I. Ahmad, S. Shahabuddin, T. Kumar, J. Okwuibe, A. Gurtov, M. Ylianttila, Security
for 5G and beyond, IEEE Commun. Surveys Tuts. 21 (4) (2019) 3682 –3722. 4th Quart.
[30] A. Banchs, D.M. Gutierrez-Estevez, M. Fuentes, M. Boldi, S. Provvedi, A 5G mobile
network architecture to support vertical industries, IEEE Commun. Mag. 57 (12)
(2019) 38 –44.
About the authors
R. Satheeshkumar (1984) was born in
manalmedu town, Nagapattinam, Tamilnadu,
India. He received Diploma in Computer
Technology (2003) from the Directorate of
Technical Education, Tamilnadu, India. He
received a B.E. degree in Electronics and
Communication Engineering (2007) from
Anna University, Chennai, India. He received
M.E. degrees in Electronics and Control
Engineering (2013) from the Sathyabama
University, Chennai, India. He was an503 5G—Communication in HealthCare applicationsEmbedded Software Engineer with the Orbit Controls and Services from
2007 to 2010. He was an Embedded Engineer with the Lumisense
Technologies from 2010 to 2011. He was an Embedded Engineer with
Porus Technologies from 2013 to 2015. At present Ph.D. thesis submitted
in Robotics under the Department of Electrical Engineering in Annamalai
University, Annamalai Nagar, India. His area of interest is embedded systems,
RTOS, and Robotics.
Kavita Saini is presently working as
Professor, School of Computing Science and
Engineering, Galgotias University, Delhi
NCR, India. She received her Ph.D. degree
from Banasthali Vidyapeeth, Banasthali. She
has 18 years of teaching and research experi-
ence supervising Masters and Ph.D. scholars
in emerging technologies.
She has published more than 40 research
papers in national and international journals
and conferences. She has published 17
authored books for UG and PG courses for
a number of universities including MD University, Rothak, and Punjab
Technical University, Jallandhar with National Publishers. Kavita Saini
has edited many books with International Publishers including IGI
Global, CRC Press, IET Publisher Elsevier and published 15 book chapters
with International publishers. Under her guidance many M.Tech and Ph.D.
scholars are carrying out research work. ( kavitasaini_2000 @yahoo.com )
She has also published various patents. Kavita Saini has also delivered
technical talks on Blockchain: An Emerging Technology, Web to Deep
Web and other emerging Areas and Handled many Special Sessions in
International Conferences and Special Issues in International Journals.
Her research interests include Web-Based Instructional Systems (WBIS),
Blockchain Technology, Industry 4.O, and Cloud Computing.504 R. Satheeshkumar et al.Dr. A. Daniel is currently working as
an Associate Professor in School of
Computing Science and Engineering in
Galgotias University, Greater Noida, Uttar
Pradesh. He completed his B.E and M.E both
in Anna University. He has completed his Ph.
D. in Computer Science and Engineering at
Shri Venkateshwara University, Uttar
Pradesh. His research interests are Cloud
Computing, Networking etc. He has publi-
shed several articles in reputed international
journals. He has membership in IEEE,
ACM, IFERP, IAENG and CSTA.
(danielarockiam @gmail.com )
Dr. Manju Khari is an Associate Professor in
Jawaharlal Nehru University, New Delhi,
prior to the university she worked with
Netaji Subhas University of Technology,
East Campus, formerly Ambedkar Institute
of Advanced Communication Technology
and Research, Under Govt. Of NCT
Delhi. Her Ph.D. in Computer Science
and Engineering from National Institute of
Technology Patna and She received her mas-
ter’s degree in Information Security from
Ambedkar Institute of Advanced Communi-
cation Technology and Research, affiliated with Guru Gobind Singh
Indraprastha University, Delhi, India. She has 80 published papers in refereed
National/International Journals and Conferences (viz. IEEE, ACM, Springer,
Inderscience, and Elsevier), 10 book chapters in a Springer, CRC press, IGI
Global, Auerbach. She is also co-author of two books published by NCERT
of XI and XII and co-editor in 10 edited books. She has also organized
05 International conference sessions, 03 Faculty development Programme,
01 workshop, 01 industrial meet in her experience. She delivered an505 5G—Communication in HealthCare applicationsexpert talk, guest lecturers in International Conference and a member of
reviewer/technical program committee in various International Conferences.
Besides this, she associated with many International research organizationsas Associate Editor/ Guest Editor of Springer, Wiley and Elsevier books, anda reviewer for various International Journals.506 R. Satheeshkumar et al.CHAPTER EIGHTEEN
The integration of blockchain
and IoT edge devices for smartagriculture: Challenges
and use cases
Swati Nigama, Urvashi Sugandha, and Manju Kharib
aDepartment of Computer Science, Faculty of Mathematics and Computing, Banasthali Vidyapith,
Banasthali, India
bSchool of computer and System Sciences, Jawaharlal Nehru University, New Delhi, India
Contents
1.Introduction 508
1.1 Requirement of IoT in agriculture 509
1.2 Requirement of blockchain in agriculture 511
2.Blockchain technology: An overview 512
3.Working of blockchain 514
4.IoT: An overview 515
5.Working of IoT 515
6.Edge computing: An overview 517
7.A proposed model for smart agriculture using blockchain and IoT 518
8.Advantages of blockchain, edge computing and IoT based agriculture 523
9.Summary of the research for applying blockchain and IoT in agriculture 524
10. Challenges and open issues 529
11. Conclusion 530
References 531
About the authors 535
Abstract
Growth of IoT (Internet of Things) has proved its importance in the various sector. Butdue to some limitations of security, privacy, etc. it is not possible to use IoT devices in the
field of agriculture at its fullest. To overcome these limitations Blockchain is used as
it provides security, privacy and it helps in monitoring, examining, to authenticatethe agriculture data. With the help of Blockchain, traditional methods of collection, rear-ranging and distribution of Agri-products can be replaced by more trust-worthy,decentralized, vitreous, and immutable style. In agriculture sector, Blockchain andInternet of things can be amalgamated to have better results which leads us to onelevel up in the field of agriculture and may control or improve the supply chain in proper
Advances in Computers , Volume 127 Copyright #2022 Elsevier Inc.
ISSN 0065-2458 All rights reserved.
https:/ /doi.org/10.1016/bs.adcom.2022.02.015507manner. The consequences of using blockchain and IoT in combination will result in
better understanding to supervise and managing the agriculture effectively. This chap-
ter will illustrate the importance of using blockchain and IoT collectively to developsmart agriculture from traditional agriculture. A model is also proposed to overcomethe challenges encountered in agriculture sector, based on IoT applications with thehelp of blockchain. Also, a review is mentioned about the main characteristics and func-tions of blockchain used in agriculture sector such as livestock grazing, crops andfood supply chain. Finally, some of the open issues Blockchain and security challengesare elaborate.
1. Introduction
Many limitations have been overcome due to the continuous growth
and development of Internet of things (IoT). With the implementation of
IoT, various devices can share the data or information to other devices with-out using wired network. IoT may be defined as the interconnection of the
devices for collecting and exchanging the information among themselves
[1]. IoT may be identified as an infrastructure consists of gateways and sen-
sors.
With the help of sensors and gateways it, IoT devices create a network
and can communicate with the surroundings any time to give rise the massproduction of various data resources such as audio-video, images, etc. IoT
has find out its application in various fields which belongs to our day-to-dayactivities such as in hospitals, households, universities, and agriculture [2].
Agriculture
sector uses IoT to convert the farming techniques into
smart farming techniques. Basically, IoT devices operated by various orga-
nizations through which data is first collected, then distributed as per need,
data is stored, analyzed by the experts and then proper action will be taken
[3,4]. In smart agriculture large number of IoT nodes are placed in wide
range
resulting in large amount of data sharing, consequently, it gives rise
to network traffic, latency issues, high energy consumption, etc. Due toreasons only limited or essential data could be collected instead of the com-plete data. Also, the mentioned limitations signifies that there is no existenceof communication in between the nodes, excepting for some defined and
conditions.
Internet of things technologies also help farmers with precision farming.
It used to be that farmers had to map the entire landscape of an area and then
plot their plots into precise dimensions, which required using sophisticated
equipment [5,6]. Now, even the most intricate forms of farming can be
monitored
digitally by modern devices. Irrigation systems can map precise508 Swati Nigam et al.areas where water conservation is most critical, for example. High-resistance
grass seed used in place of traditional turf, or modified tillage techniques, can
be easily managed with the assistance of livestock monitoring tools.
1.1 Requirement of IoT in agriculture
IoT devices holds the capability to collect and send the important data viainternet to the server where the storage of data is done to use it in future bythe user. This is done by the constant monitoring of the data receive by theIoT enabled devices. Monitoring activity may help the user to take appro-
priate action or decisions.
Devices enabled with the IoT make use of sensors to collect and transmit
the data to another device or a server and vice-versa. After analyzing the
received data, IoT devices may take appropriate action to improve the envi-
ronment and further again, consequences of the actions taken are to be
analyzed to avoid risks or to further improved the surroundings [7,8].
In
current scenario, IoT enabled devices are needed in the agricultural
sector for the better understanding of the actions to be taken for the better
results.
Smart sensing systems are used in Precision agriculture to manage the
agricultural produces, animals, fields, automatically [9].
Internet
of things’ technologies help in precision agriculture because
they let farmers precisely measure and map specific areas. This accuracy isparticularly significant when it comes to irrigation systems that rely on water
levels and flows [10]. With precise measurements and maps, farmers can plan
on 
the quantity of water that their fields will need. They no longer have to
guess how much water a field needs, which is often incorrect or settle for a
lower rate than they want to pay for.
IoT has various applications in agriculture field but due to some limita-
tions it does not fulfill the requirements needed to upgrade the farming. But
use of IoT in combination with the blockchain has overcome from its lim-
itations. Also, there is lacking trust in centralized system due to which it
needs the decentralization [11].
Livestock
production is another aspect of smart agriculture that relies
heavily on the Internet of things. In the past, farmers had to send out theirentire fleet of horses, pigs, and cattle in order to keep up with the demand of
the marketplace. Nowadays, tractors, combine harvesters, and livestock-
delivery systems can provide farmers with an efficient way of maintainingand harvesting their produce [12]. Drones are making this possible at a509 Integration of blockchain and IoT edge devices for smart agriculturefraction of the cost of traditional farming, since they allow for greater pre-
cision in handling and transporting massive quantities of produce.
Internet of things technologies enable farmers to get access to their own
data as well, through things like GPS. Modern devices have onboard com-
puters that store information, and software enables them to present thisinformation in graphical formats such as dashboards or charts. Agricultural
researchers can view historical data or conduct statistical analysis of current
farming practices. Internet of things technologies allow you to easily accessand share this information with other people who work in the industry, all-owing collaboration and communication between team members [13,14] .
One
application of Internet of things technology for agriculture is food
production. Modern farming practices are geared more toward conservingresources than increasing production. Thanks to the Internet, farmers caneasily see how things are done in another location and apply the same
principles in their own fields. With this knowledge, they can increase their
yield and reduce water consumption. With the improved food quality,greater value for money and reduced environmental impact, an Internetof things’ technology is a key part of the future of agriculture [15].
The
Internet of things (IoT), is revolutionizing the way farming is done
today by allowing farming equipment to communicate with one another tocoordinate activities, improve efficiency, and to make a better, more envi-ronmentally friendly crop or product. With the Internet of things, farmerscan access information about their crops or soil conditions easily and quickly.
They can also receive visual feeds of their crops or soil from a mobile
phone or web cam [16]. These feeds provide critical data on what crops need
to
be improved upon so that they can be delivered more quickly to con-
sumers. With all of the information available at the touch of a button, farmers
can make better decisions about how to grow their crops and increase
profitability.
Blockchain is a distributed ledger system where the data is not controlled
by single or centralized authority. It is a decentralized system and give
permission to numerous systems to keep a copy of all the transaction records
taken place in the network. Basically, blockchain had been noticed in2009 and first, implemented in Bitcoin as cryptocurrency [17,18] . In this
sy 
stem, manipulation of information is quite impossible as the copy of all
the transaction distributed among the specific nodes. Currently, Blockchain
has been implemented in various fields such as healthcare, finance, banking,
agriculture, etc.510 Swati Nigam et al.After IoT, blockchain brings next step upgradation in the field of agri-
culture. It has the superb capability to convert traditional farming into
smart farming, shares the data in a transparent manner and results in effectiveuse of shared data. Use of blockchain in combination with IoT leads tosecure data routing done by IoT devices and can avoid the attacks on thesystem [19].
1.2 Requirement of blockchain in agriculture
Concepts of blockchain enables to transform the current agriculture intosmart agriculture. Fundamental property of transparency ensures to resist
any kind of fraud in the collection or sharing of the information. Also,
it is helpful in tracking of the agricultural products to avoid the wastageof the foods and blocking of the products. With the implementation ofblockchain, users keep an eye on the food supply chain [20].
Using
the Blockchain in agriculture is one of the most unique uses
for this emerging technology. Traditional methods of securing private datasuch as credit card information or sensitive financial transactions are oftensusceptible to hacks. By decentralizing data entry and communication,the Blockchain allows for a more secure and faster transaction processing
by multiple participants in the agricultural industry [21]. The supply chain,
for
example, is very long, involving many different parties using their own
distinct siloed monitoring systems. However, using the Blockchain provides
the solution of an extended version of the traditional supply chain, using
the same technology and methodologies but applied to a much larger
portion of the agricultural market.
The benefits of the use of the Blockchain in agriculture is multi-faceted,
not only for the farmers, and producers they employ, but also for the public
at large. The applications for this technology are practically endless. With
the ability to track each phase of the food supply chain, from growingthrough to marketing, there are fewer risks of trade-offs between speedand safety. For instance, if a manufacturer senses a customer demand forlower cost or lower quality food, they can easily adjust production or reduce
processing times to meet those standards without incurring financial loss
[22,23] . If a farmer senses a trend toward lean meat or lower volume, they
can
also adjust accordingly. This provides the public with a clearer picture
of the food supply chain, providing an accurate depiction of where thewaste is being generated and which assets require the most attention to
improve quality and efficiency.511 Integration of blockchain and IoT edge devices for smart agricultureIn addition to using the Blockchain for agriculture, it has been used as an
innovation platform in other sectors. Distributed ledger technology has been
increasingly used by hedge funds and capital markets, to reduce the oppor-tunities for corruption and strengthen the integrity of the transactions andportfolios. Similarly, big data has been used to improve traceability of naturalresources across different industries. Transparency and traceability are two
important tenants of the Blockchain ecosystem, and its use in agriculture
shows how its application can streamline and optimize the agricultural sectorand help create a more transparent public space [24].
2. Blockchain technology: An overview
Blockchain is a robust technology gained attention after the imple-
mentation of cryptocurrency named as “Bitcoin.” This cryptocurrencycomes in notice after publishing the white paper of Bitcoin, with contri-bution of number of experts under the guidance of “Satoshi Nakamoto”
in 2008. Blockchain name suggests itself a “chain of blocks.” Therefore,
blockchain consists of verified blocks connected with each other and eachblocks contains the information about transactional data, timestamp, andthe hash address of the last block. In blockchain data is stored on the distrib-
uted ledger and works in a decentralized manner. The very first block in
the blockchain is known as “genesis block” [25].
Components
in the blockchain architecture are as explained below:
Block: Block is the main and basic element in the blockchain structure
which is used to store the complete set of transaction data and is further
shared among the existing nodes in the network.
Chain : This chain is to be referred as the series of the block in a
particular order.
Transaction : It a very basic unit of information in this architecture which
serves its purpose.
Nodes : In blockchain architecture, nodes are defined as the user s or the
computer system who has the complete information about Blockchain
ledger.
Miners : Miners are the special type of nodes and performing the function
of verification of the block before they get added in the blockchain structure.
Consensus : Consensus are defined as a procedure which helps in the
smooth operation of Blockchain.
All the above, mentioned components belongs to the architecture of the
blockchain are the major units. Table 1 shows the fields of blocks and
their
size.512 Swati Nigam et al.As it is mentioned that Genesis block is the first block existing in the
blockchain and having no parent in the blockchain. Every connected in
the chain has its own specific address known hash value considered as the
identity of that block. Due to the presence of components in the block,
it is considered as a unit. Basically, a block is consisting of block header
and block identifiers. First, all the field of a block will be explained based
on its size. Components of a block header are as follows ( Fig. 1 ):
Version : it defines the upgradation level of the software or protocol which
are in use. This field occupies 4 bytes.
Timestamp : It provides the time of creation and updating of the block
to avoid any kind of fraud. This field is of 4 bytes.
Previous Block Hash : This is also known as parent block hash and point to
the previous last block. It is a main component to build a linking in between
the chain due to which blockchain is formed in proper manner. This field is
of 32 bytes.Table 1 Fields of blocks and size.
Sr.no. Fields of block Size (bytes) Illustration
1 Block 4 Size hold by block itself
2 Transaction
counter1–9 Holds the count of transaction taken
3 Block Header 80 Consists of several field
4 transaction Not fixed Details of the transaction are to be recorded
BLOCK
BLOCK HEADER
BODY OF THE BLOCKTX1 TX2 TX3
TX6 TX7 TX8 TXnTX4 TX5Prev Block Hash
Merkel Root Hash NonceTimestamp Version
Difficulty Target1st Block
Genesis
Blockhash Data hash Data hash Data2nd Block 3rd Block 4thBlock
Fig. 1 Structure of a block in blockchain.513 Integration of blockchain and IoT edge devices for smart agricultureNonce : It is a random number and generally starts with 0. This
number increases on the progression of the hash computation. This field
is of 4 bytes.
Markle root Hash : It is represented as a binary tree and each hash codes
serves as a node. Markle root hash is used for the identification of transaction
takes place in the block. This field is of 32 bytes.
N-bits : it is also known as difficulty target. It defines the difficulty level
to find the targeted hash of the current block. This field is of 4 bytes.
3. Working of blockchain
Concept of blockchain is based on distributed ledger technology col-
lecting the records of digital transactions in an immutable and transparent
manner. Therefore, Blockchain may be defined as a decentralized and
distributed or shared ledger holds the information of digital transactions
performed in the network without including any third party or any inter-
mediaries. Blockchain works in an e fficient manner where the data is
immutable and secure. At very first, a transaction request in initiated by
the user in network. in response of t he request, a block is created to
represent the creation of transaction. Af ter the creation of the block, trans-
action request is broadcasted to every node existing in the network will
verify and validate the transaction. Being a verified transaction, it may
get contracts, cryptocurrency, records as a reward and then transaction
is added to a new block for the ledger. Further on this block is added to
the blockchain in a permanent and immutable way [26].Fig. 2 shows
the working of blockchain.
Initiate a transaction
request
Transaction is
completed.when a new added block is
full of transactions then it is
added to the existing
blockchain in an irrevocable
and permanent wa yonce a transaction is
verified then
transaction is added
to a new block.Nodes will receive a reward
for transaction validation and
verification.A block will represent
the transactionThe transaction is broadcast to
every node( or the computers)
in a peer-to peer networkThe network of nodes verifies the
validity of the transaction based on
agreed rules or agreements.
Fig. 2 Working of blockchain.514 Swati Nigam et al.4. IoT: An overview
The Internet of things (IoT) has played an important role in our day-
to-day needs. This technology is also known as Internet of Everything
and make a link in between the devices to connect with each other. It isacknowledged in a very short duration by the organizations as one of theessential fields for future use. As a result, existence of communication in
between the devices represents importance of this technology for the indus-
tries. The concept of this technology has been proposed in 2009 by Ashton.He makes it possible to establish communication in between human-to-things, human-to-human and things-to-things. By using IoT, each object
gained its unique identity. IoT is implemented in combination with other
technologies to bring revolution in existing fields [27,28] .
Some
prerequisites are needed to implement the Internet of Things
(IoT) in a successful manner. These are follows as:
Hardware : Importance of hardware or devices is explained in its name by
itself. Hardware needed in IoT are sensors, cameras, actuators, CCTV, andembedded hardware require to establish communication in betweendevices.
Middleware : These are the computing tools and storage devices used to
store the data and for data analysis with big data and cloud analytics.
Presentation : At every representation of data is quite important due to the
reason need of interpretation and visualization are required. Better the usageof tools may lead to better understanding of the data.
5. Working of IoT
1.Sensors
With the help of sensors, a large data can be collected as it is capable
to
capture every minute data, and this collected can have complexities
at different level. This data may be temperature monitoring data,video, etc.
2.Linking
Data collected with the help of sensors is to be stored on the cloud
infrastructure,
which is done by using Wi-Fi, satellite networks,
Bluetooth, etc.
3.Data processing
Data collected on the cloud will be processed by using the tools or
software.
It may be processed in terms of monitoring a particular data515 Integration of blockchain and IoT edge devices for smart agriculturewhether it resides in defined limits or not. This activity will be performed
by the user to take the appropriate actions.
4.User Interface
Now the collected information must be represented to the user, and
it is done by using interface. It might be mobile phone, Laptop, com-
puter system or PLC.
IoT has many applications in many areas but now in this chapter main
concerned is about the use of IoT to convert classical agriculture into smart
agriculture. Fig. 3 shows the working of IoT.
Smart agriculture is a strategic system used to enhance crop production
and reduce expenses. By using Internet of things technologies, agricultural
experts can collect, collate, analyze, and distribute real-time information
about weather patterns, soil composition, and climate conditions. The
Internet of things is expanding the reach of such technology to not only
improve the quality of farming but also make the process of farming easier
and more accurate [29,30] . With the Internet of things, farmers no longer
have to wait for information, weather conditions, and other critical data
during their field work. With the help of the Internet of things, they can
now conduct research on weather patterns, crop production, and other
aspects of the land with just a few clicks of a mouse.
In the past, agriculture relied largely on land surveys to accurately map
the physical structure of the land and to determine the optimum spacing and
nutrients needed for different crops [31]. With the advent of modern com-
puter software, the process has been made even more precise and accessible
to the point where precise and complete data about crops can be analyzed at
the push of a button. This allows farmers to know what crops should be
planted where, when they should be planted, and how much space each
one must occupy. Through the Internet of things, land surveys are becoming
IoT sensor devices IoT Gateway Wireless connectors Data Carrier Cloud and Data server Data Processing
& Aaal ysis
Fig. 3 Working of IoT.516 Swati Nigam et al.obsolete because the actual physical structure of the land can now be viewed
in real time through digital imaging. Computer programs that provide accu-
rate topographical and soil moisture mapping can be accessed instantly fromany location.
6. Edge computing: An overview
Edge computing is an example of distributed computing that pulls
together data and computation closer to the real-world sources of data,bringing real-time computing and information storage closer to the actualdevices that need access to it. This is said to dramatically increase responsetimes, reduce bandwidth and boost productivity. In the case of vehicle fleets,
this means that fleet managers can make informed decisions about vehicle
maintenance and scheduling. Foreseen failures could be mitigated throughpredictive maintenance procedures [32,33] . On the other hand, existing
vehicles
can still be kept in good running order, as long as they receive
regular maintenance, which most truck and van fleets already do. The com-bination of improving efficiency and reducing costs has real-world value forboth current as well as future generations of vehicles.
In case of agriculture, it is reshaping the future of agriculture.
Autonomous tractors and robots have been developed and can be operated
automatically without human interference. Edge computing technology is
capable to transform the agriculture area. In IoT system or IoT based devicesperforms some essential jobs and are represented in the figure as a pyramidincluding the three layers (1) at the bottom is edge computing layer (2) above
that fog computing layer is present and at the top is (3) Cloud computing
layer. Fig. 4 shows the working model of edge computing.
Edge
computing : This layer performs the processing on the edge servers
which are in direct contact of millions of controllers and sensors. These edge
servers have potential of analyzing and take decisions on real time site.
Fog Computing : This layer acts as an intermediate in between the edge
computing layer and cloud computing layer. It can perform the computing
on the proliferation of the data. Moreover, it has the capability to perform
the additional analytics and filtrating.
Cloud computing : Basically, this layer is used to store the data. Important
data is stored in this layer collected from the edge nodes and fog nodes. After
then multiple analytics is done to make decisions.517 Integration of blockchain and IoT edge devices for smart agriculture7. A proposed model for smart agriculture using
blockchain and IoT
Smart agriculture using blockchain and IoT comprising of some of the
activities and data including the agricultural parameters to sense, knowledge
about location where it has to be identified, tracking of data from source to
destination to take decision, to view results by using an application [34,35] .
Our proposed model defined on the basis of six layers after the employment
of blockchain and IoT are as follows ( Fig. 5 ):
Sensor layer and edge computing layer : This layer consists of sensors which
are used to sense the environment, analyze and after analyzing proper
actions have to be taken. In market there are various types of sensors,
and each sensor has its specific application [36,37] . These sensors can
be planted under the soil, i.e., underground, above the crops. The sen-
sors put down under the soil are water resistant generally used to take
measurement of moisture existing in the soil, chemical properties of
the soil, pH level, etc. While edge computing increases the speed of data
collection activity and hence its efficiency [38,39] .
Link Layer : Under this layer, currently existing routing and networking
technologies are implemented to make information exchange in
between the sensors. Generally, IoT uses WSNs (Wireless Sensors net-
works) for the efficient management of the crop and field. Use of WSN
provides various benefits to monitor the fields, to optimize the quality of
crops [40,41] .
Fog Computing layer : With the help of fog layer information can be shared
with the user or farmers will be on time [42]. This layer is in direct
Cloud Computing Layer
IoT Cloud Platform
Fog Nodes & LAN
Edge Server & Sensors   Data Warehousing
   Data analysisCloud
Fog
EdgeThousands
Millions
Billions   Data Analysis
   Data Reducing
   Data filter   Real-Time data processing   Business rule
Fog Computing Layer
Edge Computing Layer
Fig. 4 Working model of edge computing.518 Swati Nigam et al.Blockchain
layer
Data Processing
& Management
layer
Cloud Computing
Layer
Fog Computing
Layer
Link Layer
Senor & edge
computing Layerwind speed
sensor
Soil Temp
sensorAir temp
sensor
photosynthetic
active radiationbarmetric
pressure
sensorrelative humidity
sensorwind direction
sensorLeaf wetness
    sensor
Soil mositure
 sensor
barmetric
pressure sensor
Fig. 5 Hybrid model for IoT, Edge computing & blockchain integration.519 Integration of blockchain and IoT edge devices for smart agriculturecontact of cloud computing layer and works on the time-sensitive infor-
mation. Due to which the farmers can get the information on time and
may reduce the loss or risk factors [43,44] .
Cloud computing layer : This layer lies in between the fog computing layer
and data processing and management layer. Cloud computing layer maygenerate the distributed pool of It resources such as storage, software
information, network with the help of a suitable infrastructure ensuring
the good level of food security [45–47].
Data processing and management layer :
 Basic function of this layer is to
process the collected data and do analysis of the same. This layer manages
the data and mine the data related to interset by using efficient data min-
ing techniques [48,49] . In every field data analysis is most important
activity to make prediction and decision to avoid risks and losses. Thisis the reason this layer plays an important role in the proposed model
[50,51] .
Blockchain layer :
 After releasing of processed data, this data will enter into
the the blockchain layer where it will become advantageous for the
farmers. This data will be verified by the special nodes. After verification
data will be added in the block and futher this block would be added in
the chain hence become a part of the bockchain [52,53] .
As per the above Fig. 6 a model is proposed for the for the monitoring of food
su
pply chain. This model consists of three layers named as (1) Physical data
layer (2) Logical data layer and (3) Web interface layer. By introducing these
three layers in the proposed model it consists of extendable, efficient and scal-
able implementation. Physical layer consists of IoT nodes, IoT gateway
[54,55] . IoT nodes keep the track of the farm to monitor the environment
an
d conditions of the growth. IoT nodes gather the data from the various
farms which is then forward to the base station with the help of gatewaysand routers. Also, another gateway known as RFG gateway is used to syn-chronize the two irrelevant data and to maintain the management. To storethe data, receive from physical data layer, in the database or server, logic data
layer is used [56,57] . As per requirement data is used to analyze or to make
de
cision could be received from the logic data layer in appropriate form.
Some codes are generated for the linking of the data to the database. As a
result, web application layer will identify them. With the help of this modeldata can be converted to IoT networks for the smooth sharing of data and
operation in between the IoT devices [58,59] .
As 
you may be aware, food traceability is critical in this market. Tracking
items is a major concern, and most corporations seem to be hesitant to do520 Swati Nigam et al.IoT Gateway
Gateway Wireless routerDatabaseNetwork
Soil temperature DATA PROCESSINGDCT
CIFProcessorRetalerConsumerBuyerFarmer
PackerDifferent type of crop growth information
Base Station
air temperatureFarmerIoT Nodes
Fig. 6 Agriculture monitoring and supply chain.so. Consumers have no idea how or in what method the food was prepared
if sufficient monitoring is in place. As a result, the industry needs assistance
at this time. As a result, blockchain in food traceability has the potential to
change the situation for everyone. The blockchain is well-equipped to track
food from its suppliers to the customer who purchases it. As a result, businesses
may use this power to improve their visibility on the manufacturing line
and provide higher-quality food to the market. In the field of food safety,
blockchain can put customers at rest and allow them to acquire all of their
goods without fear of contamination or other issues. Because the food business
is currently ill-equipped to identify the real source of contamination. It may
even go unnoticed until its too late in most situations. As a result, organizations
may use blockchain to track how they handle raw materials and final goods.
Furthermore, they may treat their byproducts with greater care in order to
preserve the quality and grade of their delicacies. Fig. 7 shows the proposed
architecture of food safety using blockchain technology.
Application Layer
Blockchain LayerComplete information of
food supply chain activity
Digital flow LayerSmart ContractCONSUMER
Physical Flow Layer
Farmer Processor Distributor Retailor Customer
Fig. 7 Proposed architecture of food safety using blockchain technology.522 Swati Nigam et al.8. Advantages of blockchain, edge computing and IoT
based agriculture
Due to benefits of implementing blockchain in agriculture, it is in
high demand. Blockchain is considered as a disruptive technology because
of its significance. Advantages of blockchain are as follows:
Agriculture supply chain : Implementation of blockchain in agriculture
supply chain leads to convert the industry into more efficient industry
which was not before due to unavailability of automation and the proper
use of technology. Therefore, use of blockchain in food supply chaindeduct the costing of farming activities. This may increase the overall effi-ciency of the process [60,61] .
Transparency:
Concept of blockchain also provides the transparency
regarding the data. The subsides generated for the farmers by the govern-ment. By this tracing of the actual facts of the subsidies can be followed
[62]. User can check clearly about the flow of money.
Fair
prices: Use of Blockchain can update the prices to be fair. Many
brokers are involved in the process of buying of crops and not providethe fair prices to the farmers. By using blockchain, scenario will becometotally different [63,64] . Farmers can sell their crops or products to genuine
buyers
from whom they can negotiate.
Inventory management: Condition of Farm inventory could get better by
reducing the wastage in them. And farmers to paid for the wastages in director undirect way. With the help of blockchain, storage condition or sur-roundings could be tracked so that the propre action will be taken [65–67].
Updating
of farm management software: In current scenario, software is used
in agriculture are based on client server architecture only not working attheir maximum efficiency. But these software’s can be updated on theimplementation of blockchain [68,69] .
Secure
for IoT optimization : IoT devices are very efficient in keeping track
of their products and provides safety to the devices also. These devices cancollect real time data could analyze it and take appropriate action. On theother hand, sometimes these devices become so vulnerable about predictionof natural calamities [70,71] . Cloud services required for storing the data or
information
may get easily prone to cyber-attacks. All these issues can be
solved by using blockchain.523 Integration of blockchain and IoT edge devices for smart agricultureTable 2 Summary of the research for applying blockchain and IoT in agriculture.
Ref. no Author name Use case Year Blockchain contribution
[1] C. Xie, Y. Sun, and
H.
LuoAgricultural ProductsTracking2017 Double-chain storage structure and build a highly secured method for monitoring
agricultural goods based on blockchain. Double-chain storage assures that farm
produce data cannot be intentionally tampered with or destructed by storing it in a
blockchain transaction hash
[2] Y. P. Lin et al. ICT e-agriculture 2017 Blockchain-based e-agriculture platform for regional and local application is
suggested 
in this paper. Evaluating blockchain technology for use in ICT
e-agriculture applications is made easier with the introduction of this tool
[4] M. Tripoli andJ.
SchmidhuberSupply Chain 2018 DLTs in agri-foods: Possibilities, Advantages, and Strategies is the goal of this article.
Institutional and technical limitations and obstacles to its implementation are also
identified. Agricultural supply chains and rural development initiatives may both
benefit from digital records, cryptography, and disintermediation of transactionprocessing and data storage, thanks to DLTs
[5] M. P. Caro, M. S.Ali,
M. VecchioAgri-Food supplychain2018 IoT devices that produce and consume digital data throughout the supply chain may
be seamlessly integrated into the AgriBlockIoT blockchain-based traceability system
presented in this study
[6] K. Leng, Y. Bi, L. Jing. Supply Chain 2018 In this research, we present a public blockchain of agricultural supply chain network
based 
on the double chain architecture, focusing on the dual chain structure and its
storage mode, resource rent-seeking and matching mechanism, and consensus
algorithm. Openness and security in transaction information may be taken into
consideration by using a double-chain structure for agricultural supply chains,
according to these findings
[7] J. Hua, X. Wang,M.
KangSupply Chain 2018 Here, we propose the use of blockchain methods to create an agricultural provenance
system with decentralized, community maintenance and consensus trust in order toalleviate the trust issue inside the supply chain. Management activities like (fertilizing,
irrigation, etc.) as well as a specific data structure are stored9. Summary of the research for applying blockchain and IoT in agriculture (Table 2 )[8] J. T. Hao, Y. Sun, and
H.
LuoAgricultural ProductsTracking2018 Based on IPFS and the blockchain, we provide a new data storage mechanism in this
article. As a starting point, the sensors’ real-time data is stored in IPFS and accessedthrough web browsers. The IPFS hash address of the origin data is then stored on the
blockchain to prevent a hostile user from performing a data faking attack. A
blockchain-based authentication process is then designed. It is able to authenticatethe data and protect it effectively
[9] J. Lin, Z. Shen,A.
ZhangFood Traceability 2018 As part of this research, researchers suggest a blockchain-based food traceability
system that includes all stakeholders in a smart agricultural ecosystem, even if theydon’t trust each other
[13] A. Croxson, R. S.
SharmaAgri-Food
 supply
chain2019 Q-methodology was used to analyze whether or not blockchain technology may be
the answer of problems associated with agro-food supply chain and give advice for
organizations on how they can implement this technology. Four unique groups werefound in the sector, each with a different take on the possibilities of blockchain
[14] Z. Wang and P. Liu Agricultural products
tracking2019
 Blockchain technology is used to store the traceability data of farm commoditiessecurely, and this paper proposes an agricultural product traceability model that cancovering the full industrial chain of agricultural goods. Buyers can retrieve the data
from the truthful source of traceability for agricultural products through this model,
which uses blockchain technology to store traceability data
[19] M. Kim, B. Hilton,Z.
BurksFood Traceability 2019 End-to-end food traceability, “farm to fork” in this context, may be achieved using
the Blockchain platform and Internet of things devices exchanging GS1 messagingstandards, according to this article. Supply chain stakeholders will have full access to
the distributed ledger
[23] K. Salah,N.
NizamuddinSupply Chain 2019 With the suggested method, there is no longer a requirement for a trusted central
authority, intermediaries and transaction records, which improves efficiency and
safety. The suggested approach is based on the use of smart contracts to control andmanage all exchanges and transactions among the supply chain network players
[24] A. Kamilaris, A. Fonts Agri-Food supply
chain2019
 Examines the influence of blockchain technology on agricultural and food supplychain, provides active projects and efforts, evaluates the overall implications,
problems and possibilities with a critical perspective of the development of these
projects in this article
ContinuedTable 2 Summary of the research for applying blockchain and IoT in agriculture. —cont’d
Ref. no Author name Use case Year Blockchain contribution
[27] T. Surasak,
N.
Wattanavicheanagricultural productstracking2019 Our traceability solution benefits greatly from the combination of the blockchain
database and the Internet of Things since all of the data is collected in real time andstored in a highly secure database. Supply chain management and Food traceability
might become more dependable and public awareness of monitoring and quality
control in Thailand could be resurrected with our approach
[28] S. Madumidha, P. SivaRanjaniAgri-Food
 supply
chain2019 Decentralizing blockchain-based traceability for agriculture is presented in this
article, which allows the construction of blocks that are constantly integrated withIoT devices, from providers to consumers
[32] X. Li and D. Huang Agri-Food supply
chain2020
 From the source through manufacturing, service, security, and sales, the articleexamines how Internet technology has evolved over time in relation to the
agriculture industry’s supply chain. The paper also proposed the development of a
blockchain-based system for tracking agricultural products
[33] W. Lin et al Farm Overseeing 2020 In this article, we conduct a survey to learn more about the agricultural uses of
blockchain 
technology. Technical aspects, including data model, cryptographic
algorithms and consensus procedures are presented in great depth first. Second,
current agricultural solutions are classified and examined to show the utilization of
blockchain methods
[34] Y. Chen, Y. Li, andC.
LiFarm Overseeing 2020 This approach includes the complete ecological farm’s circular agriculture cycle onto
the blockchain. Data may now be shared more widely thanks to the adoption ofnumerous smart gadgets that automatically gather and upload data
[35] M. D. Borah, V. B.Naik,
R. PatgiriSupply Chain 2020 Blockchain technology incorporated into the supply chain to increase product
traceability and use. In order to accomplish these objectives, authors plan to use BCT
to construct a value chain that is transparent from the farm to the fork
[36] P. Chun-Ting,L.
Meng-JuAgriculturalTraceability2020 This project uses IoT sensors and a blockchain-based agricultural service platform to
track food from farm to fork[38] V. S. Yadav, A. R. Singh Supply Chain 2020 Authors want to find out what the biggest obstacles are to Indian ASC using
blockchain.
A thorough literature search and the opinion of experts from five
agribusiness, academia, and agro-stakeholders were used to identify the constructions
[42] S. K. Singh, S. Rathore,
and
J. H. ParkFarm Overseeing 2020 A Blockchain-enabled Smart IoT Framework with Artificial Intelligence is proposed
in this study, which offers an effective means of integrating blockchain and AI for IoTwith existing IoT applications and approaches
[43] S. S. Patra, C. Misra Agricultural
Traceability2021
 According to an AgriChain system, there are anticipated to be a large number oftransactions that are waiting in queues before they can be entered into blocks, andthese transactions are evaluated in detail in this chapter using a queuing model that
gauges the performance of this system
[45] S. J. Anand,
K.
PriyadarsiniFarm Overseeing 2021 In order to save energy, we’ve developed the Improved LEACH protocol. A new
energy efficiency threshold limit is implemented in this protocol. Blockchain withthe ILEACH protocol may be used to create an intelligent agricultural system
[49] Y. Zhang, J. Li, andL.
GeAgricultural productstracking2021 Agricultural product supply chains under blockchain technology are examined, and
the existing state of agricultural supply chain networks is examined
[50] Y. Bai, K. Fan, K. Zhang Trust Management 2021 It is our goal to build a sustainable supply chain strategy that incorporates
non-cooperative 
game theory, in which the Bayesian formula is used to integrate
previous experience and present conditions
[53] S. Al-Amin, S. R.
SharkarSupply Chain 2021 For agro merchants, we have developed a model that is both effective, efficient, and
satisfying, as well as a food traceability system which uses blockchain and IoT to make
their company smarter and wealthier
[54] A. A. Mukherjee, R. K.SinghSupply
 Chain 2021 An increase in the global desirability index for a blockchain-enabled supply network
over the conventional supply chain indicates that the supply chain’s sustainability maybe improved by using blockchain technology. Practitioners will benefit from the
findings of this research, which attempts to provide actionable advice on how they
may use this technology
ContinuedTable 2 Summary of the research for applying blockchain and IoT in agriculture. —cont’d
Ref. no Author name Use case Year Blockchain contribution
[56] T. H. Pranto, A. A.
NomanFarm Overseeing 2021 This study demonstrated that blockchain is irreversible, accessible and transparent in
agriculture while also underlining the robust mechanism presented by its partnershipwith smart contracts and IoT
[57] M. Biswas, T. M. N. U.AkhundSupply
 Chain 2021 The suggested model integrates Artificial intelligence technology, Internet-
of-things, and Blockchain to build a smart and future agricultural system that gives
farmers with a safe and open transaction method to rich, novel, and effective decision
assistance
[58] I. Eluubekkyzy,
H. Song, A. Vajdi,
Y. WangSupply Chain 2021 Our design considers trustworthiness, scalability, and share allocation. We use a
cyber-physical system to assure product quantity and quality. Scalability is addressedwith a novel consensus mechanism and a public service platform model
[62] S. Hu, S. Huang,
J.
HuangSupply Chain 2021 Author use the data integrity of blockchain and the edge computing paradigm to
build an OASC trust architecture that has a substantially higher cost-efficiency ratio
[63] W. Liu, X. F.Shao,
C. H. Wu, and
P. QiaoFarm Overseeing 2021 This review lays the groundwork for ongoing research into the use of ICTs and BTs
in agriculture, with implications for technical advancement and agricultural
sustainability
[64] M. H. Ronaghi Supply Chain 2021 The peculiarity of the study is that it identifies the relevance of blockchain aspects in
agriculture
and provides the management framework of blockchain in supply chain
operations from an applied perspective
[65] G. S. Sajja, K. P.Rane,
K. PhasinamFarm Overseeing 2021 This article examines the use of blockchain technology in food supply chains, crop
insurance, smart agriculture, and agricultural products transactions
[67] S. Saurabh and K. Dey Agri-Food supply
chain2021 According to the findings of the research, supply chain players’ adoption-intentiondecision processes may be influenced by dis-intermediation, traceability, pricing,
trust, compliance and coordination and control. Additional considerations for a
sustainable and scalable supply chain collaboration and sustainability architecture areprovided by the adoption factors10. Challenges and open issues
Amalgamation of blockchain and IoT may overcome the challenges of
IoT in very efficient manner. But still some of the new challenges and issues
comes in notice the are considered as strong obstruction. The challenges and
issues are defined as below ( Fig. 8 ):
1.Handling of big data in the blockchain : In the blockchain, on confirming
of the new block is broadcasted over the entire network. Being a
peer-to-peer network, this new will be appended with each node.
And this procedure will consume very amount of storage which approx-
imate of 720 GB.
2.Trade-off between power consumption and performance : Blockchain algorithms
requires a very high computational power which acts as a restriction in
the applications based on this technology. Due to the reasons user are not
confirmed about the efficiency of Blockchain.
3.Transparency and privacy : In certain applications, such as banking, trans-
parency is critical, and this is where blockchain comes in. However,
when storing and retrieving IoT data from specific IoT applications such
Challenges and
Open issues of
blockchain and
IoTHandling
big data
Regulating
challenges of
blockchain in
IoTMaintaining
both
transparency
and privacyData
concurrency
and thoughput
issueThe trade-off
between power
consumption,
performance
Connectivity
challenges of
IoT
Fig. 8 Challenges and open issues of blockchain and IoT.529 Integration of blockchain and IoT edge devices for smart agricultureas eHealth on the blockchain, user confidentiality may be significantly
damaged. IoT access control must be cost-effective in order to retain
a reasonable level of openness and privacy.
4.Regulating challenges of blockchain in IoT : Decentralization, anonymity,
immutability and automation are all properties of blockchain technologythat have the potential to improve the security of a wide range of IoT
applications. Immutability refers to the fact that data is permanently
stored in the distributed transaction ledger (DTL) on the P2P networkand cannot be erased or updated. Additionally, records cannot be vettedfor privacy before being published on the blockchain owing to a lack
of governance. Code such as smart contracts on a DTL might result
in illegal action. Because of the DTL’s anonymity, it is difficult toidentify the persons involved in unlawful service transactions.
5.Data concurrency and throughput issue : In blockchain the throughput is to
a limited amount due to existing protocol and consensus mechanism
used in this technology while in IoT, high data concurrency is existeddue to the non-stoppable streaming of data. Hence, blockchain technol-ogy is required to increase the throughput to synchronize with IoT
devices.
6.Connectivity challenges in IoT : IoT devices are connected with highly
efficient devices and networks for the sharing of the data with the user.To connect the devices with blockchain, IoT is restricted up to a limitfor the execution of applications.
11. Conclusion
The goal of this research was to find out how critical it is to include
IoT and blockchain technology with edge computing into the developmentof smart systems and precision agricultural applications. Due to this technical
integration, it has been shown that blockchain may provide new solutions
to chronic security and performance concerns in IoT-based precisionagricultural systems. With the help of Blockchain, traditional methods ofcollection, rearranging and distribution of Agri-products can be replaced
by more trust-worthy, decentralized, vitreous, and immutable style. In
agriculture sector, Blockchain and Internet of things can be amalgamatedto have better results which leads us to one level up in the field of agricultureand may control or improve the supply chain in proper manner. This
chapter presents an overview of blockchain technology, IoT with edge
computing and how these three technologies used in smart agriculture.530 Swati Nigam et al.Chapter proposed a hybrid architecture for IoT, edge computing and
blockchain technology integration. Two framework is also proposed for
smart agriculture, one for monitoring and supply chain management, andsecond for food safety. In the last, a tabular summary is also present fordifferent research carried out in the related area from 2017 to 2021.
References
[1] C. Xie, Y. Sun, H. Luo, Secured data storage scheme based on block chain for agricul-
tural products tracking, in: Proc. - 2017 3rd Int. Conf. Big Data Comput. Commun.
BigCom 2017 , 2017, pp. 45 –50,https:/ /doi.org/10.1109/BIGCOM.2017.43 .
[2] Y.P. Lin, et al., Blockchain: the evolutionary next step for ICT e-agriculture, Environ.
-
MDPI 4 (3) (2017) 1 –13,https:/ /doi.org/10.3390/environments4030050 .
[3] E.Y.T. Adesta, D. Agusman, A. Avicenna, Internet of things (IoT) in agriculture indus-
tries,
Indones. J. Electr. Eng. Informatics 5 (4) (2017) 376 –382, https:/ /doi.org/10.
11591/ijeei.v5i4.373 .
[4] M. Tripoli and J. Schmidhuber, “Emerging Opportunities for the Application of
Blockchain
in the Agri-Food Industry Agriculture,” FAO, United Nations, http:/ /
www.fao.org/3/CA1335EN/ca1335en.pdf .
[5] M.P. Caro, M.S. Ali, M. Vecchio, R. Giaffreda, Blockchain-based traceability in
Agri-food
supply chain management: a practical implementation, in: 2018 IoT Vert.
Top. Summit Agric. - Tuscany, IOT Tuscany 2018, 2018, pp. 1 –4,https:/ /doi.org/
10.1109/IOT-TUSCANY.2018.8373021 .
[6] K. Leng, Y. Bi, L. Jing, H.C. Fu, I. Van Nieuwenhuyse, Research on agricultural
supply
chain system with double chain architecture based on blockchain technology,
Futur. Gener. Comput. Syst. 86 (2018) 641– 649, https:/ /doi.org/10.1016/j.future.
2018.04.061 .
[7] J. Hua, X. Wang, M. Kang, H. Wang, F.Y. Wang, Blockchain based provenance for
agricultural
products: a distributed platform with duplicated and shared bookkeeping,
in:IEEE Intell. Veh. Symp. Proc. , vols. 2018-June, 2018, pp. 97 –101, https:/ /doi.org/
10.1109/IVS.2018.8500647 .
[8] J.T. Hao, Y. Sun, H. Luo, A safe and efficient storage scheme based on blockchain and
IPFs
for agricultural products tracking, J. Comput. 29 (6) (2018) 158 –167, https:/ /doi.
org/10.3966/199115992018122906015 .
[9] J. Lin, Z. Shen, A. Zhang, Y. Chai, Blockchain and IoT based food traceability for smart
agriculture,
in: ICCSE’18: Proceedings of the 3rd International Conference on Crowd
Science and Engineering, 2018, pp. 1 –6,dl.acm.org .https:/ /doi.org/10.1145/3265689.
3265692 .
[10] O. Bermeo-Almeida, M. Cardenas-Rodriguez, T. Samaniego-Cobo, E. Ferruzola-
Go´mez,
R. Cabezas-Cabezas, W. Baza ´n-Vera, Blockchain in agriculture: a systematic
literature review, Commun. Comput. Inf. Sci. 883 (2018) 44 –56,https:/ /doi.org/10.
1007/978-3-030-00940-3_4 .
[11] D. Kos, S. Kloppenburg, Digital technologies, hyper-transparency and smallholder
farmer
inclusion in global value chains, Curr. Opin. Environ. Sustain. 41 (2019)
56–63,https:/ /doi.org/10.1016/j.cosust.2019.10.011 .
[12] O. Lamtzidis, D. Pettas, J. Gialelis, A novel combination of distributed ledger technol-
ogies
on internet of things: use case on precision agriculture, Appl. Syst. Innov. 2
(3) (2019) 1 –31,https:/ /doi.org/10.3390/asi2030030 .
[13] A. Croxson, R.S. Sharma, S. Wingreen, Making sense of blockchain in food
supply-chains,
in:Australas. Conf. Inf. Syst, 2019, pp. 97 –107. Accessed: Nov. 15,
2021. [Online]. Available: https:/ /aisel.aisnet.org/acis2019/10/ .531 Integration of blockchain and IoT edge devices for smart agriculture[14] Z. Wang, P. Liu, Application of blockchain technology in agricultural product trace-
ability system, in: Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif.
Intell. Lect. Notes Bioinformatics), 2019, pp. 81 –90,https:/ /doi.org/10.1007/978-3-
030-24271-8_8 .
vol. 11634 LNCS.
[15] M.C. Aldag, The use of blockchain Technology in Agriculture, Zesz. Nauk.
Uniw.
Ekon. w Krakowie 4 (982) (2019) 7 –17,https:/ /doi.org/10.15678/znuek.
2019.0982.0401 .
[16] X. Shi, et al., State-of-the-art internet of things in protected agriculture, Sensors
(Switzerland)
19 (8) (2019), https:/ /doi.org/10.3390/s19081833 .
[17] R. Mavilia, R. Pisani, Scaling blockchain for agricultural sector: the Agridigital case,
in:3rd
Int. Sci. Conf. ITEMA Recent Adv. Inf. Technol. Tour. Econ. Manag. Agric ,
2019, pp. 55 –60,https:/ /doi.org/10.31410/itema.2019.55 .
[18] S. Umamaheswari, S. Sreeram, N. Kritika, D.R. Jyothi Prasanth, BIoT: blockchain
based
IoT for agriculture, in: Proc. 11th Int. Conf. Adv. Comput. ICoAC 2019 , 2019,
pp. 324– 327, https:/ /doi.org/10.1109/ICoAC48765.2019.246860 .
[19] M. Kim, B. Hilton, Z. Burks, J. Reyes, Integrating blockchain, smart contract-tokens,
and
IoT to design a food traceability solution, in: 2018 IEEE 9th Annu. Inf. Technol.
Electron. Mob. Commun. Conf. IEMCON 2018 , 2019, pp. 335– 340, https:/ /doi.org/
10.1109/IEMCON.2018.8615007 .
[20] F. Antonucci, S. Figorilli, C. Costa, F. Pallottino, L. Raso, P. Menesatti, A review on
blockchain
applications in the Agri-food sector, J. Sci. Food Agric. 99 (14) (2019)
6129 –6138, https:/ /doi.org/10.1002/jsfa.9912 .
[21] V.S. Yadav, A.R. Singh, Use of blockchain to solve select issues of Indian farmers, in:
AIP
Conf. Proc. , vol. 2148, 2019, https:/ /doi.org/10.1063/1.5123972 .
[22] S. Wingreen, R. Sharma, P. Jahanbin, S. Wingreen, R. Sharma, A blockchain traceabil-
ity
information system for trust improvement in agricultural supply chain, in: 2019
European Conference on Information Systems At: Stockholm-Uppsala, Sweden,
2019, pp. 5 –15. Accessed: Nov. 15, 2021. [Online]. Available: https:/ /aisel.aisnet.
org/ecis2019_rip/10 .
[23] K. Salah, N. Nizamuddin, R. Jayaraman, M. Omar, Blockchain-based soybean trace-
ability
in agricultural supply chain, IEEE Access 7 (2019) 73295 –73305, https:/ /doi.org/
10.1109/ACCESS.2019.2918000 .
[24] A. Kamilaris, A. Fonts, F.X. Prenafeta-Bold ύ, The rise of blockchain technology in
agriculture and food supply chains, Trends Food Sci. Technol. 91 (2019) 640 –652,
https:/ /doi.org/10.1016/j.tifs.2019.07.034 .
[25] M. Shyamala Devi, R. Suguna, A.S. Joshi, R.A. Bagate, Design of IoT blockchain based
smart
agriculture for enlightening safety and security, Commun. Comput. Inf. Sci. 985
(2019) 7 –19,https:/ /doi.org/10.1007/978-981-13-8300-7_2 .
[26] V.S. Yadav, A.R. Singh, A systematic literature review of blockchain technology
in
agriculture, in: Proc. Int. Conf. Ind. Eng. Oper. Manag , 2019, pp. 973– 981.
Accessed: Nov. 15, 2021. [Online]. Available: http:/ /ieomsociety.org/pilsen2019/
papers/256.pdf .
[27] T. Surasak, N. Wattanavichean, C. Preuksakarn, S.C.H. Huang, Thai agriculture prod-
ucts
traceability system using blockchain and internet of things, Int. J. Adv. Comput.
Sci. Appl. 10 (9) (2019) 578– 583, https:/ /doi.org/10.14569/ijacsa.2019.0100976 .
[28] S. Madumidha, P. Siva Ranjani, U. Vandhana, B. Venmuhilan, A theoretical imple-
mentation:
agriculture-food supply chain management using blockchain technology,
in:Proc. 2019 TEQIP - III Spons. Int. Conf. Microw. Integr. Circuits, Photonics Wirel.
Networks, IMICPW 2019 , 2019, pp. 174– 178, https:/ /doi.org/10.1109/IMICPW.
2019.8933270 .
[29] J. Potts, Blockchain in agriculture, SSRN Electron. J. (2019), https:/ /doi.org/10.2139/
ssrn.3397786.532 Swati Nigam et al.[30] T. Alam, Blockchain and its role in the internet of things (IoT), IJSRCSEIT 5 (1) (2019)
151– 157, https:/ /doi.org/10.32628/cseit195137 .
[31] P. Singh, N. Singh, G.C. Deka, Prospects of Machine Learning with Blockchain in
Healthcare
and Agriculture, igi-global.com, 2020, pp. 178 –208.
[32] X. Li, D. Huang, Research on value integration mode of agricultural E-commerce
industry
chain based on internet of things and blockchain technology, Wirel.
Commun. Mob. Comput. , vol. (2020) 2020, https:/ /doi.org/10.1155/2020/8889148 .
[33] W. Lin, et al., Blockchain Technology in Current Agricultural Systems: from tech-
niques
to applications, IEEE Access 8 (2020) 143920 –143937, https:/ /doi.org/10.
1109/ACCESS.2020.3014522 .
[34] Y. Chen, Y. Li, C. Li, Electronic agriculture, blockchain and digital agricultural
democratization:
origin, theory and application, J. Clean. Prod. 268 (2020), https:/ /
doi.org/10.1016/j.jclepro.2020.122071 .
[35] M.D. Borah, V.B. Naik, R. Patgiri, A. Bhargav, B. Phukan, S.G.M. Basani, Supply
Chain
Management in Agriculture Using Blockchain and IoT, Springer, 2020,
pp. 227 –242, https:/ /doi.org/10.1007/978-981-13-8775-3_11 .
[36] P. Chun-Ting, L. Meng-Ju, H. Nen-Fu, L. Jhong-Ting, S. Jia-Jung, Agriculture
blockchain
service platform for farm-to-fork traceability with IoT sensors, in:
International Conference on Information Networking , vol. 2020, 2020, pp. 158– 163,
https:/ /doi.org/10.1109/ICOIN48656.2020.9016535 .
[37] G. Mirabelli, V. Solina, Blockchain and agricultural supply chains traceability: research
trends
and future challenges, Procedia Manuf. 42 (2020) 414 –421, https:/ /doi.org/10.
1016/j.promfg.2020.02.054 .
[38] V.S. Yadav, A.R. Singh, R.D. Raut, U.H. Govindarajan, Blockchain technology
adoption
barriers in the Indian agricultural supply chain: an integrated approach,
Resour. Conserv. Recycl. 161 (2020), https:/ /doi.org/10.1016/j.resconrec.2020.
104877.
[39] L. Hang, I. Ullah, D.H. Kim, A secure fish farm platform based on blockchain for agri-
culture
data integrity, Comput. Electron. Agric. 170 (2020), https:/ /doi.org/10.1016/j.
compag.2020.105251 .
[40] X. Li, D. Wang, M. Li, Convenience analysis of sustainable E-agriculture based on
blockchain
technology, J. Clean. Prod. 271 (2020), https:/ /doi.org/10.1016/j.
jclepro.2020.122503 .
[41] S. Benedict, Serverless blockchain-enabled architecture for IoT societal applications,
IEEE
Trans. Comput. Soc. Syst. 7 (5) (2020) 1146 –1158, https:/ /doi.org/10.1109/
TCSS.2020.3008995 .
[42] S.K. Singh, S. Rathore, J.H. Park, BlockIoTIntelligence: a blockchain-enabled intelli-
gent
IoT architecture with artificial intelligence, Futur. Gener. Comput. Syst. 110
(2020) 721 –743, https:/ /doi.org/10.1016/j.future.2019.09.002 .
[43] S.S. Patra, C. Misra, K.N. Singh, M.K. Gourisaria, S. Choudhury, S. Sahu,qIoTAgriChain:
IoT blockchain traceability using queueing model in smart agri-
culture, in: EAI/Springer Innovations in Communication and Computing, 2021,pp. 203 –223.
[44] Q.N. Tran, B.P. Turnbull, H.-T. Wu, A.J.S. de Silva, K. Kormusheva, J. Hu, A survey
on 
privacy-preserving blockchain systems (PPBS) and a novel PPBS-based framework
for smart agriculture, IEEE Open J. Comput. Soc. 2 (2021) 72 –84,https:/ /doi.org/10.
1109/ojcs.2021.3053032 .
[45] S.J. Anand, K. Priyadarsini, G.A. Selvi, D. Poornima, Iot-based secure and energy
efficient
scheme for precision agriculture using blockchain and improved Leach algo-
rithm, Turk. J. Comput. Math. Educ. 12 (10) (2021) 2466 –2475. Accessed: Nov.
15, 2021. [Online]. Available: https:/ /www.turcomat.org/index.php/turkbilmat/
article/view/4857 .533 Integration of blockchain and IoT edge devices for smart agriculture[46] M. Sandeep Kumar, V. Maheshwari, J. Prabhu, M. Prasanna, R. Jothikumar, Applying
blockchain
in agriculture: a study on blockchain technology, benefits, and challenges,
in: EAI/Springer Innovations in Communication and Computing, Springer Scienceand Business Media Deutschland GmbH, 2021, pp. 167 –181.
[47] H. Patel, B. Shrimali, AgriOnBlock: Secured Data Harvesting for Agriculture Sector
Using 
Blockchain Technology, ICT Express, 2021, https:/ /doi.org/10.1016/j.icte.
2021.07.003 .
[48] U. Sengupta, H.M. Kim, Meeting changing customer requirements in food and agri-
culture
through the application of blockchain technology, Front. Blockchain 4
(2021), https:/ /doi.org/10.3389/fbloc.2021.613346 .
[49] Y. Zhang, J. Li, L. Ge, Research on agricultural product supply chain based on
internet
of things and blockchain technology, Adv. Intell. Syst. Comput. 1343
(2021) 11 –17,https:/ /doi.org/10.1007/978-3-030-69999-4_2 .
[50] Y. Bai, K. Fan, K. Zhang, X. Cheng, H. Li, Y. Yang, Blockchain-based trust manage-
ment
for agricultural green supply: a game theoretic approach, J. Clean. Prod. 310
(2021), https:/ /doi.org/10.1016/j.jclepro.2021.127407 .
[51] S. Dong, L. Yang, X. Shao, Y. Zhong, Y. Li, P. Qiao, How can channel information
strategy
promote sales by combining ICT and blockchain? Evidence from the agricul-
tural sector, J. Clean. Prod. 299 (2021), https:/ /doi.org/10.1016/j.jclepro.2021.126857 .
[52] C.S. Bhusal, Blockchain Technology in Agriculture: a case study of blockchain Start-up
Companies,
Int. J. Comput. Sci.Iinf. Technol. 13 (5) (2021), https:/ /doi.org/10.5121/
ijcsit.2021.13503 .
[53] S. Al-Amin, S.R. Sharkar, M.S. Kaiser, M. Biswas, Towards a blockchain-based
supply
chain management for e-agro business system, Adv. Intell. Syst. Comput.
1309 (2021) 329– 339, https:/ /doi.org/10.1007/978-981-33-4673-4_26 .
[54] A.A. Mukherjee, R.K. Singh, R. Mishra, S. Bag, Application of blockchain technology
for
sustainability development in agricultural supply chain: justification framework,
Oper. Manag. Res. (2021), https:/ /doi.org/10.1007/s12063-021-00180-5 .
[55] G. da Silva Ribeiro Rocha, L. de Oliveira, E. Talamini, Blockchain applications in agri-
business:
a systematic review, Future Internet 13 (4) (2021), https:/ /doi.org/10.3390/
fi13040095 .
[56] T.H. Pranto, A.A. Noman, A. Mahmud, A.B. Haque, Blockchain and smart contract
for
IoT enabled smart agriculture, PeerJ Comput. Sci. 7 (2021) 1 –29,https:/ /doi.org/
10.7717/PEERJ-CS.407 .
[57] M. Biswas, T.M.N.U. Akhund, M.J. Ferdous, S. Kar, A. Anis, S.A. Shanto, BIoT:
blockchain
based smart agriculture with internet of thing, in: Proceedings of the 2021
5th World Conference on Smart Trends in Systems Security and Sustainability, WorldS42021 , 2021, pp. 75 –80,https:/ /doi.org/10.1109/WorldS451998.2021.9513998 .
[58] I.E. Kyzy, H. Song, A. Vajdi, Y. Wang, J. Zhou, Blockchain for consortium: a practical
paradigm 
in agricultural supply chain system, Expert Syst. Appl. 184 (2021), https:/ /doi.
org/10.1016/j.eswa.2021.115425 .
[59] R. Mavilia, R. Pisani, Blockchain for Agricultural Sector: The Case of South Africa,
African
J. Sci. Technol. Innov. Dev, 2021, https:/ /doi.org/10.1080/20421338.2021.
1908660 .
[60] M. Verma, Smart contract model for trust based agriculture using blockchain technol-
ogy,
Int. J. Res. Anal. Rev. 344 (2) (2021) 2348 –2349. Accessed: Nov. 15, 2021.
[Online]. Available: www.ijrar.org .
[61] N. Niknejad, W. Ismail, M. Bahari, R. Hendradi, A.Z. Salleh, Mapping the research
trends
on blockchain technology in food and agriculture industry: a bibliometric anal-
ysis, Environ. Technol. Innov. 21 (2021), https:/ /doi.org/10.1016/j.eti.2020.101272 .
[62] S. Hu, S. Huang, J. Huang, J. Su, Blockchain and edge computing technology enabling
organic
agricultural supply chain: a framework solution to trust crisis, Comput. Ind.
Eng. 153 (2021), https:/ /doi.org/10.1016/j.cie.2020.107079 .534 Swati Nigam et al.[63] W. Liu, X.F. Shao, C.H. Wu, P. Qiao, A systematic literature review on applications of
information and communication technologies and blockchain technologies for preci-
sion agriculture development, J. Clean. Prod. 298 (2021), https:/ /doi.org/10.1016/
j.jclepro.2021.126763 .
[64] M.H. Ronaghi, A blockchain maturity model in agricultural supply chain, Inf. Process.
Agric. 8 (3) (2021) 398 –408, https:/ /doi.org/10.1016/j.inpa.2020.10.004 .
[65] G.S. Sajja, K.P. Rane, K. Phasinam, T. Kassanuk, E. Okoronkwo, P. Prabhu, Towards
Applicability of Blockchain in Agriculture Sector, Mater. Today Proc, 2021, https:/ /
doi.org/10.1016/j.matpr.2021.07.366 .
[66] W. Ren, X. Wan, P. Gan, A double-blockchain solution for agricultural sampled data
security in internet of things network, Futur. Gener. Comput. Syst. 117 (2021)
453–461, https:/ /doi.org/10.1016/j.future.2020.12.007 .
[67] S. Saurabh, K. Dey, Blockchain technology adoption, architecture, and sustainable
Agri-food supply chains, J. Clean. Prod. 284 (2021), https:/ /doi.org/10.1016/j.
jclepro.2020.124731 .
[68] P.R. Srivastava, J.Z. Zhang, P. Eachempati, Blockchain technology and its applications
in agriculture and supply chain management: a retrospective overview and analysis,
Enterp. Inf. Syst. (Oct. 2021) 1 –24,https:/ /doi.org/10.1080/17517575.2021.1995783 .
[69] T. Narayanaswamy, P. Karthika, K. Balasubramanian, Blockchain Enterprise: use
cases on multiple industries, in: EAI/Springer Innovations in Communication and
Computing, Springer Science and Business Media Deutschland GmbH, 2022,
pp. 125 –137.
[70] B. Bera, A. Vangala, A.K. Das, P. Lorenz, M.K. Khan, Private blockchain-envisioned
drones-assisted authentication scheme in IoT-enabled agricultural environment,
Comput. Stand. Interfaces 80 (2022), https:/ /doi.org/10.1016/j.csi.2021.103567 .
[71] N. Kamalakshi, Naganna, Role of blockchain in agriculture and food sector: a summary,
EAI/Springer Innov. Commun. Comput (2022) 93 –107, no. 978-3-030-76215 –5.
https:/ /doi.org/10.1007/978-3-030-76216-2_6 .
About the authors
Urvashi Sugandh is an assistant professor
at the Department of Computer Science
Engineering, HMR Institute of technology
and management, affiliated to Guru Gobind
Singh Indraprastha U niversity, Delhi by
Govt. of NCT Delhi. Currently, she is
pursuing Ph.D. from Banasthali Vidyapith,
Rajasthan. She has been awarded master’s
degree in Master of Technology from
Department of Information Technology of
Banasthali Vidyapith, Rajasthan in 2014. She
has published two patents and four research papers in International/National
Conferences. Her Research area is Blockchain, Data Mining and software
engineering.535 Integration of blockchain and IoT edge devices for smart agricultureManju Khari is an associate professor in
Jawaharlal Nehru University, New Delhi,
prior to the university she worked with
Netaji Subhas University of Technology,
East Campus, formerly Ambedkar Institute
of Advanced Communication Technology
and Research, Under Govt. Of NCT
Delhi. Her PhD in Computer Science and
Engineering from National Institute of
Technology Patna and she received her
master’s degree in Information Security from
Ambedkar Institute of Advanced Commu-
nication Technology and Research, affiliated
with Guru Gobind Singh Indraprastha University, Delhi, India. She has
80 published papers in refereed National/International Journals and
Conferences (viz. IEEE, ACM, Springer, Inderscience, and Elsevier),
10 book chapters in a Springer, CRC Press, IGI Global, Auerbach. She is
also co-author of two books published by NCERT of XI and XII and
co-editor in 10 edited books. She has also organized five international con-
ference sessions, three faculty development programme, one workshop, one
industrial meet in her experience. She delivered an expert talk, guest
lecturers in International Conference and a member of reviewer/technical
program committee in various International Conferences. Besides this,
she associated with many International research organizations as Associate
Editor/Guest Editor of Springer, Wiley and Elsevier books, and a reviewer
for various International Journal.
Swati Nigam is currently an assistant
professor at the Department of Computer
Science, Banasthali Vidyapith, Rajasthan,
India. She has been awarded PhD degree in
Computer Science from Department of
Electronics and Communication, University
of Allahabad, India in 2015. She has been a
post-doctoral fellow under National Post-
Doctoral Fellowship scheme of Science and
Engineering Research Board, Department
of Science and Technology, Government of
India. Earlier she has been awarded Senior
Research Fellowship by Council of Scientific and Industrial Research,536 Swati Nigam et al.Government of India. She has authored more than 20 articles in
peer-reviewed journals, book chapters and conference proceedings.
She has also published an authored book in Springer publications. Sheis a designated reviewer of several SCI journals like IEEE Access,Computer Vision and Image Understanding, Journal of ElectronicImaging, Multimedia Tools and Applications, etc. She has been publication
chair, publicity chair, TPC member and reviewer of various reputed con-
ferences. She is a professional member of IEEE and ACM. Her researchinterests include object detection, object tracking and human behavioranalysis.537 Integration of blockchain and IoT edge devices for smart agricultureThis page intentionally left blankHiren/uni00A0Kumar/uni00A0Thakkar
Chinmaya/uni00A0Kumar/uni00A0Dehury
Prasan/uni00A0Kumar/uni00A0Sahoo
Bharadwaj/uni00A0Veeravalli/uni00A0 /uni00A0/uni00A0Editors
Predictive 
Analytics in 
Cloud, Fog, and 
Edge Computing
Perspectives and Practices 
of/uni00A0Blockchain, IoT, and 5GPredictive Analytics in Cloud, Fog, and Edge
ComputingHiren Kumar Thakkar • Chinmaya Kumar Dehury 
Prasan Kumar Sahoo  Bharadwaj Veeravalli
Editors
Predictive Analytics in
Cloud, Fog, and Edge
Computing
Perspectives and Practices of Blockchain, IoT,
and 5GEditors
Hiren Kumar Thakkar
Department of Computer Science and
Engineering
Pandit Deendayal Energy University
Gandhinagar, Gujarat, IndiaChinmaya Kumar Dehury
Institute of Computer Science
University of Tartu
Tartu, Estonia
Prasan Kumar Sahoo
Department of Computer Science and
Information Engineering
Chang Gung University
Guishan, TaiwanBharadwaj Veeravalli
Department of Electrical and Computer
Engineering
National University of Singapore
Singapore, Singapore
ISBN 978-3-031-18033-0 ISBN 978-3-031-18034-7 (eBook)
https://doi.org/10.1007/978-3-031-18034-7
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland
AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandPreface
In the recent past, the number of connected devices has grown exponentially, leading
to enormous amount of raw data generation. However, abundant amount of raw datais meaningless unless analysed to mine the informative patterns. In this regard, raw
data need to be process and analysed at device level (edge computing), network level
(fog computing), and in the data centre (Cloud computing). Designing an efﬁcientpredictive algorithm is a challenging task at device level as well as network level
considering the limitations of computation power. On the contrary, cloud computing
supports massive computation capacity to design an efﬁcient predictive algorithm,but it suffers due to the high latency. Additionally, attempts are made to integrate
the cross technologies such as blockchain, IoT, and 5G with cloud computing
for better application designing and support. This book attempts to provide acomprehensive review of edge, fog, and cloud computing with detailed description
on their applicability, limitations, and how each technology complements each
other. Moreover, the book focuses on predictive analytics in cloud, fog, and edgecomputing as well as on perspectives and practices of blockchain, IoT, and 5G. It
covers the domains such as healthcare security in cloud computing, watermarked
medical image transmission over the cloud, role of blockchain in cloud computing,cloud-based smart controlled environment designing, serverless data pipelines for
IoT data analytics, impact of 5G technologies on cloud analytics, and 5G-enabled
smart city using cloud environment.
Gandhinagar, Gujarat, India Hiren Kumar Thakkar
Tartu, Estonia Chinmaya Kumar DehuryGuishan, Taiwan Prasan Kumar Sahoo
Singapore, Singapore Bharadwaj Veeravalli
vAcknowledgement
We would like to ﬁrst thank the Almighty for proving us the strength to pursue
the idea and carry it forward to make a comprehensive edited book on thepredictive analytics of cloud, fog, and edge computing. Our sincere thanks to
all the contributors who have provided their valuable time, support, and timely
contributions to make this book successful. We would also like to thank all thereviewers for their informative and constructive suggestions to the contributors
to improve the chapters. Finally, we would like to thank our institutions, Pandit
Deendayal Energy University (PDEU), India; University of Tartu, Estonia; ChangGung University, Taiwan; and the National University of Singapore, for providing
all the required resources for drafting, proofreading, and editing the book.
viiContents
Collaboration of IoT and Cloud Computing Towards Healthcare
Security ............................................................................ 1
Shwetank Kumar, Anjana Mishra, Amisha Dutta, and Aditya Raj
Robust, Reversible Medical Image Watermarking for
Transmission of Medical Images over Cloud in Smart IoT Healthcare .... 23
K. Jyothsna Devi, M. V . Jayanth Krishna, Priyanka Singh, José
Santamaría, and Parul Bakaraniya
The Role of Blockchain in Cloud Computing ................................. 37
Hiren Kumar Thakkar, Kirtirajsinh Zala, Neel H. Dholakia, Aditya Jajodia,
and Rajendrasinh Jadeja
Analysis and Prediction of Plant Growth in a Cloud-Based Smart
Sensor Controlled Environment ................................................ 61
Aritra Nandi, Arghyadeep Ghosh, Shivam Yadav, Yash Jaiswal,
and Gone Neelakantam
Cloud-Based IoT Controlled System Model for Plant Disease
Monitoring ........................................................................ 75
Aritra Nandi, Asmita Hobisyashi, Shivam Yadav, and Hiren Mewada
Design and Usage of a Digital E-Pharmacy Application Framework ...... 91
Shatabdi Raut, Samikshya Moharana, Soumya Sahoo, Roopal Jena,
and Payal Patra
Serverless Data Pipelines for IoT Data Analytics: A Cloud
Vendors Perspective and Solutions ............................................. 107
Shivananda Poojara, Chinmaya Kumar Dehury, Pelle Jakovits,
and Satish Narayana Srirama
Integration of Predictive Analytics and Cloud Computing for
Mental Health Prediction ....................................................... 133
Akash Nag, Maddhuja Sen, and Jyotiraditya Saha
ixx Contents
Impact of 5G Technologies on Cloud Analytics ............................... 161
Kirtirajsinh Zala, Suraj Kothari, Sahil Rathod, Neel H. Dholakia,
Hiren Kumar Thakkar, and Rajendrasinh Jadeja
IoT Based ECG-SCG Big Data Analysis Framework for
Continuous Cardiac Health Monitoring in Cloud Data Centers ........... 177
Hiren Kumar Thakkar and Prasan Kumar Sahoo
A Workload-Aware Data Placement Scheme for Hadoop-Enabled
MapReduce Cloud Data Centers ............................................... 185
Hiren Kumar Thakkar
5G Enabled Smart City Using Cloud Environment .......................... 199
Parul Bakaraniya, Shrina Patel, and Priyanka Singh
Hardware Implementation for Spiking Neural Networks
on Edge Devices .................................................................. 227
Thao N. N. Nguyen, Bharadwaj Veeravalli, and Xuanyao FongCollaboration of IoT and Cloud
Computing Towards Healthcare Security
Shwetank Kumar, Anjana Mishra, Amisha Dutta, and Aditya Raj
1 Introduction
With use of an internet connection, utility computing provides amenity in the fact
that may be retrieved from anyplace on this planet. Internet of Things (IoT) as a
platform gathers real-time data, makes it easier to examine and analyze obtained
data, and so creates an interdependent environment that can be shared with a
variety of stakeholders. The method valetudinarian and therapeutical personnel
interrelate and work on a day to day basis is changing in the health-care sector. The
Internet of Things (IoT) revolution will be critical in linking abundance of modish
items, tablets, apparel gadget, android, i-phone, etc. and cloud health apps using
a variety of conveying procedure such as wireless detector networks, etc. Health
UOI apps ﬂuctuate from forbearing observation to persistence illness management
will save healthcare costs while also improving the timeliness and efﬁciency of
service. Because different diseases might occur at any time, healthcare services are
always a challenge. IoT have been extensively used to link accessible medical assets
and assist patients with chronic conditions with dependable, effective, and smart
healthcare services. Healthcare monitoring has made signiﬁcant improvements.
These accomplishments have proved the value of IoT in health management systems
and its brilliant time ahead [ 1].
The internet is now used by over 2 billion individuals all over the world for
different domains such as wireless sensor networks [ 2], healthcare [ 3], and robotics
etc. Kevin Ashton devises the idiom “Internet of Things” to describe growing
worldwide Internet-based information service architecture. The Internet of Things
combines ideas from pervasive, ubiquitous, and ambient computing, all of which
have matured over the last two decades and have now reached a point of maturity.
S. Kumar · A. Mishra ( /envelopeback) · A. Dutta · A. Raj
C.V . Raman Global University, Bhubaneswar, Odisha, India
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_112 S. Kumar et al.
The ‘Internet of Things,’ which will provide as a worldwide program to connect
physical entity, things, and humans, authorizing new methods of functioning,
interfacing, Interrelating, pleasing, and maintaining, will rule the future. UOI or
IoT is based on notion of object hyperlinking, which promises people a modish,
highly accessible world with a vast range of interactivity with the surroundings. This
shift from the Internet which is previously used for connecting end-user devices
to internet used for interconnecting physical objects with humans will open a
multiple business and market opportunities. The cloud and the Internet of Things
are collectively reliant. To compensate for its technological limitations, IoT may
take advantage of the Cloud’s practically limitless capabilities and resources (e.g.,
storage, processing, and energy). IoT can improve the cloud by broadening its reach
to include real-world objects and offering a enormous number of new assistance in
a dispersed and beneﬁt from new.
UOI or IoT is a new solution that dispenses the transmission architecture that
every innovation healthcare system need. While the Internet of Things may be
employed in a extensive range of industries and implementation, its importance in
health management is apparent. Detector collecting patients’ vitals, primary care
equipment, institution, health centers, rehabilitation department, dotage homes, and
home automation all beneﬁt from the Internet of Things’ core communications
foundation.
2 Inspiration
This system will allow for the analysis of communities and healthcare facilities,
as well as the provision of information regarding healthcare outcomes to patients.
Healthcare data is currently kept in massive databases and disseminated through a
variety of electronic channels. Why, therefore, might such information pique the
curiosity of well-organized, very deadly criminal gangs? These organizations can
utilize the information for a variety of purposes, including blackmailing individuals
with information about their ailments, selling information to marketing corporations
for use in product advertising campaigns, and obtaining fake prescriptions by
inventing forged identities. This motivation stems from the fact that IoT security
has never been addressed using portable computing before. The quantities of data
that may be generated on a daily basis need both storage and processing, which is
why cloud architecture is gaining popularity. As a result, evaluating the conventional
security systems usage of mobile computing ideas will become a future research
directions trend [ 4]. The omnipresent and widespread nature of IoT devices was the
primary impetus for doing this investigation. The fast growth of IoT devices and
computer security need strong protection.Collaboration of IoT and Cloud Computing Towards Healthcare Security 3
3 Related Work and Background
With such a enormous expand in community, security has ﬂatter a subject matter
which has acquired a plenty of observation in the latest senility. Because it handles
one end to another interchange across particular devices, soundness is crucial from
machine to machine. As the increasing development in IoT appliance and computer
hackers, adequate security is a censorious necessity for UOI or IoT.
Pervasive healthcare sensors have spawned a slew of research projects. The
majority of them purvey with information processing on appliance (for example,
utilizing SD cards as storage), or use intermediary nodes (for example, i-phones,
androids etc.), or stock details personally on computer network. However, there are
hardly any service that explicitly inscription the matter of information processing
and administration in the Cloud [ 5].
Many multinational tech companies are also taking interest in healthcare system.
Microsoft has inaugurated a World Wide Web document where customer utilizing
distinctive Health documentation from google health can have their distinctive
Health documentation assigned to a Microsoft Health Vault description. PHR
will perceive a 33% acquire in income as medical practitioner and the sick
person are using it in health IT system. Microsoft Cloud for Healthcare enables
healthcare companies to superintend health speciﬁcs at extent, creating it simpler
towards them to ameliorate patient involvement, synchronize care, and increase
functioning organization while also ensuring health records reliability, conformity,
and interoperation.
The given platform is private, and the ﬁrst assessment ﬁndings are based on
simulated sensors rather than actual equipment. On the contrary, there are currently
a variety of Cloud-based services specialized to storing detector-based data. Some
examples are Dropbox, Salesforce, etc. For ubiquitous data management, a variety
of Cloud Computing systems are currently available, both free (e.g., Google Drive,
icloud, etc.) and commercial (e.g., Microsoft’s Azure, Amazon Web Services, etc.).
Apart from Amazon AWS, the most of them do not give signiﬁcant developer
help for creating bespoke apps and incorporating Cloud Computing capability. None
of them are well-suited to provide services to healthcare-related apps.
In the not-too-distant future, individualized health examinations will become
the norm. Individuals will be given a personalized plan to combat disease, and
communal technology will authorize us to manage our own well-being. Data from
the actual society and backstop details provided by apparel medical devices with
correlated healthcare data will be extremely useful to nurturer and a valuable origin
of information for research groups, as they can easily obtain patient data and
backstop details outside of the equipment walls [ 6].
On-demand computing is a viable option for effective administration of prevalent
wellness program information due to its ﬂexibility and capacity to approach
measured assets and common conﬁguration in a distributed network way.
There have been numerous technological advancements in the ﬁeld of IOT in
healthcare systems in recent years. The development of a streamlined common4 S. Kumar et al.
transmission agreement linking cabled and cable less medical equipment is one of
the key reasons for this. Beneﬁcial to promote the utilization of IoT in medical,
many producers are disclosing all of the requisite APIs to connect with their
outcomes, authorizing for even more customisation. The architectural design of a
health observing and examining structure is the focus of current research on IOT
Cloud platform for pervasive healthcare scenarios. For example, VIRTUS is an e-
health middleware that provides a dependable, scalable, and secure communication
connection. It’s a publish/subscribe system that uses the XMPP protocol to securely
communicate data across the internet.
Because it is more suitable and less expensive than providing each employee a
job-related cell phone, healthcare experts and healthcare workers frequently bring
their personal devices in the workplace to utilize health treatment and apps. This
will result in BYOD (bring your own device) risks can be minimised in the IoT
health cloud environment [ 7].
4 Cloud Computing Deployment Models
Simply described, cloud computing or utility computing is a technology that
permits buyers to connect to a pool of divided provisioning. This might involve
information processing, spreadsheets, servers, interacting, instrument, or any other
web-reachable assets.
4.1 Public Internet
It’s a sort of cloud arranging where cloud computing assistance are supplied through
a publicly reachable webbing. This concept is an accurate depiction of cloud
computing [ 8]. The service provider in this cloud model delivers essential services to
a variety of clients. Users have no inﬂuence over where the infrastructure is located.
Besides the amount of security supplied by cloud web hosting companies for
ﬁnancial services offered to public cloud users, there may be few or no distinction in
the structural architecture of public and private clouds. The public cloud is ideal for
businesses that need to manage their load. The public cloud concept is cost-effective
due to lower capital and operating costs [ 9].
4.2 Corporate Cloud
It is also known as internal cloud or private cloud or Intestinal cloud. This
cloud computing framework is designed on a internet-based ﬁxed surrounding
and is guarded by a ﬁre barrier wall or ﬁre wall managed by a corporation’s IT
division. Only authorised users have access to the private cloud, which providesCollaboration of IoT and Cloud Computing Towards Healthcare Security 5
the company more supervision over their information. Physical computers, which
may be presented either locally or remotely, supply assets to corporate cloud
services from a separate pool. Private cloud is better suited to organizations with
unforeseen or dynamic needs, assignments with effective managerial expectations,
and dependability criteria. There are no extra security requirements or bandwidth
constraints in a secure cloud that are present in a public cloud system.
4.3 Cloud Hybrid
It’s a form of utility computing that is all present in one. It can be said that it is a
layout of two or more internet servers, such as commercial, and collaborative clouds
that are paired or joined together even though remain different entities. Hybrid
clouds are capable of transcending provider borders and surpassing isolation; as
a result, they cannot be easily classiﬁed as public, private, or community clouds. By
incorporation, consolidation, and modiﬁcation with another cloud package/service,
the user can improve capacity and capabilities. The assets in a cloud hybrid system
are managed either in-house or by third-party services. It is a hybrid of two layout or
structure in which workloads are moved in between the corporate and public internet
or cloud based on the necessity and demands of the enterprise.
4.4 Cloud Provider
It is a kind of cloud presenting wherein the layout is divided and shared by a huge
number of establishment belonging to the same society, for e.g., banks and trading
enterprises. It’s a multi-national arrangement that’s shared across numerous ﬁrms.
These people are frequently concerned about the same things when it comes to
privacy and scalability. The communities’ primary goal is to fulﬁl enterprise-related
purposes. The cloud provider can be supervised internally or by another party
suppliers, and it can be presented either outside or inside. Because the price is
shared by several groups in the society, the internet or cloud environment may save
money [ 10].
5 Utility Computing Service Models
5.1 Software as a Service (SaaS)
SaaS (Software as a Service) is quickly expanding. It usage the internet to distribute
apps with a client-side interface that are administered by a arbitrator purveyor.6 S. Kumar et al.
Although SaaS programmes may be used directly from an internet browser without
the requirement for acquisition or installation, they still require extensions. The
purchaser can deploy an application on a cloud infrastructure thanks to the cloud
provider. SaaS eradicate the need to inaugurate and operate apps beside individual
devices due to its online distribution approach. Because all of this can be handled
by vendors in this approach, organisations may enhance their service and repair.
This includes apps, compile time, ammunition, service-oriented, System software,
hypervisor, cyberspace, arsenal, and intercommunicating. Email and interaction, as
well as health insurance applications, are popular SaaS services. Web interfaces are
common among SaaS companies.
5.2 Infrastructure as a Service (IaaS)
Infrastructure as a Service (IaaS) is a technology that helps to track and manage
distant data centre facilities including computation, arsenal, and intercommunicat-
ing. Customer perhaps pay for it entrenched on how much they use it, just as they
would for any other commodity. Applications, data, runtime, and infrastructure are
all within the control of IaaS users. Virtualization, servers, storage, and networking
are all things that providers can still handle. Above the virtualization layer, IaaS
companies also provide analytics, communications channels, and other capabilities.
5.3 Platform as a Service (PaaS)
It’s a type of utility enumerating system which gives a framework for users to
design, execute, and deploy applications without having to worry about the archi-
tecture. The Cloud Service Provider will take care of the bottom level architecture,
network architecture, and security for you. Third-party suppliers can administer the
OS, virtualization, and PaaS applications with this innovation. The programmes are
managed by developers. Cloud characteristics like as extensibility, cross functional,
SaaS sanctioning, high accessibility, and further are inherited by PaaS applications
[11]. This paradigm beneﬁts businesses since it simpliﬁes code, simpliﬁes business
strategy, and aids in the migration of programs to a hybrid approach.
6 Security Issues
Cloud computing services not only supply customers with various sorts of services,
but they also divulge data, adding to the security considerations and hazards asso-
ciated with cloud computing platforms. IaaS, is a base service which directly offers
the most remarkable properties of a cloud. IaaS furthermore concede technocrat to
launch castigate which requisite a clique of computer potentiality, like brute-force
thumping. IaaS supports several implicit computers, making it a perfect tenet forCollaboration of IoT and Cloud Computing Towards Healthcare Security 7
Table 1 Security issues on cloud and IoT
Security threats Illustration Prevention Centre of attention
Physical attack Physical attacks are a
kind of cryptanalysis,
which is the study of
systems engineering
to deeply nested
elements of
equipment and
software by
exploiting
administration
peculiarities.After analysing the
ﬂaws and threat
platform that the IoT
gateway encounters,
the appropriate
security safeguards
may be deployed.IoT software based
Equipment collapse One might have a
valid gadget,
although if you don’t
safeguard it with the
appropriate measure
of assurance, hackers
will be able to infect
it with virus or
ransomware.Different-factor
conformation or
authentication
protects against
unauthorized party
by needing at least
two security
mechanisms, either
of which should be
physically held by
the owner.IoT and cloud based
Cloud malware attacks Malware infestation
in the utility
computing
architecture is one of
the most major
safety threats caused
by inappropriate
conﬁguration and
insufﬁciency of
protection at the
software level.User segmentation:
Though
segmentation is not
perfect, it can be
used to slow down
the spread of
malware in the cloudUtility or cloud
computing based
technophile to undertake pounce requiring a huge number of pouncing exemplar. A
further security issue associated with cloud architectures is data loss [ 4].
Unwanted pooled liveware, into the bargain as outsiders technophile, can easily
avail statistics in cloud systems. Intrinsic organization have untroubled outpouring
to statistics, further on occasion or by omission. Foreign technophiles may use
castrating tools like bout expropriating and web mechanism espionage to acquire
ingress to the records in equivalent framework. Viruses and Trojans can be
distributed and cause signiﬁcant damage to cloud systems.
It’s censorious to determine feasible cloud threat so that a architecture with
upgraded safety or security approaches can be put in place to secure cloud
architecture. In Table 1the security threats, illustration, prevention and the centre of
attention is mentioned brieﬂy for the clear understanding.8 S. Kumar et al.
7 Threats in Cloud Computing
7.1 Compromised Identities and Broken Security
Attempting to issue rights relevant to the participant’s work function, associations
may struggle with authentication and authorization. When a job character commutesor a user evacuates company, they occasionally neglect to delete user access.
Stolen process of authentication were the cause of the Anthem hack, which
exposed more than 80 million customer information. Anthem had neglected toimplement multifactor authentication, which meant that once the intruders acquired
the information, it was game over. Many programmers have made the fatal mistake
of storing credentials and encryption techniques in external ﬁles by embedding themin programming language.
7.2 Data Infringement
Cloud settings suffer many of the same dangers as conventional enterprise networks,but since cloud hosting contain so much data, they’ve become a tempting target.
The extent of the harm is usually determined by the sensitivity of the data disclosed.
While thefts regarding ﬁnancial details make the news, hacks involving governmentdocuments and commercial secrets may be even more damaging. When a data
breach occurs, a corporation may face legal consequences. Costs associated with
data breaches and consumer warnings can add up quickly. Indirect impacts such asbrand harm and revenue loss can have a long-term inﬂuence on an organizational
objective.
7.3 Hacked Frontier and APIs
APIs are at the moment obtainable for each cloud amenity and software. These
gateways and APIs are used at information technology teams to regulate andamalgamate with cloud amenities, like cloud deployment, administration, surveil-
lance. The API certainty established the performance and protect of cloud services.
Arbitrator who bargains on APIs and enlarge on these connections run scurry thepossibility of revealing more amenities and accreditations, which increases the risk.
APIs and weak interfaces can put businesses at risk for security vulnerabilities
including anonymity, traceability, and scalability. APIs and conﬂuences are the mostrevealing parts of organizations since these are accessible over the online Platform.Collaboration of IoT and Cloud Computing Towards Healthcare Security 9
7.4 Manipulated System Vulnerabilities
The emergence of cross functionality in cloud enumerating, system vulnerabilities
and exploitable faults in programmes have become a major problem. As organisa-
tions splits cache, directory, and assets in immediate closeness, new attack surfaces
emerge. When compared to other IT expenses, the costs of addressing systemvulnerabilities are quite low.
7.5 Permanent Data Loss
Hackers had been continuously destroying data since last many years from cloud
storage to cause damage to the organizations. So, in order to increase security, cloud
providers also advise splitting data and apps over various zones. It’s important tohave sufﬁcient data backup and disaster recovery mechanisms in place. It is the
responsibility of both the cloud service provider and the data supplier to prevent
data loss. A client can encrypt the data and information before uploading it to thecloud, but that customer must be careful about protecting the encryption key. In
case of missing encryption keys, the data will be lost as well. Many compliance
regulations stipulate how long ﬁrms must keep audits and other documentation onﬁle. The loss of such sensitive information might have signiﬁcant implications.
7.6 Inadequate Assiduity
Accepting cloud computing without a complete knowledge of the environmentalhazards connected with it might cause many economic, ﬁnancial, technical, legal,
and compliance issues to the businesses. A lot of effort is required if a company is
seeking to transfer to the cloud or merge with another cloud-based company. Forexample, organizations that do not properly review a contract may not know about
the provider’s obligation if the data is lost or in case of any data breaches. If an
organization’s development team is unknown with cloud technologies, operationaland architectural challenges may occur when apps are provided to a speciﬁc cloud.
Since there are lots of risks involved with cloud computing, a company must go
through each and every situation before making the switch .
7.7 Cloud Service Inattention
Some of the most common types of attacks are DDoS assaults, spam, and phishing
emails. DDoS assaults must be recognised, and providers must supply tools for10 S. Kumar et al.
clients to look through the health of their cloud architecture. Purchaser should also
check with the providers to see whether they have to report ant issues. Even if users
aren’t directly selected by criminal actors, misuse of cloud service can result in
service outages and information loss.
7.8 DoS Attacks
DoS attacks have been prevalent for a long time, but due to evolvement of cloud
computing they have resurfaced and causes frequent disruptions. The systems will
run slowly or just time out because of these attacks. These DoS assaults consume a
lot of processing power, which have to be paid by the uses at last. Organizations
should be wary of asymmetric and application-level DoS assaults, which target
database weaknesses and web servers, as well as high-volume DDoS attacks.
Sometimes users are less prepared to deal with DoS assaults than cloud providers
[12]. The main focus is to have a strategy to mitigate the assault before it happens,
so management can get to those data when they’re needed.
7.9 Security Challenges in Cloud Infrastructure
7.9.1 Security Challenges
The Cloud Computing offers several Advantages but there are also some security
issues in cloud computing. The all security challenges in cloud computing represent
in Fig. 1discussed below.
7.9.1.1 Malicious Attacks
Menaces to a ﬁrm’s reliability may transpire from twain the exterior and interior.
Participants were accountable for 21% of cyber-attacks, as stated by 2011 Cyber
Security Watch Survey. Workers smacks, as stated by 33% of appellants, are
additionally expensive and detrimental to professions. Unlicensed ingress to and
wield of business particulars (63%) and larceny of psychological possessions were
the bulk habitual employee smacks (32%). Malicious users may get access to
sensitive information, resulting in data breaches. Unauthorized individuals have
carried out malicious assaults on the sufferer’s IP address and corporal web,
according to Farad Sabah. Data theft to revenge are examples of wicked agendas. In
a cloud environment, an insider has the ability to destroy whole infrastructures as
well as change or steal data.Collaboration of IoT and Cloud Computing Towards Healthcare Security 11
Fig. 1 Security challenges in cloud computing [ 9]
7.9.1.2 Storage and Backups
The cloud dealer ought-to guarantee a certain statistics is backed up on a regular
basis and that all security steps are taken. However, backup data is frequently found
in an unencrypted format, exposing the data to unwanted access. As a consequence,
facts reinforcements constitute a number of safety dangers. The additional server
hypervisor is used, the increase strenuous it becomes to reserves and save statistics.
The sole way to minimise reserves and ofﬂine depository capacity is to utilize
statistics de-replication.
7.9.1.3 Service Abduction
Unauthorized users acquiring illicit control over some permitted services is familiar
amenity seizing. Hacking, software exploitation, frauds are equitable procedures
this may be utilized. Sole of the mass acute threats has been spotted as narrative
seizing. Because no native language is spoken, the possibilities of an account being
hijacked are quite high.
7.9.2 Challenges of Deployed Models
The cloud deployment model classiﬁes the particular type of cloud environment
based on ownership, measure, cloud’s nature, scale and purpose. The server’s12 S. Kumar et al.
Fig. 2 Cloud computing
deployment models [ 13]
locations and utilizing community is by a cloud deployment model. Different types
of cloud computing deployment models are shown in Fig. 2.
7.9.2.1 Security Issues in Platform as a Service (PaaS)
Platform as a Service (PaaS) authorized cloud-based applications to be positioned
unaccompanied by the requirement to acquire and nourish the cardinal apparatus
and freeware layers [ 14]. PaaS depends on a stout and taut network. Reliability of
the PaaS manifesto alone and pledge of consumers applications entreaty installed on
a PaaS manifesto are the two software layers that makeup PaaS application security.
7.9.2.2 Relationships of Third-Party
Platform as a service (PaaS) gives many components to third-party web services
such as mashups and standard programming languages. In mashups, many source
elements combine into a single integrated unit. Therefore many issues related to
security may arise in a mashup. Users of PaaS mainly depend on the security of
web-based development tools. They rely on these tools as third-party services.
7.9.2.3 Life Cycle Development
Developers may resist the challenges of creating secure apps which can be hosted
in the cloud from the point of view of application development [ 15]. Cloud
applications evolving will have an impact on System Development Life Cycle
(SLDC). Software developers must bear in mind that PaaS apps must be modiﬁed
regularly, thus they must ensure that their application development procedures are
adaptable to accommodate changes. However, developers must be aware of theCollaboration of IoT and Cloud Computing Towards Healthcare Security 13
fact that any changes to PaaS may jeopardize application security. Developers
must be trained and informed on data legal issues in addition to safe development
methodologies so that data is not stored in improper areas [ 16].
7.9.2.4 Infrastructure Security
Since it is difﬁcult for software developers to have access to the underlying layers in
PaaS. It is the responsibility of the providers to protect the underlying infrastructure
as well as the application services. Instead of having security control for the
developers, they cannot ensure that the development environment tools offered by a
PaaS provider are safe [ 17].
7.9.3 Resource Pooling
Cloning is the process of duplicating or copying data. It can result in many
issues such as leakage of data, exposing the machine’s legitimacy. While it was
deﬁned, resource pooling as a service supplied by a provider to consumers that
allows them to access and share various resources based on their application needs.
Unauthorized access occurs as a result of resource sharing across the same network
[18]. According to a study on cloud and virtual computing, a Virtual Machine may
be supplied quickly, reversed to earlier circumstances, halted and simply resumed,
and transferred between two servers, posing non-auditable security risks.
7.9.4 Unencrypted Data
Data encryption is a method for dealing with a variety of external and hostile threats.
Because it lacks any security measure, unencrypted data is extremely exposed to
susceptible data. Unauthorized individuals can readily get access to unencrypted
data. Unencrypted data expose user information, allowing unauthorized users to
access data stored on cloud servers. For example, a very popular cloud-based ﬁle
sharing service Dropbox has been accused of using a single encryption key to protect
the data of all users [ 8]. These unencrypted and insecure data of the users allow
malevolent people to misuse the data in some way.
7.9.5 Identity Management and Authentication
Private data can be accessed by a user and it can be made available to other
services across the network when they use the cloud. Users’ credentials are used
to authenticate users through identity management. The lack of interoperability
caused by multiple identical tokens and identity negotiation methods, as well as
the constructive design, is a major challenge with Identity Management [ 19].14 S. Kumar et al.
7.9.6 Network Issues
Cloud computing makes use of the internet and distant computers to store and
operate data for a variety of purposes. This network is where all data is uploaded.
Security challenges with the cloud network are a top priority for H.B. Tabakki.
It offers users on-demand essential resources, software, and high bandwidth.
Actuality, this cloud’s network topology is prone to assaults and security concerns
such as cloud spyware injection, browser security difﬁculties, ﬂooding attacks,
incomplete data erasure, data shielding, and XML signature element wrapping, to
name a few [ 20].
7.9.6.1 Signature Element Wrapping
A well-known online service exploit is XML Signature Element Wrapping. This
protects the hostname and identiﬁcation value from unauthorized parties, but it
does not secure the location in the documents. The attacker sends SOAP messages
to the host computer and inserts jumbled data that the host machine’s user will
not comprehend. The XML Signature Wrapping attack changes the content of the
inscribed component of communication while leaving the signature intact which
may make it difﬁcult for the user to comprehend [ 12].
7.9.6.2 Browser Security
The information is sent over the network by the user via the browser. These browsers
use SSL technologies to encrypt the user’s identity and information. However,
utilizing sniffer software placed on the intermediate host, hackers from the mediator
host may be able to access this information. One should have only one identity, but
this information and data should allow for different levels of assurance, which can
be acquired by digital approvals [ 21].
7.9.6.3 Flooding Attacks
Sometimes the invader sends a lot of requests for cloud resources in a short period,
ﬂooding the cloud with demands. According to IBM research, the cloud can scale
depending on the number of requests. It will grow to meet the demands of invaders,
rendering resources unavailable to normal users.
7.9.6.4 SQL Injection Attacks
These are known to be the most aggressive cloud computing attacks in which
harmful codes are injected into SQL code. The attacker can use this techniqueCollaboration of IoT and Cloud Computing Towards Healthcare Security 15
to get unauthorized access to a database and other sensitive data. Any form of
SQL database may be attacked with SQL injection [ 22]. SQL injection and other
vulnerabilities are feasible because security is not prioritized enough throughout
development.
7.10 Point at Issue in the IoT Health Care Framework
An insulin push might be digitally commandeered and full insulin delivery admin-
istered to the patient might result in deadly insulin shock, according to a study
published in 2011. Cybercriminals breached the infrastructure connecting 206
clinics in the U.S.A in August 2014, obtaining 4.5 million patient information,
comprising personally identiﬁable information such as personalities, residences,
birth certiﬁcates, phone numbers, and personal data. In aggregate, IoT sensors
can help patients and improve healthcare, but they also can inform patients and
healthcare practitioners about hazards. Among several dangers and vulnerabilities,
we’ll go through the intricacies of the most relevant ones here:
7.10.1 Reliability
In a situation when gadgets connected to the network are vital to one’s wellbeing,
guaranteeing reliability is critical. A hack that changes the number of steps on
a wristband is likely innocuous, but malware on an insulin pump can be very
dangerous as it may cause death. Additionally, when linked to an unsecured or
easily attackable network, every connected device can act as an endpoint for possible
cyber assaults or wireless routers to a larger network including patient data [ 17].
On the illicit market, health data is now more valuable than credit card data.
Medical insurance data are ten times more valued than credit card validation value
(CVV) numbers and it may be used to buy pharmaceutical prescriptions and lodge
outstanding invoices on behalf of others.
7.10.2 Discretion
It refers to the requirement of all professionals who are involved in the acquisition
and treatment of health records or documentation to maintain the conﬁdentiality of
such data, ensuring that health information is not shared with third parties without
their explicit agreement. There may be advantages to having all of your health data
in one single place, but there is also a risk [ 13]. It is particularly beneﬁcial for
physicians to develop a diagnosis, and it may aid the job of analytical algorithms
that, given a large amount of data, attempt to anticipate a patient’s health state. But16 S. Kumar et al.
what if health insurance companies or employers had access to this information?
A person with a chronic disease may face discrimination, such as being denied
health insurance or employment. It is once again vital to develop systems that ensureprivacy protection.
7.10.3 Solitude
Even if patient records are anonymized and scrubbed of individuals’ identiﬁabledetails, they may readily be consolidated with other data to offer a complete
image of each individual. It was identiﬁed that many levels of privacy that
must be addressed as follows: (1) Device anonymity, or unauthorized devicehardware/software deception; (2) Privacy during an interaction, such as data
transmission; (3) Privacy in storage, which should only impact the absolute neces-
sities of documentation and be unencrypted for identity theft protection; and (4)Conﬁdentiality at computation, which must be done under the data owner’s wishes.
7.10.4 Unintended Efforts
It describes persons who change their behavior in response to physiological datadisplayed on their gadgets. For example, an individual may become less energetic
when they observe that many others have become less active; others may be hooked
to their athletic training; others may become concerned about their health, yet othersmay attempt to self-diagnose by analyzing their statistics. The argument is that
data isn’t the same as understanding. Only experts can transform the raw data into
meaningful data and valuable information. Cardiac data may be transformed intoinsights by a doctor, who can then make decisions and act correctly, but it can also
be misinterpreted by a layperson. As a result, users must be informed that these
detectors produce data rather than insight.
7.11 Challenges
Although the Internet of Things may provide limitless opportunities for healthcareprofessionals and private individuals, it is vital to overcome signiﬁcant and diverse
difﬁculties to create a big workforce. Although some offered solutions, similar
difﬁculties continue to exist today. The most essential ones, as well as similartopics, are highlighted here and should inspire additional inquiry to identify good
explanations.Collaboration of IoT and Cloud Computing Towards Healthcare Security 17
7.11.1 Security
An intelligent item collects data (such as vital signs) and sends it to a processing
facility, where database analytics software converts it into intelligence. To transfer
this data, the tangible thing must, of course, be attached to a network connection.
As a result, smart technologies, like any other networked device, may becomevulnerable to assaults that result in data destruction and/or data tampering. It’s
important to have secure connections (i.e., no harmful and/or unintentional assaults)
if we want IoT in healthcare to become ubiquitous and pervasive. Basic securitysolutions (such as encryption) may appear to be adequate, however, the IoT situation
has unique peculiarities.
7.11.2 Conﬁdentiality
In the Digital Age, a lot of consumers are becoming completely conscious thatconﬁdential data is frequently used as a form of exchange for free services. For
example, they are conscious that a searching engine’s service is paid for with theknowledge of the terms they employ, and that utilizing a social media platform is
paid for with private information. Even in the dynamic environment, there is a rising
understanding that free programs are paid for using the personal data of the users.The IoT health scenario is a little hazy.
7.11.3 Assimilation
The Internet of Things is a fragmented setting, with a wide range of devices,vendors, and interoperability. As a result, there are a variety of devices with distinct
properties. The innovation and deployment of IoT devices are hampered by this
diversity. How can I be assured that the gadget I purchased to monitor my heartrate is interoperable with my smartphones, for example, as a ﬁnal user? And
a manufacturer may think that what expertise they should incorporate into the
product so that it doesn’t become obsolete right away. Since so many differentmanufacturers are competing against each other and attempting to impose their
standard or ecosystem is not conducive for the growth of the IoT health industry as
a whole. On the converse, it causes it to slow down. A single standard that may beutilized by any IoT device is required. Interestingly, the picture appears to be going
in this approach, as seen by the Open Connectivity Foundation 4, which delivers a
free software IoT platform for creating a linked environment.
7.11.4 Business Illustration
Despite the great prospective in terms of perspectives that can be acquired fromArtiﬁcial Intelligence and machine learning evaluation on health records for the18 S. Kumar et al.
avoidance of deadly infections, the possibility of records being put up for sale to
third parties for marketing gain stresses the need for users to trust enterprises that
are generally and inevitably interested in proﬁt; even though the great possibility
in terms of perspectives which can be procured from AI and machine learning
interpretation on health data for the mitigation of deadly infections, the consequence
of data being sold to the third party emphasizes the need for users to trust enterprises
that are typically and subsequently Although some manufacturers have made
signiﬁcant investments in IoT health (for example, ﬁtness ﬁrms), the market lacks a
deﬁned economic model.
7.12 Dispensing Reﬁned Patient Supervision
The Body Guardian Remote Monitoring Device is a program that provides this
feature, allowing doctors to ﬁne-tune their treatment while letting individuals live
their lives as they like. The system manages security concerns in several ways. For
starters, it isolates patient identity data from observation data. The software then
encodes data on the device, while it is being transmitted and stored. This technique
is frequently employed with two categories of people who have a high type of
healthcare need: severely ill people and the aged. Medical professionals may keep a
careful eye on their patients’ health and act if necessary.
7.13 Character of IoT in Healthcare
The act of doing precautionary or essential treatments to enhance a patient’s well-
being is described as healthcare. This can be accomplished by surgically, the
administration of drugs, or other lifestyle changes. Typically, these services are
provided by a healthcare system comprised of institutions and clinicians. IoT is
playing a signiﬁcant role in healthcare in many sectors. Aged care is keeping track of
senior residents and patients at a nursing facility or institution [ 23]. Data collection
is the most developed ﬁeld in healthcare, and it includes many items that we see
at the bedside in hospitals, such as the EKG monitor. With new advancements in
the realm of IoT, this is an area that keeps on growing. At a lesser cost, real-time
location is utilized to track persons and property. The interrelated features of cloud
and IoT represent in Table 2.
7.14 Conclusion
There have been no recorded incidents of hostile attackers attacking a pacemaker,
although experts have demonstrated that it is conceivable. In addition, ForresterCollaboration of IoT and Cloud Computing Towards Healthcare Security 19
Table 2 Interrelated feature of cloud and IoT
Speciﬁcation IoT Cloud
Dismissal Prevalent Consolidate
Extendable Restricted Omnipresent
Elements Non-ﬁctional thing Virtual assets
Depository Finite or not any Illimitable
Large data-set Origin Mode to supervise statistics
Research predicts that malware for medical devices or wearables may emerge soon.
In clinics, the platforms to which those devices are connected frequently include
a lot of legacy technology with very old software and devices which cannot be
easily upgraded. Devices undergo the impermanent condition by the practitioner
in different ways, such as BYOD. It has now become very difﬁcult to identify the
operating systems and device’s life cycles administrations.
Hold devices that access the network may have acquired connectivity and
connection issues. Although these gadgets are not issued through conventional way,
they are unable to understand the weaknesses that cybercriminals may exploit these
devices. IoT in the condition of public and personal health is a luring situation.
Many phenomenon of the miracles that can be done by connecting cheap sensors to
the human body is present throughout the internet. The computational intelligence
paradigm is set to alter not just health avoidance and therapeutic interventions,
but also the whole health business, which includes pharmacological ﬁrms, research
centers, insurance companies, hospital centers, public health, and health status.
All of that is changing, but not every new offering, operation, or equipment helps
to the improvement of the situation. There are many black clouds on the horizon:
cybersecurity, conﬁdentiality, compatibility, and business models are just a few of
the issues that might undermine the advantages that IoT health can bring to our
society. This may be done by enforcing security rules and putting in place solutions
that focus on weaknesses, conﬁguration evaluations, ransomware defenses, and
behavior and event tracking.
7.15 Future Work
IoT is gaining a lot of traction throughout the world because of its high request in
the sector of telecommunication. In the Internet of Things, items are everywhere in
some way or the other. Because gadgets must be linked to the internet in order to
work properly, will this expand request and utilization? We may infer that it has
a bright time ahead based on its current insistence and prospective. Digital city
evolution will be aided by technological advancements. The Internet of Things isn’t
only for humans. Smart or inventive technologies are increasingly being used by
businesses and towns in order to conserve money and time. This will aggregate20 S. Kumar et al.
in an computerized city that can be administered remotely, with details collected
via Internet of things (IoT) devices and numerous telecommunication. We plan
to provide a futuristic vision of how IoT might impact our daily lives and how
utility computing can improve IoT services in the time ahead. The IoT-information
management system (IMS) communication platform’s purpose is to establish a
common foundation for the Future Internet that is built on existing IoT, Utility
Computing, and Information management system (IMS) technologies. IMS is the
appropriate option for combining IoT and Internet networks and maximizing their
mutual beneﬁts. We will combine IoT objects with standardized IMS infrastructure
as the fabric in the future, and then resume to superintend operations and analyze
trafﬁc.
Cloud or utility computing indicates to the on-request distribution of com-
puter assets through the web with pay-as-you-go pricing. Instead of purchasing,
hosting, and maintaining physical information centers and servers, you may use
telecommunication services. Computing assistance, storehouse, and databases are
examples of services that you may use when you it is required, thanks to the
service provider. We can now link any smart device to utility computing. It opens
up a whole new universe of opportunities in terms of jobs, apps, assistance, and
infrastructure. Utility computing’s future may be seen as a mix of internet-based
software and on-premises computation, which will aid in the creation of hybrid
IT answers. The redesigned cloud is ascendable and versatile, allowing for data
center dependability, security and administration. The structured procedure and a
preferable technique to procedure information will be important aspects of utility
computing. Numerous ﬁrms have previously migrated their work duties to the cloud
or internet, according to innovecs.com . Utility computing is now used by more
than 80% of big organizations. By 2024, this ﬁgure will have risen to greater than
90%. The wonderful new automation future of which we’ve always dreamed has
ultimately come.
It’s clever and jam-packed with information. The Internet of Things, abbreviated
as IoT, refers to household gadgets, automobiles, transportable devices, and other
electronic equipment that are linked to the cloud or internet and share details or
information with one another. It’s a reasonable chance that you possess at least one
of these devices in 2019. By the twenty-second century, the Internet of Things is
expected and predicted to grow to more than 80 billion appliances or devices and
many more.
Internet of Things and utility computing are the most signiﬁcant technologies
together when we think of delivering a better IoT solution and technical services.
IoT data are stored in cloud computing which is then used for different purposes.
A cloud is a centralized server with computer data that can be used at any time.
Massive data packages created by the Internet of Things (IoT) can be conveyed
easily by cloud computing. This process can also be done by Big Data. Real-time
management and data scanning can be done easily in a cost effective manner when
IoT and cloud computing are used together in an automated way.Collaboration of IoT and Cloud Computing Towards Healthcare Security 21
References
1.Singh P, Devi KJ, Thakkar HK, Kotecha K (2022) Region-based hybrid medical image
watermarking scheme for robust and secured transmission in IoMT. IEEE Access 10:8974–
8993
2.Sahoo PK, Thakkar HK (2019) TLS: trafﬁc load based scheduling protocol for wireless sensor
networks. Int J Ad Hoc Ubiquitous Comput 30(3):150–160
3.Rai D, Thakkar HK, Rajput SS, Santamaria J, Bhatt C, Roca F (2021) A comprehensive review
on seismocardiogram: current advancements on acquisition, annotation, and applications.
Mathematics 9(18):2243
4.Sahoo S, Das M, Mishra S, Suman S (2021) A hybrid DTNB model for heart disorders
prediction. In: Advances in electronics, communication and computing. Springer, Singapore,
pp 155–163
5.Thakkar HK, Sahoo PK, Veeravalli B (2021) Renda: resource and network aware data
placement algorithm for periodic workloads in cloud. IEEE Trans Parallel Distrib Syst
32(12):2906–2920
6.Dang LM, Piran MJ, Han D, Min K, Moon H (2019) A survey on internet of things and cloud
computing for healthcare. Electronics 8(7):768. https://doi.org/10.3390/electronics8070768
7.Singh N, Raza M, Paranthaman V , Awais M, Khalid M, Javed E (2021) Internet of things and
cloud computing. In: Digital Health, pp 151–162
8.Ramachandra G, Agrawal M, Nandi A, Samanta D (2022) An overview: secu-
rity issue in IoT network. [Online]. Available: https://www.researchgate.net/publication/
331421760_An_Overview_Security_Issue_in_IoT_Network
9.Mishra S, Jena L, Pradhan A (2012) Fault tolerance in wireless sensor networks. Int J
2(10):146–153
10.Zakaria H, Abu Bakar N, Hassan N, Yaacob S (2019) IoT security risk management model for
secured practice in healthcare environment. Procedia Comput Sci 161:1241–1248
11.Darwish A, Hassanien A, Elhoseny M, Sangaiah A, Muhammad K (2017) The impact of the
hybrid platform of internet of things and cloud computing on healthcare systems: opportunities,
challenges, and open problems. J Ambient Intell Humaniz Comput 10(10):4151–4166
12.Dutta A, Misra C, Barik RK, Mishra S (2021) Enhancing mist assisted cloud computing toward
secure and scalable architecture for smart healthcare. In: Advances in communication and
computational technology. Springer, Singapore, pp 1515–1526
13.Rath M, Mishra S (2020) Security approaches in machine learning for satellite communication.
In: Machine learning and data mining in aerospace technology. Springer, Cham, pp 189–204
14.Jena L, Mishra S, Nayak S, Ranjan P, Mishra MK (2021) Variable optimization in cervical
cancer data using particle swarm optimization. In: Advances in electronics, communication
and computing. Springer, Singapore, pp 147–153
15.Gupta P, Maharaj B, Malekian R (2022) A novel and secure IoT based cloud centric
architecture to perform predictive analysis of users activities in sustainable health centres
16.Tesse J, Baldauf U, Schirmer I, Drews P (2022) Extending internet of things enterprise
architectures by digital twins
17.Raza M, Aslam N, Le-Minh H, Hussain S, Cao Y , Khan N (2022) A critical analysis of
research potential, challenges, and future directives in industrial wireless sensor networks,
Ieeexplore.ieee.org , 2022 [Online]. Available: https://ieeexplore.ieee.org/document/8057758
18.Niaz F, Khalid M, Ullah Z, Aslam N, Raza M, Priyan M (2022) A bonded channel in cognitive
wireless body area network based on IEEE 802.15.6 and internet of things
19.Mishra S, Mahanty C, Dash S, Mishra BK (2019) Implementation of BFS-NB hybrid model
in intrusion detection system. In: Recent developments in machine learning and data analytics.
Springer, Singapore, pp 167–175
20.Jena L, Kamila NK, Mishra S (2014) Privacy preserving distributed data mining with evolu-
tionary computing. In: Proceedings of the international conference on frontiers of intelligent
computing: theory and applications (FICTA) 2013. Springer, Cham, pp 259–26722 S. Kumar et al.
21.Ni J, Lin X, Shen X (2019) Toward edge-assisted internet of things: from security
and efﬁciency perspectives. IEEE Netw 33(2):50–57. Available: https://doi.org/10.1109/
mnet.2019.1800229
22.Zhang R, Liu L (2022) Security models and requirements for healthcare application clouds,
Ieeexplore.ieee.org , 2022. [Online]. Available: https://ieeexplore.ieee.org/document/5557983
23.Mishra S, Mallick PK, Tripathy HK, Jena L, Chae GS (2021) Stacked KNN with hard
voting predictive approach to assist hiring process in IT organizations. Int J Electr Eng Educ,
0020720921989015Robust, Reversible Medical Image
Watermarking for Transmission of
Medical Images over Cloud in Smart IoT
Healthcare
K. Jyothsna Devi, M. V . Jayanth Krishna, Priyanka Singh, José Santamaría,
and Parul Bakaraniya
1 Introduction
In these emerging day-to-day technologies, we have seen a lot of developments in
the world and especially in the ﬁeld of medical sciences. Among these develop-
ments the Smart IoT health care applications have predominantly risen amidst the
pandemic. Services like telemedicine, telemonitoring and online mobile healthcare
came into existence. In these IoMT based applications the treatment and diagnosis
are being done remotely. The medical data that is being generated from the sensors
and also the data such as X-rays, CT, MRI etc reports are transmitted regularly
for various needs. This data when referred by the medical staff needs to be intact
without any tampering. The data should be authentic and should not be tampered,
since the whole diagnosis and treatment will be based on this data. Thus, the
security of this data should be given importance because when it is compromised
this will lead to incorrect diagnosis which causes severe damage to patient health.
A comprehensive study [ 1] on current medical image authentication and recent
trends motivated us to work on reversible watermarking and data hiding techniques.
Various previous researchers proposed their models which have high computational
costs and less embedding capacities and less security [ 12,13]. These difﬁculties can
K. Jyothsna Devi ( /envelopeback) · M. V . Jayanth Krishna · P. Singh
Department of Computer Science and Engineering, SRM University–A.P., Amaravati, India
e-mail: jyothsna_devi@srmap.edu.in ;jayanthkrishna_m@srmap.edu.in ;
priyanka.s@srmap.edu.in
J. Santamaría
Department of Computer Science, University of Jaén, Jaén, Spain
P. Bakaraniya
Department of Computer Science and Engineering, Sardar Vallabhbhai Patel Institute of
Technology, Vasad, India
e-mail: parulbakaraniya.comp@svitvasad.ac.in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_22324 K. Jyothsna Devi et al.
Fig. 1 Cloud based e-healthcare infrastructure
be overcome in our proposed model by employing a quadratic difference expansion
algorithm and also some encryption techniques, which allow us to achieve high
hiding capacity, data protection [ 11] and low processing cost. The block diagram of
Cloud based healthcare infrastructure as shown in Fig. 1.
The proposed scheme employs both the linear and quadratic difference expan-
sions. The cover image is ﬁrst divided into Border Region (BR) and Non-Border
Region (NBR) in this scheme (NBR). To initiate, the hospital logo is embedded
in the border region, ensuring the image’s authenticity. In addition, the patient
Electronic Healthcare Record (EHR) that must be concealed during transmission
should be watermarked.The pixels with the values 0 and 255 are removed in
the Non-Border Region to prevent image pixel overﬂow. The linear difference
expansion must then be applied to two successive pixels selected from left to right.
Half of the conﬁdential info is embedded in the ﬁrst level watermarked images.
The quadratic difference expansion has now been used, and the remaining secret
information is embedded within it. The removed pixels of 0 and 255 are then re-
added to the cover image. This is the ﬁnal watermarked image, which includes all the
data. This will be the reversible watermarking scheme proposed in this paper, which
employs the quadratic difference expansion for low computational complexity.
2 Related Work
In the ﬁeld of IoMT the most sensitive data regarding the health conditions of the
patients has been transmitted between various departments. Hence the processingRobust, Reversible Medical Image Watermarking for Transmission of Medical ... 25
and the transmission of this information is of vital importance. Over the past years
numerous methods were proposed in order to transmit the data securely in the
ﬁeld of healthcare with less computational time. Among these methods the most
focus has been given on the reversibility,imperceptibility, tamper detection and
less computational time [ 14–16]. Wang and Wang [ 2] created the Reversible data
hiding (RDH) method using difference expansion approach. Memon and Alzahrani
[3] proposed prediction-based reversible watermarking scheme to ensure high
imperceptibility and embedding capacity Wu [ 4] proposed breadth-ﬁrst prediction
RDH to ensure high embedding capacity, but this scheme has high computational
cost. Yan et al. [ 5] introduced the RISS technique, which is centred on the
Chinese Remainder theorem and restores the secret image losslessly using modular
operations while retrieving the original image using binarization operations. The
scheme has a high computational cost as well. Soni and Kumar [ 6] presented DWT-
BCH transform based watermrking scheme to ensure high robustness. Guo and
Zhuang [ 7] proposed a region based lossless watermarking scheme to ensure high
embedding capapcity. The scheme proposed [ 8] performed embedding using pixel
value difference to ensure high imperceptibility and embedding capacity.
A number of reversible watermarking techniques were reviewed in the literature
review, each with its own set of pros and weaknesses. However, no one is able
to meet all of the watermarking requirements for smart healthcare applications. In
this paper, we propose a MIW method that meets the majority of watermarking
characteristics in the spatial domain and therefore is appropriate for smart IoT
healthcare applications.
3 Proposed Work
In the proposed scheme for EHR insertion and retrieval, a cover image of size M×M
and an EHR of size P×Qare considered. The proposed scheme is broken down
into two modules: (1) EHR insertion and retrieval, and (2) EHR encryption and
decryption.
3.1 EHR Insertion (Embedding) and Retrieval (Extraction)
To achieve a low computational cost, the scheme employs quadratic difference
expansion for EHR embedding. Furthermore, the quadratic difference expansion
enables us to achieve high embedding capacity and reversibility of medical cover
images. In the proposed watermark scheme, the EHR is encrypted using secret key
generated by MPN process. This encrypted EHR is then embedded in the image
in two steps by dividing the EHR into two parts such as even and odd positioned
pixel parts then encrypting them using secret key. The division of even and odd
helps in the recovering of the EHR from the cover image. The embedding is done26 K. Jyothsna Devi et al.
Fig. 2 Block diagram for EHR embedding process
by performing the ﬁrst level difference expansion followed by quadratic difference
expansion to embed even and odd EHR parts. This embedding process can be
depicted in Fig. 2. and steps for embedding as described below:
Steps in the Embedding Procedure
Step 1 Firstly, the pixel values of 0 and 255 are removed; this will avoid the
overﬂow in the pixel values during the embedding.
Step 2 The ﬁrst level difference expansion is performed to the cover image after
selecting two consecutive pixels from left to right and top to bottom. The
ﬁrst level watermarked image is created by embedding encrypted even
positioned pixels.
Step 3 The ﬁrst level watermarked image is quadratic difference expanded, and
then encrypted odd positioned pixels are embedded to generate the second
level watermarked image.
Step 4 Finally, the removed pixels of 0 and 255 are replaced to get ﬁnal
watermarked image.
Quadratic Difference Expansion
In the quadratic difference expansion technique, a pixel pair of xandyis selected
and linear difference expansion is applied. The transformation for linear difference
expansion as shown in Eqs. 1and2.
a=(x+y)
2(1)
s=x−y (2)
where a is the average and s is the difference of the two pixels x and y.
Inverse transformation:Robust, Reversible Medical Image Watermarking for Transmission of Medical ... 27
x1=m+(s1+1)
2(3)
y1=a−(s1
2) (4)
The EHR bit w is embedded using Eq. 5by moving the dto the left by one bit and
embedding it in the position of the s’s Least Signiﬁcant Bit (LSB).
s1=2s+w (5)
Now in the quadratic difference expansion the following transformations are applied
on the pixel pair x1andy1that are generated from the ﬁrst level difference
expansion. a1is the average and s1is the difference of the pixels respectively.
a1=(x1+y1)
2(6)
s11=x1−y1(7)
The embedding of the watermark bit is done by using the following equation.
s111=(s11
2)+w (8)
Inverse transformation:
x11=a1+(s111+1)
2(9)
y11=a1−(s111)
2(10)
EHR Extraction
The reverse process of embedding is EHR extraction. For odd EHR, difference
expansion is used ﬁrst, followed by linear expansion for even EHR. Finally, EHR is
constructed by combining odd and even EHR. Figure 3depicts the EHR extraction
procedure. Let us consider the x11andy11be the two successsive pixels in the
watermarked image generated by applying the quadratic difference expansion on
thex1andy1.
Steps for Watermark Extraction Process
Step 1 Remove the pixels of the values 0 and 255 in the watermarked image.28 K. Jyothsna Devi et al.
Fig. 3 Block diagram for extraction process
Step 2 Then a pixel pair from left to right and top to bottom are selected from the
watermarked image.
Step 3 On the pixel pair, the inverse quadratic difference expansion has been
applied.
Step 4 The odd part of the EHR is extracted.
Step 5 The generated pixels after the inverse quadratic expansion are adjusted
according to the following 4 rules:
1.ifxandyare even and odd and the extracted EHR bit w=1 then the
restored xis unchanged whereas the y becomes w+1
2.ifxandyare even and odd and the extracted EHR bit w=0t h e
restored pixels xandyare unchanged.
3.ifxandyare even or odd and the extracted EHR bit w=1 then the x
becomes x−1 andyremain unchanged.
4.Ifxandyare even or odd and the extracted bit w=0 then the both
pixel values xandyremain unchanged.
Step 6 Then the inverse transformations are applied on the adjusted pixels in
order to get the remaining half of the even EHR.
Step 7 The watermark extraction is completed by adding back the removed 0 and
255 pixels. Then the cover image is completely restored.
3.2 EHR Encryption and Decryption
The substitution method is used for the encryption of the EHR in the proposed
scheme.A binary secret key is generated using the combination of magic square,
polybius square and musical notes (MPN).Robust, Reversible Medical Image Watermarking for Transmission of Medical ... 29
Magic Square
A magic square is structured in such a way that the sum of all the distinct numbers in
each row, column and the diagonal elements is the same [ 9]. With an exception for
the matrices of order 2 there are the magic squares for all the order of n. Proposed
scheme utilized 6×6magic matrix for our encryption process. Here the sum is
given as an input and then the corresponding elements in the rows and columns are
arranged accordingly. In general, if the order is n then the elements in the magic
square are in the range of 1 to n2.
Polybius Square
Polybius square is used in secret key generation which is an order of 6×6. There
are 36 alphanumeric characters present in this square [ 10]. These alphanumeric
characters are structured in square by assigning each character A–Z and 1–9 in a
sequential way from 1 to 36.
Musical Notes
Generally, in the world of music, various audio vibrations are combined and the
sequencing of these vibrations generates the desired music. Hence these vibrations
need to be named in order to organize these vibrations. A musical note is a name
that is given to the pitch that corresponds to the speciﬁc audio vibrations. There
are 7 musical notes present;they are A-B-C-D-E-F-G. Proposed scheme use six
musical notes for the indexing purpose and the seventh musical note is reserved
for the escape characters. A major scale E-F-G-A-B-C sequence is used in this
recommended scheme. The rows and columns are indexed with these musical notes
in the sequential way.
Steps for Encryption of EPR Using MPN
Step 1 In the encryption process the magic square is created with the sum as the
input and the elements are structured in order to satisfy the requirements
of the magic square.
Step 2 Then the Polybius square is generated by placing the alphanumeric
characters A–Z and 1–9 in their relevant positions based on the magic
square elements.
Step 3 The musical notes are used for the indexing square. The increasing E
major scale E-F-G-A-B-C is used and D is used for the escape characters.
With this the rows and columns are indexed with the music notes as shown
in Fig. 4.
Step 4 The secret key is generated for the given key of 16 characters, with the
help of our MPN encryption technique. For each character in the key
the corresponding index characters are substituted based on the row and
column position. This generates a cipher key of size 32 characters.
Step 5 The cipher key characters thus generated should be converted to the binary
format of 8 bits as shown in Fig. 5.
Step 6 The second bit and sixth bit of the binary converted characters are selected
and placed in a two-dimensional vector.30 K. Jyothsna Devi et al.
Fig. 4 MPN squares
Fig. 5 Secret key, cipher key and binary form of the cipher key
Step 7 Now the 2D vector ( IKey)o fs i z e 8×8is formed as shown in Fig. 7.
Step 8 TheIKeyis rotated by 90◦, 180◦and 270◦that gives us the three distinct
8×8vectors.
Step 9 The encryption is performed by taking the odd EHR, performing the XOR
operation with the IKeyandIKey90◦. Similarly, the even EHR is taken and
the XOR operation is performed by taking the IKey180◦andIKey270◦.
In this MPN encryption technique, let us consider the combined magical square,
Polybius square that is indexed with the musical notes. The magic square 6×6with
the input sum 111. And the elements in the square are placed in a way that the sum
of the elements in the row and sum of the elements in the column and sum of the
elements in the main diagonals is the same and equal to 111. Now the elements in the
Polybius square are arranged according to the elements in the magical square. Here
the characters from A–Z and 1–9 are used in this square by assigning them to the
number 1–36. Then these rows and columns are indexed with the increasing E major
scale E-F-G-A-B-C and the D is reserved for the spaces and endings of the words.
The secret key is generated for the given key by substituting the corresponding indexRobust, Reversible Medical Image Watermarking for Transmission of Medical ... 31
Fig. 6 2D Vector generated from the Binary form of cipher text
of that particular character in the key. The cipher key of 32 characters is generated
for the key of 16 characters. This cipher key is converted into the binary form of
each 8 bits. The second and sixth bits in each character binary format are taken and
represented in the 2D vector as shown in Fig. 6.
This 2D vector ( IKey)i st h e nr o t a t e d9 0◦, 180◦and 270◦. Then the encryption
is done by taking the odd watermark and the even watermark into 8×8matrices
and the XOR operation is performed on the odd EHR with the IkeyandIkey 90and
similarly the XOR operation is performed on the even watermark by taking Ikey 180
andIkey 270for the encryption as shown in Fig. 7.
EHR Decryption
In the decryption process the random keys Ikey,Ikey 90,Ikey 180andIkey 270
are generated from secretly received secret key along with the MPN square. The
decryption process also follows the same steps as the encryption process.32 K. Jyothsna Devi et al.
Fig. 7 Random keys generated from MPN
4 Experimental Results and Discussion
The watermark is 128×128, and the standard images used in the study are 256×245.
To authenticate the receiving images, the binary EHR is encoded in the medical
image. The testing is done with colour and grayscale images, as shown in Fig. 8.
The system is evaluated in terms of security, imperceptibility, and computational
time Imperceptibility is analyzed using metrics such as Peak Signal to Noise Ratio
(PSNR) and Structural Similarity Index Metric (SSIM). Robustness is observed by
Normalized Correlation (NC), Bit Rate Error (BRE). Correlation Coefﬁcient (CC)
is used to test security of the suggested scheme. The disparity between the original
image and the watermarked image, retrieved EHR is less trivial, as seen in Fig. 8.
Table 1shows that the PSNR value for grayscale and colour images is above the
threshold value of 35dB, and the SSIM is approximately equal to 1. NC and BER
are nearing ideal values of 1, 0 correspondingly. Therefore, the PSNR, SSIM, NC,
and BER ﬁndings reveal that the proposed system is undetectable and resilient.
Further to assess the performance of the suggested scheme, various attacks like
noising, ﬁltering,geometric, compression are applied on watermarked images. ForRobust, Reversible Medical Image Watermarking for Transmission of Medical ... 33
Fig. 8 Test greyscale original images ( 1–4) and color images ( 5–8) and grayscale watermarked
images ( a–d) and color watermarked images ( e–f) and extracted EPR of corresponding images
(I–VIII )
all the attacks, proposed scheme shows NC, BER nearly equal to threshold values
and able to extract EHR in under attacks with little distortions.
As a result of the experimental outcomes of PSNR, SSIM, NC, and BER for
grayscale and colour images under zero assaults and under attacks, it can be claim
that the suggested technique is highly imperceptible and robust.
Further to asses the reversibility of the proposed scheme, NC is evaluated
between cover and recovered image as recorded in Table 2. From Table 2,N C
of all the recovered images are nearly equal to ideal value which indicates that
recommended scheme is capable of obtaining cover image after extraction of EHR.
The proposed encryption approach’s performance in terms of the Correlation
coefﬁcient (CC) assessment is shown in Table 3. Original and encrypted images,
as well as decrypted and original images, are subjected to CC in horizontal,
vertical, and diagonal directions. The values of encrypted and original images differ
signiﬁcantly, and the CC values are approaching 0 or negative. This implies that the
original and encrypted images are unrelated or negatively connected. As a result,
the proposed scheme is more secure.
5 Conclusions
For effective communication in the smart healthcare area, a reversible medical
watermarking scheme for medical images is presented. The proposed approach is
evaluated in terms of all watermarking characteristics using a variety of grayscale
and colour medical images. The results reveal that the recommended approach is34 K. Jyothsna Devi et al.
Table 1 PSNR, SSIM, NC and BER of grayscale and color images
Grayscle image PSNR SSIM NC BER Color image PSNR SSIM NC BER
CT lung 41.24 0.9892 0.9992 0.0018 PET liver 41.51 0.9982 0.9962 0.0028
X-ray teeth 38.27 0.9919 0.9993 0.0009 Retina 39.81 0.9962 0.9968 0.0082
Ultrasound 38.19 0.9819 0.9990 0.0021 MRI brain 42.61 0.9982 0.9991 0.0009
MRI brain 39.28 0.9904 0.9962 0.0048 Endoscopy stomach 58.72 0.9994 0.9982 0.0028Robust, Reversible Medical Image Watermarking for Transmission of Medical ... 35
Table 2 NC, BER under attacks for X-ray teeth (NC of recovered image)
Attacks NC BER NCRI Attacks NC BER NCRI
Median ﬁlter 0.8362 0.3728 0.9827 Resize 0.8518 0.3592 0.9928
Sharpening 0.5528 0.4017 0.9838 JPEG (70%) 0.9917 0.5278 0.9928
Gaussian ﬁlter 0.9810 0.0428 0.9901 Histogram equivalent 0.9019 0.09278 0.9929
Butterworth ﬁlter 0.9910 0.0310 0.9904 Salt and pepper noise
(0.0002)0.9978 0.0062 0.9972
Translation 0.6927 0.0529 0.9728 Gaussian noise (0.0002) 0.6489 0.4892 0.9728
Cropping 0.8428 0.3728 0.9738 Poisson noise (0.0002) 0.6489 0.4892 0.9728
Table 3 CC between original and encrypted image, CC between original decrypted image ( H
horizontal, Ddiagonal, Vvertical)
CC between original and encrypted image CC between original and decrypted image
Test image H D V H D V
EPR 0.1271 0.2021 0.2031 1 1 1
Ultrasound 0.0052 0.0083 0.0017 1 1 1
MRI brain −0.0418 −0.0392 −0.0381 1 1 1
X-ray teeth 0.0189 0.0245 0.1927 1 1 1
Retina −0.1072 −0.1482 −0.1392 1 1 1
able to maintain robustness, security, and imperceptibility while requiring minimal
computation time. We wanted to enhance the proposed scheme in the future to attain
better results for attacks translation and shear.
References
1.Gaurav A, Psannis K, Perakovi ´c D (2022) Security of cloud-based medical Internet of Things
(MIoTs): a survey. Int J Softw Sci Comput Intell 14(1):1–16
2.Wang W, Wang W (2020) New high capacity reversible data hiding using the second-
order difference shifting. IEEE Access 8:85367–85379. https://doi.org/10.1109/ACCESS.
2020.2993604
3.Memon NA, Alzahrani A (2020) Prediction-based reversible watermarking of CT scan images
for content authentication and copyright protection. IEEE Access 8:75448–75462. https://doi.
org/10.1109/ACCESS.2020.2989175
4.Wu H (2020) Efﬁcient reversible data hiding simultaneously exploiting adjacent pixels. IEEE
Access 8:119501–119510. https://doi.org/10.1109/ACCESS.2020.3006139
5.Yan X, Lu Y , Liu L, Song X (2020) Reversible image secret sharing. IEEE Trans Inform
Forensics Secur 15:3848–3858. https://doi.org/10.1109/TIFS.2020.3001735
6.Soni M, Kumar D (2020) Wavelet based digital watermarking scheme for medical images.
In: 2020 12th international conference on computational intelligence and communication
networks (CICN), pp 403–407. https://doi.org/10.1109/CICN49253.2020.9242626
7.Guo X, Zhuang Tg (2009) A region-based lossless watermarking scheme for enhancing
security of medical data. J Digit Imaging 22:53–64
8.Hussain M, Wahab AWA, Javed N, Jung K-H (2018) Recursive information hiding scheme
through LSB, PVD shift, and MPE. IETE Tech. Rev. 35(1):53–6336 K. Jyothsna Devi et al.
9.Prajapati R, Jain J (2017) A study on magic squares
10.Arroyo JC, Dumdumaya CE, Delima AJ (2020) Polybius square in cryptography: a brief review
of literature. Int J Adv Trends Comput Sci Eng 9(3):3798–3808
11.Anand A, Singh AK (2021) Watermarking techniques for medical data authentication: a survey.
Multimedia Tools Appl 80(20):30165–30197
12.Shehab A et al. (2018) Secure and robust fragile watermarking scheme for medical images.
IEEE Access 6:10269–10278. https://doi.org/10.1109/ACCESS.2018.2799240
13.Soualmi A et al. (2021) Multiple blind watermarking framework for security and integrity of
medical images in e-health applications. IJCVIP 11(1):1–16
14.Su Q, Yuan Z, Liu D (2019) An approximate Schur decomposition-based spatial domain color
image watermarking method. IEEE Access 7:4358–4370. https://doi.org/10.1109/ACCESS.
2018.2888857
15.Su Q, Chen B (2018) Robust color image watermarking technique in the spatial domain. Soft
Comput 22:91–106
16.Su Q, Wang H, Liu D et al. (2020) A combined domain watermarking algorithm of color image.
Multimed Tools Appl 79:30023–30043The Role of Blockchain in Cloud
Computing
Hiren Kumar Thakkar, Kirtirajsinh Zala, Neel H. Dholakia, Aditya Jajodia,
and Rajendrasinh Jadeja
1 Blockchain
1.1 Introduction
The idea of Blockchain like protocol was ﬁrst proposed by a cryptographer David
Chaum in his 1982 dissertation “Computer Systems Established, Maintained, and
Trusted by Mutually Suspicious Groups” . The idea was then further worked upon
and the ﬁrst decentralized blockchain was conceptualized in 2008 by Satoshi
Nakamoto. Nakamoto signiﬁcantly improved the design by presenting a complexity
parameter to balance the rate during which blocks are provided to the chain with
the use of Hash cash technique to checksum blocks without demanding them
to be authorized by a trustworthy party. The design was then incorporated as a
necessary feature of the cryptocurrency by Nakamoto the following year, in which
it functioned as the shared ledger recording all transactions.
First prototype of a blockchain was developed in the year 1990s when computer
scientist Stuart Haber and physicist W. Scott Stornetta used the cryptographic
techniques in a chain of blocks so as to secure digital documents from data
tampering. Blockchain can be deﬁned as “a digital, public ledger that records
online transactions”. In layman’s terms, blockchain is a block in which records
H. K. Thakkar (/envelopeback)
Department of Computer Science and Engineering, School of Technology, Pandit Deendayal
Energy University, Gandhinagar, Gujarat, India
K .Z a l a·N .H .D h o l a k i a·A .J a j o d i a
Department of Computer Engineering, Marwadi University, Rajkot, India
R. Jadeja
Department of Electrical Engineering, Marwadi University, Rajkot, India
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_33738 H. K. Thakkar et al.
are collected and it stores data openly and chronologically. The data is then encoded
using cryptography to make sure the user’s data’s security and privacy.
1.2 Characteristics
Blockchain technology is much more than just a recovery network for cryptocurren-
cies. The following are Blockchain’s features:
1.2.1 Immutability
Immutable means that something which cannot be changed or alter. Blockchain isimmutable. Data present in the blocks cannot tamper and If someone tries to do sothen he/she would get caught very easily as data stored in the blocks are chained
through the hash key, also any changes with the data would led to invalidation of the
next blocks.
1.2.2 Distributed
The blockchain network is distributed that means it’s not owned or governed by anygovernment authority or a solitary person in charge of the framework. Rather thereare groups of junction which maintains network making it completely decentralized.
Being decentralized, blockchain puts us users in a straightforward position and since
the system is not governed by any government authority, the user can directly accessit from the web and store our assets there. This enables the user to stockpile anything
from necessary records to cryptocurrencies or vital digital assets, all of which are
directly under the control of its owner.
1.2.3 Enhanced Security
To gain the trust of its users to store their valuable digital assets and information,blockchain has an enhanced security system, helping its users to secure their data.This security is obtained through the use of cryptography, which is an advanced
math methodology which serves as a ﬁrewall against attacks. Each bit of data
on the blockchain is cryptographically hashed. Any input data passed through amathematical method that generates a different type of value, however the length
remains constant. Every block in the ledger has its own unique hash, which contains
the previous blocks. Modifying or interfering with the data thus requires exchangingall of the hash IDs, which is practically impossible.The Role of Blockchain in Cloud Computing 39
1.2.4 Distributed Ledgers
It is a digital system for recording the transaction of assets in which the transactions
and their details are recorded in multiple places at the same time. This dispersed
computational power throughout the computers results in a better outcome.
1.2.5 Faster Settlement
The blockchain technology is way too faster in comparison to that of the traditional
banking systems. With the usage of new and advanced technology, blockchain gains
an advantage of being fast and highly reliable as compared to the banking system
which we already have.
1.2.6 Working of Blockchain
First a transaction is being requested by the user. After that, as each transaction
takes place, it is taken down as the “block” of the data. This transaction displays the
motion of the assets. The data block can then be used to record the information.
Every block is then linked to that before and after it, forming a data chain as
a resource moves through one location to another or ownership loses value. The
blocks verify time and sequence of transactions, and they are securely linked with
each other to prevent any block from being changed or inserted between two existing
blocks. These transactions are now linked on in an irreversible chain called a
Blockchain. Each block contains information from the previous and subsequent
blocks, creating blockchain tamper evident and providing critical robustness of
immutability, as a result, it becomes a trusted ledger of transactions for you and
other network users. The transaction is now complete. As show in Fig. 1.
1.3 Major Implementations
Blockchain being a new era technology, it has a lot of implementations in the
industrial front. Some of the implementations are:
1.3.1 Cryptocurrencies
Cryptocurrencies are digital currencies (or tokens), such as Bitcoin, Ethereum
etc., that can be used to buy goods and services. Just like a digital form of
cash, cryptocurrency can be used to buy stuffs. Unlike cash, cryptocurrency uses
blockchain to act as both a public ledger and an enhanced cryptographic security
system, so online transactions are always recorded and secured.40 H. K. Thakkar et al.
Fig. 1 Working of blockchain
1.3.2 Smart Contracts
Smart contracts on the blockchain are suggested agreements that can be explicitly
or implicitly performed or imposed without the need for human intervention. The
blockchain network implements the contract on its own, eliminating the need for a
trusted third party to act as a mediator between contracting entities. Thus, making
the transaction automated.
1.3.3 Monetary Services
As per a survey, many banks want to use distributed ledgers in banking
through which they are collaborating with companies that are developing private
blockchains. Blockchain has spawned initial coin offerings (ICOs) and also a new
category of digital asset security token offerings (STOs). STOs can be performed
conﬁdentially else on a crowd.The Role of Blockchain in Cloud Computing 41
Table 1 Blockchain types with Advantages, Disadvantages and Cases
Public Private Hybrid Consortium
Advantages Independence
Transparency
TrustAccess control
PerformanceAccess control
Performance
ScalabilityAccess control
Scalability
Security
Disadvantages Performance
Scalability
SecurityTrust
AuditabilityTransparency
UpgradingTransparency
Use cases Cryptocurrency
Document
validationSupply chain
Asset ownershipMedical records
Real estateBanking
Research
Supply chain
1.3.4 Games
Blockchain technology, like cryptocurrencies, is being used to monetize video
games. Many live-service games include in-game customization options like char-
acter skins or other in-game products that gamers can win and barter with several
players using currency in the game. A few of them allow you to trade digital
items for real money. First recognised game for using blockchain technologies was
“CryptoKitties,” which was released on November 2017 and allowed players to buy
NFTs with Ethereum cryptocurrency.
1.4 Blockchain Types
1.5 There Are Mainly 4 Types of Blockchain as Shown
in Table 1
1.5.1 Public Blockchain Networks
A public blockchain is one that anyone can join and participate in, such as Bitcoin.
The drawbacks for such a network are weak security, computational power and
privacy in transactions.
1.5.2 Exclusive Blockchain Networks
This type of blockchain network is localized multi user network similar to public
blockchain. Here, the network is taken care by solo organization. Depending on the
scenario it can promote trust and conﬁdence among users.42 H. K. Thakkar et al.
1.5.3 Hybrid Blockchain Networks
This type of network places restrictions on who is allowed to participate in
the network and in what transactions. Participants need to obtain an invitation
or permission to join. A permissioned blockchain network is usually set up by
businesses who create a private blockchain.
1.5.4 Consortium Networks
The maintenance of a blockchain can be shared amongst multiple companies.Who can initiate transactions or access data is determined by these pre-selectedorganizations. When all members need to be permissioned and share responsibility
for the blockchain, a consortium blockchain is perfect.
1.6 Advantages
1.6.1 Secure
Each transaction here is made public as it abolishes the likelihood of deception. The
block chain’s integrity is taken care by minors who looks on all transactions.
1.6.2 There Will Be No Intervention from Third Parties
No government or ﬁnancial organisation has control over cryptocurrency. As aresult, no government can inﬂuence the value of the money or the data of the clientsaved on the blockchain.
1.6.3 Safe Transactions
It cannot be modiﬁed. A transaction’s data from both sides can be viewed at anyinstance, making online transactions more safe.
1.6.4 Automation
Transactions done using the blockchain could also be automated with the help of“Smart Contracts”. Once pre-speciﬁed conditions are being full ﬁeld, the next step in
transaction or process is automatically triggered. This in addition saves time money
and energy.The Role of Blockchain in Cloud Computing 43
1.7 Disadvantages
1.7.1 High Implementation Cost
This technology has minimal expenses for users but ﬁrms will incur signiﬁcant
implementation expenses, delaying its broad acceptance and execution.
1.7.2 Incompetency
Various network users validating the same operations is inefﬁcient because the
mining process only beneﬁts one person. Hence, this requires a large amount of
energy and hence is not so environment friendly.
1.7.3 Private Keys
When it comes to private keys, once they’ve been lost, it’s nearly impossible to
retrieve them, posing a challenge for all cryptographic value holders and resulting
in a signiﬁcant loss.
1.7.4 Storage Capacity
The hard discs’ capacity will increase as the users will increase.
2 Cloud Computing
2.1 What Is Cloud Computing?
The era in which we are living contains a lot of data around us and data centres are
there all around the world to store this data, also servers are needed for maintaining
the websites which are hosted all around the world and for maintenance of all these,
lots of money is needed. To overcome this issue Cloud Computing is used. So, Cloud
Computing is used instead of local server/PC to stock, process and head the data.
Cloud provides three services namely Software as a Service (SaaS), Platform as a
Service (PaaS) and Infrastructure as a Service (IaaS) [ 1].44 H. K. Thakkar et al.
Fig. 2 Public cloud
2.2 Deployment Models in Cloud
2.2.1 Public Cloud
Public Cloud is a model through which services are provided and managed by
the third party service providers [ 2]. Here the services are open for public which
is accessed using the internet. Examples of public cloud are Google App Engine,
Azure, Blue Cloud, etc. [ 3]. Figure 2shows the public cloud model.
2.2.2 Private Cloud
Private Clouds ensures high level of security. It is an environment in which either a
single user uses it or a tenant i.e. organization uses it. No third party providers can
access the sensitive data. It also has some drawbacks like bandwidth limitations,
regulations on security. As compared to Public Cloud, Private Cloud is costlier.The Role of Blockchain in Cloud Computing 45
Fig. 3 Private cloud
Fig. 4 Hybrid cloud
Example of private cloud are Ubuntu, Elastra-private cloud, HP Data Centres, etc.
[4]. Figure 3shows the private cloud model.
2.2.3 Hybrid Cloud
Hybrid Cloud as the name says it’s combination of two or more cloud deployment
models. It is used to create an effective cloud environment for an organization
working on computing workloads. It is complex and difﬁcult to implement. Example
Amazon Web Services, Google Cloud etc. Figure 4shows the Hybrid cloud model.
2.2.4 Community Cloud
Community Cloud is a platform on which several organizations can work on having
similar concerns. All the organizations associated with this platform can access
services and system in cloud infrastructure for sharing information and hence can
manage and operate. Example of community cloud are IBM SoftLayer cloud,
Facebook etc. Figure 5shows the community cloud model.
The technological and ﬁnancial potential, as well as risk appetite, inﬂuence the
deployment model to be considered. For example, if a company already possesses
Information and communication technology equipment such as servers and matrices
with a low disposal rate, it can consider developing its own “Cloud” and managing
internal risk control. This option is also available if the customer does not have
Information and communication technology equipment but has the ﬁnancial ability46 H. K. Thakkar et al.
Fig. 5 Community cloud
to purchase it. On the other hand, if the customer lacks the necessary equipment or
funding and is willing to rely on a cloud provider, a public model may be the best
option. Here, Fig. 6shows all deployment models of the cloud in one frame.
2.3 Implementations of Cloud Computing
2.3.1 Web Based Services
Web Based Services allows data to be exchanged between different systems. It
includes sets of open protocols and standards. Examples are JSON-RPC, XML
Interface for Network Services etc. Figure 7shows working of Web Based services.The Role of Blockchain in Cloud Computing 47
Fig. 6 Deployment models of cloud
Fig. 7 Web based services48 H. K. Thakkar et al.
Fig. 8 Software as a service
2.3.2 Software as a Service
SaaS provides software and applications through internet as a service. It can be
accessed through internet. It allows several cloud users to access the application
at the same time because it is the most widely utilised cloud service. It offers
a comprehensive solution to customers it is a cloud platform that allows clients
to exchange ﬁles, documents, and bills, lowering the danger of data theft and
duplication [ 5]. Figure 8shows SaaS in one frame.
2.3.3 Infrastructure as a Service
Hardware as a Service is another name for IaaS. Customers can outsource their IT
infrastructures, such as servers, networking, processing, storage, virtual machines,
and other resources, to the company. Customers employ a pay-per-use basis to
access these resources over the Internet. Infrastructure services cover all of the
essential elements of cloud computing. Network functions, virtual computers,
storage space, and dedicated hardware are all involved. The use of the cloud
as infrastructure is the essential foundation for any organisation. Examples are
DigitalOcean, Linode, Rackspace, Amazon Web Services (AWS), Cisco Metapod,
Microsoft Azure, Google Compute Engine (GCE).
2.3.4 Platform as a Service
The difﬁculty of setting resource provision strategies for applications in such
complex contexts leads to considerable inefﬁciencies, prompting the establishment
of a new infrastructure category known as Platform-as-a-Service (PaaS) [ 6]. TheThe Role of Blockchain in Cloud Computing 49
platform enables the company to create, run, and manage business applications
without having to invest in the infrastructure that traditional software development
processes necessitate. Examples are AWS Elastic Beanstalk, Windows Azure,
Heroku, Force.com , Google App Engine, Apache Stratos, and OpenShift are just
a few of the services offered.
2.4 Comparison of Cloud Computing Model with Traditional
Model
The concept behind cloud computing is simple: it is cost-effective, scalable, and
can cover a lot more ground as compared to traditional computing. As a result
of it we become more efﬁcient, secure, and ﬂexible. Cloud computing has pushed
the concept of computing to new heights, and many ﬁrms have transferred their
computing resources from traditional to cloud computing. Nowadays, it’s rare to
ﬁnd a corporation that isn’t protected by the cloud and has its data stored on
the cloud’s safe servers. It is also far more efﬁcient than any other computing
infrastructure.
Traditional computingis distinguished using physical data centres for digital asset
storage and the procedure of an entire networking system for everyday operations.
Data, software, and storage are all accessible but limited to the devices or authorized
network to which users are connected. Clients can only access data stored in this
computing system on the device where it is kept. These datacentres are lacking in
many aspects when compared to cloud computing technologies, such as the need
for regular updates to stay on top of the latest security metrics and the occurrence
of hardware-related issues. In the end, companies spend far too much money to
maintain their data centres working and independent of security holes.
2.4.1 Persistency
If you’re looking for something that will stick with you and won’t abandon you,
cloud computing is the way to go. Nothing beats a good old cloud server in terms
of resiliency and elasticity. If the server that houses all of the critical data for your
business or website goes down for some reason, cloud technology can seamlessly
transfer everything to a new server that is up and operating, so you won’t even notice
anything is wrong. This is the nature of cloud computing’s durability. In traditional
computing, you’d have to replace broken pieces, add new gear, and pay more for
data centre administration to keep it running well. It is adaptable, but only to an
extent, but it isn’t persistent.50 H. K. Thakkar et al.
2.4.2 Automation
Automation is the fundamental essence of business success; if you can automate
many of your activities without requiring human intervention, you’ve already started
down the right path. Consistency, timely upgrades, security, cost-effectiveness, and
smooth operations around the clock are all advantages of automation. You havenothing without automation, which is why many tech-based businesses strive to
automate as many processes and systems as possible in order to achieve a certain
degree of technological satisfaction. Cloud computing keeps everything workingsmoothly and under the supervision of the cloud providers, which implies more
opportunities for automation, the ability to scale up and down as needed, and
lower costs. In the traditional computing sector, on the other hand, everythingmust be done manually, and there is a slim chance that the industry will ever
achieve automation. That’s the reason cloud computing is superior than traditional
computing.
2.4.3 Cost
Every ﬁrm has a formula in mind for determining how much money they canspend on this project. They can only go slightly above the amount, not all theway; no logical company model would be built on spending money in a free fall
scenario. You only pay for the resources you consume in the cloud computing world;
you don’t have to pay anything extra. Furthermore, you will not be charged formanagement, repair, or upgrade charges because the cloud provider will take care of
them. So, in the end, cloud computing is more cost-effective and reasonable, that’s
why so many ﬁrms are quickly adopting cloud computing. Traditional computing,comes with a higher price tag for repairs, upgrades, and upkeep. Annually, millions
of dollars’ worth tools are discarded and replaced with a new and more advanced
version of it. Only established and growing organisations or corporations can affordtheir own data centres in today’s market; start-ups cannot, and huge businesses and
companies do not use their own data centres either.
2.4.4 Security
Cloud computing appears to be hiding in the shadows, as the security metrics of acloud-based facility aren’t quite as promising as one might expect. The fundamental
reason for this strategy is that anyone with an internet connection and possiblylegitimate access to your cloud could seize control, endangering your online
presence and, with it, the data on which you rely so heavily. Classical computing
systems, on either hand, place you in charge of data storage and security. You cantake steps to secure your physical presence and data by restricting the amount of
people who have access to that location and ensuring that security bypass systems
are changed more frequently.The Role of Blockchain in Cloud Computing 51
2.5 Advantages of Cloud Computing
2.5.1 Cost Efﬁciency
Cloud computing is the most cost-effective way to use, maintain, and upgrade
technology. Traditional desktop software is quite expensive for businesses. When
the licence fees for multiple users are summed up, the cost to the company could
be rather considerable. On both sides, the cloud is far less expensive, and so candrastically cut a company’s IT costs. There are also a variety of one-time payment,
pay-as-you-go, and other customisable choices available, making it extremely cost
effective for the company in question.
2.5.2 Backup and Recovery
Because your data is saved in the cloud rather than on a physical device, backingit up and retrieving it is much easier. Furthermore, majority of cloud serviceproviders can usually handle data recovery because of latest advancement in Applied
Computer Science and Virtual Services. As, a result, compared to other traditional
data storage techniques, the backup and recovery process is simpler.
2.5.3 Integration of Software
Integration of software on the cloud is a common occurrence. Customers that usethe Cloud won’t have to go out of their way to conﬁgure and attach their apps totheir preferences. Normally, it’s managed by their own.
2.5.4 Information Availability
After registering in the cloud, users can access their data from anywhere with anInternet connection. This useful feature allows users to circumvent time zone and
geographic location issues.
2.5.5 Deployment
Finally, and most signiﬁcantly, Cloud computing allows for fast deployment. Thecomplete system will be fully operational in just a few minutes when this style is
implemented. The time consumed here, however, will be determined by the type oftechnology required by the company.52 H. K. Thakkar et al.
2.5.6 Easier Scale for Services and Delivery of New Services
It allows businesses to grow their services that respond to customer requirements
more easily. It allows for the creation of new forms of interactive applications as
well as the provision of new services.
2.6 Challenges of Cloud Computing
2.6.1 Technical Problems
In Cloud system might also fail. Organizations have to always be conscious of the
technology’s vulnerability to breakdowns with other technical problems. Even thebest Cloud service providers have issues, despite maintaining high maintenance
standards.
2.6.2 Certainty
The Cloud’s security is the other major concern. Beneﬁciaries should be informedthat by utilizing this technology, they are entrusting company’s conﬁdential data to
a third-party service provider of cloud. This may result to a huge danger for thecompany. As a result, organisations must choose the most reliable service provider
to ensure that their data is entirely protected.
2.6.3 Vulnerable Attacks
Storing data in the cloud has risk of data being stolen by cyber-attacks.
2.6.4 Suspension
As a result of cloud computing, small businesses rely heavily on their internet
connections uptime.
2.6.5 Inﬂexibility
Selecting a Cloud computing provider usually entails committing to utilize the ven-dor’s exclusive applications or formats. A document written in another application,
for example, cannot be imported to a Spreadsheet (in Google Docs). Furthermore, as
a company’s business expands or contracts, it must be able to add or remove Cloudcomputing customers as per their needed.The Role of Blockchain in Cloud Computing 53
2.6.6 Lack of Assistance
Anita Campbell has written, “Customer service for Web apps leaves a lot to be
desired – all too many cloud based applications make it difﬁcult to get customer
service promptly – or at all. Sending an email and hoping for a response within 48
hours is not an acceptable way for most of us to run a business”. New York Times
says: “The bottom line: If you need handholding or if you are not comfortable trying
to ﬁnd advice on user forums, the cloud probably is not ideal”.
2.7 Integration of Cloud Computing with Block Chain
2.7.1 The Advantages of Combining Cloud and Blockchain Technology
The introduction of Blockchain technology, it’s a distributed ledger with a well-
structured framework and a completely different notion in cloud. This is essential
for incorporating cloud-based peer-to-peer networking services. Our Blockchain
technology supports in the creation of a decentralized cloudledger [ 7], and in this
regard [ 8], we propose a decentralized cloud development architecture based on
an autonomous operational framework. However, the proposed architecture lacks
standard design and communication, adding a sense of uncertainty and scalability
to the system [ 9].
Blockchain technologies drive the most recent technological advancements in
cloud settings while also anonymizing user data and information. Blockchain is an
online wallet for users’ privacy and it appears to be a potential technique of ensuring
privacy in large clouds [ 10]. Theonline wallet is an authenticated blockchain tool
that allows us to securely delete our data and safeguard it from third-party access.
Evidence of concept in Blockchain handles the cloud network’s data security
and scheduling chores. Each Peer to Peer network junction has a cloud service
provider (SP). One of the service provider nodes serves like a main node for local
data, while the other serves as a compute. Furthermore, the blockchain system
will track the implementation of the ideal plan set in order to generate cloud
resource suppliers, cloud services, and data storage server recommendation lists
[11]. The usage of Bitcoin with blockchain has been subjected to security issues, and
situations of privacy have been tested with Bitcoin’s with the use of blockchain. The
security of data is deﬁned by cloud computing technology in terms of privacy and
honesty. These investigations, however, are insufﬁcient. The symbolic cryptographic
technology used in blockchain is well-known. Blockchain may be turned into a
scalable service like when it is combined with cloud storage it provides increased
security. The privacy of the user can be ensured when implementing blockchain
technology to save user information in the cloud storage world. To overcome the
following privacy issue, look at the previously discussed online wallet dilemma.
They proposed a remedy in [ 12], which would allow them to safely install and delete
the electronic wallet. To trust an intermediary is risky since the dealer could sneak54 H. K. Thakkar et al.
or alter the intimate information as Cloud computing allows us the key to access
third party. Blockchain technology, similar to Bitcoins, may be used to save data in
a distributed decentralized network [ 13]. It works in the same way as cloud storage,
but without the need for a third-party service. A person can design database and
collect the information as blocks upon various hard disc devices. Hash links are
used to connect such blocks. Each block has its own hash as well as the previous
block’s hash. Because each hash is distinct from the others, it can be compared to
a ﬁngerprint. It will be tough for a hacker to access every blocks if they seek to
steal sensitive information from an agency. However, if he hacks a block, which is
risky because he can’t change another block’s hash. After all, the two are closely
connected. Changing the hash and overwriting the data would take a long time.
2.7.2 Blockchain Support for Cloud Computing
2.7.2.1 Encryption of Data
The data is decoded before being saved on the cloud, raising concerns about
its reliability. Complete block data is converted to hash code via cryptographic
techniques in the blockchain network, and a hash key is generated for each block.
Consider the usage of blockchain to keep the cloud scheduling process safe. The
data from job scheduling is received by the control system, which generates hash
code and stores it dynamically in the blockchain network to maintain duration
and streaming data authenticity. Because the blockchain provides mechanisms for
revealing blocks through convergence, the data blocks’ validity is retained. Every
transaction is duplicated on every node in the network, providing the network with
the stability and durability as it needs to avoid undesirable faults and assaults [ 12].
2.7.2.2 Cloud-Based Data Management
The data in Cloud is unstructured by data management. The blockchain system is a
really well-organized system. The generated hash key can be used to trace the data
for every block. To sustain network tracking, each block in the network saves the
hash key of the previous block with its own key [ 13]. The network nodes could now
be accessed when the block data is conﬁrmed.
2.7.2.3 Service Level Agreements (SLAs)
In countries where equal justice doesn’t really exist, these cloud arrangements
beneﬁt the service provider or consumer. Blockchain smart contracts can be used to
ﬁx this problem. A smart blockchain contract facilitates the creation of faith between
parties who are unfamiliar with each other [ 14].The Role of Blockchain in Cloud Computing 55
2.7.2.4 Interoperability
External transmission isn’t permitted in public databases, which prevents many
businesses from making use of the cloud. At time when a cloud is merged with block
chain various clouds are referred as junctions. Communication takes place between
the blockchain internal nodes which are internally connected in the same network
and data is also shared among them as a result each node has a duplicate of the
transactions. Hence, we can network openness. Each of the following transaction
is added to the ledger and then transmitted to all nodes. Organizations can add
a number of different networks while maintaining data availability, which adds
authenticity to the network.
2.7.3 Deduplication of Data in the Cloud with Blockchain
2.7.3.1 Cloud Data Deduplication
According to a research by IDC, [ 15], by 2025, the cloud would have stored
almost 88 Zettabytes of data, with 75% of that being replicated data [ 16]. The
majority of CSPs, such as DropBox and Google Drive, in their Storage-as-a-Service
(StaaS), they implemented data deduplication technology products to maximise
cloud storage efﬁciency. This technology has the potential to beneﬁt both CSPs
and consumers by reducing bandwidth usage, boosting storage efﬁciency, and
minimizing the energy and infrastructure costs, which results in lower service prices
for clients. Despite this, deduplication methods still face signiﬁcant security issues.
To maintain the anonymity of outsourced data, cloud-based data storage servers
could be in the form of cypher-texts. CSPs, on the other hand, often refused to
allow clients to encrypt their data which is outsourced using standard encryp-
tion techniques (example: AES) [ 17], that hampered deduplication efﬁciency. To
achieve cypher text deduplication, convergent key encryption [ 18] was used instead.
Message lock encryption (MLP) [ 19] was later proposed as a type of convergent
encryption. In addition, the authors demonstrated that the MLP wasn’t really
semantically secure. A following study [ 20] developed a TTP to send tags that
aided in duplicate detection. According to our observations, the centralized design
of this strategy poses the most risk. A TPP single point failure would disable this
deduplication technique. Hackers could obtain ﬁle tags for further side channel
assaults depending on source which leads to deduplication systems by entering the
TTP [ 21].
Furthermore, integrity for the data was risked throughout the deduplication
process. After deduplication, there was just one copy left, which can be themain
target for attackers. Which means that a service interruption or a malevolent
administrator might simply and irreversibly wipe stored content. In the cloud, where
deduplication was utilised, data auditing was crucial in securing users’ data. The
deployment of a trusted administrator that reliesbased on the concept of a single
point of failure [ 22] was one way for providing reliable veriﬁcation.56 H. K. Thakkar et al.
2.7.3.2 Cloud Data Deduplication Based on Blockchain
Existing blockchain-based solutions have largely focuses on a decentralised multi-
cloud deduplication approach. Due to the inducement of a signiﬁcant deduplication
rate and fault detection performance, blockchain technology was employed to
control multi-cloud deduplication activities. CloudShare [ 23], for example, used
blockchain to manage multi cloud deduplication. Here, user side encryption was
being used to protect against assaults by hostile servers working together. Temper-
resistant blockchain transactions ensured the integrity and ownership of user data.
Multiple CSPs were able to immediately synchronize ﬁle information in order to
dynamically instruct the deduplication mechanism using blockchain.
A cloud deduplication system based on smart contracts was presented by Li et
al. [24], [25]. To ensure ﬁle integrity and retrievability, as well as to protect against
side channel assaults, the Business Smart Contract (BSC) a request and respond
protocol was applied to execute periodic Proof-of-Retrievability (PoR). BSC was in
charge of ﬁle pointer management as well as Transaction Smart Contracts (TSC),
that are initiated once the server successfully completed the PoR challenge along
with executing payment and transaction operations automatically [ 26] outperformed
[24] thanks to its automatic ﬁle reconstruction, which took advantage of distributed
storage.
Outline Despite the importance of blockchain-based cloud deduplication, there
has been minimal research in this area to date. A major roadblock was the
clash between the goal of deduplication and high-redundancy blockchain data.
There was no way that could completely rebuild a cloud storage system using
the deduplication technologies using blockchain. Blockchain appeared to be a
subsystem for maintaining the security of cloud storage. File tags were stored on-
chain in previous work, but ﬁles were kept off-chain. With this conﬁguration, only
a little amount of storage space was being used, yet data consistency and security of
the system were improved.
2.7.4 Access Control Based on Blockchain in Cloud
2.7.4.1 Cloud Computing Access Control
Access control acted as an important element of cloud data security and privacy,
because it prevented unauthorised users to access cloud data. Well-deﬁned access
control policies were a big part of traditional cloud access control approaches.
There were four types of policies in the past: DAC (discretionary access control),
MAC (mandatory access control), RBAC (role-based access control), and ABAC
(attribute-based access control) are all types of access control (ABAC).
In DAC, the authenticated user, such as a service provider, is in charge of
determining how other users gain access to the objects (e.g., cloud users). Since
no ﬁxed rule was enforced in DAC, following method provided. In contrast toThe Role of Blockchain in Cloud Computing 57
DAC, a predeﬁned trusting policy was used to implement MAC which can’t be
altered synchronously. Since the system administrator was in charge of the access
restrictions rather than objects, the approach was focused on conﬁdence rather than
integrity [ 27].
In RBAC approach, subjects were given access rights rather than their identities,
based on their positions and duties in the system. The lack of consideration in other
elements of subjects generated a downside due to the nature of RBAC [ 27]. To
address these difﬁculties further, the ABAC was proposed. It set up the access rule
based on a study of the attributes of objects and subjects [ 28]. The main advantage
of ABAC was that it took into account everything during the authentication process.
Although the ABAC authentication was a time-consuming process, it used very little
computing resources in the cloud.
Traditional access control systems have a common ﬂaw in that they rely heavily
on a centralised setting that are in short of Visibility, traceability, tamper-resistance,
and multi-party governance. An exchange between security and efﬁciency exists in
the application environment, and it is difﬁcult to resolve in nature.
2.7.4.2 Cloud Access Control Using Blockchain
As compared to conventional access control systems, blockchain-based access
control (BAC) offers a few beneﬁts derived from blockchain properties. There are
mainly two advantages on which we will concentrate:
BAC incorporates consensus into the implementation of network access, allow-
ing all stakeholders to logically participate in the process. Establishing a decision
usually needs agreement level consent from participating voters or decision
makers, which improves decentralisation security.
For access control, blockchain-enabled traceability provides a visible and
immutable governance mechanism. This feature increases the difﬁculty of
opponents.
Because of the layered structure of cloud architecture, access control in clouds
primarily served two purposes. The ﬁrst was the function of cloud service, which
was in charge of restricting cloud customers’ access to cloud data and services.
Secondly, it had a visible role that it required governance for Virtual Machines’
(VMs) access to actual machines in the event, dangers posed by side channel
study [ 29]. A blockchain-based decentralised access control system could reduce
the risk of a failover and data theft by outsiders. Data owners could control
access of their own information more ﬂexibly and completely by using blockchain
technology [ 30]. According to a study [ 31] showed that BAC can allow data
transfer in an untrustworthy setting. Decentralization can mitigate the risks posed
by untrustworthy third parties or participants [ 32].
Zyskind and Nathan [ 33] created a decentralized private data management
system for an off chain storage of mobile data. In this blockchain network, there
are two types of transactions. The very ﬁrst type of transaction, Taccess, was58 H. K. Thakkar et al.
created to manage access control, Tdata, was in charge of storing the data. By
deﬁning different policy sets in the Taccess transaction, owners of the data were
able to change the access authentications. Tdata additionally uses the check control
protocol to regulate read/write operations.
Users would have complete control by using digitally-signed transactions, which
would prevent hostile invasions (from unauthorised users) here blockchain enhanced
DAC paradigm. And more explicit, the protocol-based transaction featured com-
pound key creation, authorization check, control for the access, and data on/off chain
engagement, and provided a dynamic and good access control protocol.
In addition, the writers discussed blockchain expansions in their paper. The
extension could be realised in the ﬁrst phase by effective off-chain data processing,
hence off chain data security had to ensure throughout data processing. To address
this problem, an analytical approach was presented that used a secure multi-party
computation paradigm to partition data into shares [ 33]. The extension’s second
phase was a test of the blockchain network’s trustworthiness. The sigmoid function
of the change in the number of “wise” and “bad” activities is used to calculate the
new trust score. The results of the test revealed that this method of measuring trust
might defend the blockchain system from malicious attacks.
As transaction based access control got ﬁrst introduced, we also noticed that
scalability was a problem. To address this problem, BBDS [ 34] was created,
to improve the system’s efﬁciency and scalability, it used a lightweight block
architecture. This concept was used to safeguard conﬁdential medical information
stored in cloud. As we entered the blockchain 2.0 age, smart contracts were another
frequently utilised alternatives which can be used to improve access control.
A blockchain-based therapeutic management framework was created by Rahman
et al. [ 35] While the recipient’s medical data was housed in off-chain clouds, the
smart contract included an off-chain data access policy. The reliance on trusted
third parties, such as physiotherapy centres, caregivers, and therapists, was a major
disadvantage of this technique.
The reliance on trusted third parties, like physiotherapy centres, caregivers, and
therapists, was a major downside of this system. To provide access control, the
MedShare [ 36] system uses various types of smart contracts in conjunction with
cloud provenance data. Some contracts were in charge of judging misbehaviours
and threats on basis of provenance data, while others were in charge of carrying
out potential misbehaviours and threats based on provenance data. Furthermore,
malevolent cloud users’ access rights were revoked as a kind of punishment.
MedShare, on the other hand, could only cancel access authentication; other critical
access control activities were overlooked.
Outline Access control was a critical tool for preventing unauthorised intruders
from accessing user data. Traditional access control techniques encounter chal-
lenges such as signal point loss, untrustworthy trusted third party, and a lack
of user control. Users could have complete control over their data by deploying
blockchain technology, which eliminates the risk of a single point of failure. Smart
contracts also allowed for automatic access management and the identiﬁcation andThe Role of Blockchain in Cloud Computing 59
punishment of misbehaviours. Furthermore, all of these access control approaches
were used to provide secure cloud storage.
Conclusion Cloud computing is used by public in the whole world since long
time. In Cloud Computing, there are some areas of concern, such as security,
data management, and so on. Blockchain although is a quite new technology has
these drawbacks of cloud computing in the form of its advantages providing a
highly secure data encryption, excellent data management, etc. The simultaneous
establishment and working of Blockchain with cloud computing would result into a
highly secure system which can therefore be further used to keep and manage the
data more securely and efﬁciently.
References
1.CH. V . N. U. B. Murthy et al (2020) Blockchain based cloud computing: architecture and
research challenges. IEEE Access 8 (2020): 205190–205205
2.https://www.vmware.com/topics/glossary/content/public-cloud.html
3.https://www.cloudways.com/blog/what-is-public-cloud/
4.https://www.javatpoint.com/private-cloud
5.https://dcirrus.com/implementations-of-cloud-computing/
6.Boniface M, et al (2010) Platform-as-a-service architecture for real-time quality of service
management in clouds. In: 2010 ﬁfth international conference on internet and web applications
and services, pp 155–160. https://doi.org/10.1109/ICIW.2010.91
7.Liang X, et al (2017) Provchain: a blockchain-based data provenance architecture in a cloud
environment with enhanced privacy and availability. In: Proceedings of the 17th IEEE/ACM
international symposium on cluster, cloud and grid computing, IEEE Press
8.Skulj G et al (2017) Decentralised network architecture for cloud manufacturing. Int J Comput
Integr Manuf 30(4–5):395–408
9.Barenji A V , Guo H, Tian Z, Li Z, Wang WM, Huang GQ (2019) Blockchain-based cloud
manufacturing: decentralization. arXiv preprint arXiv:1901.10403
10.Park JH, Park JH (2017) Blockchain security in cloud computing: use cases, challenges, and
solutions. Symmetry 9(164). Retrieved from www.mdpi.com/2073-8994/9/8/164
11.Kolodziej J, Wilczynski A, Fernandez-Cerero D, Fernandez-Montes A (2018) Blockchain
secure cloud: a new generation integrated cloud and blockchain platforms general concepts
and challenges. Eur Cybersecur J 4(2):28–35
12.Ingole MKR, Yamde MS (2018) Blockchain technology in cloud computing: a systematic
review
13.Harshavardhan A, Vijayakumar T Mugunthan SR (2018) Blockchain technology in cloud
computing to overcome security vulnerabilities. In: 2018 2nd international conference on I-
SMAC (IoT in social, mobile, analytics, and cloud)(ISMAC) I-SMAC (IoT in social, mobile,
analytics, and cloud)(I-SMAC), 2018 2nd international conference on, IEEE. pp 408–414
14.Tosh D, Shetty S, Liang X, Kamhoua C, Njilla LL (2019) Data provenance in the cloud: a
BlockchainBased approach. IEEE Consum Electron Mag 8(4):38–44
15.Reinsel D, Gantz J, Rydning J (2018) The digitization of the world: from edge to core
16.IDC, “Idc report,” https://www.emc.com/collateral/analyst-reports/idcdigital-universe-are-
youready.pdf
17.Shin Y , Koog D, Hur J (2017) A survey of secure data deduplication schemes for cloud storage
systems. ACM Comput Surv, 49(4):7460 H. K. Thakkar et al.
18.Douceur J, Adya A, Bolosky W, Simon P, Theimer M (2002) Reclaiming space from duplicate
ﬁles in a serverless distributed ﬁle system. In: The 22nd Int’l Conference DCS. IEEE, pp 617–624
19.Bellare M, Keelveedhi S, Ristenpart T (2013) Message-locked encryption and secure dedupli-cation. In: Ann’l Int’l conference on the theory and applications of cryptographic techniques.Springer, pp 296–312
20.Keelveedhi S, Bellare M, Ristenpart T (2013) Dupless: server-aided encryption for dedupli-cated storage. In: Presented as part of the 22
nd{USENIX }Security Sym., pp 179–194
21.Harnik D, Pinkas B, Shulman-Peleg A (2010) Side channels in cloud services: deduplication
in cloud storage. IEEE Secur Privacy 8(6):40–47
22.Yuan J, Yu S (2013) Secure and constant cost public cloud storage auditing with deduplication.In: IEEE conference on communications and network security. IEEE, pp 145–153
23.Li Y , Zhu L, Shen M, Gao F, Zheng B, Du X, Liu S, Yin S (2017) Cloudshare: towards acost-efﬁcient and privacy-preserving alliance cloud using permissioned blockchains. In: Int’lconference on mobile networks and management. Springer, pp 339–352
24.Liu H, Zhang Y , Yang T (2018) Blockchain-enabled security in electric vehicles cloud andedge computing. IEEE Netw 32(3):78–83
25.Li J, Wu J, Chen L, Li J (2018) Deduplication with blockchain forsecure cloud storage. In:CCF conference on big data. Springer, pp 558–570
26.Liu C, Lin Q, Wen S (2018) Blockchain-enabled data collection and sharing for industrial IoTwith deep reinforcement learning. IEEE Trans Indus Inf
27.Salman T, Zolanvari M, Erbad A, Jain R, Samaka M (2019) Security services using
blockchains: a state of the art survey. IEEE Commun Surveys Tuts 21(1):858–880. 1st Quart
28.Qiu M, Gai K, Thuraisingham B, Tao L, Zhao H (2018) Proactive user-centric secure data
scheme using attribute-based semantic access controls for mobile clouds in ﬁnancial industry.Future Gener Comput Syst 80:421–429
29.Zhang Y , Kasahara S, Shen Y , Jiang X, Wan J (Apr. 2019) Smart contract based access controlfor the internet of things. IEEE Internet Things J 6(2):1594–1605
30.Rouhani S, Deters R (2019) Blockchain based access controlsystems: State of the art andchallenges. [Online]. Available:arXiv:1908.08503
31.Sukhodolskiy I, Zapechnikov S (2018) A blockchain-based access control system for cloudstorage. In: Proceeding of the IEEE EIConRus, pp 1575–1578
32.Wang S, Wang X, Zhang Y (2019) A secure cloud storage framework with access control basedon blockchain. IEEE Access 7:112713–112725
33.Zyskind G, Nathan O (2015) Decentralizing privacy: Using blockchain to protect personal data.In: Proceeding of the IEEE security privacy workshops, pp 180–184
34.Xia Q, Sifah E, Smahi A, Amofa S, Zhang X (2017) BBDS: Blockchain-based data sharing forelectronic medical records in cloud environments. Information 8(2):44
35.Rahman M et al (2018) Blockchain-based mobile edge computing framework for securetherapy applications. IEEE Access 6:72469–72478
36.Xia Q, Sifah E, Asamoah K, Gao J, Du X, Guizani M (2017) MedShare: trust-less medical datasharing among cloud service providers via blockchain. IEEE Access 5:14757–14767Analysis and Prediction of Plant Growth
in a Cloud-Based Smart Sensor
Controlled Environment
Aritra Nandi, Arghyadeep Ghosh, Shivam Yadav, Yash Jaiswal,
and Gone Neelakantam
1 Introduction
The demand for food is directly proportional to the population of a country. With
the increase of population in India, the consumption of food is also increasing.
According to the United Nations Food and Agriculture Organization(FAO), food
security is often achieved if all individuals, at all times, have physical, social as well
as ﬁnancial access to adequate, safe, and healthy food that meets the consumption
intake of food for an energetic life.
Although India has enough capacity for food grains supply, many Indians still
suffer from food insecurity. A large chunk of people does not have enough food
available to meet their dietary needs to keep healthy. Also, the quality of the diet
that these people consume does not provide various vital micronutrients. A nutrition
survey of children by the Comprehensive National Nutrition Survey 2016–2018
reveals that 35% of kids below the age of ﬁve were stunted, 22% of school-age
children were stunted, and 24% of teenagers were under weighted by their age.
In Fig. 1, we can see the amount of crop wastage every year is increasing due to a
lack of consciousness among farmers. The crop loss over the year 2014 was around
595 thousand tons and it is predicted to be as high as 1514 thousand tons in the year
2026. Although with proper measures we can reduce these numbers signiﬁcantly.
Another factor for this food insecurity is the quality of food being consumed. India’s
mineral-deﬁcient soils are a signiﬁcant reason that limits the variety of crops being
grown. This causes the lack of micronutrients in a regular diet. Also, the drastic
A. Nandi (/envelopeback) · A. Ghosh · S. Yadav · Y . Jaiswal
School of Computer Engineering, KIIT (Deemed to Be) University, Bhubaneswar, Odisha, India
G. Neelakantam
Department of Computer Science and Information Engineering, Chang Gung University,
Guishan, Taiwan
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_46162 A. Nandi et al.
Fig. 1 Amount of crop wastage vs year
changes in the climate and water scarcities affect the quality of crops. In urban
areas, the availability of agricultural land is a real challenge as most of the sites
are ﬁlled with buildings and skyscrapers. These issues need to be resolved with
some new practices and modern methods. Due to a shortage of agricultural lands,
people can utilize the unused part of their home (terrace, backyard, etc.) for a
small cultivation land or gardening. More speciﬁcally, a greenhouse. Also, smart
farming and monitoring can be implemented to improve the quality of crops in the
greenhouse.
A lot of technologies are being used these days to capture the conditions based
on which certain crops can grow. IoT system is one of the emerging techniques
which is evolving at this time. With the introduction of IoT in farming, farmers can
easily get knowledge about the climate, soil, and moisture conditions by which the
productivity of crops grown in certain regions increases. After data gets generated
through the sensors, these data are then sent to the cloud for further analysis.
Various machine learning techniques can be applied to this data to gain more reﬁned
information [ 13,14]. This research paper deals with data generated from sensors
like humidity, temperature, NPK, and pH, and uses a machine-learning model i.e.
LightGBM on this gathered data which predicts the most suited crop that can be
grown in the given greenhouse environment. This model is capable of training and
handling large-scale data efﬁciently, acquiring low memory usage, and giving the
highest efﬁciency, making it the most suitable model for this job. The ﬁnal prediction
also gives a better understanding of their current environmental conditions like pH,
moisture levels, nutrient content in the soil, temperature, and humidity.Analysis and Prediction of Plant Growth in a Cloud-Based Smart Sensor ... 63
2 Literature Survey
“ZIGBEE BASED GREENHOUSE ENVIRONMENT MONITORING AND
CONTROL SYSTEM”, [ 1] by T. Likhitha and A. Sirisha, et al. used a Zigbee
based WSN network. The automated wireless system can control greenhouse
climate. Apart from this, the system is capable of performing tests on temperature
and humidity sensors. Sensor Station (SS), Coordinator Station (CS), and Central
Control Station (CCS) are the major units used and ZigBee modules are used for
wireless network connection.
Ahmet Murat Turk, et al. used a SQL database to store data like temperature and
humidity for future prediction in “An Automation System Design for Greenhouses
by Using DIY Platforms”, [ 2] DIY platforms like raspberry pi and Arduino
provide ﬂexibility and low power consumption to work on embedded systems. The
automation system can be controlled via both smartphones and the web for which a
network package and database design are suggested.
“A Remote Monitoring System for Greenhouses Based on the Internet of
Things”, [ 3] by Zhenfeng Xu et al. is based on LPL charged with solar panels,
which reduces energy consumption. The system also uses ACK which has a better
quality of wireless communication.
J Muangprathub, et al. in “IoT and agriculture data analysis for smart farms”, [ 4]
proposed the IoT and agriculture data analysis. Their work was to develop a control
system with the help of sensors that can be managed by a web app.
“Smart Agriculture System in India Using Internet of Things”, [ 5]D r .R a m a
Krushna Das, et al. proposed an IoT-based smart agriculture system with different
sensors and Raspberry Pi. Several models are also used to get the value of soil
content like pests, moisture, etc.
“Crop yield prediction using machine learning: A systematic literature review”,
[6] Thomas van Klompenburg, et al. used several algorithms based on machine
learning and deep learning for crop yield prediction. Convolutional Neural Net-
works (CNN), Long Short Term Memory (LSTM), and Deep Neural Network
(DNN) were mostly used as deep learning algorithms.
Abdul Rehman, et al. used machine learning and IoT for smart farming in
“Machine Learning Prediction Analysis using IoT for Smart Farming”, [ 7]f o rt h e
prediction of soil conditions (moisture and temperature), they have used KNN,
ANN, SVM, and many other machine learning algorithms.
“A Temperature Compensated Smart Nitrate-Sensor for Agricultural Industry”,
[8] by Md Eshrat E. Alahi, et al. used a nitrate sensor to get an accurate concentration
of nitrate in the soil. A Wiﬁ-based IoT system is also used which is connected to the
IoT-based web server for storing data for future use.
Akshay Badhe, et al. proposed an “IoT Based Smart Agriculture And Soil
Nutrient Detection System”, [ 9] which predicts the suitable crop for the environ-
mental conditions. They used sensors like NPK Sensor, DHT 11, pH value, and soil
moisture.64 A. Nandi et al.
Anand Nayyar and Er. Vikram Puri, et al. in “IoT Based Smart Sensors
Agriculture Stick for Live Temperature and Moisture Monitoring using Arduino,
Cloud Computing & Solar Technology”, [ 10] have proposed a Novel Smart IoT
based Agriculture Stick. It includes a live data feed used with various sensors along
with Arduino Technology. The tested product results give high accuracy over 98%
in data feeds.
Pradorn Sureephong, et al.in “The comparison of soil sensors for integrated
creation of IoT-based Wetting front detector (WFD) with an efﬁcient irrigation
system to support precision farming”, [ 11] implemented a deliberate Wetting front
Detector to support precision farming. The study conducted the comparison of 2
sensors, the Frequency Domain Reﬂectometry sensor (FDR) and the Resistor-based
sensor (RB). The results showed a positive edge for the use of IoT-WFD.
3 IoT in Greenhouse
As various kinds of sensors are used in the Greenhouse to measure physical
conditions like temperature, pH, humidity, etc., it is important to constantly monitor,
analyze and store the information generated from these sensors. Also, it is beneﬁcial
for the users if they can get access to this data remotely from their computers or
smartphones. The best and most popular solution is connecting these sensors to
the internet. In broad terms, IoT can be implemented to access data and control
devices remotely [ 12–15]. IoT connects devices via a network and monitors their
performances and features from a distant location. This device has the capability
to monitor the environmental conditions inside a greenhouse remotely. The sensor
nodes are based on a microcontroller like NodeMcu (hardware) which connects to
web-based software. A remote monitoring system can be developed with the help of
web Technology. The monitoring terminal uses Node.Js as a backend which is used
to serve the frontend. The web app is user-friendly so that the farmers can operate it
with great ease. The system has been installed in a glass greenhouse.
3.1 Architecture
In the smart Greenhouse Monitoring System, IoT is implemented for controlling the
physical devices and to visualize and analyze the data produced by the sensors. This
architecture consists of three layers: The perception layer, the transport layer, and
the Application layer. In the perception layer, different sensors are used to monitor
the physical conditions of the Greenhouse. They collectively form this layer. Each
sensor is connected to a microcontroller creating a sensor node that can send data to
the network layer.
All the data from the sensor nodes are processed by a local monitoring system
whose function is to collect this data and send it to the transport layer. The controllerAnalysis and Prediction of Plant Growth in a Cloud-Based Smart Sensor ... 65
nodes can be managed by receiving the control commands [ 16]. The transport
layer provides end-to-end message transfer capability. The data collected from
IoT devices are stored in cloud databases which can be further accessed. In the
application layer, the outcomes of analysis and processing of data is shown. In this
layer, the data is visualized in the form of graphs and various other representations
like the ﬁnal prediction of the model after applying machine learning to this data. A
Web app has been implemented where users can log in to their accounts and monitor
the readings from their smart devices.
3.2 Cloud Implementation
The data generated from the sensors is collected by the IoT devices. These sensors
are connected to the wireless network which is accessing the internet. Hence this
data is sent to the Cloud database using a REST API. The API is managed by
AWS Lambda which handles the data collected from IoT devices and stores it in the
database. The Lambda function is a highly available, fault-tolerant infrastructure
that seamlessly deploys the API without any complications. It also provides a
continuous scaling feature that can handle a large number of requests at a time
automatically [ 17]. After getting the data, it is stored in the MongoDB database
which is hosted on AWS. It is a No-SQL-based database that stores information in a
documented model. So all the data can be stored in a single document which makes
the queries run much faster than SQL based database. Also, it is easy to scale as it
uses internal memory for storing the working set. MongoDB enables fast access to
data which is helpful in displaying real-time updates about the environmental data
on the web app.
3.3 Hardware Components (Fig. 2)
The four components that make a sensor node consist of a temperature and humidity
sensor, a pH sensor, an NPK sensor, and a controller board to interact with the sensor
and send data to the web servers. For this GreenHouse system, the sensor used is
the DHT11 temperature and humidity sensor and Fig. 3. NodeMcu (ESP8266) as
a controller board. It is a basic, low-cost digital temperature and humidity sensor.
NPK and pH sensors were used to measure soil nutrients (Nitrogen, phosphorus,
and potassium) and the acidity of the soil respectively. To read and send these sensor
values to the server, Nodemcu was used. It is a low-cost open-source IoT platform.
It runs on ESP8266 (microcontroller) Wi-Fi SoC from Espressif Systems. NodeMcu
is used to get data from the sensors and send it to the webserver. NodeMcu can be
programmed in many ways but the easiest one is to use Arduino IDE. An HTTP
POST request can be sent to the webserver from NodeMcu through an API. In this
way, data can be posted to the server at a regular interval of 10 min. This data is66 A. Nandi et al.
Fig. 2 DHT11 sensor
Fig. 3 NodeMCU
stored using a database [ 18]. Here MongoDB is used which is mostly preferred in
this type of work due to its scalable nature and storing data in document format.
4 System Overview
From Fig. 4, it can be observed that after collecting the data from the IoT sensors the
data is stored in the cloud database. As the data which is stored is of high volume
there were possibilities of garbage values. The second step is data preprocessing
which excludes those unwanted values and makes the quality of the data better. This
step leads the model again to higher accuracy. The next step includes the training
and building of the model. The lightGBM model was built to recommend the crop.
The lightGBM model is built from a collection of decision trees, which makes the
model more efﬁcient. LightGBM gives us a very fast speed to train the model which
reduces its training time. Now let’s see the methodology.Analysis and Prediction of Plant Growth in a Cloud-Based Smart Sensor ... 67
Fig. 4 System overview
4.1 Dataset
The augmented dataset was collected from various samples of soil throughout India.
It was collected using IoT devices and stored in the cloud database [ 19]. The
information contains seven different data ﬁelds, the ratio of nitrogen, phosphorus,
potassium, the temperature in degrees Celsius, humidity in percentage, the value
of pH in the soil, and the amount of rainfall in mm (millimeter). The dataset also
consists of raw data of the major crops with respect to the data ﬁelds according to
the cultivation.
4.2 Data Preprocessing
While collecting the data the values in the data ﬁelds were varied because the
data was collected from multiple sources. Due to this data duplicity was observed.
Dataset was collected from various sensors and due to the malfunction of these
sensors, garbage values were found in the dataset [ 20]. That is why data pre-
processing was a major step in this methodology. The process started with data
cleaning. Data .isnull().any() was used to check for null values and data.dropna() for
removing these null values from the dataset. Data.duplicated() was used to remove
duplicate values so that it does not affect the accuracy and efﬁciency. Comparing
multiple data variables across the dataset, data consistency was maintained. With
the use of this technique, the quality of the dataset was improved and was further
imported to gain higher accuracy.68 A. Nandi et al.
Fig. 5 Tree representation
4.3 LightGBM
Nowadays Machine Learning is one of the fastest-growing ﬁelds. LightGBM is
one of the most powerful machine learning algorithms which is based on gradient
boosting, which not only acts as a regressor but also a classiﬁer. The most unique
feature of this algorithm is that it grows trees vertically (leaf-wise) and is not like
others. Due to this feature, at the time of training, the validation loss reduces to the
minimum value [ 21–24]. The dataset that is used here for training the model is of
huge volume. In the case of lightGBM when the volume of the dataset is large, very
few overﬁtting conditions are observed. And thus, using this algorithm makes the
analysis more compatible. LightGBM also works well for multiclass classiﬁcation
problems (Fig. 5).
4.4 Training and Building the Model
After processing the data, the distribution of the target variable and the dependent
variable is checked, and then the dataset is split into a training set and test set
using the train_test_split function which is provided by sklearn for randomly
splitting data, taking the parameter value that is test_size as 0.3(which means
the 30 percent of the observation will go in the testing dataset and the rest
70 percent in training data set), shufﬂe as True and random_state as 0 which
improves and optimizes the further process. Now the next step is to prepare a
dataset for the LightGBM model. For training the model we need to change the
training data (X_train and Y_train) into lightGBM dataset format. For converting
into lightGBM the function used lgb.LGBMClassiﬁer() where parameters like
max_depth, learning_rate, random_state, and many more are taken as default. To
get rid of overﬁtting conditions we initialize the max_depth parameter which has the
control of tree growth and depth, learning_rate, which affects how each tree impacts
the ﬁnal outcome. Here GBM starts by taking an initial estimate which gets updated
with each output of the upcoming tree. The learning parameter is responsible for the
change in the magnitude of estimates [ 25–27]. The accuracy of the model is totally
dependent on the parameter value and parameter itself so, After the successfulAnalysis and Prediction of Plant Growth in a Cloud-Based Smart Sensor ... 69
conversion model.ﬁt() function has been used in order to achieve better accuracy
where the parameters like batch_size and epoch value are set as default. Now the
LightGBM model is ready for prediction, so bypassing X_test as a parameter in the
model.predict() function we will get a prediction for the test data set. LightGBM
grows leaf-wise as its tree grows vertically. It chooses the leaf with max delta loss
to grow, as the leaf-wise algorithm reduces loss when compared to the level-wise
algorithm. Below is a comparison of both the leaf-wise algorithm (based on LGBM)
and the level-wise algorithm (based on other boosting algorithms).
5 Results and Explanation
Figure 6shows the real-time data analysis which is generated from the sensors
without any human intervention thus preventing risk factors. The data which is
generated gives insights immediately and efﬁciently. The data is ﬁrst generated from
sensors like DHT11, soil moisture sensor, and NPK sensor and sent to the cloud via
REST API. This data is stored in MongoDB from which data can be accessed at a
rapid pace for real-time analysis.
Figure 7shows the Nitrogen, Phosphorus, and Potash value (in kg/ha) compar-
ison for different crops. This bar chart basically implies the number of nutrients
required in the soil for the proper growth of different crops. From the diagram, it
can be observed that the potash value of apples and grapes is around 200 kg/ha. So
these plants need potash-rich soil for proper growth. At the same time, a low amount
of nitrogen (about 20 kg/ha) and moderate levels of phosphorus (about 130 kg/ha)
are sufﬁcient for them.
Figure 8shows the rainfall (in mm), temperature (in degree Celsius), and relative
humidity (in %) value comparison for different crops. This graph denotes the
Fig. 6 Real-time data analysis70 A. Nandi et al.
Fig. 7 N, P, K Comparison between crops
Fig. 8 Comparison between rainfall, temperature, and humidity
environmental conditions that are suitable for the proper growth of different crops
like rice, apple, banana, coffee, cotton, and many more. From the diagram, it can
be observed that rice requires the most amount of rainfall i.e. about 230 mm, a
low temperature of 23 C, and high relative humidity of 80%. So an environment of
high moisture, low temperature, and high humidity should be maintained inside the
greenhouse for the proper growth of rice.
The main beneﬁt of using LightGBM is that it uses an algorithm that is based
on the histogram. In the training phase, the continuous features present are being
grouped into different bins which makes the training process faster and reduces
memory usage. Thus it takes less amount of time to execute this process. As we
have seen that the structure of lightGBM follows a leaf-wise split approach whichAnalysis and Prediction of Plant Growth in a Cloud-Based Smart Sensor ... 71
Fig. 9 Model accuracy
leads to better accuracy among all the boosting algorithms. After training, the model
reached the overﬁtting condition, so to avoid this max_depth() function was used to
convert it to the best ﬁt.
Figure 9shows the validation accuracy i.e. the accuracy increases with the
increase in the number of epochs. As per the analysis, the validation accuracy in
the ﬁrst epoch was found to be 0.7953, and till the last epoch accuracy reached
0.9938. LightGBM is the best ﬁt model which gives an accuracy of 99.38%.
6 Conclusion
The proposed method suggests the best-ﬁt crops that can be grown in the envi-
ronment (Greenhouse) of the users as well as farmers (on farming land). This
system preprocesses the data given by the user like the amount of moisture, rainfall,
humidity, and the nutrients present in the soil, and recommends the most suitable
crop that can be grown in such an atmosphere. Also, a visual representation of
nutrients and environmental conditions of the suggested crop is also displayed to
understand the requirements of that crop with ease. Also with the help of IoT,
real-time data can be generated and monitored. This data can be used for further
analysis which will help to improve the recommendation system. More sensors can
be included in this system for a better understanding of the environment. The most
important aspect of introducing a cloud environment to the system was to analyze
real-time data. The data generated was so accurate that no error was found.
This experimentation system was set up for the greenhouse but can also give very
good results if being used in the real world. Indian Farmers can get a lot of beneﬁts
with the help of this type of architecture. This will give them prior knowledge about72 A. Nandi et al.
the environmental situation and they can plan accordingly for better results. We
know that India is a vast region and due to this, various crops are grown widely
across the region with respect to the soil. Soil present in different regions of India isdifferent from each other if we see it with respect to the nutrients they contain. As the
system also detects the number of nutrients required for a certain plant, farmers will
be able to identify the particular crop which can give the best product in that speciﬁcregion. This is will lead to an increase in crop production in India and minimize the
wastage of crops that happens every year due to a lack of knowledge in farmers.
References
1.Likhitha T, Sirisha A. Zigbee based greenhouse environment monitoring and control system
2.Turk AM, Gunal ES, Gurel U (2016) An automation system design for greenhouses by
using DIY platforms. In: The international conference on science, ecology and technology(Iconsete’2015–Vienna), pp 257–266
3.Xu Z, Chen J, Wang Y , Fan Z (2016) A remote monitoring system for greenhouse based on theinternet of things. In: MATEC web of conferences, vol 77. EDP sciences, p 04001
4.Muangprathub J, Boonnam N, Kajornkasirat S, Lekbangpong N, Wanichsombat A, Nillaor P(2019) IoT and agriculture data analysis for smart farm. Comput Electron Agric 156:467–474
5.Das RK, Panda M, Dash SS (2019) Smart agriculture system in India using internet of things.In: Soft computing in data analytics. Springer, Singapore, pp 247–255
6.Van Klompenburg T, Kassahun A, Catal C (2020) Crop yield prediction using machinelearning: a systematic literature review. Comput Electron Agric 177:105709
7.Rehman A, Liu J, Keqiu L, Mateen A, Yasin MQ (2020) Machine learning prediction analysisusing IoT for smart farming. Int J 8(9)
8.Alahi MEE, Xie L, Mukhopadhyay S, Burkitt L (2017) A temperature compensated smartnitrate-sensor for agricultural industry. IEEE Trans Ind Electron 64(9):7333–7341
9.Badhe A, Kharadkar S, Ware R, Kamble P, Chavan S (2018) IOT based smart agriculture andsoil nutrient detection system. Int J Future Revolut Comput Sci Commun Eng 4(4):774–777
10.Nayyar A, Puri V (2016) Smart farming: IoT based smart sensors agriculture stick for live
temperature and moisture monitoring using Arduino, cloud computing & solar technology. In:
Proceeding of the international conference on communication and computing systems (ICCCS-2016), pp 9781315364094-121
11.Sureephong P, Wiangnak P, Wicha S (2017) The comparison of soil sensors for integratedcreation of IOT-based wetting front detector (WFD) with an efﬁcient irrigation system tosupport precision farming. In: 2017 international conference on digital arts, media andtechnology (ICDAMT). IEEE, pp 132–135
12.Kajol R, Akshay KK (2018) Automated agricultural ﬁeld analysis and monitoring system usingIoT. Int J Inf Eng Electron Bus 10(2):17
13.Tripathy HK, Mallick PK, Mishra S (2021) Application and evaluation of classiﬁcation modelto detect autistic spectrum disorders in children. Int J Comput Appl Technol 65(4):368–377
14.Joshi J, Polepally S, Kumar P, Samineni R, Rahul SR, Sumedh K, ..., Rajapriya, V . (2017,January). Machine learning based cloud integrated farming. In: Proceedings of the 2017international conference on machine learning and soft computing, pp 1–6
15.Ke G, Meng Q, Finley T, Wang T, Chen W, Ma W et al (2017) Lightgbm: a highly efﬁcientgradient boosting decision tree. Adv Neural Inf Proces Syst 30
16.Mishra S, Thakkar HK, Mallick PK, Tiwari P, Alamri A (2021) A sustainable IoHT basedcomputationally intelligent healthcare monitoring system for lung cancer risk detection.Sustain Cities Soc 72:103079Analysis and Prediction of Plant Growth in a Cloud-Based Smart Sensor ... 73
17.Ju Y , Sun G, Chen Q, Zhang M, Zhu H, Rehman MU (2019) A model combining convolutional
neural network and LightGBM algorithm for ultra-short-term wind power forecasting. IEEEAccess 7:28309–28318
18.Rao TVN, Manasa S (2019) Artiﬁcial neural networks for soil quality and crop yield predictionusing machine learning. Int J Future Revolut Comput Sci Commun Eng 5(1):57–60
19.Pantazi XE, Moshou D, Alexandridis T, Whetton RL, Mouazen AM (2016) Wheat yieldprediction using machine learning and advanced sensing techniques. Comput Electron Agric121:57–65
20.Malik S, Huet F (2011) Adaptive fault tolerance in real time cloud computing. In: 2011 IEEEworld congress on services. IEEE, pp 280–287
21.García-Valls M, Cucinotta T, Lu C (2014) Challenges in real-time virtualization and predictablecloud computing. J Syst Archit 60(9):726–740
22.Liu S, Quan G, Ren S (2010) On-line scheduling of real-time services for cloud computing. In:2010 6th world congress on services. IEEE, pp 459–464
23.Tripathy HK, Mishra S, Thakkar HK, Rai D (2021) Care: a collision-aware mobile robotnavigation in grid environment using improved breadth ﬁrst search. Comput Electr Eng94:107327
24.Tripathy HK, Mishra S, Suman S, Nayyar A, Sahoo KS (2022) Smart COVID-shield: an IoTdriven reliable and automated prototype model for COVID-19 symptoms tracking. Computing,1–22
25.Mishra S, Tripathy HK, Thakkar HK, Garg D, Kotecha K, Pandya S (2021) An explainableintelligence driven query prioritization using balanced decision tree approach for multi-level
psychological disorders assessment frontiers in public health:9
26.Mishra S, Dash A, Ranjan P, Jena AK (2021) Enhancing heart disorders prediction with
attribute optimization. In: Advances in electronics, communication and computing. Springer,Singapore, pp 139–145
27.Roy SN, Mishra S, Yusof SM (2021) Emergence of drug discovery in machine learning. In:Technical advancements of machine learning in healthcare. Springer, Singapore, pp 119–138Cloud-Based IoT Controlled System
Model for Plant Disease Monitoring
Aritra Nandi, Asmita Hobisyashi, Shivam Yadav, and Hiren Mewada
1 Introduction
Plants are an indispensable source of oxygen because they take in carbon dioxide
during the day and release oxygen during the process of photosynthesis. Plants are
the primary source of nutrition for all terrestrial species, including humans. But
plant or crop diseases and pest infestation are major issues faced by farmers every
year. There are two ways by which growing crops are getting damaged. One is
the direct harm caused to plants by insects eating leaves and burrowing holes in
stems, fruit, and roots. And the other way by which they are getting damaged is
by transmitting bacterial, viral, or fungal infection. Pests and diseases cause the
withering of crops or parts of plants, resulting in reduced food production that
may cause food insecurity. For effective analysis of data for disease detection,
several sensor based IoT devices are used across the domains such as healthcare
[1], wireless sensor networks [ 1], etc.
The ﬁrst step toward successful disease management is to understand the disease
and methodology. There are a few special conditions that are conducive to disease
development. First, each crop is susceptible to some disease. Then there are
abiotic factors (i.e. sunlight, humidity, rain, temperature, etc.) that affect the plants
signiﬁcantly [ 1]. Therefore, the plant becomes host to all the pathogens. So the
pathogens act as a cherry on top. And thus the disease occurs due to the combination
of the aforementioned factors. That is called the Plant Disease Pyramid.
A. Nandi (/envelopeback) · A. Hobisyashi · S. Yadav
School of Computer Engineering, KIIT (Deemed to Be) University, Bhubaneswar, Odisha, India
H. Mewada
Department of Electrical Engineering, Prince Mohammad Bin Fahd University, Al Khobar,
Kingdom of Saudi Arabia
e-mail: hmewada@pmu.edu.sa
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_57576 A. Nandi et al.
Fig. 1 Economic loss
The wastage of crops due to diseases and pests are approximately observed to
vary from 10% to 30% of the crop production. If an average crop loss of 20% is
considered and the present gross value of our agricultural production as Rs 700,000
crore, the loss sums to Rs 140,000 crore which is very large (estimated in 2013).
From a survey, it was known that in India about 30–35% of crops get damaged
by pests. Nematodes, consisting of roundworms, threadworms, and eelworms, are
causing the loss of crops to the tune of almost 60 million tonnes or 10–12% of crop
production every year (estimated in 2017). As much as 40% of the world’s crops are
lost to pests each year, according to a recent report. Figure 1. [Taken from a survey
done by Economic Times] represents the economic losses with respect to the type
of crops [ 2].
This methodology involves the use of IoT based application that can capture
plant images and store them in a cloud-based system. Drone technology is being
developed these days in many such areas. In this process, a drone system with some
speciﬁc sensors for image capturing and processing can be used. Now the captured
data is stored with the help of the cloud. Cloud helps in storing the huge volume
of data that is being generated from time to time and can be remotely accessed
anywhere. The main advantage of using the cloud is the backup and restoration and
further won’t be having any problems in the detection part. The drone system is one
of the advanced technologies which is used these days for the mass monitoring of
plant disease. In this process, the drone covers the whole ﬁeld and takes images of
plants periodically. The dataset containing pictures of crop images is then analyzed
by software. This speciﬁc software is trained to identify whether the plant is infected
or not, and thus provides information about the presence and location of diseaseCloud-Based IoT Controlled System Model for Plant Disease Monitoring 77
Fig. 2 Drone system
Fig. 3 Image captured by drone
in the crop. This advanced methodology is now used for the identiﬁcation and
detection of plant disease in large commercial farming operations and thus reduces
cost and increases food security. In Fig. 2given below, the drone system which
is being used is shown to analyze crops on a large scale, and in Fig. 3the image
captured by the drone can be seen [Image taken from Drones Africa website].78 A. Nandi et al.
With the help of this technology plant diseases and their pathogens can be rapidly
detected and thus it prevents widespread epidemics. It creates a lot of impact on
the environment, economy, and social well-being by detecting diseases while it
is in the early phase. Early intervention can also save money on any containment
or eradication effort. Because of the diverse and complicated nature of plant pests
and diseases, apparent signs may take longer to develop after the initial infection,
allowing plant pathogens to go unnoticed for extended periods of time. The new
technologies can help us in multiple ways like recording and analysing aerial
imagery to detect the outbreak of newly prone diseases faster. These also facilitate
a more targeted and quick response. Machine learning (ML) has made it very
straightforward and quick to identify these diseases and pests at a very early stage.
The advancements in artiﬁcial intelligence are solely responsible for these beneﬁts.
The prediction model is created by an artiﬁcial neural network trained on a massive
dataset of images of plant diseases and pests collected in the cloud. Thus the model
learns about plant diseases and pests and helps in efﬁciently predicting them. For
the classiﬁcation and detection of plant leaves diseases, several ML models are used
such as Artiﬁcial Neural Network, and Convolutional Neural Network [ 3].
2 Literature Survey
“Smart Agriculture using Clustering and IoT” by Aher et al. [ 4] implements a cloud-
based IoT application that can help farmers in adapting a smart way of agriculture.
It collects data from multiple locations on a farm and provides the farmers with this
data for them to handle their respective operations wisely.
“Design and implementation of a cloud-based IoT scheme for precision agricul-
ture” by Ahmed et al. [ 5] uses precision agriculture which utilizes IoT by which the
efﬁciency of crop production can be increased, considering all the important factors
like growth, yield, stability, and consistency of the agricultural ﬁelds, etc.
“A smart agricultural model by integrating IoT, mobile and cloud-based big data
analytics” by Rajeswari et al. [ 6] makes use of big data analysis and cloud-based
architecture to send necessary details to the farmers as smart agriculture is highly
beneﬁcial for them. Data mining is used to gather the necessary information.
“A Cloud-Based IoT Platform for Precision Control of Soilless Greenhouse
Cultivation” by Sagheer et al. [ 7] is based on the formation of a soilless medium
along with the installation of all the essential sensors and actuators in a greenhouse
environment which is proven to generate a hike in the amount of crop production as
well as facilitate the reduction of energy consumption. “Internet of Things (IoT) for
Precision Agriculture Application” by Dholu et al. [ 8] proposes the utilization of the
beneﬁts of IoT in precision agriculture. IoT with the help of actuators ensures that
the various important factors that affect plant growth, like, light intensity, humidity,
temperature, pesticides, etc. are provided in the correct amount. “Cloud Computing
for IoT Applications in Climate-Smart Agriculture: A Review on the Trends and
Challenges Toward Sustainability” by Symeonaki et al. [ 9] is a review of variousCloud-Based IoT Controlled System Model for Plant Disease Monitoring 79
surveys made to facilitate the application of climate-smart agriculture in areas that
are not technologically sound. It helps identify the obstacles as well as the perks of
having an IoT-based agricultural setup in rural areas. “Development of IoT based
smart security and monitoring devices for agriculture” by Baranwal et al. [ 10]i s
based on the development of a smart device that can detect the attack of pests or
rodents in the very initial stage and make the user alert without human intervention
so that major steps can be taken to eradicate further spread. A success rate of 84.8%
has also been achieved against test cases on using this device thereby increasing the
security of crops from being wasted. “Remote sensing and controlling of greenhouse
agriculture parameters based on IoT” by Pallavi et al. [ 11]u s e sI o Tt op r o v i d e
plants with the right amount of sunlight, soil moisture, temperature, CO 2, and so
on by controlling their excess supply in a greenhouse in order to increase crop
yield and take a step ahead in organic farming. “A novel technology for smart
agriculture based on IoT with cloud computing” by Mekala et al. [ 12]u s e sI o T
and cloud computing to perform three tasks. With the help of a remote-controlled
process, it performs weeding, spraying and so on which is followed by humidity
and temperature control along with theft detection in a warehouse. Finally, after
analyzing real-time ﬁeld data, the best decision is taken for smart irrigation. “Cloud-
based Decision Support and Automation for Precision Agriculture in Orchards
by Tan” [ 13] provides an IoT-based technique for accumulating information and
helping in taking the best-suited decision for the growth of crops. With the help
of a cloud computing platform, the ﬁeld devices are also closely monitored to
ensure that they are being controlled safely and the operations are being carried out
properly. “IoT based Soil Nutrition and Plant Disease Detection System for Smart
Agriculture” by Suhag et al. [ 14] proposes the use of IoT to collect data related to
soil fertility, moisture, minerals, etc., and this data is passed to a robotic arm which
helps in harvesting the crops without the farmer’s labor. “Automated Agricultural
Field Analysis and Monitoring System Using IOT” by Kajol R et al. [ 15]u s ea
camera to detect soil moisture and pests and utilizes solar energy efﬁciently.
3 IoT Controlled Device
IoT or the Internet of Things helps us to connect with the computer world directly
and gives better accuracy. IoT devices have replaced human intervention to a
great extent, as this technology deals with wireless sensors to protect the plant
by monitoring various aspects associated with the same [ 16]. IoT with the help
of emerging technology is helping farmers get accustomed to the new trends. A
drone system can be used for surveillance. Crop production usually covers a huge
plot and it is difﬁcult for farmers to cover the entire area and analyze the quality. In
such a case, the drone system comes to the rescue by helping in covering the entire
area and analyzing the crops as well as their surrounding environment including
the temperature, humidity, and so on. The Drone system contains multiple sensors
which can detect plant disease through image processing and monitor the weather80 A. Nandi et al.
Fig. 4 Hyperspectral image capturing sensor
condition at the same time. With this monitoring system, a farmer can also be
beneﬁtted by getting a proper understanding of the weather situation and planning
accordingly. The drone contains a camera with a hyperspectral imagining sensor as
shown in Fig. 4. [Image taken from Specim] that could capture the image of the
crop, and store it in the cloud. This image can further be used for detecting plant
disease. It also contains temperature and humidity sensors that help in monitoring
and providing real-time data about its surroundings and thereby ﬁnding out if a
particular crop is advisable to be grown in that area or not taking into account its
weather conditions [ 17].
Hyperspectral sensors are used to capture the image from a distance known as
Ground Sampling Distance which is the altitude at which the drone is required to
be present in order the get the best resolution and radiometric accuracy. Once the
image is captured, the data is stored in the form of x, y, and λvalues. These points
altogether take the form of a hypercube. The (x, y) pair and the λvalue represent the
spatial and spectral information respectively. These details also help in observing
minute information like variation in the mineral content or the category of crop
grown with every change in the value of x, y, and λ. The temperature sensor in the
drone helps in measuring the temperature which is done by sending electrical signals
and then getting the most accurate measurements that can be read. Humidity sensors
function by detecting the change in air temperature or the variation in electrical
current. Every little variation is closely monitored by these sensors to provide the
most accurate result [ 18].Cloud-Based IoT Controlled System Model for Plant Disease Monitoring 81
4 Cloud Architecture
Cloud architecture is implemented to ensure accuracy, transparency, and security.
All the technological components together form the cloud architecture as shown in
the Fig. 5. These components can be broadly divided into two. The front-end is the
component of cloud architecture that deals with the operations of the client-side.
It is responsible for the smooth functioning of the user interfaces and applications.
The client can make use of the cloud computing services without any problem.
The cloud platform can be operated using the web browser and client infrastructure
comprises all the components that the user interface has. The cloud service provider
is contained in the backend. It is responsible for the storage of resources as well
as its management with proper security ensured. Multiple virtual machines and
applications added to a huge storage and deployment models are also involved.
The drone captures the image from a certain speciﬁed altitude with the help of
hyperspectral imaging and then sends it to the backend as resources to be stored
in the cloud. It also determines the temperate and humidity of the surrounding
environment which can also be used to ﬁnd out the suitable weather conditions
required for growing crops. These images further undergo feature extraction and are
segregated into healthy and diseased leaves. The usage of cloud storage is highly
beneﬁcial as it ensures high security by protecting the data from illegal usage,
improves performance as it provides the best ways to manage resources, and allows
recovery and retrieval of data as and when required [ 19].
Fig. 5 Cloud architecture
82 A. Nandi et al.
5 Methodology
The cloud storage will contain a collection of images of both healthy as well as
diseased leaves. The ﬁrst task is to distinguish between healthy and diseased plants
and bring uniformity in the size of all the images. There are a lot of image processing
and feature extraction steps involved, but as hyperspectral imagining sensors has
been used, HOG feature extraction will be implemented. The HOG technique has
been used as a feature descriptor that describes the outline of the images by their
intensity gradients. One of the most essential advantages of HOG feature extraction
is that it only works on the cells that have been formed, therefore any modiﬁcations
have no effect on the process. The histogram of oriented gradients (HOG) is an
element descriptor that is mostly utilized for object detection in computer vision
and image processing. Figure 6shows the main system architecture [ 20].
5.1 HOG Filter
This whole feature extraction process can be further divided into three steps that are
as follows:
Hu moments – This is used to determine the shape of the leaves. It helps us to
understand the outline of the particular leaf selected. To calculate Hu moment,
only a single channel is used. This process begins with converting RGB to
Fig. 6 System overviewCloud-Based IoT Controlled System Model for Plant Disease Monitoring 83
Fig. 7 RGB to HSV and gray scale
Fig. 8 Histogram Plot for healthy and disease cucumber leaf
Grayscale which is followed by calculating the Hu moments as shown in Fig.
7. As an end result, an array of shape descriptors is obtained.
Haralick Texture –Haralick textures are used to differentiate between healthy
leaves and diseased ones. This property is based on an adjacent matrix that stores
the position of (i, j). The frequency of pixel i occupying the location next to pixel
j can be used to calculate this texture. It is mandatory to convert the image to
grayscale for calculating the Haralick texture [ 21].
Colored histogram –It is used to represent the different colors in the image .In
this process, RGB is converted into HSV-color space and then the histogram is
plotted. Converting RGB to HSV is important because it aligns the model with
how the human eye perceives colors in an image. Plotting a histogram as shown
in Fig. 8describes the number of pixels available in a given colour spectrum is
quite beneﬁcial.84 A. Nandi et al.
6 Experimental Analysis
After the feature extraction step, the dataset can be further divided into two subsets,
train and test data. The train dataset will be used for training the model and the test
for testing the accuracy of the model. Two separate models will be built using two
different algorithms and the ﬁnal one to be decided based upon the accuracy.
6.1 Analysis Using Artiﬁcial Neural Network
This Table 1shows how much the efﬁciency is affected with the change in the count
of hidden layers. The number of states of neurons in a network is represented by
the number of hidden layers. The efﬁciency of the network can reach its optimum
state when the number of hidden layers is at least n*n. The number of features in the
training set is represented by n. When the number hidden neurons are considered to
be 50, the network is at its optimum state. This can be concluded from the graphical
representation of analysis of the Number of Hidden Neurons v/s Neural Network
Efﬁciency.
In Table 2the maximum tolerable error is represented by the termination error
rate in the classiﬁcation of values in a neural network. The efﬁciency of the network
reaches its optimum state when the termination rate increases and thereby the
performance of the neural network improves. The network achieves its optimum
state when the termination error is set to 0.00001. This can be inferred from the
above graphical model which is the representation of analysis with respect to the
Termination Error Rate v/s Neural Network Efﬁciency. Table 3shows the graphical
representation of recognition rate with uniform background.
Table 1 The graphical representation of Hidden neurons vs NN efﬁciency
Cloud-Based IoT Controlled System Model for Plant Disease Monitoring 85
Table 2 Termination error rate v/s neural network efﬁciency
Termination
error rate (ms)Recognition rate
of alternaria (%)Recognition rate
for BBD (%)Recognition rate
for anthracnose (%)Overall neural
network efﬁciency (%)
0.1 78.5 73 81.5 79
0.01 76.5 83.5 96 85
0.001 73 82.5 94.5 84
0.0001 77 86 93 84
0.00001 81.5 94 97.5 91
Table 3 The graphical representation of recognition rate with uniform background
In this approach, the network was trained on 140 samples from which 8 samples
were Alternaria, 26 samples were BBD and 89 samples were Anthracnose were used
for training and testing. The performance of the neural networks depends depends
upon the number of features, the number of hidden neurons, the termination error
rate, and the quality of the sample image. In order to correctly classify the samples
to their corresponding classes, it is essential to ensure that the optimized values are
tested with the number of feature values, the number of hidden neurons, and the
termination error rate in various input conditions. Based on its ability to correctly
classify samples to their corresponding classes, the exact system performance can
be extracted. Hence this experimental analysis helps conclude that the network
achieves a better efﬁciency when the variables on which it is dependant, reaches
certain values. In the above example, when the number of features for an image
is 168, the number of hidden neurons is 50, the termination error rate is .00001,
and images are with a uniform background in a light environment with a minimum
distance of 1 or 2 feet between the input image and the camera, the network performs
more accurately [ 22].86 A. Nandi et al.
6.2 Analysis Using Convolutional Neural Network
This CNN model is composed of three parts, the ﬁrst one being convolutional layers,
the second one named pooling layers, and the third one is activation functions which
are commonly known as Rectiﬁed Linear Units (ReLUs). The whole process is
divided into 3 steps. Firstly collect the data from the cloud. Next, is Segmentation
preprocessing that involves three processes, namely image segmentation, image
enhancement, and color space conversion. In this process, the digital version of the
image is improved with the help of a ﬁlter. Then it converts each image into an array.
After the image detection part, it comes to data Annotation which labels information
so that machines can use it. In the next step i.e. data augmentation, a subset of the
training data was created. From testing, training, and validation the diseased samples
are collected then this sample is again trained and after the performance veriﬁcation
then we come to our ﬁnal result.
The images that were stored in the cloud were then converted into an array. The
input ﬁle was processed after scaling the information points from [0,255] (minimum
image and most RGB values) to the varying [0,1]. Then the dataset was split into
70% comprising of the training images and the rest 30% consisting of testing
images. A random rotation, movement, inversion, culture, and part of our image
library were made using an image generator. “Last channel” architecture was used in
the standard model, but built backend switches that support “the ﬁrst channel” were
also utilized. Then Conv =>R e l u => Pool was performedﬁrst. 36 ﬁlters were
present in the Conv layer, with 3 ×3 core and Relu activation (linear correction
module). Batch normalization, maximal aggregation, and a reduction (0.26) of 27%
was used. By inhibiting the rectiﬁcation of complex collaborative data for training,
dropout technology was utilized to lessen neural network readjustment. For the
averaging of neural network models, this technique was very effective. Then two
sets (Conv => Relu) * 2 => Pool blocks were created. This was followed by
just a series of fully connected layers (fully connected layers) => Relu. Adam’s
Hard Optimizer was used for the model. The network started where the model ﬁt
generators were called. The goal was to add data, train test data, and the number
of training epochs. An epoch value of 26 was chosen for this project. According to
this study, controlling plant diseases can help enhance crop yields by up to 50%.
Figure 9shows the graph of validation and training loss and Fig. 10shows the trend
of training and validation accuracy.
7 Conclusion
In this experimental analysis, a drone system containing multiple sensors was used.
This system was automated and no human intervention was required. As crop
production takes place covering a huge area, the drone system can capture the view
of the entire land by itself without the farmers needing to check each and everyCloud-Based IoT Controlled System Model for Plant Disease Monitoring 87
Fig. 9 Shows the validation loss and training loss
Fig. 10 Shows the training and validation accuracy
small span of area. The drone captures the image in RGB format and is stored in
the cloud which further undergoes the feature extraction step. This step is highly
efﬁcient and is capable of converting the images into HSV or Grayscale format.
The images captured are stored in the cloud which helps in maintaining the security
of the data. It also enables data backup immediately in case of system failure. The
entire process of storage of resources is carried out efﬁciently.88 A. Nandi et al.
Further, a series of steps are involved which helps in distinguishing between the
healthy and diseased leaves and thereby predicting the plant disease. Two different
machine learning models were used for a discrete analysis. Both Artiﬁcial NeuralNetwork and Convolutional Neural Network performed well in terms of prediction.
The main aspect of using the Neural Network algorithm to build the model is that
it trains the model itself and also the output it produces performs well on the testdataset. Though it can be observed that CNN gave the best accuracy i.e. 96.4% on
test data. The feature extraction process that was being used is one of the major
reasons behind such a good accuracy as it converted all the images into differentlayers and then important features were extracted. Crop production usually takes
place on a large scale and if a certain plant is diseased, the entire crop production is
prone to get affected. As this experimental analysis is done over a large area and thattoo precisely, if a particular crop is predicted to have a disease, the further spread
can be immediately curtailed by applying the required pesticides or disinfectants to
the diseased plant. This measure can be taken anywhere and the problem of wastageof plants can be avoided to a huge extent.
References
1.Rai D, Thakkar HK, Rajput SS, Santamaria J, Bhatt C, Roca F (2021) A comprehensive review
on seismocardiogram: current advancements on acquisition, annotation, and applications.Mathematics 9(18):2243
2.Rajeswari S, Suthendran K, Rajakumar K (2017) A smart agricultural model by integratingIoT, mobile and cloud-based big data analytics. In: 2017 international conference on intelligentcomputing and control (I2C2). pp 1–5. IEEE
3.Mishra S, Dash A, Ranjan P, Jena AK (2021) Enhancing heart disorders prediction withattribute optimization. In: Advances in electronics, communication and computing. Springer,Singapore, pp 139–145
4.Dholu M, Ghodinde KA (2018) Internet of things (IoT) for precision agriculture application.In: 2018 2nd international conference on trends in electronics and informatics (ICOEI), IEEE,
pp 339–342.
5.Symeonaki EG, Arvanitis KG, Piromalis DD (2017) Cloud computing for IoT applications
in climate-smart agriculture: a review on the trends and challenges toward sustainability. In:International conference on information and communication technologies in agriculture, food& environment. Springer, Cham, pp 147–167
6.Baranwal T, Pateriya PK (2016) Development of IoT based smart security and monitoringdevices for agriculture. In: 2016 6th international conference-cloud system and big dataengineering (conﬂuence) (pp. 597-602). IEEE
7.Pallavi S, Mallapur JD, Bendigeri KY (2017) Remote sensing and controlling of greenhouseagriculture parameters based on IoT. In: 2017 international conference on big data, IoT anddata science (BID). IEEE, pp 44–48
8.Mekala MS, Viswanathan P (2017) A novel technology for smart agriculture based on IoTwith cloud computing. In: 2017 international conference on I-SMAC (IoT in social, Mobile,analytics and cloud)(I-SMAC). IEEE, pp 75–82
9.Tan L (2016) Cloud-based decision support and automation for precision agriculture inorchards. IFAC-PapersOnLine 49(16):330–335Cloud-Based IoT Controlled System Model for Plant Disease Monitoring 89
10.Suhag S, Singh N, Jadaun S, Johri P, Shukla A, Parashar N (2021) IoT based soil nutrition
and plant disease detection system for smart agriculture. In: 2021 10th IEEE internationalconference on communication systems and network technologies (CSNT). IEEE, pp 478–483
11.Kajol R, Kashyap AK (2018) Automated agricultural ﬁeld analysis and monitoring systemusing IoT. Int J Inf Eng Electronic Business 10(2)
12.Ahmed E, Shakhnarovich G, Maji S (2014) Knowing a good hog ﬁlter when you see it: efﬁcientselection of ﬁlters for detection. In: European conference on computer vision. Springer, Cham,pp 80–94
13.Li C, Guo L, Hu Y (2010) A new method combining HOG and Kalman ﬁlter for video-based human detection and tracking. In: 2010 3rd international congress on image and signalprocessing, vol 1. IEEE, pp 290–293
14.Xu F, Gao M (2010) Human detection and tracking based on HOG and particle ﬁlter. In: 20103rd international congress on image and signal processing, vol 3. IEEE, pp 1503–1507
15.Balaji M, Arshinder K (2016) Modeling the causes of food wastage in Indian perishable foodsupply chain. Resour Conserv Recycl 114:153–167
16.Mishra S, Tripathy HK, Thakkar HK, Garg D, Kotecha K, Pandya S (2021) An explainableintelligence driven query prioritization using balanced decision tree approach for multi-levelpsychological disorders assessment frontiers in public health. Front Public Health 9
17.Nikish Kumar SV , Balasubramaniam S, Sanjay Tharagesh RS, Kumar P, Janavi B (2020)An autonomous food wastage control warehouse: distributed ledger and machine learningbased approach. In: 2020 11th international conference on computing, communication andnetworking technologies (ICCCNT). IEEE, pp 1–6
18.Jena L, Mishra S, Nayak S, Ranjan P, Mishra MK (2021) Variable optimization in cervicalcancer data using particle swarm optimization. In: Advances in electronics, communicationand computing. Springer, Singapore, pp 147–153
19.Khirade SD, Patil AB (2015) Plant disease detection using image processing. In: 2015international conference on computing communication control and automation. IEEE, pp 768–771
20.Tripathy HK, Mishra S, Thakkar HK, Rai D (2021) Care: a collision-aware mobile robotnavigation in grid environment using improved breadth ﬁrst search. Comput Electr Eng94:107327
21.Mishra S, Tripathy HK, Panda AR (2018) An improved and adaptive attribute selectiontechnique to optimize dengue fever prediction. Int J Eng Technol 7:480–486
22.Mishra S, Koner D, Jena L, Ranjan P (2021) Leaves shape categorization using convolutionneural network model. In: Intelligent and cloud computing. Springer, Singapore, pp 375–383Design and Usage of a Digital
E-Pharmacy Application Framework
Shatabdi Raut, Samikshya Moharana, Soumya Sahoo, Roopal Jena,
and Payal Patra
1 Introduction
In the current scenario, technical advancements is a vital aspect in all kinds of
coordination to effectively automate the majority of society needs. Furthermore, the
administration of the manual approach encountered various obstacles in a speciﬁed
direction, which can be described as time consuming accessibility, managing
the shop, and searching for skilled employees to match the needs of employer
expectations [ 1]. One of the promising medical solutions is healthcare information
technology, which is utilized to eliminate as many types of pharmaceutical mistakes
as feasible. To address these issues, an urgent need exists to create an online
pharmacy management system, i.e. an e – pharmacy website, which would be
helpful to the Pharmacy. We can create bills, keep goods in good condition, save
money, and manage inventory control by utilizing this program. This approach can
assist pharmacies in handling incomings and outgoings more swiftly and efﬁciently.
Managing a large pharmacy using paper records would be time consuming and
difﬁcult to maintain track of inventory in terms of the medications in the store,
expiration date, number of medicines available based on the categories and their
functions. The key distinction between other pharmaceutical websites and our
website is its user-friendliness. Certain beneﬁts have been assured for patients by the
e-pharmacy marketplace: Patients who are homebound or disabled have easy access
to medications. There is an almost limitless choice of medicinal goods accessible, as
well as conﬁdentiality, which may encourage patients to inquire about embarrassing
difﬁculties and high charges [ 2]. One of the objectives of this project is to develop
an integrated data management system for hospital pharmacies that will address the
S. Raut · S. Moharana · S. Sahoo ( /envelopeback)·R .J e n a·P .P a t r a
C.V Raman Global University, Bhubaneswar, Odisha, India
e-mail: soumya.sahoo@ccgu-odisha.ac.in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_69192 S. Raut et al.
majority of the existing system issues. By linking the system with a SQL Server
database, we have achieved reliability, strong performance and high capacity. The
proposed system manages as much of the hospital pharmacy’s work as possible. The
presented system has two components: a database developed in SQL server and a
graphical user interface constructed with HTML, CSS, and JavaScript.
2 Literature Survey
The e-pharmacy practices will serve as a stepping stone for the establishment
of online pharmacies in the kingdom. This study paper examines the different
important tools and approaches used in website creation. They also go over the steps
involved in creating a website, with a particular emphasis on a local host known as
the XAMPP tool [ 3].
Between July and September 2001, a survey of public information provided
on worldwide e-pharmacy web sites was done. They identiﬁed a sampling frame
of worldwide pharmacies using a meta-search engine, Copernic, with the search
phrases ‘online’ or ‘internet,’ and ‘pharmacy,’ ‘pharmacies,’ and medicines.’ The
purpose of this study was to look at the quality of worldwide e-pharmacies, which
are described as websites that sell controlled (drugs with the potential for addiction
or misuse), prescription-only, pharmacist-only, or pharmacy-only medications [ 4].
Many more online medications purchasing web apps are available these days,
and this is how consumers will order drugs whose sale is not necessary without
prescriptions. They attempted to address this issue by including certain authorized
online pharmaceutical sales. To begin, the consumer will upload the prescription
for the needed drugs; in the second step, that prescription will be examined by
the Doctor on the site; and ﬁnally, only authorized prescriptions will be allowed
to complete the purchase. This reduces the potential of unlawful sales while also
protecting consumers from negative effects caused by self-medication [ 5].
Future studies should look at the side effects of drugs obtained online. New
developing technologies, such as machine learning algorithms used in “Big Data,”
serve as the foundation for a new ﬁeld of study known as “digital” surveillance”. As
a result, the real patient safety risk in outpatients can be recognised inside the health
care system through the collection of data gathered during medical examinations
and anamnesis, through the study of patient records, or online using modern data-
science approaches [ 6].
Because e-pharmacy use was self-reported, it was susceptible to recall bias
and untruthful reporting by the user, thereby underestimating true prevalence.
Legitimate and illicit players are not distinguished in our research, primarily because
online retailers may mislead clients or be unable to distinguish between them. In
Southern Hungary, we conducted our research in hospitals, general practitioners’
ofﬁces, and community pharmacies. As a result, it reﬂects the national patient
population rather than the total Hungarian population. Moreover, this might be a
potential strength since people are more inclined to acquire prescriptions and areDesign and Usage of a Digital E-Pharmacy Application Framework 93
more exposed to the risks connected with e-pharmacies. The authors previously
assessed and published the prevalence and attitudes of inpatients [ 7].
Users of internet pharmacies, whether legal or not, purchase drugs for both
acute and chronic diseases, including narcotics. All medicines (prescription and
over-the-counter) and even dietary supplements can be harmful if used without the
proper guidance and supervision of a medical practitioner or pharmacist. Inadequate
or erroneous information regarding the patient’s health state and prescriptions,
improper self-diagnosis, or inadequate treatment of drug-related difﬁculties. The
main issues include polypharmacy, therapeutic duplications, adverse effects, and
drug-to-drug or drug-herbal interactions. Illegitimate internet pharmacies obviously
represent a signiﬁcant risk to patients by selling counterfeit drugs and poor items.
However, even legal performers have concerns with their utilisation [ 8].
Globally, national pharmaceutical budgets are growing. As a result, economic
constraints may limit consumer access to medications and encourage customers
to purchase medicines in a price competitive market over the internet. It is both
hard and difﬁcult to protect consumers and improve the quality of websites that
sell drugs across state and national borders. The goal is to prevent the creation
of fraudulent and deceptive websites while allowing for the development of new,
ethical pharmaceutical services [ 9].
3 Utilization of Cloud in Health Care
Healthcare is described as a service provided by healthcare service providers to
individuals or populations in order to promote, maintain, monitor, or restore health.
Healthcare’s ultimatum is fully anticipated on a global scale, since it is expected
to continue to grow indeﬁnitely in the future due to tangible factors such as
life expectancy, predicted demographic trends among the elderly population, and
lifestyle disorders [ 10].
A cloud based model for healthcare domain is shown in Fig. 1. The communi-
cation of patient data across clinicians, departments, and even patients is unusual
and challenging. An organization’s reliance on vendors to connect its various
technologies indicates that it is too costly to conduct untested data experiments.
Various countries have handled this issue in a variety of ways, ranging from
the UK’s central national clearinghouse to Canada’s provincial health centers to
more granular health information exchanges, all of which have achieved a variety
of degrees of success. Furthermore, countries that have moved away from paper
records and toward diagnostic images are expected to win in a limited way, but have
yet to win in patient records due to their larger equipment [ 11].94 S. Raut et al.
Fig. 1 Cloud system in health care
4 Redeﬁning E-Pharmacy Domain
E-pharmacy refers to the purchase and sale of medications and other pharmaceutical
commodities through the use of e-commerce. Legitimate internet pharmacies are
granted particular operational permits in various countries. There were various
disputes in India about e-pharmacies between 2010 and 2015. The legality of
e- pharmacies in India, the sale of prescription medications without a doctor’s
prescription, and the appearance of ‘cyber doctors’ on some websites were all
hotly argued and discussed [ 12]. The question was whether prescriptions may be
dispensed from both physical (bricks and mortar or B&M pharmacies) and internet
pharmacies via an electronic prescription or solely from a physical pharmacy. The
1940 Drugs and Cosmetics Act, as well as its Rules, made no distinction between
online and ofﬂine pharmacies. Until 2018, Indian regulations were completely silent
on the operation of e-pharmacies. The drug laws enacted before independence were
not revised to reﬂect advances in electronic and information technology (IT), as well
as innovations and changes in pharmacy practice and dispensing. Some states’ food
and drug administrations (FDAs) have ﬁled legal charges against internet pharma-
cies, including Maharashtra, Gujarat, Telangana, and Karnataka. In May 2015, the
Maharashtra FDA ﬁled a FIR (First Information Report) against the online vendor
‘Snapdeal’ for selling prescription pharmaceuticals as well as over-the-counter
(OTC) drugs on the internet. It was based on a raid they conducted in April 2015 at
Snapdeal’s Mumbai ofﬁce. In Gujarat 2, 3, similar raids and FIRs were made against
Delhi-based Mchemist, Mumbai-based Pharmeasy, and MeraMedicare. There was
no problem with the internet sale of non-prescription medications. Only prescription
medicine sales were the subject of complaints and concerns. Pharmacies will
provide several advantages over present physical community pharmacies, including
professional services. Medicine Authenticity and Quality, Improved Accessibility,Design and Usage of a Digital E-Pharmacy Application Framework 95
Data Tracking, Generic Dispensing by Professional Pharmacists, Cost Beneﬁt, Drug
Information and Patient Counseling, Issue of Medicine Safety are a few of them.
5 Impact of Cloud Computing in Pharmacy
Cloud computing has evolved as a ﬂexible, scalable, and cost-effective IT infras-
tructure option for many businesses and organizations. Cloud computing began
as a database service provider and has now expanded to include software, apps,
platforms, and fully functional IT infrastructures for clients. The beneﬁts of cloud
computing to customers, such as stability, near-zero downtime maintenance, and a
signiﬁcantly cheaper alternative compared to having their own IT infrastructures,
have paved the road for cloud computing to be fully adopted in the corporate
world. This tendency is predicted to continue in the ﬁeld of healthcare information
systems. Adoption of cloud-computing in the industry is going to make the health-
care system more efﬁcient, but will also allow for the deployment of an integrated
healthcare information system [ 13]. In the research, we evaluate the present status
of cloud-computing research and technology, with a focus on cloud computing
adoption in healthcare information systems, and we also propose a basis for
developing a framework for cloud-based e-prescription systems utilizing cloud
computing infrastructure. In cloud computing, databases are kept in multiple data
centers in different location. This distinguishes the cloud databases in comparison
with the traditional relational database management systems. The Data centers
placed in various geological areas, there are multiple junctions in the cloud database
established for querying service. Data connection is required to allow simple access
to data on the cloud computing system. The pace of transmitting data to a data
center in a cloud database is substantially quicker than the accessing speed the via
the Internet data centre. This is referred to as an efﬁciency bottleneck. Regardless of
this obstacle, the cloud database has various advantages that make it preferred and
ﬂexible for end users and companies [ 14].
The pay-as-you-go cost structure is one of the strategies underpinning cloud
computing’s economic efﬁciency. The framework allows operators, software and
applications service organizations, and physical service providers to deliver on-
demand services, with users paying for things based on how they were utilised.
Furthermore, cloud computing can reduce on-premise infrastructure expenditures
(costs for installing and maintaining software, hardware, design, building, down-
time, maintenance, and workers). Cloud computing can help increase the longevity
of customers’ devices, lowering long-term expenditures [ 15].
Cloud computing has provided us on-demand network access to a variety of
online computing resources based on a shared pool conﬁguration, such as networks,
servers, storage, and applications, according to the National Institute of Standards
and Technology (NIST). Cloud-computing has altered the method, we use IT in
our daily lives and for the rest of our lives. The majority of healthcare-related
cloud computing administrators are ecstatic about the beneﬁts that cloud computing96 S. Raut et al.
brings, such as the ability to dig into infrastructure management and save costs.
The healthcare industry, like any other services, require systematic and continual
innovativeness for providing cost effectiveness and high-quality services. Many
managers and experts, as per Kuo, feel that cloud computing will improve healthcare
services, aid researchers, and change the face of IT. The research of Kuo’s addresses
the idea, obstacles, and prospects of cloud -computing in the healthcare-domain, and
it ﬁnishes with a strategic plan for organizations who aim to transition to the new
service model [ 16].
Cloud computing applications provide signiﬁcant technical beneﬁts, such as
electronic medical records. Electronic medical records are a boon to the healthcare
industry. Cloud computing solutions provide a novel method for storing patient
records, simple access to the database, and improved system security. Cloud
computing also facilitates cooperation by utilizing video conferencing, mobile
devices, and speciﬁc programmes designed for healthcare needs [ 17]. Cloud
computing has data analytics capabilities that allow for real-time tracking and
data calculation. End-users and medical practitioners can receive information for
a variety of objectives, including medical research, trend detection, and referral
creation. Apps in the cloud computing environment provide high-powered data
solutions and research procedures that are tough to handle on modest devices.
Through mobile app technologies, cloud computing enables telemedicine. These
applications facilitate the provision of healthcare solutions such as telemedicine and
consultation from anywhere, permitting people to be monitored without needing to
seek medical treatment [ 18].
6 Model Design and Implementation
The applications being utilized on mobiles and tabs, and it operates on the platform
of Android and iOS, indicating that it is a responsive website. The mobile app is
utilized for prescriptions administration and provides savings to app users. Because
it includes a variety of capabilities, the programme has proven to be useful in
medication prescription management. It enables users to regulate which prescription
medications they must take and when they must take them. This functionality, which
we built using ML, will anticipate the ailment that a person is suffering from based
on its symptoms, as well as the medications he or she will purchase based on the
prescription. The application has shown to be beneﬁcial to the elderly, who are
more prone to ailments as they age. Customers may miss when or how much to
take their medicine, but the app reminds them and helps them avoid missing doses.
HTML and Cascading Style Sheet tools were used to develop the client side of
the programme, and Django was used to implement the server side. Django data
models are deﬁned as Python objects, and Django facilitates sending these to aDesign and Usage of a Digital E-Pharmacy Application Framework 97
database. Cloud Run is a managed serverless platform that allows each server to
run statelessly. It also integrates with many other components of the Google Cloud
ecosystem, such as Cloud SQL for managed databases, Cloud Storage for uniﬁed
object storage, and Secret Manager for secret management. Using the python-
spanner-django database backend, we can deploy Django with a Cloud Spanner
backend. Django is a high-level Python web framework, and we also used it to
store the medication database. The purpose of the Online Pharmaceutical Website
was to minimize individual user usage. Because the application is designed for
pharmacy customers as a whole, rather than being customized for individual usage,
the problem of drug misuse has already been reduced with the use of the application.
Therefore, only authorized people and people having proper doctor’s prescriptions
are allowed to place medicine orders with drug producers. To enable record keeping,
all transactions are recorded in the system.
7 Basic Structure of the Cloud Based E-Pharmacy
Application
This application is a user-friendly website. We have provided all of the user
necessities medication section in the form of card data on the home page, which
will assist a user in ﬁnding its necessary medicine faster by clicking those cards.
We have supplied drug descriptions such as indications, dosage, and adverse
effects, as well as medicine images. If we continue following the description by
clicking it, if the medications are accessible on our website, it will display the
Go to cart feature, and if the drug is not available, it will display the Add to Cart
functionality to the user. We have launched a blog area on our website where users
may access current health updates, stories, and relevant information. We have also
established a contact page; if someone needs a drug that is not accessible on our
website, they may put the name of the medicine on our contact us page, and we
will give them the medicine within roughly 2 days. The basic system model for
e-pharmacy is highlighted in Fig. 2.
8 Security Provided by the Application
8.1 XSS Security (Cross Site Scripting)
Hackers can access your website, inject malicious code and change the layout of
your project which is very harmful for our website so our website provides this XSS
security. Figure 3demonstrates the security aspect of the application model.98 S. Raut et al.
Fig. 2 Model diagram of the cloud e-pharmacy application
Fig. 3 Hackers injecting malicious script
8.2 CSRF Token (Cross Site Request Forgery)
It is provided in Forms like signup and login etc. where authenticated users can
login. Hackers cannot login by putting malicious things this security will provide
on our website.Design and Usage of a Digital E-Pharmacy Application Framework 99
Fig. 4 Sql injection attack
8.3 SQL Injection Security
In our website parameterized SQL query was there because it has an integrated
cloud based database. A sample sql injection attack is shown in Fig. 4.
8.4 User Upload Security
Users can upload the valid ﬁles which are related to our website. They cannot upload
the malicious ﬁle so that our website will not be destroyed.
9 Results and Discussion
The data ﬂow diagram model is shown in Fig. 5. Before actually implementing the
entire code, we are supposed to test the code after the completion of each part of the
code to remove any bugs that may exist, so that our software will give smooth and
correct results after the completion of the entire code and our users who will use our
software in the future will not encounter any problems. The testing step of a software
is critical; the output of the test run should match the intended outcomes. When a
project is created, compiled, and made operational, it is individually tested using
the prepared test data. Any unfavorable occurrence has been noticed and debugged,
which implies that the faults have been corrected. We examined the database of
all our drugs available, and the result was right; it is delivering correct indications,
recommendations, and use, such as how much a person should take. At this point,
the test has been run on actual data. The code’s output or results are examined at
each stage of execution. During the result analysis, it was discovered that the outputs
corresponded to the system’s intended output. And if we meet any mistakes in any
particular area in the future, we will correct it and then test it to acquire the desired
result, which will match the expected output. Figure 6shows the home screen of the
model. The types of medicine available is depicted in Fig. 7. The blog web portal
of the application is shown in Fig. 8. Oxygen cylinders and medicine availability is
highlighted in Figs. 9and10respectively. Google translator functionality is shown
in Fig. 11.100 S. Raut et al.
Fig. 5 Data ﬂow diagram of the cloud e-pharmacy application
10 Important Features of the Application
The administrator’s rights are role-based, allowing for role-based access control
for security.
For security reasons, each user’s IP address is saved in the database.Design and Usage of a Digital E-Pharmacy Application Framework 101
Fig. 6 Home screen of the application
Fig. 7 Categories of medicines available in the model
After it exceeds four unsuccessful authentication attempts, users will be banned.
Users can view the products
Each user has a unique identiﬁer
The items that are out of stock can be view but can not be added to the Cart
We have used Cloud database systems which save people time.102 S. Raut et al.
Fig. 8 Blog screen of the application
Fig. 9 Oxygen cylinders web portal of the application
11 Critical Goals of the Application
The main aim of this application are:
1.To develop a user friendly application for pharmacists.
2.To be able to manage all sections of Pharmacy from medicine management to
billings etc.Design and Usage of a Digital E-Pharmacy Application Framework 103
Fig. 10 Medicine availability portal and feedback screen of model
Fig. 11 Google translator options to switch between different languages
3.To develop a affordable and customer friendly system.
4.To fully utilize the information and communication technology for the maximum
beneﬁt in the healthcare industry.
5.To provide a simple interface that provides high security and less executing time104 S. Raut et al.
12 Beneﬁts of the Model
Most Pharmacies are still doing their whole work manually which may lead to
mistakes by workers and lead to major problems, so the E – Pharmacy website isdesigned to overcome such problems.
This software helps in effective management of the pharmaceutical store.
It helps our users to know which medicine they are in need of by predicting fromthe prescription.
It assists users in understanding when they need to take their medicines.
It can generate a total bill amount by calculating the total amount of the medicinespurchased by our user in an organized manner.
13 Summary/Conclusion
Breaking down the information barrier that was previously held by professionals,
such services are now available to the general public at any time and from
any location. The power of information technology to break down boundaries isa distinguishing quality. It beneﬁts e-governance, e-health, and e-pharmacy. E-
pharmacy is a need in this day and age of globalization. The cloud is utilized for
storage, and the database may be accessed and calculated from anywhere. In orderto scale up, a substantial number of web applications require distributed storage
solutions. It allows the user to outsource resources and services to a third-party
server. The most recent trend in cloud service is to base it on a database managementsystem and offer it as one of the cloud services. In the future, it is critical to build
physical infrastructure for pharmacies. E-pharmacies improve pharmacists’ societal
services. It has the potential to provide consumers with easy and cost effectiveaccess to medicines at their doorstep, and it is expected to generate signiﬁcant
demand in the coming months. The ease of access and convenience elements
connected with e-pharmacies are extremely beneﬁcial not just for the elderly andsick, but also for rural communities who must travel to obtain medications. Indian
pharmacists have a responsibility to ensure that e-pharmacies are professional and
ethical in their operations. E-pharmacy is fast spreading in countries with low andmoderate incomes, and this trend is anticipated to continue, aided by COVID-19
and expanding e-commerce ecosystems. Under-regulated e-pharmacy marketplaces
offer severe risks to public health due to medication misuse; yet, the opportunitye-pharmacy brings to expand access and quality should not be neglected. Recent
rules have not kept up with technological progress, and HICs have yet to produce
successful models for LMICs to rely on, while the latter are still dealing withwidespread regulatory violation in brick-and-mortar pharmacies. Rigid research will
be required to accompany the rise of this business and the regulatory reaction as it
expands.Design and Usage of a Digital E-Pharmacy Application Framework 105
References
1.Alomi YA (2016) A new guidelines on hospital pharmacy manpower in Saudi Arabia. J Pharm
Pract Community Med 2:30–31
2.Zhu M, Guo DH, Liu GY , Pei F, Wang B, Wang DX et al (2010) Exploration of clinicalpharmacist management system and working model in China. Pharm World Sci 32:411–415
3.Suman S, Mishra S, Sahoo KS, Nayyar A (2022) Vision navigator: a smart and intelligentobstacle recognition model for visually impaired users. In: Mobile Information Systems, 2022
4.Mishra S, Tripathy HK, Thakkar HK, Garg D, Kotecha K, Pandya S (2021) An explainableintelligence driven query prioritization using balanced decision tree approach for multi-levelpsychological disorders assessment. Front Public Health 9 795007
5.Hu Y , Lu F, Khan I, Bai G (2015) A cloud computing solution for sharing healthcareinformation. In: The 7th International Conference for Internet Technology and SecuredTransactions (ICITST), IEEE
6.Yaw AAS, Twum F, Hayfron-Acquah JB, Panford JK (2015) Cloud computing framework fore-health in Ghana: adoption issues and strategies: case study of Ghana health service. Int JComput Appl, 118(17)
7.Tripathy HK, Mishra S, Suman S, Nayyar A, Sahoo KS (2022) Smart COVID-shield: an IoTdriven reliable and automated prototype model for COVID-19 symptoms tracking. Computing104:1233
8.Rolim CO, Koch FL, Wetphall CB (2010) A cloud computing solution for patient’s datacollection in health care institutions. In: Proceeding of the second international conference
on eHealth, Telemedicine, and Social Medicine (ETELEMED’10), pp 95–99
9.Mishra S, Mohanty S (2022) Integration of machine learning and IoT for assisting medical
experts in brain tumor diagnosis. In: Smart healthcare analytics: state of the art. Springer,Singapore, pp 133–164
10.Desai V (2016) Opportunity and implementation of cloud computing in Indian health sector.Int J Recent Innov Trends Comput Commun 4(7):333–338
11.Dutta P, Mishra S (2022) A comprehensive review analysis of Alzheimer’s disorder usingmachine learning approach. In: Augmented intelligence in healthcare: a pragmatic andintegrated analysis, pp 63–76
12.Tsai WT, Sun X Balasooriya J. Service-oriented cloud computing architecture. In: IEEE 7thinternational conference on information technology: new generations, pp 684–689
13.Bessel T, Silagy C, Anderson J, Hiller J, Sansom L (2002) Quality of global e-pharmacies: canwe safeguard consumers? Eur J Clin Pharmacol 58(9):567–572
14.Kouﬁ V , Malamateniou F Vassilacopoulos G (2010) Ubiquitous access to cloud emergencymedical services. In: Information Technology and Applications in Biomedicine (ITAB), 201010th IEEE international conference on. pp 1–4. IEEE
15.Kumar NM, Senthilkumar K (2013) Proposed architecture for implementing privacy in cloudcomputing using grids and virtual private network. Int J Technol Enhanc Emerg Eng Res1(3):12–15
16.Mohapatra SK, Mishra S, Tripathy HK, Bhoi AK, Barsocchi P (2021) A pragmatic inves-tigation of energy consumption and utilization models in the urban sector using predictiveintelligence approaches. Energies 14(13):3900
17.Khanghahi N, Ravanmehr R (2013) Cloud computing performance evaluation: issues andchallenges. Comput 5(1):29–41
18.Panda AR, Mishra M (2018) Smart phone purchase prediction with 3-NN classiﬁer. J Adv Res
Dyn Control Syst, 674–680Serverless Data Pipelines for IoT Data
Analytics: A Cloud Vendors Perspective
and Solutions
Shivananda Poojara, Chinmaya Kumar Dehury, Pelle Jakovits,
and Satish Narayana Srirama
1 Introduction
Industry 4.0 revolution accelerated the large scale adoption of IoT in manufacturing
and other allied industries [ 1]. The signiﬁcant change towards digitization and smart
manufacturing heavily boosts the increased efﬁciency and automation. According to
Bitkom prediction by 2025 [ 2], Germany alone itself will have increased efﬁciency
in its production up to EUR 78.5 billion. The primary components of an industry 4.0
solution are sensor technology and interfaces, and transferring and analyzing data
that includes the use cases such as Predictive Analytics and Maintenance, Trace-
ability and Asset and Plant Performance Monitoring. In this direction, our proposed
article would focused on proposing solutions to the challenges in processing such
IoT data.
In smart manufacturing hub, huge set of sensors implanted on machinery robots,
assembly lines and production ﬂoor and connected vehicle ﬂeets produce large
amounts of data continuously in the form of streams. To exploit the large amounts
of such heterogeneous data they produce, it needs to be fusioned, preprocessed,
transported, transformed, and analyzed before useful knowledge can be extracted
from it [ 3]. To simplify the design of such data processing services, developers
commonly use the pipelines to construct and coordinate the data processing
components in to single service, where output of one component acts as input to
other component. This allows the reuse of components and build complex data
processing pipelines.
S. Poojara (/envelopeback) · C. K. Dehury · P. Jakovits
Institute of Computer Science, University of Tartu, Tartu, Estonia
e-mail: poojara@ut.ee
S. N. Srirama
School of Computer and Information Sciences, University of Hyderabad, Hyderabad, India
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_7107108 S. Poojara et al.
The data processing pipelines can be deployed as software services seamlessly on
the cloud and on the on-premise servers. On the other-side, IoT data processing can
be leveraged with large data processing compute clusters such as Apache Spark,
Flink, and Storm [ 4]. However, IoT applications are more event driven [ 5] and in
need of performing actions in real time, which often makes using such compute
clusters more expensive. Alternatively, using new cloud computing service model
known as Serverless computing, also named as Functions as a Service (FaaS), which
has a function level billing model and granular scaling, provides an opportunity to
design real-time event driven IoT data processing in super simpliﬁed manner.
Furthermore, cloud-centric IoT data processing approaches yield major chal-
lenges such as higher latency, higher cost and essentially require huge bandwidth
to upload and communicate from the premises of devices to clouds. Hence, edge
computing is introduced to harness the computing capability of the on premise edge
devices (such as routers, switches and servers). Aligning to this, IoT data processing
tasks are designed with integration of serverless and data pipelines deployed across
multi-layered heterogeneous devices in IoT continuum (Edge to Cloud or vice
versa).
In data pipelines, each task or component is a process that consumes the data and
produces the output that feed into the next task or component in a pipeline manner.
This makes potentially easy to compose simple to complex design data processing
tasks in the form of a pipeline, wherein part of the pipeline components deployed
near to IoT devices at the edge and over the clouds. In the serverless computing
model, deployed services are easily invoked or triggered on certain events (REST
invocation) with input data, then process and produce output. Such sequence of
Serverless functions processing the data in pipeline manner constructs the Serverless
Data Pipelines (SDP). Due to ﬁne grained scaling and reuse-ability of serverless
functions running over edge or cloud makes more beneﬁcial for IoT data processing
pipelines.
However, Serverless functions are stateless and its frameworks only deal with
run time not the data management. This separation would be beneﬁcial but makes
challenging for data intensive and stream processing pipelines. To expedite this
challenge, using intermediate storage units or integration with off the shelf data
pipeline engines makes simpliﬁed and more reliable in design of SDP. In this regard,
our previous work [ 6] investigated the use of message queues (such as MQTT or
RabbitMQ), object storage service (MinIO) and off the shelf data pipeline tools
such as Apache NiFi to handle intermediate data between the serverless functions,
however we designed and tested different SDP approaches, but did not considered
the scale-ability, cost aspects. Further, fully focused with open software services
and now, its essential and opportunistic to investigate the SDP architecture using
managed services due to large user base by public cloud vendors in IoT spectrum.
The current IoT solutions by public cloud vendors such as Amazon Web Ser-
vices, Microsoft Azure and Google Cloud Platform follows the similar architecture
of IoT continuum. One example would be, AWS Greengrass conﬁgured on edge
devices and that uses the lambda serverless framework for local processing or pre-
processing of IoT data and further data processing tasks pushed in to AWS IoTServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 109
service managed on the clouds. This type of architectures are designed with multiple
managed services, over-here cost and latency will be the major challenges because
it changes according to public CSP’s service limits, scaling techniques and billingmodels.
1.1 Motivation
Considering the smart factory, millions of sensors are connected in the factory
ﬂoor for automated and efﬁcient manufacturing. This sensors produces a massive
amounts of data continuously that’s needs to be processed rapidly to extractinsightful information and actuate accordingly. The most of data operations in this
context are event centric with short running time. On other-side, due to bandwidth
and latency issues, its expected that most data operations performed on the edgedevices in the factory ﬂoor.
However, processing such data over off the self data processing platforms such
as Apache Spark or Flink is challenging due to cost and resource constraints. Toaddress this, Serverless computing is an efﬁcient way of executing data operations
using tiny virtual functions that are deployed and scaled on edge devices. Further,
complex data operations such as video data analytic operations moved to cloud forextreme processing and actuate the further business processes or control the other
devices in the factory ﬂoor. So, building and deploying such end to end serverless
data processing pipelines running over edge and clouds makes more advantageousfor developers in terms of granular scaling and build once deploy any where and for
factory community beneﬁt in terms of cost and achieving the latency requirements
that yield higher production.
Aligning to this, currently public cloud vendors have awful solutions that
leverages the edge and cloud resources to design, integrate and deploy end to
end serverless data processing pipelines. For example, AWS Greengrass or AzureIoT edge are typical frameworks those easily bring cloud services near to the
factory ﬂoor (edge devices). However, building serverless data processing pipelines
involves computation and communication cost. Each serverless function is invokedin the data pipeline using HTTP or web-hooks with data to process. Further,
serverless function consumes the data and produces the output by pushing in to
intermediate storage unit such as object storage (AWS S3 or Azure Blob) or messagequeues (AWS Simple Queue Service). This further continues in the pipeline till it
reaches to the designated data sink.
The cost of each serverless function invocation and its limit varies according
to cloud vendors subscription plan, further insert and retrieve data operations of
intermediate storage units like S3 or SQS or Azure Blob have variable cost and
service limits for accessing the data concurrently. Due to latency constraints for IoTapplications, serverless invocation warm/cold start times are extremely important
and are typical varies provider to provider due to their architectural styles. Since,
in SDP the set of sequence of serverless functions and each function in the pipeline110 S. Poojara et al.
performs different data operation, warm/cold start will add signiﬁcant cascading
latency’s. In addition to this, how cloud providers support the scale-ability of
functions, data units in processing the rapid massive concurrent data from factory
ﬂoor sensors?
So considering all the above challenges, it motivates us to investigate the
solutions for design of SDPs and performance of each cloud provider for SDP
deployment at edge and cloud environments w.r.t to computation and communica-
tion cost, warm/cost starts of serverless entities and end to end latency in processing
the data in the pipeline. This investigation results can be impact-full in designing
the IoT applications using public cloud vendors.
1.2 Contributions
In the above context, the primary contributions of this work are summarized as
follows:
–We provide an overview and comparative analysis of three public cloud vendors
(Amazon Web Services (AWS) and Microsoft Azure) IoT solutions w.r.t their
cost, service limits and other parameters.
–We designed a PdM system architecture for real time fault failure prediction
system.
–We proposed Serverless Data Pipeline architectures for PdM use-case using
Azure and AWS solutions.
–We evaluated the proposed SDPs using performance metrics and provided future
directions.
The rest of the is organized as follows. In Sect. 2, we provide background of IoT
architecture and foundations to SDP for IoT data processing and provided survey
on current research works in Sect. 3. Further, in Sect. 4we highlight and compare
the IoT solutions of AWS and Azure Cloud providers and we describe the real time
use-case for bearing failure prediction in industrial motor in Sect. 5. In Sect. 6,w e
propose SDP architecture using Azure and AWS services and compare them with
various performance metrics in Sect. 7. Finally, the concluding remarks and the
future works are discussed in Sect. 8.
2 Background
Based on the introduction and motivation, In this section, we will describe fun-
damental components of IoT system and its three-tier architecture that consists of
different services at each layer. We also describe the serverless data pipelines and
its approaches for IoT data processing.Serverless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 111
2.1 Internet of Things
This section describes the three-tier architecture of an IoT system as shown in
Fig.1. The ‘Cloud Customer Architecture for IoT (CCAIoT)’ [ 7] created a reference
architecture that includes end to end service layers that encompassed with managing
end user devices to enterprise data processing. Our three-tier architecture in Fig. 1
follows the same architecture of CCAIoT but we would be interested in edge and
cloud data processing rather than cloud only processing.
The rapid growth in Internet and Communication Technology (ICT) accelerated
the growth of IoT deployments in Industry 4.0 ecosystem. This massive use
of IoT devices in various applications including industrial use cases (Predictive
maintenance of factory machines) to smart health care solutions necessities the
processing of sensor data in faster manner by leveraging the on premise and far away
clouds seamlessly. Several solutions[ 8] exists for Cloud centered IoT platforms from
different CSPs such as thingspeack [9],Cayenne [10], etc, but such platforms are
fully focused on collecting the data from IoT devices and performing entire data
processing and controlling the device operations from the centralized clouds.
However, using only cloud-centric IoT platforms are challenging due to that
current IoT applications are latency agnostic and demand for huge bandwidth
to upload live data streams to clouds for processing (for example using thermal
video cameras for predictive maintenance of heavy electrical machines or detecting
safety position of workers using video cameras). To expedite the above mentioned
challenges, three-tier architecture [ 11] is advantageous as similar to Fig. 1. Here,
focus is to perform preliminary operations near to the source in a gateways such as
video compression or data aggregation and further transported to cloud systems.
Figure 1shows a high level three-tier IoT architecture that includes Device tier,
edge and cloud tiers. The device tier includes large set of sensors implanted on the
monitoring device (robotic arm, motor or electric motor) using PLC or SCADA
Robotic Sensor
Temp. Sensor
Camera SensorSwitch
Device Tier Edge TierData Storage Service
Data Processing 
and analytics
Data VisualizationDevice Integration
Device Managment
Device Data routingData Flow
Control FlowData aquistion
Data
Preprocessing
Device ControlData Flow Data Flow
Control Flow Control FlowCloud TierContol and
Managment layerData Processing 
and Storage layer
Fig. 1 High level three-tier IoT architecture112 S. Poojara et al.
systems and connected over different data communication protocols like HTTP,
MQTT or OPC-UA to edge tier. An edge tier is an on-premise very near to the
sensor device responsible for data collection and performing basic operations on the
data such as data conversion, aggregation and resizing etc. The cloud tier is divided
in to two layers such as Device control and management layer which is responsible
for connecting, conﬁguring and manage, monitor and data routing of the IoT Sensor
devices and gateways, this essentially a single window system manage for billions of
IoT devices in scale-able manner. The data processing and storage layer performs
the IoT data using processing clusters, than storage in the time series data base,
further can be visualized using managed services. The cloud tier is far away from
the edge tier and responsible for performing data stream processing, storing in scale-
able manner by utilizing the cloud resources seamlessly. Further, cloud tier performs
business analytic and actuates business processes based on analytic decisions and
deliver the notiﬁcations to end users and other data sinks.
Currently, managed services for IoT data processing provided by multiple public
cloud vendors follows the similar three-tier architecture. We will provide extensive
overview of such services in Sect. 4.
2.2 Serverless Data Pipelines for IoT Data Processing
Serverless [ 12] had emerged as a new way of deploying and scaling the applications
at a functional level. This provides an opportunity for developers to concentrate
on business logic instead of focusing to scale, conﬁgure and manage. Serverless is
also known as FaaS (Function as a Service) [ 13], where the event is triggered to
compute the function and enforces to bill only during the function execution. This
billing as opposed to traditional systems, where there is a need to pay for idle CPU
and memory. Hence serverless is advantageous and essential to run the event-driven
applications. Event-driven systems consist of many short running tasks expected to
run in any sequence or in any combination, to manage and deploy leads to higher
cost in VM’s [ 14].
In Today’s world, the transformation of technology made human life easier by
automating connected objects like vehicles, home elements, and other smart entities.
These elements build a smart ecosystem and push up the events in between to
perform speciﬁc tasks. For example in the Internet of Things (IoT) environments,
there are multiple connected endpoint devices, edge devices like routers, switches
and other tiny processing elements, where an event triggered the need to be
processed. In traditional Cloud-centric IoT systems, events propagated from the
endpoint device to cloud, by increasing the latency. Now a new computing paradigm
deﬁned known as Edge computing, where a computational power of edge devices
can be harnessed to process the events. Each event considered to be a task and carries
data. The capabilities of serverless entities can be used to deploy on edge elements
for faster processing and minimizing service latency. Since each task carries theServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 113
Data
SourceSurveillance
Camera Video  
aquistionSplit in to
imagesObject
DetectionStore
SF1 SF2 SF3Data
Sink
Fig. 2 Example of serverless data processing for IoT application
data, at each level of edge elements, having serverless frameworks triggered by
events builds the data pipeline [ 15].
A simple example of serverless data pipeline shown in the Fig. 2. Consider an
use-case in smart factory, where video surveillance cameras are used to detect the
false pose of factory worker while operating a sensitive device/machine, here, video
streams are collected, split into frames (images) and detect the pose of the worker,
ﬁnally alert the administration and the worker about false pose that yield harm
to person or to the machine. The each module is dependent on previous module
for data input, which resembles as a data pipeline. The video data generated from
video camera (data source) processed in a sequence of modules and ﬁnally stored
in a storage (data sink). Here, moving entire video streams for cloud oriented
data processing yield major disadvantages in bandwidth issues to upload video,
latency and cost of processing. So essentially, part of data processing pipeline
executed near to data source and further on to the cloud systems or entire pipeline
deployed near to data source on factory ﬂoor. However, due to advancements, cloud
services can brought near to device and than streamline from edge to cloud data
processing. Its challenging to conﬁgure compute and memory hungry data pipeline
platforms in factory ﬂoor. Henceforth, due to light weight and easy synchronization
of serverless functions from cloud to edge and vice-versa by cloud providers
makes the data processing efﬁcient. Align to this, entire use case modules are
decomposed to individual serverless functions, where each function output is fed
to another serverless function for processing data from source sink by constructing
pipelines, which is termed as Serverless Data Pipelines. In Fig. 2, SF1, SF2 and
SF3 are serverless functions (SF) formed as data pipeline. Due to non stateless
nature of serverless functions, an intermediate storage units are used for a complete
end to end data pipeline. Once SF are designed can reused and deployed on
platform independent and heterogeneous hardware like X86 or ARM systems. This
drastically reduces the time to market and developers need not worry.
The serverless platform would be efﬁcient and supports the deployment of
large number event-driven applications, especially Industrial automation, Smart
city, agriculture automation, sports and many more. Subsequently, they generate
real-time data streams, need to be scaled and processed in real time with minimal
response latency. Such applications are decoupled into individual functional units114 S. Poojara et al.
that can harness the power of serverless entities deployed at edge elements for
computation. Like endpoints objects in IoT environments generates massive events,
all have to be processed in a quick, these events may be in different nature as
mentioned below and with different quality parameters like minimum latency, cost,
time and many.
3 Literature Survey
This section provides an overview of the literature review that includes the current
state of art research in the ﬁled of IoT Data processing, serverless computing and
solutions by various cloud vendors.
Some research works focused with the frameworks for processing entire IoT data
over edge devices by using data oriented programming models (R-Pulsar) [ 16] and
actor based framework (ERAIA) [ 17]. However, these works are more advantageous
for local device processing rather moving and processing data oriented pipelines
between different remote systems such as edge to cloud servers.
Multiple research works shown that the light weight serverless computing is
efﬁcient at resource constrained edge devices [ 5,18,19] specially for IoT data
processing. Renart et al. [ 20] proposed a serverless based real time data analytic
solution across the cloud and edge in a uniform manner. Salehe et al. [ 21] proposed
the data pipeline oriented architecture for video processing (gesture recognition)
using serverless framework. These works quite similar to our proposed work but
execution of function pipeline is controlled using single function which is not
reliable and can yield an extra added cost.
Recently edge computing became primary pillar in designing IoT applica-
tions due to latency and cost constraints that enforced Public Cloud Providers
to streamline their near device data processing. solutions. Pierleoni et al. [ 22]
performed comparative study with architectures and performance of services in
AWS, Microsoft Azure and Google Cloud with IoT applications. Their evaluation
measures speciﬁc to MQTT middle-ware based reference architecture and not
speciﬁcally to the data oriented serverless pipelines. Das et al. [ 23] investigated
the AWS Greengrass and Azure IoT Edge with different IoT applications to
estimate the performance metrics such as latency, cost with various payloads. These
article resembles the same as our work but we focus speciﬁcally with serverless
architecture for end to end data processing and measuring the performance metrics
such as cold/warm time of serverless invocations, end to end data processing time
and other QoS parameters. Similar study had been conducted by Ucuz et al. [ 23],
but their focus was on non functional requirements such as constraints on hubs,
analytics and security based on user perspectives.
Condition Monitoring and Predictive Maintenance are the new age applications
of IoT in Industry 4.0 eco-system. Goh et al. [ 24] proposed a data pipeline approach
for smart factory using AWS Greengrass for monitoring industry equipment.
Izquierdo et al. [ 25] proposed a serverless based architecture for data processing and
detecting anomalies in MARSIS instrument. Our work uses the similar use case andServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 115
investigate the how cloud vendor speciﬁc serverless based data processing pipelines
are efﬁcient for IoT data.
Considering the existing literature and best of our knowledge, none of the
research works focused on investigating serverless oriented data pipeline design
using public cloud vendors and comparing their performance for industrial IoT use-
case.
4 Cloud Service Providers (CSP) and IoT Solutions
In our proposed work, we primarily considered two CSPs namely AWS and
Microsoft Azure for comparison in terms of the SDP design due to their popularity
and maximum user base. The IoT services provided by these CSPs follows the
three-tier architecture and includes SDPs composed with managed services such as
Message Brokers, Queuing and Notiﬁcation services, object storage, analytic APIs,
No-SQL data bases and visualization tools. Further, AWS, Azure and Google cloud
platforms uses the open standard deployment, orchestration, and integration services
which makes easy for developers and administrators to deploy and conﬁgure IoT
applications easily.
AWS is a popular public cloud service provider with market share of 31% in
2021 over other CSPs [ 26]. As of 2022, AWS comprises over 200 plus products
including IoT. Currently, AWS provides more than 11 IoT services categorized in to
two layers as similar architecture as shown in Fig. 1namely device software (Edge
Tier), Connectivity and control services, and Analytics (cloud Tier) to manage,
control and analyze the billions of devices as shown in the Table 1.T h eA W S
IoT with ﬁve main characteristics (accelerate, build fast, secure, scale), accelerate
the designing solutions with complete set of solutions from device connectivity,
storage and analytics. Further, Build intelligent solutions using superior AI/ML in
25x faster with managed Machine Learning Services and safeguard the device and
data transmission in edge-cloud continuum. AWS elastic infrastructure supports the
connection of billions of device and trillions of messages on the ﬂy. Developers and
administrators can easily integrate with other AWS services seamlessly.
Most of the services for IoT, are quite similar in three CSP’s. Table 1shows
the list of different services mapped according to three-tier architecture described
in Sect. 2.1. The IoT solutions stack of CSPs broadly classiﬁed as edge and cloud
tiers. The following sub section provides extensive description of two tiers and their
speciﬁc services provided by CSPs.
4.1 Edge Tier
The services at edge tier focused on running the cloud services near to data source
on the edge computing devices. The primary functionality of edge tier service stack
is data acquisition, synchronization of the data and services to/from far away clouds116 S. Poojara et al.
Table 1 Cloud service providers and their IoT solutions mapping against three-tier
architecture
IoT layers and CSPs AWS Microsoft Azure
Edge tier FreeRTOSaAzure RTOSd
AWS IoT greengrassbAzure IoT edgee
AWS IoT expresslinkcIoT perceptf
Cloud tier Device control and AWS IoT coreg
management layer AWS device defenderh
AWS IoT device managementiAzure IoT hubj
AWS IoT FleetWise
Data processing and AWS IoT sitewisek
storage layer AWS IoT eventslAzure IoT central
AWS TwinMaker Azure digital twinsm
AWS LamdanAzure functionso
AWS S3pAzure blobsq
AWS SQSrAzure storage Queuess
ahttps://aws.amazon.com/freertos/
bhttps://aws.amazon.com/greengrass/
chttps://aws.amazon.com/iot-expresslink/
dhttps://azure.microsoft.com/en-us/services/rtos/
ehttps://azure.microsoft.com/en-us/services/iot-edge/
fhttps://aws.amazon.com/iot-expresslink/
ghttps://aws.amazon.com/iot-core/
hhttps://aws.amazon.com/iot-device-defender/
ihttps://aws.amazon.com/iot-device-management/
jhttps://aws.amazon.com/iot/
khttps://aws.amazon.com/iot-sitewise/
lhttps://docs.aws.amazon.com/iotevents/latest/developerguide/what-is-iotevents.html
mhttps://aws.amazon.com/iot-twinmaker/
nhttps://aws.amazon.com/lambda/
ohttps://azure.microsoft.com/en-us/services/functions/
phttps://aws.amazon.com/s3/
qhttps://azure.microsoft.com/en-us/services/storage/blobs
rhttps://aws.amazon.com/sqs/
shttps://docs.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction
using platform speciﬁc communication protocols as shown in Table 1and ﬁnally to
perform data processing operations locally.
The CSPs have typical similar solutions based on the use case and environments.
For example, AWS RTOS and Azure RTOS serves the similar purpose, they are real
time operating system (RTOS) for edge devices powered by micro-controller units
(MCUs). Google Cloud IoT SDK does the similar job to use for micro-controller.
These services mostly for most highly constrained devices (battery powered and
having less than 64 KB of ﬂash memory).
AWS ExpressLink provides secure and faster connectivity to clouds and its
managed with AWS partner devices, where security credentials conﬁgured already
with the devices during procurement. Azure IoT Precept is most advanced serviceServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 117
for comprehensive, easy-to-use platform with added security for creating edge
AI solutions. Comparatively, Google Cloud IoT have IoT device registration and
management components managed entirely on the cloud infrastructure and leastservices at edge tier. However, Google’s EdgeTPU hardware stack and Cloud IoT
edge software stack brings powerful AI capability to edge and gateway devices.
In our proposed work, we are interested in AWS Greengrass and Azure IoT edge
services. These services are mainly for resource constraint devices, that bring the
most of the cloud hosted services to on-premises edge devices. For example, AWS
Lambda Serverless functions or Azure functions can easily designed in the cloudand executed both on the cloud and edge devices for seamless data processing
based on the QoS expectations. In our SDP design, we use both of the services
for designing and deploying serverless entities with data processing pipelines on theedge and cloud environments seamlessly.
4.1.1 Comparison of A WS IoT Greengrass and Azure IoT Edge
In the following paragraph, we describe and compare these services.
–AWS IoT Greengrass: It is an open-source edge run-time and cloud service forbuilding, deploying, and managing device software. It has two components-Greengrass Client Software and Greengrass Cloud Service. The Greengrass
Client software is conﬁgured at Edge Tier that enables local processing, mes-
saging, data management and ML inference. Local processing is enabled withlambda service. The Greengrass Cloud Service helps to build, deploy and manage
your device software across Edge Tier and this is a managed service by AWS.
–Azure IoT Edge: Its software stack to deploy on premise to consolidate opera-
tional data at scale in the Azure cloud. It provides facility to deploy remotely,
securely to manage cloud native clouds such as AI, data processing, other
azure services and login to run on customer IoT devices. The interestingfeatures are certiﬁed partnered edge hardware, free and open source run time
to code, modules- Docker-compatible containers from Azure services to run
business logic at edge and cloud interface to remotely manage workload andfor synchronization of the data from edge to Azure cloud.
The key comparison of both edge tier services:
–Cost of using service: Azure IoT Edge free to use and conﬁgure, whereas AWS
Greengrass has a charge of $0.16 per device per month.
–Device setup and service provisioning: Both have ready to dump software stack
for easy setup and sync to cloud device management with secure connection.
But AWS Greengrass provides, its own X.509 certiﬁcates for secure device
connection.
–Security: AWS and Azure IoT uses X.509 certiﬁcate for authenticating devices,
however for Azure, device owners has to get their own signed certiﬁcates where
as AWS provides the signed certiﬁcates by AWS’s own certiﬁcate authority.118 S. Poojara et al.
–Components or modules: The software modules run native in device’s software
environment in AWS Greengrass, whereas in Azure IoT Edge supports to run
using docker containers. Modules can be written any language as device software
supports in AWS but in Azure will support speciﬁed languages.
Edge tier as mentioned in the We design the SDP using the AWS Greengrass and
Azure IoT edge for deploying serverless functions with data processing pipelines.
In the further Sect. 5, we described the usage of these services to design and deploy
serverless data processing pipelines for real time IoT usecase.
4.2 Cloud Tier
The cloud tier services are categorized in two layers namely (1) Device control
and management layer, (2) Data processing and analytics layer. The device control
and management layer responsible for seamless integration and synchronization
of billions of devices from edge tier and further management of connectivity,
security conﬁguration etc. The data processing and analytics layer accommodates
the services related to event processing, storing the data using object storage service
or time series database and further processing by leasing managed data stream
processing clusters. The scale-able queuing service for storing events and data and
notiﬁcation services to generate the alerts and control signal to devices.
AWS and Azure has a specialized IoT services for managing and controlling the
on-premise devices on the cloud, for example AWS Fleet-Wise service dedicated
to automakers to collect, transform and transfer vehicle data to cloud in near real
time. It aimed to build applications for faster analytics and machine learning to
improve the vehicle quality, safety, and autonomy. Aligning to this, AWS IoT device
management service essentially provides a platform to manage billions of devices
in terms of easy and securely register, organize, monitor, and remotely manage IoT
devices at scale. It automates the ﬁrmware updates, trouble shooting, query the
state of any IoT device and manage other conﬁguration settings of huge number
of devices. The AWS Device defender service continuously audits the security
conﬁguration of devices to cross validate the deviation from best security practices.
However, we would be interested in managed services that sync and route the
edge data to managed serverless entities in the cloud. So as part of our study, AWS
IoT core and Azure IoT Hub are primary building blocks of the SDP architecture
as describe in the further Sect. 6. The AWS IoT Core does the job of connecting
and routing data and control signals to/from (bi-directional communication) end
user devices securely such as sensors, actuators and smart appliances over MQTT,
HTTPS, and LoRaWAN. The AWS IoT core need to be conﬁgured with routing
rules that forward the device to AWS service that is speciﬁed in the routing rule.
The routing rule would be inserting data object to S3, or invoke lambda function
etc., Similarly, Azure IoT hub and Google Cloud IoT provides bi directional
communication between the edge devices and cloud to receive/send the data and
control signals. Azure IoT Hub is also a managed service hosted in the cloud thatServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 119
acts as a central message hub for communication between an IoT application and its
attached devices. It supports MQTT, HTTPS, AMQP over Web Sockets. Azure IoT
hub supports SAS token-based authentication or X.509 certiﬁcate authentication.
Some of the key difference between AWS IoT Core and Azure IoT Hub as follows:
–Authentication: Both uses TLS based authentication. However, Azure uses only
user authentication but AWS uses the mutual authentication.
–Cost of using the service: Azure IoT hub pricing for basic B1 service is $10
per month with 400,000 messages/day with message size of 4 KB. The pricing
of AWS IoT core estimated monthly cost would be $11.45 with 1000 device
connectivity/month with 5 million messages per month with message size of
1 KB. While considering the overall pricing strategies, Azure is efﬁcient in terms
of cost and message size considerations.
–Security: Both uses the X.509 certiﬁcates but AWS provides self signed certiﬁ-
cate by AWS owned CA.
–Communication Protocols: Both supports HTTP, MQTT. Azure supports AMQP
over Web Sockets where as AWS supports LORAWAN.
–Routing rules and other service integration: Both support integration and routing
rules for most of the other managed services such as object storage (AWS S3,
Azure Cosmos db), serverless functions (AWS lambda or Azure functions),
analytic services such as PowerBI etc.
There exists awful other manged IoT services from AWS, Azure and Google for
ML and analytics spectrum, for example AWS IoT Site-Wise is a managed service
that simpliﬁes collecting, organizing, and analyzing industrial equipment data. The
developers and architects can replicate the real time scenario of several domains
including industrial applications using AWS TwinMaker and Azure Digital Twins.
All the CSPs provides a wayto store the multi variety data for example video/images
in the form object using object storage service. The AWS Simple Storage Service
(S3), Azure COSMOS DB and Google Storage are examples of object storage. It
is an scale-able object storage service and an efﬁcient way to store any kind of
data along with metadata and accessed with unique URI. In most of the serverless
oriented data processing implementations, object storage quite useful to store the
intermediate data between serverless functions. The object notiﬁcation service is
very much useful in such scenarios and even powerful that AWS S3 action writes
the data from an MQTT message to an AWS S3 bucket.
In our study, we are focused with Serverless platform oriented data processing,
in this direction we are using managed Serverless services (AWS lambda and Azure
functions), time series storage and event queuing and notiﬁcation services and
further, visualization tools for IoT data processing as described in Sect. 6.I nt h e
further paragraph, we will describe and compare such services used in designing
the SDP implementation.
1.Serverless platforms : Serverless is also know as Function as Service (FaaS).
This is the primary component of the data processing pipeline in our proposed
system. The AWS Lambda and Azure Functions are managed serverless plat-
forms used in our SDP design and implementation. Both platforms support120 S. Poojara et al.
variety of programming run time environments such as Python, NodeJS, Java etc.
Key comparisons of the Serverless platforms of AWS and The key comparison
of the Serverless platforms as follows:
–Supported run time: Azure supports run time such as C#, JavaScript, F#, Java,PowerShell, Python, where as AWS supports Java, Go, PowerShell, Node.js,C#, Python, and Ruby code.
–Pricing: In lambda functions, cost is measured on invocation requests tofunction and GB/s Memory duration. However, for Azure functions price iscalculated on per-second resource consumption and executions.
–Free limits based on user requests: Both platforms have similar free limitsfor example one million invocations a month. After free quota, $0.20 per onemillion.
–Free limits based on Memory usage: Azure Functions have limit of 400 KGB-s per month. After this $0.000016 per GB-s. AWS lambda have free limit400,000 GB-s of compute time per month and after that $0.000017 per 1 GB-s.
–Timeout: In lambda functions invocation time out is 900 s and in Azurefunctions is unbounded.
–Code size limit: The lambda functions supports 75 GB in archives and Azure
does not have code limit.
2.Queuing Service : Due to the stateless nature of serverless functions, queuing
service serves as intermediate storage between chain of serverless functions.
The queuing service is a distributed message queues that enables ti scale micro
services and serverless applications. The AWS Simple Queuing Service is apopular scale-able distributed queuing service that offers secure and durable
features. Azure Storage Queues are popular service provided by Microsoft Azure
and for storing large numbers of messages.
Considering the IoT solutions of CSPs mapped according to the architecture, we
are not essentially using all o for the services in our proposed system. We will stickto limited services in both AWS and Azure services to build the IoT data processing
pipelines with serverless architecture. We use AWS Greengrass, AWS IoT Core,
Lambda and SQS services to design the pipelines. In Azure services, we use AzureIoT Edge, AZure IoT Hub, Azure functions and Azure Storage Queues for SDP
design. In next section, we will describe about real time IoT application and further
section, will design the SDP approach using AWS and Azure IoT solutions,
5 Real-Time IoT Application: Predictive Maintenance of
Industrial Motor
This section, describes a real time IoT application that is Predictive Maintenance
(PdM) system for predicting the bearing faults for electrical motors in Industrial
systems. For this use case, we have considered run to failure data set of bearingsServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 121
OPC-UA
Server
OPC-UA
Client
ServiceData
Aggregation
ServiceEdge Device
Data Storage ServiceAnomoly
Detection
ServiceFailure
Predtion
Service
Data
VisualizationNotification
Service
Public Cloud EnviornmentEdge  EnvironmentCloud Services
O
S
Motor with IoT Sensor
OPC-UA
Server
O
S
Motor with IoT Sensor
IoT devices
Fig. 3 PdM system architecture for real time fault failure prediction
collected in NSF I/UCR Center for Intelligent Maintenance Systems (IMS) [ 27].
The data set is used to simulate the real time scenario of motor with four bearings.
Further, PdM system is designed to detect anomalies and predict the future failure-
ness of the bearings using Machine Learning (ML) algorithm. Considering all this, a
overall system architecture of the Use Case used in this article is as shown in Fig. 3.
Figure 3depicts the overall system architecture for condition monitoring that
predicts the future failure-ness of the AC motor in Industrial systems. The motors
deployed with accelometer sensors to sample the vibration signals of running
bearings in the motor. According to the description of IMS data set [ 27], Rexnord
ZA-2115 double row bearings were installed on the shaft of AC motor and PCB
353B33 High Sensitivity Quartz ICP accelerometers were installed on the bearing
housing which is a sensor to sample the vibration signals at 20 kHz for 1-s time
period. This vibration signal snapshot will result with 20,480 data points. The
interval time for each snapshot is 10-s and that streams about 20,480 data points at
each interval. Here, to generate this scenario we used docker containers with python
environment to stream the data at every 10 s that mimic the sensor behaviour.
The Open Uniﬁed Architecture (OPC-UA) standard architecture [ 28]i su s e d
for data acquisition from the factory ﬂoor machines (motors) to edge computing
device. There exist several communication protocols and architecture, however most
of the Industry 4.0 applications follow the OPC-UA standards due to interesting
features such as independent-ability, extendable, and security. Also it enables an
easy integration with IoT and M2M with Programmable Logic Controllers (PLC)
of machines [ 29]. Further, motor assembly is fabricated with Programmable Logic
Controllers (PLC) that hosts with OPC-UA Server that communicates to OPC-UA
Client hosted on the edge device. The OPC-UA Client listens the data from several
machines and forwards to cloud systems over HTTP or through message brokers
(MQTT /AMQP) or can pass through for further processing in the edge device.
The edge computing device is a tiny server (less compute, memory and storage
power respectively as compared with clouds) located in the premises of factory
ﬂoor near to the machines. The edge device capable to perform preliminary data
operations and even-more, the current modern hardware devices support to run AI122 S. Poojara et al.
applications with GPU processing capability. Few such example of edge devices
are Cisco Catalyst IR8100, Dell PowerEdge XR11 and XR12, Lenovo ThinkEdge
SE30 and ThinkEdge SE50 [ 2]. In this use-case, edge device will run with two
services namely OPC-UA Client and data pre-processing (data aggregation) service.
The OPC-UA Client receives the data from OPC-UA servers on every intervals and
forwards it to data pre-processing service. The data pre-processing service consist
of several activities such as data aggregation, transformation, ﬁltering etc. In our
use case,we use an aggregation service to minimize the data transmission cost from
edge device to clouds. Further, the pre-processed (aggregated) data is forwarded to
the cloud environment.
An aggregated data from edge device, received in cloud systems over MQTT
Publish/Subscribe messages or HTTP invocation for the particular service in the
cloud. In this use-case, multiple services are used such as anomaly detection, storage
service (time series storage) and failure prediction service along with notiﬁcation
and data visualization service accessible for the end users. The anomaly detection
service is basically ML algorithm that consumes the incoming data stream and
performs anomaly identiﬁcation. The storage service used to store the data stream
along with anomalous identify and recorded time stamp. The failure prediction
service is triggered on the stored data over period of time (each day or every hour)
and it identiﬁes the future failure-ness of the machine based on statistical analysis.
To realize use case, the PdM system architecture is implemented by simulating
a real world Industrial IoT scenario using managed services provided by AWS and
Azure IoT Solutions.
6 Building SDP for Predictive Maintenance Application
In this section, we will describe the end to end design of Serverless Data Pipelines
for the use case mentioned in the Sect. 5. Firstly, we emphasize on the need of
SDP and generalize the serverless entities and intermediate data storage units at
each tier of the IoT system. Secondly, we propose the design of SDPs using tools
and managed services leased from the public cloud vendors (AWS and Microsoft
Azure).
IoT data processing domain experienced signiﬁcant growth in the recent years
focused with use of modern data processing platforms and integrated cloud services.
IoT applications such as PdM use case are latency sensitive and event driven
in nature those can leverage the serverless computing framework composed of
serverless functions (data analytic operation) integrated with the data pipelines
deployed across IoT continuum. The data analytic pipelines are build with several
connected components wherein data consumed from data source, then pre-processed
(ﬁlter, aggregate or transform) and ultimately fed into ML component to extract the
insights and further reaches to data sink.Serverless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 123
This data analytic operations are composed with Serverless functions and
deployed at edge or cloud in-spite of heterogeneous hardware architecture. The
other beneﬁts of serverless computing such as ﬁne tuned granular auto-scaling,
re-usability of serverless functions and efﬁcient billing methods makes straight
forward candidate for resource constrained edge computing platforms [ 18,19,30].
Currently most of the CSP’s support their serverless framework (AWS lambda,
Azure functions) available for edge devices and are managed and conﬁgured from
the cloud platforms as we describe in the Sect. 4. However, Serverless functions are
stateless and most of the platforms coordinate the only run-time management rather
then data handling.
To address this challenge, our previous work [ 6] proposed different techniques
of storing intermediate data between serverless functions in the data pipeline. We
experimented the efﬁciency in use of message brokers (MQTT), Object Storage
Service (MinIO) and Apache NiFi as a intermediate storage units. Currently, public
cloud providers has typical solutions to manage the intermediate data, for example
AWS SQS or AWS S3 or Azure Blob could serve as storage components within the
pipeline. Aligning to this, we have proposed the design of end to end data pipeline
for the PdM use case as explained in the below sub sections.
6.1 Proposed Serverless Data Pipelines
The proposed SDP for PdM use case is shown in Fig. 4. The entire use case is
decomposed in to a set of Serverless Functions (SF), these functions consumes the
incoming data and produces the output. The data between the functions is managed
with Storage Units (SU). The data ﬂows from the source to the sink with sequence of
operations on the data using serverless functions. We mimic the working behaviour
of the motor connected with sensor using python program as a docker container. The
program reads the data from the CSV ﬁle (IMS data set) and pushed into OPCUA-
Failure
PredictionOPC-UA ClientSF1
data aggregationSF2
SUanomaly
detection
time-series
storageSF3
SF4SU
Notification
ServicezVisualization
Service
OPCA-ServerPython
OPCA-ServerData Source (Sensor)
CSV
CSV
Data Source (Sensor)
Data Sink Data Sink
OPCPython
Fig. 4 Proposed SDP for the PdM use case. ( SF: serverless function, SU: storage unit)124 S. Poojara et al.
Server. The data is streamed at every 10 s by reading each CSV ﬁle with 20,468
data points. This entire unit is assumed to be data source in the data pipeline. In the
further pipeline, we use four SFs and working of the those functions are as describedbelow:
–OPCA-Client (SF1) : This function acts as OPCA-Client to receive the data from
the set of OPCA-Servers. The received data is stored in the intermediate storage
unit as a input to the SF2 function.
–Data Pre-processing (SF2) :T h e Data Processing function reads the data from
storage unit and performs the data pre-processing tasks such as removing of null
values and aggregates the data points to single data point by using statistical mean
operation. Further, preprocessed data is stored in the intermediate SU.
–anomaly detection (SF3) : This function reads a data from the SU and performs
the anomaly detection using ML algorithm. It produces the binary output with
True or False. Here, we use Facebook’s Prophet anomaly detection algorithm.The trained model is exported and used in the function. The anomaly detection
model training and validation is explained in the below subsection.
–Failure Prediction (SF4) : This function predicts the future failure-ness of the
bearings based on the identiﬁed anomaly data stored in the time series data base.
It uses the statistical operation to ﬁnd out anomaly pattern on the speciﬁc time
period on the data. The function is triggered on certain intervals of time periodand will be notiﬁed with end user using notiﬁcation service.
In a pipeline, the data streams are generated using IoT devices. This raw data
is collected using SF1 and pre-processed, aggregated using SF2. Further, anomalydetection algorithm is applied over the pre-processed data using SF3 and result is
stored in the storage server. To detect the future failure, SF4 is triggered over certain
period of intervals and output is notiﬁed to the end users. Here, end users serveas the data sink. The sequence of serverless functions (SF1, SF2, SF3 and SF4) are
used for data operations and ﬁnally delivered to data sink. These serverless functions
can be design using any of run time environment (Like Python, Go, C# or Java). Inour experiments, we used a python run time based serverless functions and Prophet
anomaly detection was trained and tested using Google’s Colab environment.
Since serverless functions are stateless and to build reliable data processing
pipelines, we used SU as a intermediate data units to make data persistent over the
pipeline from source to sink. Most of the CSPs provide SU services such as message
queues, object storage and off-the-shelf data pipeline engines (such as AWS datapipeline). However, selection of SUs heavily depends cost and reliability factors. In
our experiments, we use Message queues and managed IoT servcies such as Azure
IoT Hub or AWS IoT Hub provided by AWS and Azure.
Considering the use case, the below sub sections describes the building the
anomaly detection model using Facebook’s Prophet and further we illustrate the
implementation of SPD’s using AWS and Azure Solutions.Serverless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 125
6.1.1 Building an Anomaly Detection Model
In this subsection, we describe the training and validation of the ML based anomaly
detection algorithm. For this, we considered Facebook’s Prophet algorithm which is
a unsupervised machine learning algorithm that works nicely with uni-variate time
series data.
–Dataset : As mentioned earlier in the Sect. 5,w eu s e d run to failure IMS data set
downloaded from NASA repository [ 2]. The size of the data is 5.2 GigaBytes
(GB). It contains three sets of data, each data set consists of individual ﬁles that
are 1-s vibration signal snapshots recorded at 10 s of interval. Each ﬁle consists
of 20,480 points with the sampling rate set at 20 kHz. We considered the data set
1 for training and validation. This data set has a recorded data of 8 channels for
four bearings (two channels for each bearing). The bearing 1 and bearing 2 were
failed at the end of the experiment. There were 2156 number of ﬁles and ﬁlename
indicates the recorded time stamp.
–Data pre-processing : Since, each ﬁle consists of 20,480 data points with 8
columns of channel data, for an anomaly detection we aggregated to single data
point and removed null values. then, ﬁlename contains the recorded time stamp
and this is considered as a timestamp attribute. The pre-processed data shown in
Fig.5.
–Facebook’s Prophet algorithm : The Prophet is a open source ML algorithm
for forecasting the time series data which is developed by Facebook in the year
2017 [ 31,32]. It works best with time series that have strong seasonal effects and
several seasons of historical data. Prophet is robust to missing data and shifts in
Fig. 5 IMS data set with global outliers and breakdown of the bearing along the time126 S. Poojara et al.
the trend, and typically handles outliers very well. The data set consists the large
set of global outliers as a symptom before the failure of the bearings as shown in
Fig.5.
–Training and validation of the model : We split the pre-processed data set
in to training and validation data. The training data does not cover the global
outliers and failure of the bearings. The validation data covers the unseen data
for the Prophet algorithm and that have a global outleirs with failure data. We
used Google Colab and python framework and Facebook’s Prophet package is
installed using python pip. Initially we ﬁt the model with training data and predict
with unseen validation data.
6.2 SDP Using AWS and Microsoft Azure
The section describes the design of SDP using AWS and Azure IoT services. The
Azure based SDP implementation consists of Azure IoT edge, Azure functions
and message queue at edge tier and the cloud tier consists of Azure IoT Hub
conﬁgured with routing rules. The Azure functions are used to implement the
serverless functions. The complete end to end implementation of Azure based
SDP for predictive maintenance use case is shown in Fig. 6. Here, IoT devices are
simulated using docker containers as virtual electric motor connected with vibration
sensors that read the IMS data set and stream at certain interval.
An edge tier is a edge gateway, here we used RPi4B model (arm64 architecture)
with Ubuntu 20.0.4 OS. The azure IoT edge softwares stack are conﬁgured on the
edge gateway. The IoT Hub service is created in the Azure portal with standard
subscription that allows 400,000 messages/day with 1 edge device connection. The
edge device conﬁgured on IoT Hub as IoT Edge device with X.509 authentication.
The azure functions for edge tier are developed using Microsoft Visual Code with
python environment. The functions are built as docker images and pushed in to
docker hub, further function modules are as docker images are deployed using visual
code in to the edge device.
Failure
PredictionOPC-UA Client data aggregationanomaly
detection
time-series
storage
Notification
ServicezVisualization
Service
iOPCA-ServerPython
OPCA-ServerSensor
Sensor
CSV
CSV
Rule IoT HubCloud
FunctionsCloud
FunctionsCloud
Functions
Cloud
FunctionsAzure IoT Edge
Edge GatewayMicrosoft Azure Cloud
IoT devices
PCAPython
Fig. 6 Microsoft Azure IoT SDP architectureServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 127
Failure
PredictionOPC-UA Client data aggregationanomaly
detection
time-series
storage
Notification
ServicezVisualization
Service
iOPCA-Server
OPCA-ServerSensor
SensorCSV
CSV
Rule IoT HubAWS GreengrassAWS Cloud
Edge Gateway
IoT devicesPython
Python
Fig. 7 AWS IoT SDP architecture
The OPC-UA Client is a serverless function that does the job of OPC-UA server
to connect with clients and collect the data further published in to azure Service
Bus queue. The azure Service Bus queue invokes the aggregation function. The
aggregated function routes the data to IoT Hub. The IoT Hub receives the message
data, and forwards to designated routing rule. The routing rule is conﬁgured with
events to store message in the Service bus. Further, service bus event invokes the
prophet’s anomaly detection function. This also tag the data and stores in to the
time-series database. Further, cron based event trigger is raised to invoke Failure
prediction function to read the data at certain interval and check for tagged data
and it raises the notiﬁcation, if anomalous data is highest in the previous collected
tagged data. The notiﬁcation services generate the notiﬁcation by sending alerts of
further failure (Fig. 7).
7 Experiments and Results
The proposed PdM use case is implemented using AWS and Azure based SDP
architecture as explained in the previous Sect. 6. Further, the goal is to measure the
performance of the SDP’s w.r.t to the metrics to understand and investigate the data
pipeline efﬁciency based on the scaling the data streams. In the following section,
we will discuss the metrics used to measure the performance of the pipelines and
analysed the results, further outlined the experience on the SDP implementation and
provided future directions.
7.1 Performance Metrics
In this subsection, we will describe the various performance metrics and their
mathematical formulae. We considered the cost, data processing time of the pipeline
from source to sink, cold and warm time for serverless invocations. Further, size of
data source is scaled in various size and performance metrics are measured.128 S. Poojara et al.
–Cost : The cost of running the SDP from data source in edge tier to the data sink
in the cloud is calculated. The end to end pipeline cost is inclusive of all the
service cost taken into consideration. It is measured in $.
–Processing Time : The processing time of the SDP is the time taken to process
the each data unit from data source to data sink in the pipeline. It includes
both computation and communication cost. The processing time is measured in
seconds (s).
–Cold start time : The cold and warm start time specially monitored for serverless
functions. The cold time measures the time required to setup the serverless
function for the ﬁrst time when its invoked. It is measured in milliseconds (ms).
CPU and memory utilization : The resource utilization of the edge and cloud
tier. The resource utilization includes CPU and memory metrics. It is measured
in percent (%).
7.2 Experimental Setup
As part of edge tier infrastructure, we used RPi 4B model conﬁgured with Ubuntu
20.0.4 Operating System. The Docker Container Engine v20.0.1 and python3
environment were installed on edge tier. Further, AWS and Azure edge tier solutions
(AWS Greengrass Edge and Azure IoT Core) are conﬁgured on the edge tier. The
aws and azure command line interface installed, to communicate and conﬁgure
necessary libraries in on-premise edge device. The docker environment is necessary
for Azure, to run the services as docker containers. As part of AWS SDP solution,
AWS Lambda and Azure functions services are conﬁgured on the edge device. All
the serverless functions described in the Sect. 6are designed and developed using
python3 run time environment.
7.3 Results and Discussions
We considered the scaling of the incoming sensor data to check the performance of
the SDP approaches, because rate of amounts of arrival of concurrent IoT device
data, heavily impact the pipeline performance. To measure the performance of the
SDP approaches, we scaled users from 10 to 200. In the following paragraph, we
provide result analysis of the metrics such as cost, processing time, processing time
of the pipeline components in an edge tier and resource utilization of edge tier.
An overall total processing time is measured in milliseconds. It is the summation
of the time as data unit started from data source (IoT device) and reaches to the
data sink (storage in the cloud). Figure 8shows the total processing of the both
AWS and Azure based SDPs. Here, X-axis indicates the number of concurrent
devices requesting for data processing and y-axis represents the total processing
time measured in milliseconds. Figure 8indicates that AWS based SDP yieldedServerless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 129
Fig. 8 Total processing time
Fig. 9 Time spent in green grass and Azure edge
higher processing time as compared Azure based SDP. This is due to the addition
of extra queues in the pipeline, for example initially messages are published to SQS
(Simple Queuing Service) service and then to invoke the functions, there is a need
to add SNS (Simple Notiﬁcation Service) service. So, we need to use SQS and SNS
as an intermediate storage units while designing the pipeline.
Figure 9shows an in-ﬂight time spent in edge tier application and is measured
in milliseconds. This indicates that time to process data unit by the part of SDP
deployed in edge tier before forwarding to the cloud environment. This measures
the capability of software stack running at edge tier (Azure IoT edge and AWS
Greengrass core). Here, AWS Greengrass experience lower processing time as
compared with Azure IoT edge. This is because, AWS Greengrass run its modules
native to the host environment, where Azure IoT edge run all its software stack130 S. Poojara et al.
Fig. 10 CPU utilization of edge tier
Fig. 11 Memory utilization of edge tier
in docker container which adds an extra layer and that induces the extra time in
processing. Figures 10and11, shows the CPU and memory utilization of edge tier
and are measured in percent (%). The Azure based SDP consumed overall maximum
CPU power as compared with AWS based SDP. The Azure IoT edge requires extra
software modules like Docker or moby container engine as compulsory requirement
to run its services on an edge hardware. This makes slightly higher CPU and
memory utilization as compared to AWS Greengrass.Serverless Data Pipelines for IoT Data Analytics: A Cloud Vendors Perspective ... 131
8 Conclusions
In this book chapter, we investigated the public cloud provider speciﬁc IoT solutions
to build serverless data pipelines to process the IoT data. Further, we proposed
Azure and AWS based serverless data pipeline approaches and implemented using
predictive maintenance use case of industrial motor. The public cloud providers
have variety of solutions for handling the IoT data on-premise and in cloud, but
our investigation shows that the architectural deployment at edge tier signiﬁcantly
varies in the performance of data processing. Aligning to this, Azure based SDP
experienced high consumption of CPU and memory power as compared with AWS.
However, AWS based SDP yielded higher processing time as compared with Azure
based SDP. Our investigation is limited and has room for more opportunity to
investigate in different aspects of SDP implementations like scale-ability, function
chain based using AWS Step functions or Azure functions that can improve the
overall processing time of the IoT data processing.
Acknowledgments This work is partially supported by the European Social Fund via IT Academy
program. We thank for Telia Eesti for providing hardware procurement support and also thank
ﬁnancial support to UoH-IoE by MHRD, India (F11/9/2019-U3(A)).
References
1.Lufthansa Industry Solutions. https://www.lufthansa-industry-solutions.com/de-en/solutions-
products/industry-40-iot/industry-40-sensing-the-way-to-the-smart-factory
2.Bitkom Study Report. https://www.bitkom.org/Presse/Presseinformation/IT-Unternehmen-
bauen-Angebote-fuer-die-Industrie-40-aus.html
3.Tsanousa A, Bektsis E, Kyriakopoulos C, González AG, Leturiondo U, Gialampoukidis
I, Karakostas A, Vrochidis S, Kompatsiaris I (2022) A review of multisensor data fusion
solutions in smart manufacturing: systems and trends. Sensors 22(5):1734. https://doi.org/10.
3390/s2205173
4.Cardellini V , Presti FL, Nardelli M, Russo GR (2018) Decentralized self-adaptation for elastic
data stream processing. Future Gener Comput Syst 87:171–185
5.Aslanpour MS, Toosi AN, Cicconetti C, Javadi B, Sbarski P, Taibi D, Assuncao M, Gill
SS, Gaire R, Dustdar S (2021) Serverless edge computing: vision and challenges. In: 2021
Australasian computer science week multiconference, pp 1–10
6.Poojara SR, Dehury CK, Jakovits P, Srirama SN (2022) Serverless data pipeline approaches
for IoT data in fog and cloud computing. Future Gener Comput Sys 130:91–105
7.CCAIoT Architecture. https://www.iiconsortium.org/IIC_PUB_G1_V1.80_2017-01-31.pdf
8.Ray PP (2016) A survey of IoT cloud platforms. Future Comput Inform J 1(1):35–46 (2016).
https://doi.org/10.1016/j.fcij.2017.02.001
9.Cayenne. https://developers.mydevices.com/cayenne/features/
10.ThingsSpeak. https://thingspeak.com/
11.Pierleoni P et al. (2020) Amazon, Google and Microsoft Solutions for IoT: architectures and a
performance comparison. IEEE Access 8:5455–5470. https://doi.org/10.1109/ACCESS.2019.
2961511132 S. Poojara et al.
12.Baldini I, Castro P, Chang K, Cheng P, Fink S, Ishakian V , Mitchell N, Muthusamy V , Rabbah
R, Slominski A, Suter P (2017) Serverless computing: current trends and open problems. In:
Research advances in cloud computing. Springer, Singapore, pp 1–20
13.Serverless. https://blog.g2crowd.com/blog/trends/digital-platforms/2018-dp/serverless-
computing/ . Accessed 2 Feb 2019
14.Serverless and VM. https://techbeacon.com/enterprise-it/economics-serverless-computing-
real-world-test
15.Serverless Datapelines. https://www.bsquare.com/blog/serverless-data-pipeline/ . Accessed 4
Feb 2019
16.Renart EG, Balouek-Thomert D, Parashar M (2019) An edge-based framework for enabling
data-driven pipelines for IoT systems. In: 2019 IEEE international parallel and distributed
processing symposium workshops (IPDPSW). IEEE, Piscataway, pp 885–894
17.Hernandez A, Xiao B, Tudor V (2020) Eraia-enabling intelligence data pipelines for IoT-
based application systems. In: 2020 IEEE international conference on pervasive computing
and communications (PerCom). IEEE, Piscataway, pp 1–9
18.Javed H, Toosi AN, Aslanpour MS (2021) Serverless platforms on the edge: a performance
analysis. arXiv preprint arXiv:2111.06563
19.Kjorveziroski V , Filiposka S, Trajkovik V (2021) IoT serverless computing at the edge: a
systematic mapping review. Computers 10(10):130
20.Renart EG, Balouek-Thomert D, Parashar M (2018) Edge based data-driven pipelines (techni-
cal report). arXiv preprint arXiv:1808.01353
21.Salehe M, Hu Z, Mortazavi SH, Mohomed I, Capes T (2019) Videopipe: building video
stream processing pipelines at the edge. In: Proceedings of the 20th international middleware
conference industrial track, pp 43–49
22.Pierleoni P, Concetti R, Belli A, Palma L (2019) Amazon, Google and Microsoft solutions for
IoT: architectures and a performance comparison. IEEE Access 8:5455–5470
23.Das A, Patterson S, Wittie M (2018) Edgebench: benchmarking edge computing platforms. In:
2018 IEEE/ACM international conference on utility and cloud computing companion (UCC
companion). IEEE, Piscataway, pp 175–180
24.Goh PJ, Hoe ZY , Low CY , Koh CT, Mohammad U, Lee K, Tan CF (2021) Conceptual design
of cloud-based data pipeline for smart factory. In: Symposium on intelligent manufacturing
and mechatronics. Springer, Singapore, pp 29–39
25.Izquierdo DP. Serverless architecture for data processing and detecting anomalies in MARSIS
instrument
26.www.statista.com ,https://www.statista.com/statistics/967365/worldwide-cloud-infra-structure-
services-market-share-vendor
27.Lee J, Qiu H, Yu G, Lin J, Rexnord Technical Services (2007) IMS, University of Cincinnati.
Bearing data set. NASA Ames Prognostics Data Repository. http://ti.arc.nasa.gov/project/
prognostic-data-repository . NASA Ames Research Center, Moffett Field, CA
28.Drahoš P, Ku ˇcera E, Haffner O, Klimo I (2018) Trends in industrial communication and OPC
UA. In: 2018 cybernetics & informatics (K&I). IEEE, Piscataway, pp 1–5
29.Muhammed AS, Ucuz D (2020) Comparison of the IoT platform vendors, Microsoft Azure,
Amazon Web Services, and Google Cloud, from users’ perspectives. In: 2020 8th international
symposium on digital forensics and security (ISDFS), pp 1–4 (2020). https://doi.org/10.1109/
ISDFS49300.2020.9116254
30.Wang Z, Wang P, Louis PC, Wheless LE, Huo Y (2021) Wearmask: fast in-browser face mask
detection with serverless edge computing for covid-19. arXiv preprint arXiv:2101.00784
31.Taylor SJ, Letham B (2018). Forecasting at scale. Am Stat 72(1):37–45
32.Facebook’s Prophet. https://facebook.github.io/prophet/Integration of Predictive Analytics
and Cloud Computing for Mental Health
Prediction
Akash Nag, Maddhuja Sen, and Jyotiraditya Saha
1 Introduction
According to the World health organization, health is not merely a condition of not
having any illness or any physical disruptions. Rather, it can be presumed as “The
state of complete physical, mental and social wellbeing and not merely the absence
of disease or inﬁrmity”. Mental health disorders possess a substantial threat to a
good health condition of an individual, and to society as a whole. It is due to these
reasons that the medical ﬁeld has decided to make the abstinence and treatment
of mental disorders a public health priority. However, the main hurdle in this
process is the difﬁculties faced in properly identifying and marking the symptoms
of mental health disorders among the masses. Incorrect treatment resulting due
to improper diagnosis of the symptoms can lead to further deterioration of the
health conditions [ 1]. Due to the high demand for sustainable mental health care
services, the internet gave several psychological interventions that would provide
treatment based on evidence and symptoms taken from the patient. However, most
of the studies focussed mainly on using a single matric to determine mental health
deterioration. Other studies suggest that it is unreliable to depend on only a single
matric, especially when it comes to a complicated and extrinsic system such as
our own mental health itself [ 2]. Machine learning plays an important role in
predicting the mental health and state of a person precisely than what can be done by
humans. Supervised learning is often used for prediction on the advantage that it can
account even for the complex relationships that couldn’t be identiﬁed by the other
algorithms. It becomes very handy in case the datasets become long and complex
A. Nag (/envelopeback) · M. Sen · J. Saha
School of Computer Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar,
Odisha, India
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_8133134 A. Nag et al.
and when properly deployed, can predict better than human intelligence predicts the
mental state of the person.
The above Fig. 1shows the different steps in predicting the mental state of a
person using machine learning. First, the dataset is imported which will be trained
later to predict whether a person is suffering from any mental illnesses or not. All
Fig. 1 Steps to identify mental state using machine learningIntegration of Predictive Analytics and Cloud Computing for Mental Health Prediction 135
the libraries required are imported. The dataset is ﬁne-tuned to increase the accuracy
of the model. Then several models are trained with the data. The model with the
highest accuracy is selected and it is tested. Finally, the model can be used with
any website or app and can be made user friendly. A huge portion of the studies
has used the classiﬁcation for diagnosis of the mental state of a person. The main
fault behind this technique is that it does not consider every single aspect of the
mental state of the person. It is due to the fewer insights on various assumptions of
machine learning techniques for mental state prediction [ 3]. The main purpose of
this research paper is to deliver an extensive report of all the ML strategies that can
be applied to properly determine the mental health disorders and imply the method,
which can be applied on a mass scale to make the health service accessible and
sustainable to every section of the society [ 4]. We will examine whether the mentally
disturbed patients actually manifest the internet-based cognitive behavioral therapy
(iCBT) for their acute depression and anxiety issues [ 5].
The main purpose of this paper is-
1.Give a brief introduction about the machine learning procedures and the algo-
rithms required for mental health disorder detection
2.Explore the schematic steps that are to be followed in order to generate a mental
health prediction process.
3.To explore the precision, challenges and limitations of mental health disorder
detection using Open Storage Network data.
4.Examine the efﬁciency of iCBT technology for mental health disorder detection
using SilverCloud Health
2 Method of Approach
2.1 Overview of the Subject
Research has been done in this ﬁeld for a long time, mostly by the computer science
groups. Their approach is different from the clinical groups studying behavioural
sciences. Computer scientists are more focused on problem-solving. They tend to
gather a lot of data and build classiﬁcation models then train them with the gathered
data. They try to keep the accuracy of the predictions made by the models as high
as possible and keep a check on overﬁtting and underﬁtting. This approach of
study is completely different from those of clinical scientists. They tend to focus
on conﬁrmation and their study is designed to test answers to questions. The risk
factor in the study of computer scientists is much more compared to that of clinical
scientists. Clinical scientists try to eliminate as many threats as possible and try to
reduce the risk whereas computer scientists focus on ﬁnding the best solution. In this
study machine learning is used. The main goal of machine learning is to identify the
relationship between the data and the patterns among them. With the help of these136 A. Nag et al.
relationships and patterns, the machine learning model will predict the output. We
will try to understand two types of machine learning:
1.Supervised machine learning
2.Unsupervised machine learning
2.1.1 Supervised Learning
The predictions are based on data from the previous solutions, where the joint values
of all of the variables are known. This is called supervised learning or “learning
with a teacher.” It is a type of machine learning where the machine works under
guidance. When a teacher teaches her students and prepares them for the exams, it
is an example of supervised learning. The machine is fed with labeled data and is
explicitly given the input with its corresponding output. The machine is trained with
labeled data and when it is tested with data, it will predict the corresponding output.
For example, simple linear regression. The equation of linear regression is:
y=mx+c (1)
In the above Fig. 2, ‘y’ is the output variable, ‘x’ is the input variable, ‘m’ is the
coefﬁcient of ‘x’ and ‘c’ is the intercept. The value of y is predicted depending on
x, where m and c are the constants. First, a machine learning model is trained with
data (labeled data) where corresponding values of y for x are present. Then when
the model is tested with values for x as an input. It will predict a value for y.
2.1.2 Unsupervised Learning
Unsupervised learning is a type of machine learning where the machine can
determine certain patterns in data and group similar data together. It does not get any
guidance while training. It has to ﬁgure out different patterns in data during training
and make predictions about the output. Example: K Means clustering. Clustering is
a process of dividing data into similar data groups. Points in a cluster are as similar
as possible and points in different clusters are as dissimilar as possible. The main
objective of a K Means clustering algorithm is that it identiﬁes a pattern in a mixture
of data and segregates similar data points into clusters. The ‘K’ in K Means indicates
the number of clusters.
The above Fig. 3is a representation of K Means clustering where data of similar
types are divided into groups.Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 137
Fig. 2 Linear regression
Fig. 3 Clustering
138 A. Nag et al.
2.2 Selection of Papers
This section gives a detailed explanation of the strategies and methodologies that
were adopted by the authors in order to prepare the paper. We adopted the PRSIMA
guidelines for the systematic review The ﬁelds that were looked for are-
(A) Machine Learning
(B) Natural language processing
(C) Mental health detection using ML
(D) Use of iCBT to detect Mental disorders
The main objective of this paper is to elaborate on the usage of ML and its branches
to efﬁciently detect the mental health condition of a patient.
It will list out the previous studies done and take references. This paper aims
to conclusively answer the following analytical questions regarding mental health
analysis:
1.How can sensors be used in receiving the data for mental data analysis?
2.How can ML and DL properly solve the problem of mental health analysis?
3.How to properly channel the raw data to converge at a conclusion?
4.How did patients respond to the iCBT platform?
2.3 Literature Search Strategy
Literature search for this paper was primarily done from databases like Scopus,
WoS, and ScienceDirect. Health-related databases like PsycInfo and PubMed were
also included. Each database was thoroughly covered and relevant papers werepicked up. The search was ﬁrst conducted in the Health ﬁeld papers, later the rest
papers were collected from IT databases. The sea search was carried on using the
following keywords: (“Machine learning” OR “Natural language processing”) AND(“mental health” OR “mental disorder” OR” mental disturbance”). Other keywords
that were searched for are “iCBT”, “big data”, and “OSNs”. No speciﬁc date range
was ﬁxed in the search.
2.4 Study Selection
Articles and papers were selected according to the inclusion-exclusion principlealready set by the authors.
Inclusion Principle:-
1.The paper or article listed a method by which ML could be applied in detectingmental health issues.Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 139
2.The paper explained the process of ML and NLP exclusively for mental health
analysis
3.The paper listed a way of examining iCBT effectiveness in determining mental
health.
4.The paper was published by a peer-reviewed publisher
5.The language of the paper was English
Exclusion Principle:-
1.The article didn’t raise the issue of mental health
2.The text of the paper was not available
3.The paper was not in the English language
4.The paper didn’t satisfy the inclusion principle.
We used two independent reviewers who went through the paper and supervised the
studies done.
2.5 Data Extraction and Analysis
Data were extracted from the papers according to the following criteria:
1.The topic of the paper
2.The data type used
3.ML algorithms used
4.The Discipline the paper followed. i.e., it is a medical or IT related paper
In order to analyze a wide range of data regarding ML and its application in mental
health analysis, a narrative review was adopted as a review method as shown in
Fig.4.
3 Introduction to Mental Health Research
The behavioural characteristics of a person are often the window to his feelings and
emotions, or rather, the mental state of the person itself. When we interact with
someone, we actually infer their psychological state, their emotions, beliefs and
feelings. However, it is as complex as it seems. Our behaviour and physiological
states are cross-media channelled and encrypted at different ﬁne scaling platforms,
like a slow vocal pinch to express annoyance and anger, to a smirk depicting
arrogance. Also, the behavioural characteristics vary from person to person, so there
is no justiﬁed answer to whether the same expression carries the same meaning for
all individuals at the same time [ 6].
Behavioural characteristics play an important role in determining the mental state
and disorder of a person. For example, soft and non-articulated speech often refers to140 A. Nag et al.
Fig. 4 Method of research
the symptoms of Parkinson’s disease, whereas autism can be determined by a. poor
eye contact b. poor conversational skill and c. abnormal speech rate and rhythm.
The extent of behavioural and neural disorders is vast. Not only does it require
billions of dollars every year for its treatment, but also, it depletes the health of the
patient as rapidly as any deadly disease. NIMH, in their studies, has indicated that
it takes almost US$300 billion every year for the treatment of mentally unstable
patients. Thus, research that increases the awareness regarding health awareness as
well as treatment quality [ 7].
The above Fig. 5shows the different types of mental illnesses a person
may suffer. OCD (Obsessive-compulsive disorder) where a person has fears and
obsessions, which in turn leads to repetitive behaviour and agony. Paranoia, where
a person feels threatened by others or someone wants to hurt him even though such
types of things are not happening in reality. People suffering from paranoia are not
able to trust anyone. Panic attack, is a feeling of intense fear and anxiety. It occurs
due to longtime stress and excessive physical exercise. PTSD (Post Traumatic Stress
Disorder) occurs when a person fails to recover from a traumatic event. Depression,
is a mood disorder where a person feels sad for a long term and loses interest in
things. People suffering from bipolar disorder goes through extensive depression
and mood swings. People’s experience in bipolar disorder is different from the other.
Schizophrenia is a very serious mental disorder where people interpret the reality
incorrectly. This disorder might cause hallucinations and may impair the thinking
process of that person.
The basic procedure of behaviour analysis has remained the same over time.
However, there are several limitations, even though humans are splendid signal
processors. A huge set of manpower is required to label large datasets to work
constantly. Another obstacle is that the process of labelling data by humans isIntegration of Predictive Analytics and Cloud Computing for Mental Health Prediction 141
Fig. 5 Types of Mental Illness
limited to a certain extent. We can only observe the exterior expressions, and cannot
reach a perfect conclusion considering the fact that their behaviour may change by
various factors like fatigue and mood [ 8].
Conclusively speaking, the main problem that the experts are facing is determin-
ing the subliminal feelings and emotions under superior behavioural characteristics.
These hidden attributes can be identiﬁed by various signals and novel signal
processors and using machine learning, it becomes easier to identify the symptoms
to identify various possible mental health issues. The sequence starts off with
raw data and signals from the patients, like auditory, visual and physiological
sensors to detect and perceive the behavioural signals. After localizing the signals
incoming from the various behavioural channels, noise is removed and modelled
to extract the required information, such as the way they speak, or their non-verbal
communications at large. Finally, after achieving all the behavioural signals and
applying ML algorithms to the data, an inference could be reached about the mental
state of the patient [ 9].
3.1 Machine Learning in Big Data
Every day millions of data are generated from social media, mobile phones, the
internet, etc. which cannot be managed by computer systems (traditional database
systems). Such a huge amount of data is referred to as Big Data. It is deﬁned by142 A. Nag et al.
Fig. 6 Neural Network
the 5 V’s, volume, velocity, variety, veracity and value. For ﬁrms trying to optimize
the value of their data, using machine learning algorithms for big data analytics is
a potential move. Machine learning tools use data-driven algorithms and statistical
models to examine data sets and then make conclusions or predictions based on
those patterns. In contrast to typical rule-based analytics systems, which follow
explicit instructions, the algorithms learn from the data as they run against it [ 10].
3.2 Deep Learning in Healthcare
Deep learning is advanced Machine Learning. It can be supervised, unsupervised
and semi-supervised. The deep learning architectures are used in computer vision,
natural language processing, language translation, etc. In computer vision, deep
learning is used to extract text. From images, tagging visual features here the
image description generated by computer vision is based on a set of thousands of
recognizable objects, which can be used to suggest tags for the image then it is
used for detecting objects, brands and faces. In Natural language processing, deep
learning is used for speech recognition, to convert text to speech and vice versa, the
semantic interpretation of the text, etc. Neural Network is the heart of deep learning
where there are several layers consisting of nodes. The layers are connected with the
help of the nodes. Neural networks are used to ﬁnd the hidden patterns in the data
and classify them. The training of neural networks relies on increasing the accuracy
of the predictions over time [ 11]. A simple neural network is shown in Fig. 6.
Some of the applications of deep learning areIntegration of Predictive Analytics and Cloud Computing for Mental Health Prediction 143
1.Cancer Prognosis: Images of tissues are fed to a deep learning model, where
different patterns are identiﬁed and it predicts whether the patient has cancer or
not [12].
2.Drug Discovery: Discovering a drug is a complicated task, it takes a lot of time
to ﬁnd a combination of compounds which will cure the target disease. Deep
learning makes this task a lot easier and more cost-effective. Deep learning
algorithms can predict the compounds required and their amounts to make a
certain drug which will fulﬁl the criteria and cure the target disease.
3.Medical Imaging and Diagnostics: Deep learning algorithms can analyze medical
images like MRI, CT Scans, X rays, etc. and detect if there is any anomaly
present in the medical images or not and also determine its risk factor.
4.Simplifying Clinical Trials: Deep learning algorithms can be used to segregate
people with critical health conditions from those with mild symptoms. So that
doctors can treat them accordingly. Deep learning algorithms can also be used to
monitor these procedures continuously without any human intervention and with
minimum errors.
3.3 Natural Language Processing
Natural Language Processing (NLP) is a branch of Artiﬁcial Intelligence which lets
computers understand the text and recognize speech. It can help machines to analyze
text, extract key phrases and detect entities, like places, people, etc. It also helps
in speech recognition where spoken are taken and converted into data that can be
processed else it can also be converted to text and vice versa (text to speech). NLP
can be used to analyze the semantic context of the text [ 13].
4 The Pipeline of Data Flows from the Sensors
to the Algorithmic Approach
The main objective of deploying machine learning algorithms and sensing mecha-
nisms is to transform the high volume of raw sensor information into meaningful
behavioural and psychological markers that can help us reach a conclusion about
the state of mental health of the patient. There are several methods of perceiving
the sensed information into meaningful data. But here, we are deploying a layered
approach, where, in each layer, there is a set of devices and issues with its internal
processes to conceive the required information. After converting the raw sensor
data to meaningful markers, the entire state of the data is used to conclude the
physiological state of the person [ 14].144 A. Nag et al.
4.1 Sensor Data
Sensors perceive the raw information from the patient and its surroundings. For the
most part, sensor data without denoising and classiﬁcation does not imply certainty
about the mental state of the patient. However, it is the sensors which retrieve the
raw data and transfer it for processing. The sensors could be of any device, such as
a mobile phone, wearables or even GPS or wiﬁ [ 15].
4.2 Extraction of Features
As already discussed, the raw data is of minimal value. Only the features extracted
are of importance for predicting the mental state of the patient. For example, if we
are interested in a phone call, we can track the number of phone calls at a time
and their duration. Alternatively, a high number of missed calls can also be tracked
to perceive whether the patient is in panic or not. Similarly, SMS message count
and their frequency can also contribute as features. On top, new features can also
be derived using statistics and algorithms such as stacked autoencoders and slow
feature analysis [ 16]. This can help in discovering new features.
4.3 Designing the Behavioural Markers
Behavioral and physiological markers are higher-level features that reﬂect the
cognitions and emotions derived from lower-level sensor data. For example, bedtime
and phone usage can reﬂect sleep deprivation, low SMS count and phone calls can
derive social avoidance and activity type and movement intensity can conclude the
status of the psychomotor activity and fatigue condition of the patient. Additionally,
the precision of the data can be improved by enriching it with additional features,
such as age and marital status in determining sleep deprivation along with bedtime
and phone usage. Similarly, the working condition of the patient, previous track
record, family problems and physical health issues can also act as additional
improving features [ 17].
4.4 Clinical Target
There is no use if we try to diagnose the symptoms without even thinking about
possible clinical and mental health issues [ 18]. One or two questions answered
cannot precisely determine the clinical health of the patient. It is due to this reason
that ML is applied to a large number of behavioural and physiological markers inIntegration of Predictive Analytics and Cloud Computing for Mental Health Prediction 145
Fig. 7 Architecture to decode raw data for mental state analysis
order to modestly determine the clinical target. On an important note, the symptoms
and clinical targets may not have a one-to-one relationship with each other. Some
markers may not be detectable, whereas some markers may be uncovered by
personal sensing [ 19]F i g . 7depicts the general model for mental state analysis.
5 Cloud Computing
Cloud Computing is anything that involves sharing of resources over the internet. A
cloud can be both private and public. In public, it shares the resources with everyone,
but in private the cloud shares resources with only certain organizations, those who
have paid for those resources. With the help of cloud computing sharing of resources
over the internet has become a lot faster and easier [ 20]. A sample demonstration of
cloud computing is shown in Fig. 8.
5.1 Architecture of Cloud Computing
Cloud computing is divided into two parts, the front end and the back end. It
provides the applications and the interfaces required by the cloud-based services.146 A. Nag et al.
Fig. 8 Cloud Computing
The front end is the infrastructure which is facing the internet, the user can access
the cloud with the help of the front end. The back end infrastructure of the cloud
manages all the programs and applications which are going on in the frontend
infrastructure. Whatever support is required to run the web application in the
frontend, is present in the back end. The backend infrastructure is the backbone
of the cloud computing architecture.
In healthcare cloud computing provides the user with services like patient health
records, transferring data, storing data, etc. The patient health records can be
transferred from one point to another point or they can also be stored securely with
the help of frontend and backend infrastructures. Cloud computing will ensure that
conﬁdentiality of the records is maintained and they are transferred to a preferable
location at a high speed. The architecture of cloud computing is shown in Fig. 9.
5.2 Beneﬁts of Cloud Computing in the Healthcare Industry
The on-demand availability, internet-based service and high demand availability of
cloud computing have converted healthcare into health tech.
(A) Collaboration
Details related to healthcare can be easily shared among healthcare personnel like
doctors, nurses etc. with the help of cloud computing. Conﬁdentiality is maintained
while sharing the data. This data can be remotely accessed if any changes are made,
they will be reﬂected instantly [ 21].Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 147
Fig. 9 Architecture of Cloud Computing148 A. Nag et al.
(B) Security
It is the responsibility of the healthcare department to keep the data of the patients
private while sharing. So, cloud computing provides security and maintains the
conﬁdentiality of the data when it is shared.
(C) Cost
An enormous amount of data can be stored/accessed/shared using cloud computing
at a very low cost. It follows, ‘pay-per-use’ where the users have to pay only when
they are using it [ 22].
(D) Speed
Data can be transferred at a very high speed here. The health records of the patients
can be transferred easily to the doctors and the nurses at a high speed with the help
of cloud computing [ 23].
(E) Scalability
The data generated by the healthcare industries are dynamic in nature, i.e. they
can be changed anytime. Cloud computing provides the healthcare industries with
various facilities like mobile applications, big data analytics, electronic medical
records, etc. All these facilities are scalable [ 24].
(F)Flexible
All these facilities provided to the healthcare industry by cloud computing can be
used by anyone, anywhere. Hence it is very ﬂexible.
5.3 Cloud Computing as a Solution to Mental Health Issues
Cloud computing not only makes customer service interactions more accessible to
everyone but also provides medical solutions. Medical specialists, for example, have
employed augmented and Virtual Reality driven by cloud computing to assess their
efﬁcacy in treating mental illnesses.
According to an article published by Cambridge University, virtual reality might
aid clinicians in better understanding the causes of mental illnesses including
anxiety, PTSD, and depression. Several types of research were undertaken to
determine the utility of VR and AR in the medical area, and the article was based
on their ﬁndings. The authors of the article claim that virtual reality seems “to
perform comparably in efﬁcacy to face-to-face equivalent interventions”, implying
that cloud-based computing might be a component of the future treatment of mental
illnesses.
While many consumers are apprehensive about shifting everything to the cloud,
experts and professionals are enthusiastic about the beneﬁts that cloud computing
offers. It is now up to businesses and specialists to educate more individuals about
the beneﬁts of cloud computing.Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 149
6 Review of Personal Sensing Research
After reviewing several research papers, we came to the conclusion that most of
the studies have been done on the basis of mobile phone sensors. Table 1illustrates
various factors to determine mental health of patients.
Mobile phones have become an integral part of our lives. There are several factors
why most of the studies are based on mobile phone sensors. Firstly, almost 76% of
Americans own a smartphone. Secondly, nowadays, smartphones come with a lot of
Table 1 Factors to determine the mental health of a patient
Factors Study Done Suggestions
Sleep Disturbance in mental health can be reﬂected
in the sleep period and quality. It has been seen
that the sleep cycle and sleep quality can be
determined almost with 90% precision without
any user intervention. This can be achieved by
the usage of sensors like:
1. Microphone
2. Accelerometer
3. Light sensor
4. Screen state
5. Battery usage
6. Proximity sensor
7. Running processesAbdullah et al. [ 26] suggested
that the chronotype of a person
whether he is a night owl or a
morning lark can be predicted
by determining the sleep-wake
cycle on non-work days. The
workday sleep cycle determines
the external requirements of the
person which can also be called
social jet lag. Wang et al. [ 27],
in their paper, related the
sleep-wake cycle of a person to
the severity of stress and
depression of the patient.
Social context Since the focus is being given to
smartphone-based research, the perspective of
social context can be easily judged by the
social media of that person. Population based
smartp [hone datasets are often large enough
and can give us possible results about the
proximity of the person and movement of the
same. This can, in turn, derive the results about
the possible relationships of the patient.
Calling and SMS networks can also detect the
possible relationships and mental state of the
patient. For example, calling up a
non-colleague during work hours often implies
non-satisfaction with the work environment.
Other factors that can add up to the sources
are:
1. Contact lists
2. Regularity of incoming calls and messages
3. Regularity of outgoing calls and messages,
However, this does not necessarily mean that
lower calls imply far relationships. We often
tend to have conversations with some close
people over Snapchat or WhatsApp too.Eagle et al. [ 28] Identiﬁed
friends and non-friends on a
higher precisive scale with the
help of Bluetooth sensing
devices that can detect other
bluetooth sensors up to a radius
of 15 ms.
In the case of perceiving the
patterns of call and SMS, it has
been seen that longer calls have
been associated with family
members. Work calls were
lower on Sundays and were
characterized by low SMS
frequency.
(continued)150 A. Nag et al.
Table 1 (continued)
Factors Study Done Suggestions
Mood and
stressMood and stress detection are not
easily detectable by direct sensors.
Rather, it can be predicted by
studying the trends of normal sensors.
For example, by studying the trends
of a decrease in phone calls or less
movement from the house, the stress
level can be predicted. On the other
hand, anxiety often results in more
phone calls, within a short span of
time.
The number of apps used and web
history also predicts the daily mood
of the patient with 66% accuracyCiman et al. [ 29] attempted to detect
the mood and stress level of the
patient by using swipe, roll and text
input interaction. It is evident from
the paper that stress can be perceived
keyboard and mouse interaction
Ma et al. [ 30] Used location and
motion detectors, along with the
surrounding ambience to detect the
mental state of the patient with 50%
accuracy.
Calvo and d’Mello [ 31] suggested in
their paper that mood and stress could
be detected with the help of
paralinguistic features of the person,
such as the action and tones of their
speech.
integrated sensors, which makes it easy to receive and transmit information. Also,
it has been seen that on average, we tend to check our smartphones 85 times per
day. Hence, behavioural markers are potentially more effective using smartphone
devices [ 25].
7 Result of the Research
Table 2depicts the tabular representation of study done regarding the algorithms
used for detecting the mental state of a person.
7.1 Limitations of the Study Done on the Algorithms to Detect
Mental Health
1.Personal sensing systems will need a broad user base to be universally applicable
due to the vast diversity of technology, device usage habits, lifestyle, and
environmental factors.
2.A contradiction between what is conceivable and what is practicable is expected
to emerge in the area of personal sensing, which is connected to a trade-off
that happens between tiny proof-of-concept studies exhibiting innovation and
big research proving robustness and universal application.Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 151
Table 2 Summary of studies done to detect mental health of a patient
Mental health detective
problemMachine learning technique
appliedDatasets used
1. Alzheimer’s disease A. Active learning methodology
(Qian et al.) [ 32]
B. Regression (Westman et al.)
[33]
C. SVM (Costafrede et al.) [ 34]
D. kNN (Ertek et al.) [ 35]
E. Similarity discriminative
DLA (Li et al.) [ 36]A. Electronic health and patient
records (Qian et al.)
B. Imaging data .
2. Autism Spectrum
disorderA. Authors had their own
classiﬁer developed in the
papers (Yahata et al.) [ 37]
B. K-means clustering (Liu et
al.) [ 38]
C. SVM (Jiao et al.) [ 39]
D. kNN (Oh et al.) [ 40]
E. L2LR (Pitt et al.) [ 41]A. Imaging data (Jiao et al., Pitt
et al., Yahata et al.)
B. B. biological data (Jiao et al.,
Oh et al.)
C. Photographic evidence (Liu
et al.)
Depression‘ A. Adaboost (Liang et al.) [ 42]
B. Classiﬁcation (Hajek et al.)
[43]
C. Bayes (Wang et al.) [ 44]
D. Clustering (Dipnall et al.)
[45]
E. Gaussian process (Mitra et
al.) [ 46]
F. Deep learning (Kang et al.)
[47]
G. K means clustering
(Wardenaar et al.) [ 48]
kNN (Zhang et al.) [ 49]Clinical test samples (Liang et
al.)
Social media evidence (Wang et
al.)
Audio samples (Mitra et al.)
Suicide and self-harm A. Adaboost (Pestian et al.) [ 50]
B. kNN (Tran et al.) [ 51]
C. NLP (Pestian et al.) [ 52]
D. Regression (Pestian et al.,
Zhang et al.) [ 52]
E. SVM (Zhou et al.) [ 53]
F. RF (Hettige et al.) [ 54]Audio samples
Electronic health records (Tran
et al.)
Letters and communicating
mediums (Pestian et al.)
Video samples (Zhou et al.)
Stress A. Conditional random
functions (Moulahi et al.) [ 55]
B. K means clustering (Hagad
et al.) [ 56]
C. Regression (Stutz et al.) [ 57]
D. SVM (Chiang et al.) [ 58]
E. RF (Maxhuni et al.) [ 59]
E. RF (Maxhuni et al.)Mobile and wearables (Stutz et
al., Maxhui et al.)
Physiological sensors (Hagad et
al.)
(continued)152 A. Nag et al.
Table 2 (continued)
Mental health detective
problemMachine learning technique
appliedDatasets used
Schizophrenia A. Gaussian Process (Taylor et
al., 2017) [ 60]
B. k-means clustering
(Castellani et al., 2009) [ 61]
C. Multivariate analysis (Skåtun
et al., 2016) [ 62]
D. Regression (Strous et al.,
2009; Hettige et al., 2017) [ 63]
SVM (Castellani et al., 2009,
2012; Hess et al., 2016; Mikolas
et al., 2016) [ 64–66]Imaging data (all)
Dementia A. Ensemble learning (Chen
and Herskovits)
B. NB (Bhagyashree et al.) [ 67]
C. NN (Kumari et al.) [ 68]
D. SVM (Diniz et al.) [ 69]Imaging data (Chen and
Herskovits, Kumari)
Survey data (Kumari et al.)
3.As no sensing system can be perfect, researchers, developers, and users must
agree on the amount of error that is acceptable, as well as how to effectively
communicate and demonstrate the errors to important stakeholders.
4.Certain types of data, such as GPS tracking, cannot be de-identiﬁed while still
being useful. Building trust in these systems among the participants will need
a recognition of the primacy of the users, which will be realized by allowing
individuals to comprehend, control, and own their data.
5.Enhancing systems will certainly need some participation of the user, as well as
ﬁguring out how to link actions to beneﬁts.
6.Improvements in infrastructure and workﬂow integration, as well as advance-
ments in underlying technical knowledge and algorithmic accuracy, will be
required for personal sensing.
7.2 Results Based on iCBT Test
Data was collected for a total of 54,606 people. The main inferences were:-
1.Meantime spent on the program =111.33 minutes
2.Mean Tools used =230.60
3.Mean Baseline score in PHQ-9 =12.96
4.Mean Baseline score in GAD-7 =11.85
5.Mean improvement in clinical score in PHQ-9 over 14 weeks =4.29
6.Mean improvement in clinical score in GAD-7 over 14 weeks =4.01Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 153
Table 3 Classes of patients interacting with iCBT platform
Class percentage Description
1. Class 1(low
engagers)36.5% They were seen to have the lowest engagement time and
eventually dropped out of the platform. They used fewer
sessions and engaged in fewer modules than other classes.
However, this class has been very interactive on the review
page, where they could vent their feelings with a supporter
They have been seen to engage with modules related to
mood monitoring and worry.
2. Class 2(late
engagers)21.4% They were characterized by a slower rate of disengagement
with time.
They abstained from the use of to-do lists.
Less possibility to look upon the hierarchy of fears
Engaged more with sections regarding anxiety and myths.
3. Class 3 high
engagers but
rapid
disengagement25.5% Showed a steep rate of disengagement
Initially had a high engagement curve
More likely to use advanced modules at earlier times
Less likely to use core modules like activity goals.
4. Class 4: High
engagers but less
disengagement6.0% Had almost a constant engagement rate
Took signiﬁcantly more programs and took up more
modules
They were more likely to interact with activity related goals
and attempted question which requires deep introspection
5. Class 5:
Highest
engagement10.6% Highest rate of engagement with the modules
The fact that distinguished this class from other classes is
that this group showed more interest in sleeping tips rather
than anxiety or worry related modules.
The distribution of engagement with the time was based on a Markov model. It
was a hidden layer in between, which provided the best optimal ﬁt. Its main perks
were:
1.Could identify 5 subtypes of engagement.
2.Engagement identiﬁed based on patient interaction rather than just binary
variable [ 70]
Table 3highlights some relevant studies critical for patients interacting with iCBT
platform.
8 Discussion
The review shows the rapidly growing ﬁeld of research and development of Machine
Learning in mental health. We will now address the existing approaches and future
directions based on three primary patterns and associated difﬁculties identiﬁed
during this assessment:154 A. Nag et al.
1.Identifying Important Health Care needs to inform ML development:
Our everyday actions and habits produce an ever-increasing cloud of digital
emissions. Some of the information is gathered on purpose, such as via theuse of wearables. However, most of the data is captured by our telephones,
computers, purchases, and the more sensor-enabled devices in our life as a
consequence of our daily actions. Several approaches and algorithms havebeen developed and recommended for diagnosing and treating mental health
concerns. There is still an opportunity for improvement in a number of
solutions. Furthermore, a wide range of machine learning parameters isbeing used to explore a variety of problems in terms of mental health.
The characteristics used in machine learning algorithms have a signiﬁcant
inﬂuence on classiﬁcation outcomes, given how difﬁcult it is to classify mentalhealth data in general.
According to recent studies and research, machine learning might be a valuable
tool for understanding mental diseases. Aside from that, it might help inrecognizing and diagnosing mental health conditions in individuals in advance
of future therapy. Newer strategies that make use of data generated by the
integration of several sensor modalities present in technologically advanceddevices have shown to be effective. The potential for mental health research
is enormous. However, there are signiﬁcant roadblocks. Even though the
potential of personal sensing for mental health has been established, movingfrom proof of concept to tools that are successful in broader populations has
huge hurdles. The sustained participation of users who give both passively
gathered data and some amount of active labeling will most likely decidethe long-term usefulness of personal sensing in mental health. To avoid
obsolescence, this infrastructure may need a social machine that interacts with
people.
In order to build conﬁdence in these systems, the user’s primacy must be
acknowledged, which will be done by allowing users to understand, manage,
and own their data. Despite the size of the responsibilities, the potentialrewards are enormous. The capacity to detect mental health behaviours on
a continuous basis has the potential to revolutionize care delivery by lowering
the time it takes to identify people who are at risk or in need of treatment.
2.Assessing the Real-World Efﬁcacy of ML interventions:
The bulk of the studies focused on the technical development of early machine
learning models. Despite one’s excitement, it’s vital not to exaggerate or
generalize predicted (clinical) advantages too quickly. In computers, researchis usually exploratory in character, with the goal of “ﬁnding” a solution. The
majority of clinical research is hypothesis-driven, with studies aimed to test
and “conﬁrm” answers.
These discipline differences are reﬂected in the data sources used for machine
learning analysis. Many social media studies add to the rigor of the experiment
by including “control” groups. However, they are often chosen at random fromIntegration of Predictive Analytics and Cloud Computing for Mental Health Prediction 155
a pool of online service users with little assurance that they are people with
mental health problems. More study is required as machine learning models
improve in technological advancement and accuracy.
Clinical experts can give important information on construct validity, ground
truth, and biases. During the study design and development process, MHPs
and the persons targeted by ML predictions should be included. Combiningthe Strengths of Different Data Methods to Overcome Methodological Lim-
itations and Improve Insights The data used to train ML models necessarily
limits the accuracy and dependability of the models. Error, uncertainty, andbias are all common problems with machine learning models.
3.Deeper examination of new ML systems in real-world scenarios to comprehendthe larger implications of new ML systems:
Machine learning has primarily assisted in the discovery and development
of basic (multidisciplinary) research breakthroughs in the area of mental
health. To continue on this early path toward real-world impact, HCI and ML
researchers will need to expand the invention and ﬁrst testing of innovativeML treatments. Laypeople’s Appropriate Understanding and Use of Machine
Learning Outputs For ML-enabled systems, producing interpretable and
(clinically) useful outputs is a big challenge. More research is needed toestablish how lay people may accurately grasp machine learning output.
Laypeople’s capacity to understand how speciﬁc (behavior) data and model
results connect to a health outcome may be limited. User interface designand interactive visualizations or simulations may be beneﬁcial in providing
detailed mappings for users. This is necessary so that lay people may adjust
their understanding of a system’s capabilities and limitations. Few studieshave looked at the long-term consequences of using machine learning models
in health care. Falsely diagnosing a mental disease has the potential to harm a
person’s self-esteem, reputation, and employment. This begs the question ofwho is responsible for ML system ﬂaws.
9 Conclusion
Our everyday actions and habits produce an ever-increasing cloud of digital
exhausts. Some of the data are purposefully generated but much of it is a result
of our everyday activities, gathered by our cellphones, laptops, purchases, andthe increasingly sensor-enabled objects in our lives. For assessing and addressing
mental health issues, several methodologies and algorithms have been established
and suggested. Several solutions still have room for improvement. Furthermore,numerous difﬁculties are currently being investigated in terms of mental health by
employing a broad range of parameters in machine learning. Given how difﬁcult it is
to categorize mental health data in general, the features utilized in machine learningalgorithms have a considerable impact on classiﬁcation results. Even though the156 A. Nag et al.
possibility of personal sensing for mental health has been demonstrated, enormous
challenges remain to move from proof of concept to tools that are effective in
larger populations. The long-term effectiveness of personal sensing in mental health
will most likely be determined by the continued engagement of the users who
provide both passively obtained data and some level of active labeling. To prevent
obsolescence, this may need an infrastructure i.e., a social machine that interacts
with users. The primacy of the user must be recognized for building trust in these
systems, which will be accomplished by enabling users to comprehend, control,
and own their data. Although the tasks are substantial, the potential beneﬁts are
game-changing. The ability to continually identify mental health behaviors has
the potential to transform care delivery by reducing the time it takes to identify
individuals who are at risk or who need treatment.
References
1.Miner L et al (2014) Practical predictive analytics and decisioning systems for medicine: infor-
matics accuracy and cost-effectiveness for healthcare administration and delivery including
medical research. Academic, Cambridge
2.Jena L, Mishra S, Nayak S, Ranjan P, Mishra MK (2021) Variable optimization in cervical
cancer data using particle swarm optimization. In: Advances in electronics, communication
and computing. Springer, Singapore, pp 147–153
3.Fleury A, Vacher M, Noury N (2010) SVM-based multimodal classiﬁcation of activities of
daily living in health smart homes: sensors, algorithms, and ﬁrst experimental results. IEEE
Trans Inf Technol Biomed 14(2):274–283
4.Mohapatra SK, Mishra S, Tripathy HK, Bhoi AK, Barsocchi P (2021) A pragmatic inves-
tigation of energy consumption and utilization models in the urban sector using predictive
intelligence approaches. Energies 14(13):3900
5.Jung Y , Yoon YI (2017) Multi-level assessment model for wellness service based on human
mental stress level. Multimed Tools Appl 76(9):11305–11317
6.Sangaiah AK, Samuel OW, Li X, Abdel-Basset M, Wang H (2017) Towards an efﬁcient risk
assessment in software pro jects–fuzzy reinforcement paradigm. Comput Electr Eng. https://
doi.org/10.1016/j.compeleceng.2017.07.022
7.Jena L, Kamila NK, Mishra S (2014) Privacy preserving distributed data mining with
evolutionary computing. In: Proceedings of the International Conference on Frontiers of
Intelligent Computing: Theory and Applications (FICTA) 2013. Springer, Cham, pp 259–267
8.Goodman R, Renfrew D, Mullick M (2000) Predicting type of psychiatric disorder from
Strengths and Difﬁculties Questionnaire (SDQ) scores in child mental health clinics in London
and Dhaka. Eur Child Adolesc Psychiatry 9(2):129–134
9.Mishra S, Tripathy HK, Panda AR (2018) An improved and adaptive attribute selection
technique to optimize dengue fever prediction. Int J Eng Technol 7:480–486
10.Panda AR, Mishra M (2018) Smart phone purchase prediction with 3-NN classiﬁer. J Adv Res
Dyn Control Syst:674–680
11.Milligan GW, Cooper MC (1987) Methodology review: clustering methods. Appl Psychol
Meas 11(4):329–354
12.Xu J et al (2011) On the properties of mean opinion scores for quality of experience
management. Multimedia (ISM), 2011 I.E. International Symposium on. IEEE
13.Suman S, Mishra S, Sahoo KS, Nayyar A (2022) Vision Navigator: A Smart and Intelligent
Obstacle Recognition Model for Visually Impaired Users. Mobile Information Systems, 2022Integration of Predictive Analytics and Cloud Computing for Mental Health Prediction 157
14.Gislason PO, Benediktsson JA, Sveinsson JR (2006) Random forests for land cover classiﬁca-
tion. Pattern Recogn Lett 27(4):294–300
15.Hiltz SR, Lee A, Imran M, Plotnick L, Power R, Turoff M (2020) International Journal of
Disaster Risk Reduction Exploring the usefulness and feasibility of software requirements
for social media use in emergency management. Int J Disaster Risk Reduct 42(October
2019):101367
16.Agrawal R, Srikant R (1994) Fast Algorithms for Mining Association Rules (expanded
version). Research Report IBM RJ9839. Proc. 20th Intl. Conf. VLDB:487–499
17.Mishra S, Tripathy HK, Thakkar HK, Garg D, Kotecha K, Pandya S (2021) An explainable
intelligence driven query prioritization using balanced decision tree approach for multi-level
psychological disorders assessment. Front Public Health:9
18.Khattar A, Quadri SMK (2020) Emerging role of artiﬁcial intelligence for disaster management
based on microblogged communication. SSRN Electron J
19.Andrews G, Bell C, Boyce P, Gale C, Lampe L, Marwat O et al (2018) Royal Australian
and New Zealand College of Psychiatrists clinical practice guidelines for the treatment of
panic disorder, social anxiety disorder and generalised anxiety disorder. Aust N Z J Psychiatry
52(12):1109–1172. https://doi.org/10.1177/0004867418799453
20.Tutica L, Vineel KSK, Mishra S, Mishra MK, Suman S (2021) Invoice deduction classiﬁcation
using LGBM prediction model. In: Advances in electronics, communication and computing.
Springer, Singapore, pp 127–137
21.Alyousef SM (2019) Psychosocial stress factors among mental health nursing students in KSA.
J Taibah Univ Med Sci 14(1):60–66. https://doi.org/10.1016/j.jtumed.2018.11.006
22.Mohammad M, Khan MB, Bashier EBM (2017) Algorithms and applications, vol 7. CRC
Press. https://doi.org/10.1007/978-94-017-2221-6_5
23.Akareem HS, Hossain SS (2016) Determinants of education quality: what makes stu-
dents’ perception different? Open Rev of Educ Res 3(1):52–67. https://doi.org/10.1080/
23265507.2016.1155167
24.Jabatan Pendidikan Tinggi (JPT) (2017) Direktori Universiti Awam. Retrieved from http://
jpt.mohe.gov.my/portal/ipta/institusi-pendidikan-tinggi-awam/direktori-universiti-awam
25.Khan S, Islam A, Hossen A, Zahangir T, Latiful Haque A (2018) Supporting the Treatment of
Mental Diseases using Data Mining:339–344. https://doi.org/10.1109/ICISET.2018.8745591 .
Parekh R (2018) What Is Mental Illness? Retrieved from https://www.psychiatry.org/patients-
families/what-is-mental-illness
26.Abdullah S, Matthews M, Murnane EL, Gay G, Choudhury T (2014) Towards circadian
computing: “early to bed and early to rise” makes some of us unhealthy and sleep deprived.
Proc. UbiComp ‘14: 2014 ACM Int. Joint Conf. Pervasive Ubiquitous Comput., Seattle, WA,
pp. 673–84. Assoc. Comput. Mach, New York
27.Wang R, Chen FL, Chen Z, Li TX, Farari G et al (2014) StudentLife: assessing mental health,
academic performance and behavioral trends of college students using smartphones. Proc.
UbiComp ‘14: 2014 ACM Int. Joint Conf. Pervasive Ubiquitous Comput., Seattle, WA, pp.
3–14. Assoc Comput Mach, New York
28.Eagle N, Pentland A, Lazer D (2009) Inferring friendship network structure by using mobile
phone data. PNAS 106:15274–15278
29.Ciman M, Wac K, Gaggi O (2015) Assessing stress through human-smartphone interaction
analysis. Pervasive Health ’15: Proc. 9th Int. Conf Pervasive Comput Technol Healthc, Istanbul
Brussels: Inst Comput Sci Social-Inform Telecom Eng. http://ieeexplore.ieee.org/document/
7349382/
30.Ma Y , Xu B, Bai Y , Sun G, Zhu H (2012) Daily mood assessment based on mobile phone
sensing. Proc. 2012 9th Int. conference wearable implant. Body Sens. Netw., London. IEEE,
Washington, DC, pp 142–147
31.Calvo RA, D’Mello S (2010) Affect detection: an interdisciplinary review of models, methods,
and their applications. IEEE Trans Affect Comput 1:18–37158 A. Nag et al.
32.Qian B, Wang X, Cao N, Li H, Jiang Y-G (2015) A relative similarity based method for
interactive patient risk prediction. Data Min Knowl Disc 29:1070–1093
33.Westman E, Aguilar C, Muehlboeck J-S, Simmons A (2013) Regional magnetic resonanceimaging measures for multivariate analysis in Alzheimer’s dis ease and mild cognitiveimpairment. Brain Topogr 26:9–23
34.Costafreda SG, Dinov ID, Tu Z, Shi Y , Liu C-Y , Kloszewska I, Mecocci P, Soininen H, TsolakiM, Vellas B, Wahlund L-O, Spenger C, Toga AW, Lovestone S, Simmons A (2011a) Automatedhippocampal shape analysis predicts the onset of dementia in mild cognitive impairment.NeuroImage 56:212–219
35.Ertek G, Tokdil B, Günaydın I (2014) Risk factors and identiﬁers for Alzheimer’s disease: adata mining analysis. In: Perner P (ed) Advances in data mining. Applications and theoreticalaspects. ICDM 2014. Lecture notes in computer science, vol 8557. Springer, Cham
36.Li Q, Zhao L, Xue Y , Jin L, Feng L (2017b) Exploring the impact of co-experiencingstressor events for teens stress forecasting. In: Bouguettaya A et al (eds) Web InformationSystems Engineering – WISE 2017. WISE 2017. Lecture notes in computer science, vol 10570.Springer, Cham, pp 313–328
37.Yahata N, Morimoto J, Hashimoto R, Lisi G, Shibata K, Kawakubo Y , Kuwabara H, KurodaM, Yamada T, Megumi F, Imamizu H, Náñez JE Sr, Takahashi H, Okamoto Y , Kasai K, KatoN, Sasaki Y , Watanabe T, Kawato M (2016) A small number of abnormal brain connectionspredicts adult autism spectrum disorder. Nat Commun 7:11254
38.Liu F, Guo W, Fouche J-P, Wang Y , Wang W, Ding J, Zeng L, Qiu C, Gong Q, Zhang W, ChenH (2015a) Multivariate classiﬁcation of social anxiety disorder using whole brain functional
connectivity. Brain Struct Funct 220:101–115
39.Jiao Y , Chen R, Ke X, Chu K, Lu Z, Herskovits EH (2010) Predictive models of autism
spectrum disorder based on brain regional cortical thick ness. NeuroImage 50:589–599
40.Oh DH, Kim IB, Kim SH, Ahn DH (2017) Predicting autism spectrum disorder using blood-based gene expression signatures and machine learning. Clin Psychopharmacol Neurosci15:47–52
41.Plitt M, Barnes KA, Martin A (2015) Functional connectivity classiﬁcation of autism identiﬁeshighly predictive brain features but falls short of biomarker standards. NeuroImage. Clinical7:359–366
42.Liang X, Gu S, Deng J, Gao Z, Zhang Z, Shen D (2015) Investigation of college students’mental health status via semantic analysis of Sina microblog. Wuhan University Journal ofNatural Sciences 20:159–164
43.Hajek T, Franke K, Kolenic M, Capkova J, Matejka M, Propper L, Uher R, Stopkova P, NovakT, Paus T, Kopecek M, Spaniel F, Alda M (2017) Brain age in early stages of bipolar disordersor schizophrenia. Schizophr Bull 45:190–198
44.Wang S-H, Zhang Y , Li Y-J, Jia W-J, Liu F-Y , Yang M-M, Zhang Y-D (2018) Single slicebased detection for Alzheimer’s disease via wavelet entropy and multilayer perceptron trainedby biogeography-based optimization. Multimed Tools Appl 77:10393–10417
45.Dipnall JF, Pasco JA, Berk M, Williams LJ, Dodd S, Jacka FN, Meyer D (2016b) Intothe bowels of depression: unravelling medical symptoms associated with depression byapplying machine-learning techniques to a community-based population sample. PLoS One11:e0167055
46.Mitra V , Shriberg E, McLaren M, Kathol A, Richey C, Vergyri D, Graciarena M (2014) TheSRI A VEC-2014 evaluation system. In: Proceedings of the 4th International Workshop onAudio/Visual Emotion Challenge A VEC ‘14. ACM, New York, pp 93–101
47.Kang Y , Jiang X, Yin Y , Shang Y , Zhou X (2017) Deep transformation learning for depressiondiagnosis from facial images. In: Zhou J et al (eds) Biometric recognition. CCBR 2017. Lecturenotes in computer science, vol 10568. Springer, Cham, pp 13–22
48.Wardenaar KJ, van Loo HM, Cai T, Fava M, Gruber MJ, Li J, de Jonge P, Nierenberg AA,Petukhova MV , Rose S, Sampson NA, Schoevers RA, Wilcox MA, Alonso J, Bromet EJ,Bunting B, Florescu SE, Fukao A, Gureje O, Hu C, Huang YQ, Karam AN, Levinson D,Medina Mora ME, Posada-Villa J, Scott KM, Taib NI, Viana MC, Xavier M, Zarkov Z, KesslerIntegration of Predictive Analytics and Cloud Computing for Mental Health Prediction 159
RC (2014) The effects of co-morbidity in deﬁning major depression subtypes associated with
long-term course and severity. Psychol Med 44:3289–3302
49.Zhang J, Xiong H, Huang Y , Wu H, Leach K, Barnes LE (2015a) M-SEQ: early detection ofanxiety and depression via temporal orders of diagnoses in electronic health data. In: 2015IEEE international conference on big data (big data), Santa Clara, pp 2569–2577
50.Pestian JP, Matykiewicz P, Grupp-Phelan J (2008) Using natural language processing toclassify suicide notes. In: Proceedings of the workshop on current trends in biomedical naturallanguage processing BioNLP ‘08. Association for Computational Linguistics, Stroudsburg, pp96–97
51.Tran T, Kavuluru R (2017) Predicting mental conditions based on ‘history of present illness’in psychiatric notes with deep neural networks. J Biomed Inform 75S:S138–S148
52.Pestian J, Nasrallah H, Matykiewicz P, Bennett A, Leenaars A (2010) Suicide note classiﬁ-cation using natural language processing: a content analysis. Biomedical Informatics Insights2010:19–28
53.Zhou D, Luo J, Silenzio V , Zhou Y , Hu J, Currier G (2015) Tackling mental health byintegrating unobtrusive multimodal sensing. In: Proceedings of the Twenty-Ninth AAAIConference on Artiﬁcial Intelligence (AAAI 2015). AAAI Press, pp 1401–1408
54.Hettige NC, Nguyen TB, Yuan C, Rajakulendran T, Baddour J, Bhagwat N, Bani-Fatemi A,V oineskos AN, Mallar Chakravarty M, De Luca V (2017) Classiﬁcation of suicide attemptersin schizophrenia using sociocultural and clinical features: a machine learning approach. GenHosp Psychiatry 47:20–28
55.Moulahi B, Azé J, Bringay S (2017) DARE to care: a context-aware frame work to track
suicidal ideation on social media. In: Bouguettaya A et al (eds) Web information systems
engineering – WISE 2017. WISE 2017. Lecture notes in computer science, vol 10570.Springer, Cham, pp 346–353
56.Hagad JL, Moriyama K, Fukui K, Numao M (2014) Modeling work stress using heart rate andstress coping proﬁles. In: Baldoni M et al (eds) Principles and practice of multi-agent systems.CMNA 2015, IWEC 2015, IWEC 2014. Lecture notes in computer science, vol 9935. Springer,Cham, pp 108–118
57.Skåtun KC, Kaufmann T, Doan NT, Alnæs D (2016) Consistent functional connectivityalterations in schizophrenia spectrum disorder: a multi site study. Schizophrenia 43:914–924
58.Chiang H-S, Liu L-C, Lai C-Y (2013) The diagnosis of mental stress by using datamining technologies. In: Park J, Barolli L, Xhafa F, Jeong HY (eds) Information technologyconvergence. Lecture notes in electrical engineering, vol 253. Springer, Dordrecht, pp 761–769
59.Maxhuni A, Hernandez-Leal P, Morales EF, Enrique Sucar L, Osmani V , Mu ´noz-Meléndez
A, Mayora O (2016) Using intermediate models and knowledge learning to improve stressprediction. In: Sucar E, Mayora O, Munoz de Cote E (eds) Applications for future internet.Lecture notes of the institute for computer sciences, social informatics and telecommunicationsengineering, vol 179. Springer, Cham, pp 140–151
60.Taylor JA, Matthews N, Michie PT, Rosa MJ, Garrido MI (2017) Auditory prediction errors asindividual biomarkers of schizophrenia. NeuroImage Clinical 15:264–273
61.Castellani U, Rossato E, Murino V , Bellani M, Rambaldelli G, Tansella M, Brambilla P (2009)Local kernel for brains classiﬁcation in schizophrenia. In: Serra R, Cucchiara R (eds) AI*IA2009: emergent perspectives in artiﬁcial intelligence. AI*IA 2009. Lecture notes in computerscience, vol 5883. Springer, Berlin, Heidelberg, pp 112–121
62.Skåtun KC, Kaufmann T, Doan NT, Alnæs D (2016) Consistent functional connectivityalterations in schizophrenia spectrum disorder: a multisite study. Schizophrenia 43:914–924
63.Strous RD, Koppel M, Fine J, Nachliel S, Shaked G, Zivotofsky AZ (2009) Automatedcharacterization and identiﬁcation of schizophrenia in writing. J Nerv Ment Dis 197:585–588
64.Castellani U, Rossato E, Murino V , Bellani M, Rambaldelli G, Tansella M, Brambilla P (2009)Local kernel for brains classiﬁcation in schizophrenia. In: Serra R, Cucchiara R (eds) AI*IA2009: emergent perspectives in artiﬁcial intelligence. AI*IA 2009. Lecture notes in computerscience, vol 5883. Springer, Berlin, Heidelberg, pp 112–121160 A. Nag et al.
65.Hess JL, Tylee DS, Barve R, de Jong S, Ophoff RA, Kumarasinghe N, Tooney P, Schall U,
Gardiner E, Beveridge NJ, Scott RJ, Yasawardene S, Perera A, Mendis J, Carr V , Kelly B,
Cairns M, Unit NG, Tsuang MT, Glatt SJ (2016) Transcriptome-wide mega-analyses reveal
joint dysregulation of immunologic genes and transcription regulators in brain and blood in
schizophrenia. Schizophr Res 176:114–124
66.Mikolas P, Melicher T, Skoch A, Matejka M, Slovakova A, Bakstein E, Hajek T, Spaniel
F (2016) Connectivity of the anterior insula differentiates participants with ﬁrst-episode
schizophrenia spectrum disorders from controls: a machine-learning study. Psychol Med
46:2695–2704
67.Bhagyashree SIR, Nagaraj K, Prince M, CHD F, Krishna M (2018) Diagnosis of dementia by
machine learning methods in epidemiological studies: a pilot exploratory study from South
India. Soc Psychiatry Psychiatr Epidemiol 53:77–86
68.Sheela Kumari R, Varghese T, Kesavadas C, Albert Singh N, Mathuranath PS (2014)
Longitudinal evaluation of structural changes in frontotemporal dementia using artiﬁcial
neural networks. In: Satapathy S, Udgata S, Biswal B (eds) Proceedings of the International
Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2013.
Advances in intelligent systems and computing, vol 247. Springer, Cham, pp 165–172
69.Diniz BS, Lin C-W, Sibille E, Tseng G, Lotrich F, Aizenstein HJ, Reynolds CF and Butters
MA (2016) Circulating biosignatures of late-life depression (LLD): towards a comprehensive,
data-driven approach to understanding LLD pathophysiology. J Psychiatr Res 82:1–7
70.Er F, Iscen P, Sahin S, Çinar N, Karsidag S, Goularas D (2017) Distinguishing age-related
cognitive decline from dementias: a study based on machine learning algorithms. J Clinical
Neurosci 42:186–192. Wp.nyu.edu. 2022. Cloud Computing and the Mental Health Sector.
[online] Available at https://wp.nyu.edu/insight/2021/06/30/cloud-computing-and-the-mental-
health-sector/. Accessed 1 May 2022Impact of 5G Technologies on Cloud
Analytics
Kirtirajsinh Zala, Suraj Kothari, Sahil Rathod, Neel H. Dholakia,
Hiren Kumar Thakkar, and Rajendrasinh Jadeja
1 Introduction
If there is one undisputed fact in recent technological history, it is the massive
growth of apps and connected devices, particularly when it comes to mobile
access. This growth is usually massive, especially as technology progresses. New
adventures supported by new access technologies, such as 5G, include 8 K ultra-
high-deﬁnition television, new virtual/augmented reality devices, cloud gaming,
the massive Internet of Things (IoT), and other applications. All of this progress
raises environmental concerns. Using this type of infrastructure, we are capable
of handling all of this trafﬁc. Data analytics has long been used by businesses
to guide their strategy and optimize proﬁtability. Data analytics, in theory, helps
to eliminate much of the guesswork involved in trying to comprehend clients by
systemically tracking data trends to best create business tactics and operations with
the least amount of doubt Data collection not just to determines what will draw fresh
customers, but it also analyses current data patterns to actually help satisfy current
clients, which is sometimes less costly than creating new company. Data analysis
gives businesses an advantage in detecting changing conditions and implementing
the plan to compete better in an already software industry with special features.
K .Z a l a·N .H .D h o l a k i a
Department of Computer Engineering, Marwadi University, Rajkot, India
S. Kothari · S. Rathod
Department of Information Technology, Marwadi University, Rajkot, India
H. K. Thakkar (/envelopeback)
Department of Computer Science and Engineering, School of Technology, Pandit Deendayal
Energy University, Gandhinagar, Gujarat, India
R. Jadeja
Department of Electrical Engineering, Marwadi University, Rajkot, India
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_9161162 K. Zala et al.
In this work, we aim at ﬁfth generation and wireless communication, and Cloud
analytics, change to the way communication are used will result from present and
future societal development. On-demand knowledge and information will become
more prevalent via current wireless communication systems. These developments
will result in a massive growth in digital and wireless trafﬁc levels, which is expected
to increase 1000-fold over the next few years [ 1,2].
2 Self-Organizing Next Generation Network Data Analytics
in the Cloud
2.1 What Is Network Data Analytics?
Cloud Network Analytics helps phone companies and administrators understand
their networks better. A typical network, data center, mobile communications, or
cloud, is multi-cultural. It is made up of multiple devices from different vendors,
such as an F5 load balancer, a Checkpoint ﬁrewall, Cisco and Juniper switches
and routers. Without relevant statistics and analytical information, identifying and
addressing any issue in such a network is time-consuming and a failure.
2.2 Beneﬁts of Network Data Analytics
1.Represents the current state of the network using real-time statistics
2.Predicts can provide disruptions before they happen.
3.Manages and controls the conformance and assurance processes
4.Increases network reliability and assures network performance in conformance
with SLAs
2.3 The Best Uses of Network Data Analytics
1.Social media:
Social media activity analysis is a popular software for virtual data analytics. It was
hard to process task all over various social media sites before the advent of cloud
drives, mainly if such information was stored on more than one server Cloud drives
enable the analysis of data from different social media platforms at the same time,
as well as faster quantiﬁcation and distribution of focus and effort.Impact of 5G Technologies on Cloud Analytics 163
2.Tracking Products:
E-bay, widely regarded as the king of dependability and foresight, utilizes this
information in the cloud gadgets to store records all over distribution centers and
ship items to clients in terms of item attraction. E-bay is a popular brand in data
management services, in addition to data storage and virtual investigation, thanks
to its Open-shift effort. Big data provides most of the same overview tools and
approaches as E-bay and serves as an information source, saving small companies
money on expensive hardware [ 3].
3.Tracking Preference:
Netﬂix has gotten a lot of attention in the last century or so for its movie collection
on its website. One of One of its platform’s advantages is its ﬁlm series suggestions,
which records and suggests ﬁlms and shows based on what people watch and
suggests everyone else those who might like, providing answers to users while
inspiring them about using each-others service Since all user data is wirelessly
stored on virtualized drives, usage patterns are remain consistent from machine to
machine. Because Netﬂix maintained with Based on most of their users’ interests in
ﬁlms and tv, they have been capable of creating a tv show which was proportionally
preferred by a large proportion of their viewers based on their proven taste.
4.Keeping Records:
Cloud analytics allows for data keeping records regardless of proximity to local
servers. Software can identify a product’s purchases throughout their categories or
distributors with in United States and customize creation and shipments as needed.
If an item does not sell well, they should not have to wait for inventory from local
stores and should instead be able to manage stocks wirelessly using information
that is instantly uploaded to cloud drives. Data storage helps businesses run more
smoothly and given technical knowledge of their clients’ interactions.
2.4 The Near Future
The use of connection data science in cloud technology will increase as clouds are
becoming more secure, dependable, and cost-effective. It is not unimaginable that
all of a user’s data will shortly be stored to the data center and available those in need
of understanding from anywhere. Because of webservers and computer hard drives,
all data from anywhere may be stored in the data systems more of a company’s
physical location. And some are responsible about cloud gadgets’ potential risks,
they are probable to become as reliable and effective just like any other drive or
server. Moreover, the cloud data placement is emerging solution to make the overall
system viable and reliable across the distributed cloud data centers [ 4].164 K. Zala et al.
2.5 The Opportunities
The ability of data analysis to detect patterns in a collection and predict future
occurrences accounts for the majority of its value. The procedure is frequently
referred to as data mining. This essentially implies Acknowledging models in large
datasets to improve data. Number of signiﬁcant advantages of data processing and
big data, the majority of their prospects are wasted due mainly to labor shortages.
Access to said data that is both quick and trustworthy as shown by Idc [ 5],
85% of organizations are not receiving the ﬁnancial advantages of their big data
visualization. Because of a lack of accessibility, resulting in missed chances to
interact with and fulﬁl the needs of customers. Data processing becomes more
accessible when it is stored on virtualized drives because employees can access
corporate information from any location, releasing them along with like being linked
to networks and therefore attempting to improve data accessibility Century link
recently launched its business analytics network infrastructure, which enables its
4000 members to actively use sales data in order to increase proﬁt margins.
3 Intelligent 5G Network Estimation Techniques in the
Cloud
The INCIDENT study is an online, pass survey with random selection based on
network level method. The goal of this article is to create a Neural Network
estimation model in the cloud to control the timing problem in software projects.
The 5G Network estimation and classiﬁcation of network estimation schemes is
shown in Fig. 1. To address the issue of traditional automated computer networks’
low accuracy in estimating feature, a pairing nonlinear autoregressive with external
inputs neural network estimation method in cloud is presented and applied to
lithium-ion battery state of health prediction. The majority of network estimations
assume that disturbances have indirect effects. Although there is fast-growing
research on ﬂexible connectivity methods, the main focus has been on individual
network estimation, which fails to utilize common patterns of connectivity.
3.1 Network Estimation Technique
To show that the performance of network estimation techniques in the cloud very
little research has been conducted to assess how network estimation techniques
perform when the data is ordinal. The ﬁrst database was used to train the neural
network estimation technique. Finally, early research conﬁrmed that the interference
was more successful in developing overall depression severity for group members
who scored higher on the four symptoms directly impacted by the treatment;
as a result, network estimation techniques in cloud showed promise in precision
psychiatry.Impact of 5G Technologies on Cloud Analytics 165
Fig. 1 Classiﬁcation of network estimation schemes [ 6]
3.2 Literature Review
Hong-Linh Truong and SchahramDustdar proposed a Modular network estimation
technique based on a facility for network estimating, analyzing, and tracking costs
involved with cloud-based research areas. The authors also presented techniques
in this paper for estimating service dependency and monitoring associated with
typical research areas. Composable Cost Model: Networking estimation, tracking,
and analysis on basis of requirements are needed whenever developers develop
software. Study on Delivery Network Estimation in a Cloud 301, Before utilizing
the cloud resource, the developer is well in the application. As a result, the main
objective of the programmer cost model is to assess the price of moving to a cloud
system using known performance parameters such as ﬁle transfer, processing time,
and other software deﬁned parameters. Hong-Linh Truong and SchahramDustdar,
on the other hand, developed a pricing model for multiple application designs like
OpenMP, MPI, and workﬂows. This can be ﬁnished by reviewing previous network
estimation, tracking, and analysis knowledge. Basic cost models can be used to
create network estimation, tracking, and analysis tools and services [ 7]. Moreover,
IoT devices are increasing in the past few years leading to massive amount of data
generation from different domains such as healthcare [ 8], Wireless sensor networks
[9], and robotics which require cloud data centers to process them.166 K. Zala et al.
4 5G-cloud Integration: Intelligent Security Protocol
and Analytics
4.1 Scope
This organizations system develops on the ESF Potential Attacks to 5G Facilities
white paper, which was published in May 2021 and focused on threats, risks,
and workarounds related to the implementation of 5G cloud systems. Although
this guidance can beneﬁt all 5G network partners, it is primarily aimed at serviceproviders and network operators who build and setup 5G cloud services. Core
wireless equipment distributors, service providers, service companies, and mobile
operators are all included. Throughout the series, the audience for each set ofinstructions will be identiﬁed, supplying a layered approach to hardened 5G cloud
implementation.
4.2 5G Cloud Threat
A few threat signals related to 5G cloud services were provided in the Risks
Potential Attacks to 5G Connectivity article, including Software/Conﬁguration,Internet Security, Virtualization, and Cloud Based Networking. Virtualized 5G
networks will be a valuable target for cyber threat a looking to reject or destroy
network services or otherwise compromise data. To counter this threat, 5G cloudfacilities must be built and designed safely, with the ability to detect and respond to
threats, thereby providing a hard-core environment for launching secure network
functions. It is also critical that 5G network services conform to security bestpractices. These organizations systems will address the latter, giving guidance
on modifying 5G public cloud deployments motivated by security events. This
approach is consistent with the May 2021 Executive Order on Improving theNation’s Data protection, which called for secure services as well as the ability
to detect unexpected behaviors more easily.
4.3 5G-Cloud Integration
IoT devices will expand in a wide range of devices, donating to smart city
infrastructure. These devices from different city areas will cover every corner ofsociety, requiring town networks with connectivity to retrieve and deliver the data
sets. In response to this demand, 5G has expanded its task to communicate things
other than people. When compared to solutions such as LoRa and SigFox, thegenerated 5G IoT contributes to the success of the smart city environment by
allowing companies, large and small, to set up IoT services without the need toImpact of 5G Technologies on Cloud Analytics 167
utilise their own internet network. Many smart city services are expected to run
over 5G, boosting the integration of 5G and IoT. This trend, in turn, will impose
new challenge on the on-going 5G mobile service and have an impact on 5G moralwork.
Many factories use cases, such as manufacturing, place demanding criteria on
communication networks in terms of reliability, connectivity, and teleology. Overthe last 70 years, we’ve seen a shift away from 4-20 mA current loops and toward
open ICT standards developed by the IEEE Period Community group.
IoT devices will expand in a wide range of devices, donating to smart city
infrastructure. These devices from different city areas will cover every corner of
society, requiring town networks with connectivity to retrieve and deliver the data
sets. In response to this demand, 5G has expanded its task to communicate thingsother than people. When compared to solutions such as LoRa and SigFox, the
generated 5G IoT contributes to the success of the smart city environment by
allowing companies, large and small, to set up IoT services without the need toutilise their own internet network. Many smart city services are expected to run
over 5G, boosting the integration of 5G and IoT. This trend, in turn, will impose
new challenge on the on-going 5G mobile service and have an impact on 5G moralwork.
Many factories use cases, such as manufacturing, place demanding criteria on
communication networks in terms of reliability, connectivity, and teleology. Overthe last 70 years, we’ve seen a shift away from 4-20 mA current loops and toward
open ICT standards developed by the IEEE Period Community group.
Security data analysis is an information security technique that employs data
gathering, assembling, and analytical techniques to monitor threats and security.
When a company employs security tools, it can analyze data breaches to identify
risk perceptions before they disrupt the industry’s interconnection and bottom line.
Security analytics combines big data capabilities to developed threat detection,
observation, and prevention to aid in the identiﬁcation, analysis, and preventative
measures of threats, as well as continuous cyberattacks and targeted attacks fromoutside bad actors.
4.4 Advantages of Security Capabilities
Several key advantages are provided by security analytics tools to organizations:
Detection and response to security incidents and variations
In real time, security products classify a wide range of information types, connecting
the two activities and notiﬁes to identify security events or computer viruses.
Compliance with regulations
Security tracking tools are helpful to companies in complying to government and
industry laws such as the Affordable Care Act (HIPAA) of 1996 and the Payment168 K. Zala et al.
Card Industry Data Security Standard (PCI DSS). Security analytics tools can
combine multiple data sources, providing organizations with a clear overview of
data events throughout multiple devices. Compliance managers can use this to
observe regulated data to identify potential noncompliance.
5 5G, Fog and Edge Based Approaches for Predictive
Analytics
5.1 Introduction
In recent decades, the amount of Internet of Things gadgets has grown in popularity,
facilitating the development of clearing Ito applications in a variety of areas
to improve the quality of human life [ 10,11]. With the evolution of internet
applications, fog computing is the most recent dispersed virtual world that has lately
stimulated the interest both of industrial and academic researchers for making sure
the queries of numerical apps in IoT-connected devices [ 12,13]. Because IoT smart
solutions create huge amounts of data via sensors, portable devices, and detectors,
the cloud - based data environment faces problems in connected cars, connected
home, smart manufacturing, and other hardware applications, such as delay, network
access, and security issues [ 14,15]. To address these concerns, Cisco [ 16]t r i e dt o
introduce “fog computing,” a cloud platform technology that acts as a link among
cloud storage and embedded systems in order to meet the number of iterations of
hardware software [ 17,18].
5.2 Literature Review
Fog computing refers to Internet of Things application components that can be
accessed in a cloud data center, such as icons, connectors, proxy servers, ﬁxed data
packet, smart portals, core network, or other fog devices [ 3,7]. To address the needs
of hardware applications, it enables location - based services, user mobility, context -
aware, multiple data predictive analysis, real-time relations, interface heterogeneity,
scalability, and connectivity [ 19]. However, due to its wide range of raw material
difference and ﬂexible agreements, and the highly uncertain and volatile creation
of the fog network, one of the complicated challenges to be addressed in terms
of improving fog computing performance is resource management. The rest of
this section will begin with a brief summary of the fog landscape’s three-tier
architecture, followed by a detailed of some popular observation and review
experiments on fog and edge computational resources management issues.
The 5G cellular network is set to emerge on the market in 2019, but will it be a
game changer for predictive analytics?Impact of 5G Technologies on Cloud Analytics 169
Although none of us can predict the future, 5G is supposed to bring new
possibilities and changes. This article will look at how 5G might affect facets of
predictive analytics. It will also investigate Fog and Edge techniques as changes to
the current processing approach. When data is detailed and accurate, an edge-based
approach is advantageous; however, fog can impair the reliability of predictive and
decision making. As a result, in order to ensure accuracy, data and information may
need to be handled through an edge function using a traditional analytics method.
A fog-based approach, on the other hand, would most likely method very limited
data sets with many than one exceptional case point at a time, as these unique
activities can be easily modelled by other techniques that do not advantage from
processing big data in parallel. This article will compare and contrast Fog and Edge
Coding in terms of how they will be generated, covered, and saved. It will also
try comparing their use cases and explain when every approach is suitable. Edge
computing utilise edge nodes which are nearly at the end user’s gadget than fog
computing does. Edge computing is a sort of cloud-based estimation that provides
an IT asset at the device layer across portable or ﬁxed different networks or within
application areas with free or low-latency Wi-Fi. Predictive modelling has become
increasingly complex, from business analytics to big data analysis. Fog and cloud-
related technologies have grown in popularity in recent years. In this article, we will
look at edge-based predictive analytics approaches that could provide a response to
these trends. Edge-based approaches are a subset of localized coding that provides
a different way to analyze data above what is probable with big data alone for order
to ﬁnd observations before they are needed.
6 5G and Beyond in Cloud, Edge, and Fog Computing
As shown in Fig. 2, cloud edge and fog computing is connected with each other if
we want to perform edge computing the we required fog computing as well as we
required cloud computing if we want to perform fog computing.
6.1 Edge Computing
Edge computing refers to facilities that allows data analysis as close to the area as
possible, enabling faster data analysis, faster data, and less high-speed data internet
trafﬁc. This type of virtualization will require similar computer servers as well
as the addition of microprocessors. As a method of improving this access, cloud
servers are being connected in and close 5G towers. Edge computing exists in a
shared environment, and normalization is not a choice. Simple task the “design
once, measure forever” approach used in cloud services need not apply to private
clouds. The precipice the huge range of tasks and public cloud tools available at the170 K. Zala et al.
Fig. 2 Cloud, edge, and fog computing architecture [ 20]
edge creates the need for a speciﬁc design as a result, a “Mobile operator Cloud” is
required was never more obvious.
Fog Computing Edge computing is a subcategory of the “fog computing” idea,
which deﬁnes edge computing’s functions. The purpose of fog computing is to
enhance the accuracy of data transmission from the edge cloud server for more
processing, analysis, and long-term retention.
6.2 Cloud Computing
Cloud computing, in its most basic form, is a set of functions delivered over
the internet, or “the cloud.” It involves storing and accessing data on remote
servers rather than relying on local disks and private datacenters. The conceptual
understanding of cloud computing is shown in Fig. 3.
Before cloud computing, businesses had to pay to maintain their own web server
to meet their needs. This compelled the purchase of sufﬁcient data centers to lower
the risk of rest time and outages while also accepting peak trafﬁc volume. As a
side effect, huge quantities of server space sat empty for the majority of the time.
Cloud service providers today enable businesses to eliminate the need for onsite
web server, maintenance crews, and other expensive IT resources.Impact of 5G Technologies on Cloud Analytics 171
Fig. 3 Introduction to cloud
computing [ 21]
6.3 5G and Beyond
5G provides a wide range of multi-partnership models in which standard services
are enhanced by the addition of additional third-party features. NGMN’s user offer
supplemented by partner,” in which connections are supplied, is one of these case
studies. A selection of third-party applications complements the operator’s offering.
Here’s an example. Highlights the considerable complexity of such interservice due
to ambiguous responsibility sharing across numerous parties Each member must
have trust in the other. Decisions taken by other partners to contribute to the needed
quality level of service.
As we move to 6G, application cases and business models are more focused on
intelligence slicing and risk allocation, haptic communications, web or automatons,
Electronic and hyper data frequency connectivity are pushing the technological
envelope even further. The University of Oulu’s Finnish 6G Flagship The initiative
itself had released a collection of 12 6G white papers that cast light on major radio
possibilities and an imagination for developing use cases including such three-
dimensional accessing material and very widespread aircraft vehicle connectivity,
and several others.
7 AI-Enabled Next Generation 6G Wireless Communication
The construction of a 6G network will be huge, non - linear, extremely complicated,
energetic, and sophisticated. Furthermore, 6G cellular must provide interconnection,
meet the various QoS requirements of a wide range of devices, and handle large
volumes of data generated by the external surroundings. Artiﬁcial intelligence
introduces with powerful observation, learning, enhancement, and smart recognition
capabilities that could be used in 6G networks to perform optimization, knowledge
extraction, advanced teaching, framework association, and intelligent decision
making. For 6G networks, we provide a multiple AI-enabled smart structure: smart172 K. Zala et al.
detecting, data gathering and analytics, adaptive control, and smart implementa-
tion.
Supervised Learning:
Supervised learning constructs the learning method (also known as training)
from a set with exclusive labelled data, which is area of impact into classiﬁer
subﬁelds. Classiﬁcation analysis, which includes classiﬁcation trees (DT),support vector machines (SVM), K-nearest neighbors, aims to Each input
sample should be assigned a categorical label (KNN). In regression analysis,
the SVR and Finite mixture aggregate data are used to estimate or predictcontinuous values based on statistical features input.
Unsupervised Learning:
Unsupervised learning’s task is to discover the patterns that are hidden and
extract useful functions from information sets, and it is broadly classiﬁedas hashing and dimension decrease. Clustering algorithms, which primarily
include K-means Cluster analysis and grouping, seek to group a dataset into
different clusters based on their similarities. Dimension reduction is the pro-cess of reducing a fast from a high-dimensional dataset to a low-dimensional
dataset while retaining a lot of helpful data. Two classic dimension reduction
algorithms are principal component analysis (PCA) and isometric mapping(ISOMAP).
7.1 Computation Efﬁciency and Accuracy
Big data applications collected in 6G technology, as well as large complex
conﬁgurations, pose challenges to AI-enabled training and learning. Furthermore,
in terms of meeting the learning accuracy rate, limited processing requirementsmay be insufﬁcient to wide range large amounts of high data. Deep learning
is also expensive due to its high computation. As a result, determining how to
make cost-effective AI development system that enhance both simulation accuracyand reliability is a signiﬁcant research challenge. Recent research has identiﬁed
surviving systems, graphics handling, concept detection, and encrypted training
as promising methods for increasing convergence rate, reducing computationalresources, and improving accuracy results.
7.2 Hardware Development
When developing 6G networks, hardware development is particularly complex.
On the one hand, mm Wave and THz hardware components are costly and
demand energy. Some devices/terminals, however, have limited computational andImpact of 5G Technologies on Cloud Analytics 173
data energy. Regardless of the beneﬁts of AI learning methods for learning and
recognition, they need time complexity, energy consumption, and enough computer
infrastructure. As a result, a combined effort for both hardware and AI learningmethodologies, involving substantial study, should be promoted.
7.3 Types 6 G Wireless Communication
Satellite communication Satellite communication Satellite communication, broad-
cast radio, microwave radio, Bluetooth, Zigbee, and other types of wireless commu-
nication are the most popular.
Satellite Communication
Satellite communication is a subconscious communication technology that isfrequently utilised to keep people linked practically anywhere on the earth. When
the signal (a modulated microwave beam) is delivered close to the satellite, it
is ampliﬁed and sent back to the earth’s surface transmitter receiver. Satellitecommunication is comprised of two key components: the spacecraft and the ground
station. The ground part consists of Wi-Fi or cellular transmission, reception, and
supporting equipment, whereas the space section consists mostly of the satelliteitself. Itself.
Infrared Communication
Infrared wireless communication IR radiation is utilized to transmit messages in
a device. The wavelength of infrared energy is longer than that of red light. It’s
used for security, remote control of televisions, and short-range communication.IR radiation is found between radio waves and light rays in visible radiation. As a
result, they are employed for communication.
Broadcast Radio
Accessible radio transmission was the ﬁrst communication device to seek
widespread acceptance, and it continues to play an important role today. Userscan communicate over small distances using multimedia radios, while sailors
can connect to networks using civilian group and underwater radios. Ham radio
operators communicate data and provide alert system assistance during accidentsusing powerful communications devices, and they might sometimes interact digital
information over the radio frequencies.
7.4 6G Wireless Access Use Case
6G Wireless Access will emerge and be required to meet the unexpected require-
ments resulting from exciting new apps expected in the 2030 era, which current174 K. Zala et al.
Fig. 4 6G wireless access use case [ 22]
wireless generations will be unable to support. In this post, we will attempt to list
the applications that necessitate wireless internet connectivity with high speed, as
well as other specialized features. The 6G wireless access use cases is shown in
Fig.4.
The following list contains potential new 6G use cases and software that will help
in understanding the key demands of future 6G systems.
Enhanced hotspot (e-Hotspot):
An enhanced hotspot (e-hotspot) is a scenario in which the Access Point (AP)
provides high data link data speeds to many devices within a small coverage
area with low receiver complexity limits. High-speed data hotspots that type
of system which supports HD streaming video and improved Wireless LAN
(WLAN).
Remote Areas Connectivity :
Half of the worldwide people still lacks basic internet broadband access.
Current technologies and platforms have not reached half the world. A key
goal of 6G is to assure 10 Mbps from every occupied part of the city, using
ﬁeld and space-borne virtual networks. 6G must be created in a cost-effectiveImpact of 5G Technologies on Cloud Analytics 175
manner to allow actual deployments that provide broadband to the entire
world’s population.
Autonomous Vehicle Mobility:
The smart transportation devices pioneered by 5G are expected to progress
toward self-driving systems, supplying safer and more efﬁcient transportation,
better trafﬁc management, and a better user experience.
Short-Range D2D Communications:
The D2D use case is focused on data swap between devices that are close
together, with little involvement from network infrastructure. This takes into
account a synchronous (same DL/UL data rate requirements) large data
rate, point-to-point connection with extremely strict power and complexityconstraints.
Industrial Automation:
Through computer devices, IoT networks, cloud services, and artiﬁcial intelli-
gence, Industry 4.0 imagines a digitalization of manufacturing companies andprocesses. Automatic control systems and modern communications are used
in industrial processes to achieve high accuracy manufacturing.
Extended Reality (AR/MR/VR):
This use case considers augmented, virtual reality (AR/VR) applications that
capture multi-sensory inputs and provide real-time user interaction. To deliver
a fully immersive experience, very high data rates in the Gbps scope and verylow latencies are required per user. Remote communication and interaction
enabled by holographic communications, as well as all human sensory input
data, will push the data rate and delay targets even further. Speed on thesequence of tbsp will be required for multiple-view cameras used in three
dimensional communications.
Smart Railways:
This is an essential element in a worldview in which facility, trains, passen-
gers, and products are all seamlessly connected at high data rates. Railway
interactions are developing applications to supporting a variety of network
capacity applications such as wayside HD and on-board security cameras,broadband services for passengers, passenger information broadcasting, and
virtual driving or control. These applications must be installed in at least ﬁve
different scenarios.176 K. Zala et al.
References
1.Index, Cisco Visual Networking. “Global mobile data trafﬁc forecast update.” Cisco
White Paper [Online]. Available: http://www.cisco.com/en/US/solutions/collateral/ns341/
ns525/ns537/ns705/ns827/white_paper_c11–520862.pdf (2014)
2.WWRF, KES, and L. Sorensen. “Beyond 4G: radio evolution for the gigabit experience,” July
2009. (2020)
3.Thakkar HK, Dehury CK, Sahoo PK (2020) Muvine: multi-stage virtual network embedding in
cloud data centers using reinforcement learning-based predictions. IEEE J Sel Areas Commun
38(6):1058–1074
4.Thakkar HK, Sahoo PK, Veeravalli B (2021) Renda: resource and network aware data
placement algorithm for periodic workloads in cloud. IEEE Trans Parallel Distrib Syst
32(12):2906–2920
5.Yarosh S, et al (2011) Examining values: an analysis of nine years of IDC research. In:
Proceedings of the 10th international conference on interaction design and children
6.Khan I, et al (2019) A robust channel estimation scheme for 5G massive MIMO systems.“
Wirel Commun Mob Comput 2019 (2019)
7.Malik PK, Wadhwa DS, Khinda JS (2020) A survey of device to device and cooperative
communication for the future cellular networks. Int J Wireless Inf Networks 27(3):411–432
8.Rai D, Thakkar HK, Rajput SS, Santamaria J, Bhatt C, Roca F (2021) A comprehensive review
on seismocardiogram: current advancements on acquisition, annotation, and applications.
Mathematics 9(18):2243
9.Sahoo PK, Thakkar HK (2019) TLS: trafﬁc load based scheduling protocol for wireless sensor
networks. Int J Ad Hoc Ubiquitous Comput 30(3):150–160
10.Jo D, Kim GJ (2019) IoT +AR: pervasive and augmented environments for “Digi-log”
shopping experience. Hum-centric Comput Inf Sci (HCIS) 9(1):1
11.Ghobaei-Arani M, Souri A, Baker T, Hussien A (2019) ControCity: an autonomous approach
for controlling elasticity using buffer Management in Cloud Computing Environment. IEEE
(ACCESS) 7:106912–106924
12.Miah MS, Schukat M, Barrett E (2018) An enhanced sum rate in the cluster based cognitive
radio relay network using the sequential approach for the future internet of things. Hum-centric
Comput Inf Sci (HCIS) 8(1):16
13.Deng Y , Chen Z, Zhang D, Zhao M (2018) Workload scheduling toward worst-case delay and
optimal utility for single-hop fog-IoT architecture. IET Commun 12:2164–2173
14.Souri A, Asghari P, Rezaei R (2017) Software as a service-based CRM providers in the cloud
computing: challenges and technical issues. J Serv Sci Res 9(2):219–237
15.Ghobaei-Arani M, Shamsi M, Rahmanian AA (2017) An efﬁcient approach for improv-
ing virtual machine placement in cloud computing environment. J Exp Theor Artif Intell
29(6):1149–1171
16.Bonomi F, et al (2012) Fog computing and its role in the internet of things. In: Proceedings of
the ﬁrst edition of the MCC workshop on mobile cloud computing
17.Ghobaei-Arani M, Rahmanian AA, Shamsi M, Rasouli-Kenari A (2018) A learning-based
approach for virtual machine placement in cloud data centers. Int J Commun Syst 31(8):e3537
18.Ghobaei-Arani M, Rahmanian AA, Aslanpour MS, Dashti SE (2018) CSA-WSC: cuckoo
search algorithm for web service composition in cloud environments. Soft Comput
22(24):8353–8378
19.Kertesz A, Pﬂanzner T, Gyimothy T (2018) A mobile IoT device simulator for IoT-fog-cloud
systems. J Grid Comput EarlyCite 17:529–551. https://doi.org/10.1007/s10723-018-9468-9
20.Tsai W-T, Sun X, Balasooriya J (2010) Service-oriented cloud computing architecture. In: 2010
seventh international conference on information technology: new generations. IEEE
21.Stanoevska-Slabeva K, Wozniak T (2010) Cloud basics–an introduction to cloud computing.
In: Grid and cloud computing. Springer, Berlin/Heidelberg, pp 47–61
22.Rajatheva N, et al (2020) White paper on broadband connectivity in 6G. arXiv preprint
arXiv:2004.14247IoT Based ECG-SCG Big Data Analysis
Framework for Continuous Cardiac
Health Monitoring in Cloud Data Centers
Hiren Kumar Thakkar and Prasan Kumar Sahoo
1 Introduction
Internet of things (IoT) is referred to as networking of smart devices [ 1,2]o r
wearable sensors to accomplish data collection, processing and analysis in the ﬁeld
of vehicular communication, mobile health care, elderly smart home monitoring
and industrial applications etc. [ 3]. Before IoT, collection of huge amount of data
and subsequent analysis was very tedious and time consuming task. However,
technology advancement in the ﬁeld of wearable sensors and actuators has made
it easy to collect data in continuous and uninterrupted manner [ 4].
In past few years, a tremendous growth in the ﬁeld of Microelectromechanical
systems (MEMS) and nanoelectromechanical systems (NEMS) has made it viable
to design Mobile Health (mHealth) applications consisting of large varieties of low-
cost IoT based body sensors [ 5]. Various mHealth applications are developed to
measure physiological parameters such as body temperature, pulse rate and blood
pressure etc. In addition of mHealth applications [ 6], IoT sensor based Body Area
Network (BAN) applications such as activity monitoring, drug monitoring, diet
monitoring and cardiac monitoring are also envisioned in recent years. The IoT
based health monitoring has not only reduced the cost but also made the entire
health monitoring process a continuous, hassle free and convenient with the help
of wearable devices such as smart belt, smart band, smart cloth or use smart
H. K. Thakkar (/envelopeback)
Department of Computer Science and Engineering, School of Technology, Pandit Deendayal
Energy University, Gandhinagar, India
e-mail: hiren.thakkar@sot.pdpu.ac.in
P. K. Sahoo
Department of Computer Science and Information Engineering, Chang Gung University,
Kwei-Shan, Taiwan (ROC)
e-mail: pksahoo@mail.cgu.edu.tw
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_10177178 H. K. Thakkar and P. K. Sahoo
phone based sensors. Although, IoT body sensor based systems enable collection
of physiological data in an uninterrupted manner, the existing systems face the
challenge of processing and analyzing of continuous data in the absence of proper
big data analysis framework [ 7].
This encourages us to design a data collection cum processing framework
to provide a feasible solution for continuous cardiac health data monitoring. A
cardiac big data processing model based on MapReduce is proposed to facilitate
the processing of continuous cardiac big data in cluster platforms such as apache
hadoop. In this paper, the proposed system framework is designed considering
Electrocardiography (ECG) and Seismocardiography (SCG) cardiac big data. The
rest of the article is organized as follow: Sect. 2brieﬂy describes the related works.
Section 3provides the proposed framework. Section 4reports the simulation based
evaluation results followed by conclusion and future works in Sect. 5.
2 Related Work
Now-a-days, inexpensive and reliable mobile healthcare systems are increasingly
becoming the basic need of a society. In past, efforts are made to collect, process and
analyze cardiac healthcare data using IoT sensors. In [ 8], authors have employed a
tri-axial accelerometer sensor to collect seisemocardiography (SCG) data. Recently,
Di Rienzo et al. have introduced smart garment named MagIC-SCG to facilitate the
monitoring of SCG, ECG and respiration out of laboratory settings in ambulant
subjects [ 9]. The challenges, contribution and future of mobile, cloud and big data
computing in telecardiology is explored in [ 10]. In recent years, cloud computing
usage has made it viable to remotely store and process voluminous data in an
inexpensive manner using powerful cloud resources. A cloud-ecg service platform
for efﬁcient monitoring and analysis of ECG data is introduced in [ 11].
Wearable technology based cardiac monitoring draws attention of many
researchers as it provides the convenience and reliability of data analysis at low
cost. The Human ++ [12] is one of the earliest efforts along the direction of
Body Area Network (BAN) for diversiﬁed health applications, whose primary
goal is to monitor and visualize various signals such as electroencephalogram,
electrocardiogram and electromyography. Since, cardiac health data generates
in continuous manner, they need huge storage space. Additionally, the stored
data also needs to be processed in real-time manner. The cloud computing is an
emerging solution to handle continuous big data. In [ 13], a cloud-based dynamic
electrocardiogram monitoring and analysis system is introduced. Although, cloud
based systems are efﬁcient, a novel distributed processing and analysis models are
required to monitor cardiac health in real-time manner.IoT Based ECG-SCG Big Data Analysis Framework for Continuous Cardiac ... 179
3 Proposed Cardiac Big Data Analysis Framework
The proposed cardiac big data analysis framework is consist of three modules. (1)
IoT based ECG and SCG data collection, (2) Cardiac Hadoop cluster architecture,
(3) MapReduce based cardiac big data processing cum analysis model. Each module
is described individually as follow.
3.1 ECG/SCG Data Collection Framework
In this section, an IoT based ECG and SCG data collection framework is introduce.
The primary goal of any health care data analysis is the collection of accurate and
noise free health data. Multiple accelerometer-based sensing modules and three-led
ECG modules are used to collect SCG and ECG waveforms, respectively [ 14,15].
Each SCG IoT sensing module is consist of Ultra low power high performance
3-axis digital accelerometer (LIS331DLH, developed by STMicro electronics). To
ensure the quality of data collection, sensing modules are proposed to place at
four auscultation sites such as Aortic, Pulmonic, Tricuspid and Mitral as shown
in Fig. 1. The location of placement of SCG as well as ECG sensing module greatly
affect the quality of cardiac data collection. Hence, the proposed data collection
framework is designed with great care and location of sensing modules are proposed
in consultation with expert cardiologists.
For the collection of simultaneous ECG signals, the proposed location of ECG
leads are at left arm, right arm and left leg as shown in Fig. 1. Since, the sensing
modules are capable of connecting to Internet, the cardiac ECG and SCG data
acquired by sensing modules are transferred to local smartphone based gateway. The
ECG and SCG waveforms are sufﬁciently ampliﬁed and ﬁltered using compatible
Fig. 1 IoT based ECG/SCG data collection framework180 H. K. Thakkar and P. K. Sahoo
Fig. 2 Hadoop data processing platform
android or iOS apps installed in smartphone. From smartphone gateway, ﬁltered and
ampliﬁed ECG and SCG waveforms are forwarded to remote cardiac Hadoop cluster
for further processing and analysis via cellular or wiﬁ enabled internet services.
3.2 Data Processing and Analysis Framework
To process real-time continuous data using traditional data processing platforms is
not a feasible solution and often fails to serve the purpose of continuous health
monitoring. In this section, a parallel and distributed data processing platform
Hadoop is introduced and used to deal with the real-time processing requirement
of cardiac big data processing and analysis.
The apache hadoop based cardiac big data processing and analysis framework is
shown in Fig. 2, where real-time ECG and concurrent SCG data generated during
data collection framework are taken as input. The hadoop platform provides a
resources on the top of which a MapReduce based distributed feature selection
and correlation analysis algorithm can be executed. The hadoop cluster is built on
the notion of master-slave architecture, where one machine is designated as master
(i.e. Name Node) and rest machines are designated as slaves (i.e. Data Nodes).
All of the slave machines are divided into sub-set of machines to form Racks,
where communication between any two machines within a rack was facilitated
via corresponding rack switch; on the other hand inter-rack communication was
facilitated via series of switches (Here, two Rack Switches and one N/W Switch).
In order to smoothly overcome crash failure of a cluster, hadoop supports secondary
Name Node, which helps in maintaining up to date meta information about theIoT Based ECG-SCG Big Data Analysis Framework for Continuous Cardiac ... 181
cluster. In this article, we assumed that for each patient, ECG/SCG data accumulated
in past few hours are stored in a ﬁle, which acts as an input to the cluster. Since, input
data size may range in multiply of GB (i.e., GigaBytes), ﬁles are divided into set of
small sized blocks (Here, 64 MB) and are stored in distributed fashion among slave
machines. Once the data storage is concluded, the stored data are processed in a
MapReduce (i.e., Map phase and Reduce phase) fashion for feature points selection
and subsequent co-relational analysis.
3.3 MapReduce Based Cardiac Big Data Processing Model
In this section, a MapReduce based cardiac big data processing model is proposed
to enable the distributed and parallel processing of ECG and SCG data on apache
hadoop based clusters. The MapReduce is a two phase (e.g., Map and Reduce)
programming paradigm designed to parallelize the processing of data as shown in
Fig.3. At ﬁrst, an input data ﬁle of ECG/SCG data collected in past few hours is
provided as an input. Before start of Map phase, input ﬁle is partitioned into set of
small sub ﬁles and are distributed over slave nodes. In Map stage, each partitioned
ﬁle unit processed on individual slave node in parallel and distributed fashion. The
proposed framework emphasis on selecting the ECG and SCG speciﬁc feature points
during the map phase. However, user can chose his/her own approach of analysis
instead of feature points selection. The set of important feature points of ECG
and SCG are {P,Q,R,S,T }and{AS,MC,IM,AO,IC,RE,AC,MO,RF },
respectively. For each partition, during Map phase, feature points selection algo-
rithm executes in parallel, extracts the amplitude as well as time duration which are
later compared with reference values. The reference value of various ECG feature
points is listed in Table 1. For reference value of SCG feature points, estimation
Fig. 3 MapReduce cardiac ECG/SCG data processing model182 H. K. Thakkar and P. K. Sahoo
Table 1 Normal reference
values for ECGNotation Meaning
TP Pwave time duration (80 ms)
AP Pwave amplitude range (0.1 mm, 0.2 mm)
TQRS QRS time duration (80 ms, 100 ms)
AQRS QRS wave amplitude ( ≤1 mm)
from few initial cardiac cycles is considered. Finally, the count of number of cardiac
cycles with one or more abnormal feature points is obtained.
In reduce phase, abnormal cardiac cycles observed during map phase by various
data nodes are aggregated and re-evaluated to conﬁrm the type of abnormality
detection. Additionally, Reduce phase also estimate the distance between two abnor-
mal cycles and frequency of abnormal ECG and SCG cardiac cycle appearance.
The longer the distance between two abnormal ECG/SCG cardiac cycle, lesser the
chances of abnormality in cardiac functioning.
4 Evaluation Results
In this section, performance evaluation of proposed big data analysis framework
under different conﬁguration is carried out with respect to processing time and
ability to process data in real-time. Four different data processing and analysis
conﬁgurations such as traditional (e.g., sequential), 2 nodes hadoop, 4 nodes
hadoop, 8 nodes hadoop are used for evaluation purpose. Different size of input
data size such as 1 GB, 3 GB, 5 GB, 7 GB and 9 GB are considered and processing
time for respective conﬁguration is observed. As shown in Fig. 4, the traditional
sequential processing conﬁguration takes maximum time and 8 nodes hadoop
conﬁguration takes the minimum processing time. It is expected that the increase
in number of nodes in hadoop conﬁguration by 2, 4 and 8 should reduce the
processing time by1
2,1
4and1
8, respectively. However, it is observed that in hadoop
the processing capacity gradually saturates.
To evaluate the ability of real-time data processing need, the 1 GB data collection
time of 100 subjects is compared with 1 GB data processing time of different
conﬁgurations. As shown in Fig. 5, traditional data processing takes nearly 3 times
more time than the data collection time and it is not suitable conﬁguration for
real-time processing. On the other hand, in 8 nodes hadoop conﬁguration, data
processing time is marginally less than that of data collection time. This shows that
for hadoop conﬁgurations with number of nodes more than eight can successfully
handle real-time processing of cardiac ECG and SCG data for nearly 100 subjects.IoT Based ECG-SCG Big Data Analysis Framework for Continuous Cardiac ... 183
Fig. 4 Performance evaluation of different processing conﬁguration with respect to processing
time
Fig. 5 Performance evaluation of different processing conﬁguration for real-time data processing
capacity
5 Conclusion and Future Works
In this paper, a novel cardiac big data analysis platform based on apache hadoop
and mapreduce is introduced to provide real-time cardiac health information. The
IoT sensor based ECG and SCG data collection in a hassle free and convenient way
is proposed to facilitate real-time data analysis transfer to remote cardiac hadoop
cluster. The apache hadoop big data processing cluster is employed to satisfy the
need of cardiac big data processing. A MapReduce based modeling of ECG and
SCG data processing cum analysis is designed to make the entire data analysis
compatible with apache hadoop platform. The evaluation results show that the
propose framework is highly efﬁcient and match with the need of real-time health184 H. K. Thakkar and P. K. Sahoo
monitoring. In future, the integration of hadoop cluster with cloud environment
can be introduced to provide low cost alternative to users without engaging with
costly hardware equipments of cluster. Moreover, a more focus on novel ECG/SCGabnormality detection methods can be introduced to further reduce the processing
and analysis time.
References
1.Ali HM, Liu J, Bukhari SAC, Rauf HT (2022) Planning a secure and reliable IoT-enabled
fog-assisted computing infrastructure for healthcare. Cluster Comput. 25(3):2143–2161
2.Singh P, Devi KJ, Thakkar HK, Kotecha K (2022) Region-based hybrid medical imagewatermarking scheme for robust and secured transmission in IoMT. IEEE Access 10:8974–8993
3.Rai D, Thakkar HK, Rajput SS (2020) Performance characterization of binary classiﬁersfor automatic annotation of aortic valve opening in seismocardiogram signals. In: 2020 9thinternational Conference on bioinformatics and biomedical science, pp 77–82
4.Kristoffersson A, Lindén M (2022) A systematic review of wearable sensors for monitoringphysical activity. Sensors 22(2):573
5.Rai D, Thakkar HK, Rajput SS, Santamaria J, Bhatt C, Roca F (2021) A comprehensive reviewon seismocardiogram: current advancements on acquisition, annotation, and applications.Mathematics 9(18):2243
6.Triantafyllidis A, Kondylakis H, Katehakis D, Kouroubali A, Koumakis L, Marias K, AlexiadisA, V otis K, Tzovaras D et al. (2022) Deep learning in mhealth for cardiovascular disease,diabetes, and cancer: systematic review. JMIR mHealth uHealth 10(4):e32344
7.Thakkar HK, Sahoo PK, Veeravalli B (2021) Renda: resource and network aware dataplacement algorithm for periodic workloads in cloud. IEEE Trans Parallel Distrib Syst32(12):2906–2920
8.Dinh A (2010) Design of a seismocardiography using tri-axial accelerometer embedded withelectrocardiogram. In: The world congress on engineering and computer science, pp 19–21
9.Di Rienzo M, Vaini E, Castiglioni P, Merati G, Meriggi P, Parati G, Faini A, Rizzo F (2013)Wearable seismocardiography: towards a beat-by-beat assessment of cardiac mechanics inambulant subjects. Auton Neurosci 178(1):50–59
10.Hsieh J-C, Li A-H, Yang C-C (2013) Mobile, cloud, and big data computing: contributions,challenges, and new directions in telecardiology. Int J Environ Res Public Health 10(11):6131–6153
11.Xia H, Asif I, Zhao X (2013) Cloud-ecg for real time ecg monitoring and analysis. ComputerMethods Programs Biomed 110(3):253–259
12.Gyselinckx B, Vullers R, Van Hoof C, Ryckaert J, Yazicioglu RF, Fiorini P, Leonov V (2006)Human ++: emerging technology for body area networks. In: International conference on very
large scale integration, IFIP. IEEE, Piscataway pp 175–180
13.Zhou B, Ma Q, Song Y , Bian C (2016) Cloud-based dynamic electrocardiogram monitoringand analysis system. In: International congress on image and signal processing, biomedicalengineering and informatics (CISP-BMEI). IEEE, Piscataway, pp 1737–1741
14.Thakkar HK, Sahoo PK (2019) Towards automatic and fast annotation of seismocardiogramsignals using machine learning. IEEE Sensors J 20(5):2578–2589
15.Rai D, Thakkar HK, Singh D, Bathala HV (2020) Machine learning assisted automaticannotation of isovolumic movement and aortic valve closure using seismocardiogram signals.In: 2020 IEEE 17th India council international conference (INDICON). IEEE, Piscataway, pp1–6A Workload-Aware Data Placement
Scheme for Hadoop-Enabled MapReduce
Cloud Data Center
Hiren Kumar Thakkar
1 Introduction
Gaining an insight of knowledge out of Terabyets and Petabytes of rapidly generated
structured and un-structured data is a real challenge in current set of technologies
[1,2]. For any organizations to sustain in highly competitive market, they need
to innovate their strategies and planning based on the current and future need of
customers. At present any top level decision made by top executives are merely
based on the input from various departments where data are not thoroughly analyzed
considering different dimensions such as sentiment analysis [ 3]. Further, Market
trends, customers behavior and constant market innovations are bound to happen
over the period of time which can be detected using the frequent pattern mining
[4]. Any analyses on the data from shorter period of time cannot catch the hidden
knowledge treasure and result in poor growth of an organization [ 5]. Let’s say
wallmart wants to know the response it gets on products when promotional offers
are offered, or % increase in sales on the days of festivals, or demand of newly
launched product. For organizations like wallmart, data generated during special
events or days are more important so are requested and analyzed more often than
other. These frequently requested data (Data Blocks in Hadoop) forms a set called
frequent data blocks. The phenomena of interest in small portion of data out of all is
applicable to many diversiﬁed applications from scientiﬁc, medical, space science,
business to stock market etc. These applications are the potential source of Big Data
generation. Traditional parallel programming technologies has the limitation when
data grow beyond certain limit and have speciﬁc data type requirement to work on.
To tackle the problem, Apache Hadoop is one of the open source platform built
H. K. Thakkar (/envelopeback)
Department of Computer Science and Engineering, School of Technology, Pandit Deendayal
Energy University, Gandhinagar, India
e-mail: hiren.thakkar@sot.pdpu.ac.in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_11185186 H. K. Thakkar
on the concept of MapReduce [ 6,7]. Initially, Hadoop was used to process only
large batch type of jobs, but later sharing of Hadoop cluster among users made it
possible to submit small interactive query type jobs along with Large batch type of
jobs concurrently on the same set of data blocks. Long Batch type jobs work on
the data collected over a longer period whereas Interactive query type jobs work on
the subset of data collected for the same period. For example, A query like “The
most proﬁtable product during last one year” can be called as Long Batch type job
whereas the same query issued for shorter period of time to know the selling pattern
on events, holidays, product launch are called as small interactive type jobs. Usually,
the % of Interactive query type jobs are more than the batch type jobs out of all.
An Apache Hadoop Cluster is not designed for a dedicated purpose where
highly conﬁgured nodes with speciﬁc hardware conﬁgurations are must. Generally,
commodity Hardware (Laptops or Desktops) are used in the formation of Hadoop
Cluster where nodes have different hardware conﬁguration and processing power
[8]. A same task when run on different set of nodes can take different amount of
times to complete depending on the underlying nodes processing capacity. This
inherent heterogeneity among nodes in a cluster need to be considered while
distributing data blocks [ 9]. A node having poor processing power and storing
more number of frequent data blocks can affect the overall performance of a job
[10]. A delay in completion of Map phase can increases the job completion time of
MapReduce program as reduce phase is dependent on the completion of Map Phase.
This stresses on a data distribution scheme which take heterogeneity of nodes into
consideration for better performance.
In a cluster like Hadoop, where multiple different types of jobs are running
concurrently over the same set of data blocks, the data placement scheme play
an important role. Frequent data blocks requested by many interactive queries if
placed on nodes having poor processing speed can prolong the job response time.
For interactive queries, response time must be minimum whereas for Batch type
jobs, higher throughput is desirable.
Any Data placement scheme for hadoop without due consideration of Heteroge-
neous nodes, different types of jobs and phenomena of applications to favour certain
data blocks over rest can inﬂuence the job completion time. We have proposed a
data placement scheme which considers the above mentioned concerns and aimed
to minimize the job completion time of different type of jobs.
The rest of the paper is organized as follow: In Sect. 2, Related works is shown,
Sect. 3shows problem description, Sect. 4shows Proposed Protocol description.
Section 5shows Problem Formulation, Sect. 6shows Data Locality Problem, Sect. 7
shows the conclusion and Future works.
2 Related Works
The Default Hadoop Data placement scheme has few drawbacks and has not
considered:A Workload-Aware Data Placement Scheme for Hadoop-Enabled MapReduce ... 187
1.The Heterogeneity of nodes (Hardware conﬁguration).
2.Shared environment, where different types of job do exist.
3.The scenario, where part of the data set is requested more often then others.
Though, some data placement scheme have been proposed to improve the
performance of Hadoop Cluster they lack of holistic approach. Jin et al. [ 11]h a v e
proposed a data placement scheme called ADAPT, which is based on the availability
of nodes. Based on the availability of nodes, data blocks are distributed. For this,
they designed a stochastic model, where for every node in a cluster the expected
execution time of task is calculated when interrupted. Data blocks are distributed
based on the expected execution time a node take to process a task.
The Data Placement Scheme proposed by Xie et al. [ 12] is based on the
computing capacity of nodes. Based on the response time of nodes, the computing
ratio is determined. Higher the computing ratio, more the data blocks are assigned to
a node. But the protocol has few limitations, the computing ratio of nodes changes
from one application to another. Before running the actual application a proﬁling
is done to determine the node’s computing ratio and later number of data blocks it
should hold. It is not viable to do proﬁling every time for all nodes in a cluster for
every applications prior to their actual execution.
In [13] an improvement of the work [ 12] is proposed. Apart from considering
the computing capacity of nodes they have considered the algorithmic complexity
of functions as well.
DRAW [ 14], has proposed a Data Grouping aware Data Placement scheme where
applications exhibit interest locality and Grouping semantics. In order to maximize
the parallel execution, DRAW [ 14] identiﬁes a set of frequently requested data
blocks and distributes them equally among nodes. Figure 2, shows how DRAW
[14] has distributed frequently requested data blocks equally among nodes.
Though all of the mentioned data placement scheme have improvement over
Hadoop’s default data placement scheme. They have few drawbacks. Jin et al. [ 11],
Wang et al. [ 14] have not considered the Heterogeneity of nodes having different
hardware conﬁguration. Jin et al. [ 11], Xie et al. [ 12], Arasanal and Rumani [ 13],
and Wang et al. [ 14] have not considered the shared environment where different
types of jobs runs concurrently. Jin et al. [ 11], Xie et al. [ 12], and Arasanal and
Rumani [ 13] have not considered the phenomena of applications requesting certain
data blocks more often than others.
3 Problem Description
Let’s assume, as shown in Fig. 1, Node-1 and Node-2 processes 16 and 34 tasks on
an average every hour. There also exist a set of frequently requested data blocks,
which is shown distributed on them. Let’s say Node-1 gets 70% and Node-2 gets
30% of frequent data blocks out of all. Let’s assume that Task arrival rate is 50/h
requesting frequent data blocks. Usually in Apache Hadoop, a task is ﬁrst schedule
to a node where it can get input data block before scheduling on remote node. As188 H. K. Thakkar
Fig. 1 Problem description
Fig. 2 Drawback of draw
shown in Fig. 1, based on the % of frequent data blocks nodes are storing, Every
hour out of 50 tasks Node-1 gets 35 and Node-2 gets 15. But Node-1 has capacity
to process only 16 tasks. Hence, rest 19 tasks are re-schedule on Node-2 to process,
which increases the network trafﬁc and bandwidth usage. This shows how data
placement in cluster of heterogeneous nodes play an important role.
In order to maximize the parallel executions and data locality, Wang et al. [ 14]
has proposed to distribute frequent data blocks in equal amount to nodes as shown in
Fig.2. The proposed scheme improves the performance in homogeneous cluster but
under heterogeneous cluster it results in imbalance of workload execution. As shown
in Fig. 2, every node store stores equal amount of frequent data blocks leads to equal
task arrival rate. Node-1 and Node-2 get on an average 25 tasks to process every
hour. As nodes are having different processing speed, they take different amount of
time to execute assigned number of tasks. Here Node-1, Node-2 take 1.56 and 0.7 h
respectively which in total take 2.30 h to process all 50 tasks. The efﬁcient scheme
shown beside, where frequent data blocks are distributed based on the underlyingA Workload-Aware Data Placement Scheme for Hadoop-Enabled MapReduce ... 189
node’s processing speed. The Execution time of both nodes are similar i.e 1.0 h,
which is an improvement over [ 14]b y0 . 3 0h .
Further, when the Hadoop cluster is shared by multiple users, they submit
different types of job concurrently and can access frequent data blocks. In Fig. 1,
two different type of jobs are shown requesting frequent data blocks. Let’s say
Interactive job is requesting 30% of frequent Data Blocks and is subset of frequent
Data Blocks requested by Batch type job. Without considering the type of job
requesting a frequent data block and distributing them based on only underlying
nodes processing speed can harm the respective job’s performance. If the large
number of frequent Data Blocks requested by interactive job are stored on Node-1
as shown in Fig. 2, the response time will be higher as Node-1 have low processing
speed. The better approach is to put frequent data blocks requested by interactive
jobs to place on nodes having higher processing power.
The procedure can be as follow:
1.First, Determine the percentage of allocation of frequent data blocks to nodes
based on their processing speed.
2.Within the determined percentage of node, What percentage of frequent Data
Blocks of Interactive job and batch type of job to be decided.
In this paper, we have considered:
1.Heterogeneity of Hadoop cluster where nodes have different processing speed.
2.Hadoop cluster shared between Interactive and Batch type jobs.
3.The phenomena where application requests certain Data Blocks more often then
others.
4 Proposed Protocol
We have proposed a data placement algorithm which tries to distribute frequently
accessed set of data blocks aiming to minimize the job completion time by
optimizing the imbalance of workload. Our Proposed scheme also tries to satisfy the
performance parameter of different types of job. The Proposed scheme is consist of
three phases as shown in Fig. 3.
1.In ﬁrst phase, historical data access information is collected from the log ﬁle of
Namenode to determine the frequently accessed set of data blocks, Task arrival
rate in a cluster and to individual nodes requesting frequent data blocks and Task
processing rate of nodes.
2.In Second phase, percentage of frequent data blocks allocation to nodes and clas-
siﬁcation of frequent data blocks based on their requesting jobs are determined.
3.In third phase, the data placement algorithm distributes the frequent data blocks.190 H. K. Thakkar
Fig. 3 A proposed model for algorithm
The proposed algorithm runs continuously to adapt temporal (Time) changes in
the set of frequent data blocks, Task arrival rate in a cluster, Task arrival and Task
processing rate of nodes.
4.1 System Model
The system model of the proposed scheme is shown in Fig. 4and notations are
described in Table 1. The system model has the following assumptions:
1.Nodes are heterogeneous. (Shown in different pattern)
2.Tasks are ﬁrst schedule to nodes containing input data blocks before scheduling
on remote nodes.
3.Hadoop cluster is shared between by Interactive and Batch type jobs
4.At most two tasks can run concurrently on any node iby occupying Processing
slot.
5.Tasks arrival is assumed to be Poisson distribution.
6.Each job has a soft deadline to ﬁnish.
5 Problem Formulation
5.1 Network Model
Let’s assume that there is a set R={r1,r2,...}of Racks in Hadoop cluster. Each
Rackri∈Ris consist of a set {di1,di2,...}of Data Nodes. S={s1,s2,...}is a set
of intra-rack network switches that facilitates local data communication among dataA Workload-Aware Data Placement Scheme for Hadoop-Enabled MapReduce ... 191
Fig. 4 A system model
nodes within a rack with bandwidth BL.G1andG2are inter-rack network switches
that facilitates data communication across racks with bandwidth BG. Any data node
di,jin a cluster can be deﬁned as jth data node in ith rack and has a computational
speed ci,j.
Let’s say, data blocks which are requested repeatedly form a set /Omega1(set of
frequently requested data blocks) whereas rest of the data blocks form a set /Psi1(set
of not frequently requested data blocks). A set /Phi1represents total number of data
blocks in a cluster where /Phi1=/Omega1∪/Psi1.L e t ’ ss a y /Omega1i,jand/Psi1i,jare sets of frequent
and non-frequent data blocks stored on data node di,jrespectively with total data
blocks /Phi1i,j.
Here,
|/Omega1|=|R|/summationdisplay
i=1|ri|/summationdisplay
j=1|/Omega1i,j|, |/Psi1|=|R|/summationdisplay
i=1|ri|/summationdisplay
j=1|/Psi1i,j| (1)
Let’s say, λ/Omega1andλ/Psi1are the task arrival rate in a cluster requesting data blocks
from set /Omega1and/Psi1respectively. λrepresents total task arrival rate in a cluster, where
λ=λ/Omega1+λ/Psi1.λ/Omega1i,jandλ/Psi1i,jare the task arrival rate of any node di,jin a cluster
with where λ/Omega1i,j≤λ/Omega1andλ/Psi1i,j≤λ/Psi1.
Here,192 H. K. Thakkar
Table 1 Notations
Symbol Meaning
R A set of racks in a cluster
ri A set of data nodes in a Rack i
S A set of intra-rack network switches
G1,G2 Inter-rack global network switches
di,j jth Data node in ith rack
/Phi1 A set of total number of data blocks in a cluster
/Psi1 A set of not frequently requested data blocks in a cluster
/Omega1 A set of frequently requested data blocks in a cluster
/Psi1i,j A set of not frequently requested data blocks on di,j
/Omega1i,j A set of frequently requested data blocks on di,j
λ Task arrival rate for any data block in a cluster
λ/Psi1 Task arrival rate for not frequently requested data blocks in a cluster
λ/Omega1 Task arrival rate for frequently requested data blocks in a cluster
λi,j Total task arrival rate of di,j
λ/Psi1i,j Task arrival rate of di,jfor not frequently requested data blocks
λ/Omega1i,j Task arrival rate of di,jfor frequently requested data blocks
/Theta1i,j Workload arrival of di,j
μi,j Task processing rate of di,j
BL Local bandwidth among data nodes within a rack
BG Global bandwidth among data nodes across racks
D Size of the data block
yi,j Binary variable indicating if data node di,jis free or not?
xi,j Binary variable indicating if task is processed locally on data node di,jor not?
ci,j Computational speed of di,j
Tc Task completion time
|λ/Omega1|=|R|/summationdisplay
i=1|ri|/summationdisplay
j=1|λ/Omega1i,j|, |λ/Psi1|=|R|/summationdisplay
i=1|ri|/summationdisplay
j=1|λ/Psi1i,j| (2)
Ifkpercentage of tasks arrival in a cluster are for data blocks from set /Omega1then
λ/Omega1=k.λandλ/Psi1=(1−k).λ. Higher the value of kindicates λ/Omega1approaches to λ.
Hence with kapproaches to 1, λ≈λ/Omega1.
5.2 Task Processing Model
A task in Mapreduce cluster is ﬁrst schedule on a data node di,jcontaining input
data blocks before scheduling on remote node. When λ/Omega1tasks are distributed toA Workload-Aware Data Placement Scheme for Hadoop-Enabled MapReduce ... 193
individual data nodes di,jin a cluster, task arrival rate λ/Omega1i,jdepends on the size of
the set /Omega1i,j. Higher the value of |/Omega1i,j|, higher the number of tasks arrival ( λ/Omega1i,j).
Every Task in a Mapreduce cluster is schedule to process a single data block,
w h i c hi so fﬁ x e ds i z e DMB. With ci,jMB
Sprocessing speed, a data node takesD
ci,j
amount of time to complete a task. Hence, the processing rate ( μi,j)o fa n yd a t a
nodedi,jin a cluster can be deﬁned asci,j
Dnumber of tasks in every time unit.
A task can run locally or on remote nodes depending on the availability of data
nodes having replica of input data blocks. A binary variable yi,jcan be deﬁned as
follow: Where,
yi,j=/braceleftBigg
0i fdi,jis free
1i fdi,jis busy(3)
Let’s say pindicates the number of replicas available in a cluster for every
data block. Usually in Apache Hadoop Mapreduce, p=3 replicas are made. 2D
Markov chain shown in Fig. 5decides whether a task will run locally (a data node
having replica) or on remote data node. Each state in Markov chain is represented
as (a,b). Where aindicates the availability of replica on data node and bindicates
the probability of a data node to be free.
A binary variable acan be deﬁned as follow:
a=/braceleftBigg
0 if replica is not available
1 if replica is available(4)
Figure 5shows data nodes with different probabilities where Pi>Pj>Pk.
A state (0,1)indicates any data node which is free and has no replica. The tasks
are ﬁrst schedule to data nodes having replica (i.e.a=1)and probabilities in
descending order. Based on the availability of data nodes, tasks are either processed
locally (i.e.a=1)or on remote nodes (i.e.a=0). Any Task which runs on remote
nodes takes εamount of extra time to transfer replica depending on the link
bandwidth.
The task completion time depends on the host data node. There are three different
possible ways a task can be processed:
1.Locally (On Data node with a replica).
2.Remotely within a Rack (On Data node without a replica and pull it from other
node within a Rack).
3.Remotely out of Rack (On Data node without a replica and pull it from other
node out of Rack).
A task processed on remote node (within a rack orout of rack) has and extra
overhead of Data block transfer time apart from task processing time. The task194 H. K. Thakkar
Fig. 5 2D Markov chain for local or remote processing
completion time is the addition of Data block transfer time and processing time.
It can be deﬁned as follow:
For a task running locally:
TCL=D
ci,j(5)
For a task running Remotely Within a Rack:
TCRWR=D./parenleftbigg1
ci,j+1
BL/parenrightbigg
(6)
For a task running Remotely Out of Rack:
TCROR=D./parenleftbigg1
ci,j+2
BL+1
BG/parenrightbigg
(7)
So, on an average task running remotely can be deﬁned as follow:
TCR=TCRWR+TCROR
2(8)
5.3 Workload Distribution
The workload arrival of any node di,jis the multiplication of Task arrival rate λi,j
and average task processing time of data node.
/Theta1i,j=λi,j.1
μi,j(9)A Workload-Aware Data Placement Scheme for Hadoop-Enabled MapReduce ... 195
The variance can be deﬁned as:
/Theta1variance =1
k.|R|/summationdisplay
i=1|ri|/summationdisplay
j=1/parenleftbig
/Theta1i,j−/Theta1mean/parenrightbig
(10)
where, /Theta1mean is:
/Theta1mean=1
|R|.|r||R|/summationdisplay
i=1|ri|/summationdisplay
j=1/Theta1i,j (11)
6 Data Locality Problem
In the presence of higher value of k, higher (/Theta1variance )indicates, highly un-equal
frequent data blocks distribution.
Highly un-equal frequent data blocks distribution lead to highly un-equal task
arrival rate ( λi,j) of data nodes. With μi,jprocessing rate, data nodes can be
categorised as follow:
Category 1 For any node di,j,i f/parenleftbig
λi,j>μi,j/parenrightbig
then task arrival rate is more than
processing rate.
Category 2 For any node di,j,i f/parenleftbig
λi,j=μi,j/parenrightbig
then task arrival rate is equal to
processing rate.
Category 3 For any node di,j,i f/parenleftbig
λi,j<μi,j/parenrightbig
then task arrival rate is less than
processing rate.
Asλi,j∝/Omega1i,j, data nodes in category 1 has proportionally more number of
frequent data blocks than it processes in every time interval. Likewise, data nodesin category 2 and 3 indicate equal and less proportion of frequent data blocks
respectively.
For any data node ( di,j) in Category 1, number of tasks goes for remote
processing are ( λi,j−μi,j).
Lets say ‘ l1’, ‘l2’ and ‘ l3’ number of data nodes are present in Category 1,
Category 2 and Category 3 respectively.
With ‘ l1’ number of data nodes in Category 1, the number of Remote Executions
(NR) can be deﬁned as follow:
NR=/summationdisplay
di,j∈l1(λi,j−μi,j) (12)196 H. K. Thakkar
Higher the value of ‘ l1’ indicates higher number of tasks go for Remote
Executions in a cluster. If these tasks are from Mjobs running concurrently in a
cluster, then on an averageNR
Mtasks runs remotely per job.
Every remote processing of task takes on an average TCRtime to complete.
Where ( TCR>> T C L). This affects the job completion time.
The Objective is to minimize the number of Remote Executions ( NR) deﬁned in
Eq. (12), subject to the constraints deﬁned in Eqs. ( 1)–(5), (8), and ( 12).
7 Conclusion and Future Works
In this paper, a novel cardiac big data analysis platform based on apache hadoop
and mapreduce is introduced to provide real-time cardiac health information. The
IoT sensor based ECG and SCG data collection in a hassle free and convenient way
is proposed to facilitate real-time data analysis transfer to remote cardiac hadoop
cluster. The apache hadoop big data processing cluster is employed to satisfy the
need of cardiac big data processing. A MapReduce based modeling of ECG and
SCG data processing cum analysis is designed to make the entire data analysis
compatible with apache hadoop platform. The evaluation results show that the
propose framework is highly efﬁcient and match with the need of real-time health
monitoring. In future, the integration of hadoop cluster with cloud environment
can be introduced to provide low cost alternative to users without engaging with
costly hardware equipments of cluster. Moreover, a more focus on novel ECG/SCG
abnormality detection methods can be introduced to further reduce the processing
and analysis time.
References
1.Madden S (2012) From databases to big data. IEEE Internet Comput 16(3):4–6
2.Thakkar HK, Sahoo PK, Mohanty P (2021) Dofm: domain feature miner for robust extractive
summarization. Inform Process Manage 58(3):102474
3.Feldman R (2013) Techniques and applications for sentiment analysis. Commun ACM
56(4):82–89
4.Thakkar HK, Shukla H, Sahoo PK (2022) Metaheuristics in classiﬁcation, clustering, and
frequent pattern mining. In: Cognitive big data intelligence with a metaheuristic approach.
Elsevier, Amsterdam, pp 21–70
5.Gan W, Lin JC-W, Fournier-Viger P, Chao H-C, Tseng VS, Philip SY (2019) A survey of
utility-oriented pattern mining. IEEE Trans Knowl Data Eng 33(4):1306–1327
6.Dean J, Ghemawat S (2008) Mapreduce: simpliﬁed data processing on large clusters. Commun
ACM 51(1):107–113
7.Thakkar HK, Sahoo PK, Veeravalli B (2021) Renda: resource and network aware data
placement algorithm for periodic workloads in cloud. IEEE Trans Parallel Distrib Syst
32(12):2906–2920A Workload-Aware Data Placement Scheme for Hadoop-Enabled MapReduce ... 197
8.Mavridis I, Karatza H (2017) Performance evaluation of cloud-based log ﬁle analysis with
apache hadoop and apache spark. J Syst Softw 125:133–151
9.Vavilapalli VK, Murthy AC, Douglas C, Agarwal S, Konar M, Evans R, Graves T, Lowe J, ShahH, Seth S et al. (2013) Apache hadoop yarn: yet another resource negotiator. In: Proceedingsof the 4th annual symposium on cloud computing, pp 1–16
10.Thakkar HK, Dehury CK, Sahoo PK (2020) Muvine: multi-stage virtual network embedding incloud data centers using reinforcement learning-based predictions. IEEE J Sel Areas Commun38(6):1058–1074
11.Jin H, Yang X, Sun X-H, Raicu I (2012) Adapt: availability-aware mapreduce data placementfor non-dedicated distributed computing. In 2012 IEEE 32nd international conference ondistributed computing systems. IEEE, Piscataway, pp 516–525
12.Xie J, Yin S, Ruan X, Ding Z, Tian Y , Majors J, Manzanares A, Qin X (2010) Improvingmapreduce performance through data placement in heterogeneous hadoop clusters. In: 2010IEEE international symposium on parallel & distributed processing, workshops and PhD forum(IPDPSW). IEEE, Piscataway, pp 1–9
13.Arasanal RM, Rumani DU (2013) Improving mapreduce performance through complexityand performance based data placement in heterogeneous hadoop clusters. In: Internationalconference on distributed computing and internet technology. Springer, Berlin, pp 115–125
14.Wang J, Shang P, Yin J (2014) Draw: a new data-grouping-aware data placement schemefor data intensive applications with interest locality. In: Cloud computing for data-intensiveapplications. Springer, Berlin, pp 149–1745G Enabled Smart City Using Cloud
Environment
Parul Bakaraniya, Shrina Patel, and Priyanka Singh
1 Introduction
Deﬁnitions of a wise city and its borders are many and are often used in various
ways to emphasize one or the other within it. However, it is common practice to use
new technologies to ﬁnd effective and economical solutions to urban challenges.
Precisely, the Internet penetrates the physical realm and becomes the Internet of
Things (IoT), providing unprecedented opportunities. When it comes to production,
this event is often said to change the industry; When it comes to urban migration,
people prefer the word “smart city”, but the two names sometimes change. A
smart city project usually consists of a few key components that are based on
the collection, processing, and interpretation of the data used to transform speciﬁc
aspects of the city – this can be called “activation”. Based on sensory and action
such response loops are not different from those found in living systems, for
example in individual interactions. Clearly, it can be stated that a smart city requires
Artiﬁcial Intelligence. The ubiquitous connection of mobile subscriptions is equally
important. For example, Smart City can be described as a high-technology ﬁeld
such as “Information Communication Technology (ICT), planning, power gener-
ation” and, similarly, cooperation creates citizen beneﬁts: welfare., inclusion and
collaboration, environmental quality, and intellectual development; It is governed
by departments, governed by local government and development regulations, and
may specify policies [ 8]. “.
P. Bakaraniya · S. Patel ( /envelopeback)
Department of Computer Engineering, Sardar Vallabhbhai Patel Institute of Technology, Vasad,
Gujarat, India
e-mail: parulbakaraniya.comp@svitvasad.ac.in
P. Singh
Department of Computer Engineering, SRM University, Amravati, India
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_12199200 P. Bakaraniya et al.
In recent years, the Smart City idea has gained popularity. One of the smartest
modern city models is the Internet of Things. The purpose of the paper is to
identify major research guides in the ﬁelds of IoT, Edge Computing, 5G Technology,
and Cloud Computing-based Smart Cities [ 9]. Smart cities are rapidly evolving
with technological advances in wireless networks and sensors, informatics, and
human-computer interactions. An urban computer provides the ability to process
and integrate such technologies for the betterment of conditions in the city. This
chapter deals with various aspects of cloud computing to support intelligent
cities through urban development. Provides the computing power and computer
computing required in the simulation of urban systems for timely testing of cloud
computing. The portable computer provides portability and social networking so
that citizens can report instant information in order to compile better information.
Edge Computing allows analysis of the data generated by devices, reducing data
overload in the main storage and operation. Future challenges and directions to
integrate three computer technologies to achieve advanced computer infrastructure
that supports smart cities are discussed.
Mobile networks, such as 5G, are evolving from 4G networks and will continue
to offer greater services to end customers. Over time, more Internet of Things (IoT)
with 5G networks will connect gadgets to enable lower latency and more reliable
connectivity. The process management procedure, as well as the vast volume of
data linked with IoT and 5G-based technologies, are the key ﬂaws. The acceptance
of cloud computing (CC), 5G, and IoT is therefore a very important term in its
application. Market interest in IoT has increased due to advances in 5G and CC
technology. 5G can meet current needs such as smart power applications and more
is coming in the future. 5G users can be categorized as delayed communication,
advanced mobile broadband (eMBB), and critical connectivity for large IoT clusters.
CC helps manage data generated by IoT as it enhances 5G network capacity. The
integration of these technologies such as CC, IoT, Automotive and mobility, media
and content, public/smart cities, healthcare, manufacturing, energy, and utilities are
several businesses that beneﬁt from 5G. The beneﬁts of integrating 5o-enabled
IoT systems with the cloud ecosystem are covered in this chapter. This paper is
arranged as follows. Section 2 outlines the new phenomenon models of edge and
fog computing, cloud computing, IoT, 5G, and how they are used to build smart
cities. Section 3 describes the proposed Smart City framework. Section 4 describes
the context of the use of smart cities as intelligent grids, smart transport, smart
health care, and prudent management, as these functional areas of smart cities
have contributed to the adoption of computer fog over the years to address many
challenges. The interest in attendance has increased. Section 5 focuses on some
smart city case studies. Section 6 describes challenges and issues such as hyper-
scale, virtualization, reliability, scalability, on-demand, cost savings, and green
energy, privacy, and security. Finally, Section 7 closes the study by highlighting
the most pressing concerns in public research.5G Enabled Smart City Using Cloud Environment 201
2 Technologies Used to Build the Smart City
How smart is your home and city? If it is like me, it is much smarter than it was 10
or 5 years ago. Because our homes now have smart devices like smart thermostats,
smart speakers and smart lighting. But the habit of smart space extends beyond our
homes. All over the world, every city is also smart. Smart City is an information and
communication technology (ICT) framework to develop, implement and promote
sustainable development strategies to address urban growth. Smart cities use the
Internet of Things and Information and Communication Technology (ICT) devices
to acquire and analyze data. Cities then use this data to improve infrastructure, social
services, and services. Below, we describe the technology of smart cities that use this
technology, citizens will ﬁnd a more efﬁcient and quality life, and together we will
build a smart city and achieve our goals.
2.1 Edge and Fog Computing
A fog computing system or operation is a segmented computer infrastructure or
process that connects a data source to a cloud or other data center using computer
resources. Real-time data gathering and analysis are required to enable real-time
automation. Address high delays and low bandwidth problems that occur during
network data processing. Despite the fact that cloud data has created an extended and
ﬂexible environment for analysis, communication and security issues between local
assets and clouds have resulted in inactivity and other risk concerns. Fog computing
and edge computing were developed to reduce these risks. These concepts have
brought computer resources closer to data sources, allowing these assets to be
more useful by allowing them to work with data without needing to interface with
faraway computer infrastructure. Fog computing is deﬁned by Cisco Systems as:
“Fog computing is a virtual platform that provides computer services, storage,
and network between storage devices and cloud computing data centers, often but
speciﬁcally at the edge of a network” [ 15]. The primary goal of both the Edge
computer and fog models is to swiftly store and analyze data, making critical
business applications real-time. Cloud-based services will need to be used for
recovery, leading to signiﬁcant delays. Edge server or gate service will be deployed
locally, with a computer-based solution on the edge or fog: automotive automotive
applications, virtual and unpopular objects we see and smart transport systems Fig. 1
shows the state of the fog computer in a smart fog city and examples of workplaces
that can beneﬁt [ 1].
Data is typically processed on Edge devices before being sent to servers through
Edge gates. Data is evaluated and processed by fog computers at fog nodes on
the local network’s edge. Edge Computing is sometimes mispronounced as a fog
computer by the Open Fog Consortium ( openfogconsortium.org ). A reference for
its fog computer architecture states that: (i) Fog nodes are initially with 3-phased202 P. Bakaraniya et al.
Fig. 1 cloud and fog infrastructure for smart city
architecture but can also provide n tires when required and (ii) provide a fog
computer., The cloud, IoT, and sensor devices may all be used for storage and
control, but Edge provides limited computer resources on the computer edge [ 16].
Its ﬁrst level, near the network limit, usually involves receiving data from end-to-end
devices, standard data, and sensor and actuator controls. The second level involves
ﬁltering, conveying, and converting data. The third level of data conversion is done
into information. There is an almost unanimous agreement in the literature that fog
computing is just the extended part. Many cloud computing technologies are used in
fog computing too [ 17]. In order to fulﬁll the requirement of power, additional fog
nodes can be set up, to improve durability and to provide the necessary Enterprise
Strength Requests.
How Does Fog Computing Work?
Fog ﬁlls networks – not replaces – cloud computing; Fogging offers edge-to-
edge analysis in the short term while also providing for much-needed cloud-based
analysis, and long-term analysis. Although Edge and sensory devices generate and
store data, they sometimes do not have the computational and ﬁnal resources for
sophisticated machine learning and analytical works. Despite their capabilities,
cloud servers are still distant from processing data and reacting in a timely manner.
In addition, all conclusions that connect to sending personal data over the internet
have privacy, security, and legal issues, particularly when dealing with sensitive5G Enabled Smart City Using Cloud Environment 203
material under different nations’ laws. Smart grids, smart cities, smart buildings,
automobile networks, and software-deﬁned networks are all popular fog computing
applications. Depending on how it operates, the fog computer frame has severalcomponents and functions. This includes computer gateways that receive data from
data sources or other cluster storage points such as routers and switches that connect
assets within a network.
The following processes are involved in data transfer via the building of a fog
computer in an IoT context:
1.The default controller reads codes from IoT devices.
2.The controller employs system programming to activate IoT devices automati-
cally.
3.Data is sent from the control system via an OPC base server or other gateway
agreements. (OPC is the standard for interoperability in data exchange on IoT.)
4.This information is transformed into a protocol that can be comprehended, such
as MQTT or HTTP.
Data is transmitted to a fog or IoT gateway after conversion. These endpoints capturedata for additional analysis or send data sets to the cloud for usage.
When Fog Computing can be used?Fog Computing can be used in the following situations:
1.Used when selected data only needs to be sent to the cloud. This selected data ispreferred for long-term storage and is rarely accessible by the host.
2.Used when necessary to analyze data by part of a second, i.e. the delay should be
minimal.
3.Used when large quantities of resources need to be provided in a large area in
different areas.
4.Devices under solid ﬁgure and processing should use fog computing.
5.Real-world examples use fog computing IoT devices (e.g. car-to-car consortium,Europe), sensors, cameras, etc.
Beneﬁts of Fog Computing
The quantity of data transported to the cloud is reduced using this strategy.
As the distance traveled by data is reduced, save network bandwidth.
Reduce the system response time.
Having a “data host” improves the overall security of the system.
Provides better conﬁdentiality as industries can do statistics on their local data.
The Disadvantages of Fog Computing
Increased trafﬁc (heavy data ﬂow) leads to congestion between the host and thefog area.
Power consumption increases when another layer is placed between the host and
the clouds.
Schedule work between host fog and cloudy nodes is difﬁcult.204 P. Bakaraniya et al.
Data storage and calculation and data transfer include encryption which removes
data so that data management is a hassle.
Fog Computing Applications
It can be used to monitor and diagnose patients. Doctors can be notiﬁed if there
is an emergency.
It may be used to keep a real-time eye on the train as high-speed trains require as
little delay as possible.
Can be used for the development of oil and gas pipelines. A high volume of data
is been generated and is unable for further analysis in the cloud.
Fog Computer vs Edge Computing
According to Cisco’s presentation of the Open Fog Consortium, the key distinction
between edge and fog computing lies in the ingenuity and power of the computer [ 2].
In the harshest cases, intelligence is generated through a local network (LAN) and
transmitted from data centers to fog gates, where it is sent to sources for processing
and distribution. At the computer end, intelligence and power may be at the edges or
gates. Edge Computing proponents advocate for lowering the number of failure sites
because each device operates independently and decides which data can be stored
in the central cloud for further analysis. It should be sent.
Standards are produced for each new technology concept and exist to offer
instructions or guidelines to users when using these concepts. Edge computing refers
to moving a computer closer to a data source, while fog computing is a term used
to explain its performance and performance in different contexts. The fog and edges
make it equal to both sides of the coin because they collaborate to shorten processing
times by bringing the ﬁgure closer to the data sources.
Edge Computing for Smart Cities
Many smart devices have been combined with different sensors as a result of
advancements in computer technology and hardware, allowing them to gather data.
As a result of this predicament, the interesting notion of IoT was born, in which all
intelligent objects such as smart cars, clothing, sensors, and industrial equipment
and resources are connected to networks and empowered by data analysis that
enables us to operate, survive and thrive. It dramatically changes the way and
plays. Many scientiﬁc and industrial groups have presented and applied the Internet
of Things idea in numerous domains throughout the years. Edge Computing is a
new example where many computer and archive applications (known as cloud ﬁles
or small data centers) are embedded in the internet to provide cloud computing
capabilities. Edge Computing is a decentralized system that processes data in local
data centers and pushes data obtained from the central system. By using calculations
near the edge of the network, the analysis of complex data is realized in real-time.
Different edge forms; For example, the ridge between the furniture gate and the
middle cloud of the smart home; The margin between the small data center and
the center cloud, and the cloudlet smartphone. Edge computing’s main purpose is
to collect, store, ﬁlter, and deliver data to a central cloud system. Smart City is5G Enabled Smart City Using Cloud Environment 205
a vast expansion of IoT sensor networks that provide a data network for efﬁcient
and effective service and asset management. Typical expansion possibilities include
material ranging from bus tracking to trafﬁc signal management, air quality, and
pollution monitoring. We anticipate that edge computing will have a similar impact
on cloud computing. Edge computing opens up new possibilities for IoT systems,
particularly for AI-powered functions like object identiﬁcation, facial recognition,
language processing, and obstacle avoidance [ 11].
2.2 What Price Does 5G Provide for Fog Computing?
5G offers higher bandwidth, better ratings, and fewer delays. Obviously, fast real-
time data transfer means getting a variety of applications on 5G Internet of Things
(IoT) devices. In short, IoT devices enable connection and status data analysis for
smart cities. Security cameras, door locks, and different sensors may all contribute
to a more seamless living across the city network. These products feature improved
security and communication thanks to 5G. Another signiﬁcant beneﬁt of 5G is that
it enables more efﬁcient connections per square mile. However, Only 2000 active
devices per square kilometer are supported by the 4G radio infrastructure. As IoT
devices compete with smart personal devices, this number should be very high in
the smart cities of the future. Currently, 5G is designed to support 100,000 active
devices in one place.
The ﬁfth-generation technology ensures the advancement of cellular networks
and not just to link people, but also to link and operate machines, objects,
and gadgets. High data throughput, minimal latency, ubiquitous connection, and
compatible apps and services are all key elements for 5G networks [ 18]. The
technology provides a new level of quality and services that enable new consumer
experiences and the inclusion of new industries. In addition, 5G offers multi-Gbps
data rates, ultra-low latency, large capacity, and consistent user information. For
example, Yu et al. [ 19] Fog use has been used to support 5G-enabled Internet of
Vehicles (IOV) and deliver precise positioning and troubleshooting services for the
Global Navigation Satellite System failure (GNSS). They have proposed a topology-
based GNSS emergency service in 5o-enabled IoV , which to collect trafﬁc data and
execute applications quicker, fog clusters and fog nodes are used. The support of
a high number of devices concurrently with 5G cells, as opposed to 4G, is the
important value assigned to 5G on the fog computer. The use of 5G in Smart City
environments means that Smart City’s private parts are automatically connected,
sharing, collecting, and exploiting data in real-time [ 1].
City Governments plan to expand 5G as new solutions to help achieve new goals,
investments and policies in ﬁve key areas:
By 2020, smart cities are expected to produce 16.5 jet bytes of data from the use of
IoT in public works, resources, transportation, buildings and infrastructure. 5G
is ready to help smart cities manage this explosive data growth. Their ‘cutting’206 P. Bakaraniya et al.
capabilities allow cities to control different data bandwidth levels of devices,
systems, and classes of users. It also provides an opportunity to use AI to analyze
the data which were stored. The resulting data is used to automatically execute
current processes. By 2025, more than 55% of all data generated will come
from IoT devices. It prepares real-time transport networks by analyzing such
data from connected cars, road sensors, city cameras, and parking lots. 5G smart
cities are also thriving. Cities use about two-thirds of the world’s energy and
produce 70% of their greenhouse gases. With 5G-enabled technology, urban
areas are at the forefront of green change. Smart meters can help homes and
energy-saving facilities [ 13]. 24/7 water monitoring can help detect lead and
other health risks. Connected lighting systems help reduce energy costs and
keep intelligent CO2 cities emitting 5G stable. Cities produces 70% of their
greenhouse gases on an average. Smart meters can help homes and energy-saving
facilities. 24/7 water monitoring can help to detect lead and other health-related
risk factors. Connected lighting systems help reduce energy costs and CO2
emissions. Pollution sensors update the environment in terms of air quality. IoT
trash cans help track trash and promote recycling. Travel data provide details of
better public transport [ 7].
2.3 Cloud Computing
Cloud computing offers ﬂexible computer and cost-based computer models that
allow full integration and sharing of information and resources within the “cloud”.
With cloud computing technology, scattered computer power can be integrated and
used to store and process data at a very low cost with very high returns. More volume
of data can be stored in cloud with the help of 5G. With less delays, data will take
less time to load. With its increased loading capacity, many IoT devices can be
connected to the cloud. Such cloud-edge interactions improve business efﬁciency
[3].
The features and beneﬁts of cloud computing are as follows [ 11]:
1.Hypercale. Some internet businesses have built massive computer cloud plat-
forms for business operations and have an active cloud presence. For example,
Google Cloud Computing has millions of servers; Amazon, IBM, Microsoft,
Salesforce, Ali and Tencent and other agencies with hundreds of thousands
of cloud servers. Ideally, the cloud could provide users with unprecedented
computer power.
2.Practice. Users may access resources from anywhere utilising various interfaces
and devices thanks to cloud computing. The cloud provides requested applica-
tions by separating computer applications and services into basic portable parts
using virtualization technologies. The application works on the cloud without
specifying a server. Users may utilise the most capable programmes on numerous5G Enabled Smart City Using Cloud Environment 207
devices, such as a computer, keypad, or mobile phone, with just a simple network
connection.
3.Honesty. To provide maximum dependability and availability, cloud computing
employs error tolerance, isomorphic variation of computer nodes, and other
strategies. Cloud computing is more dependable and stable than traditional
internal computer infrastructure.
4.The universe. Cloud computing is not associated with any particular application.
Under the same cloud support, it may support a variety of applications. Different
applications can share the same cloud infrastructure at the same time.
5.Strength. The capacity and level of the cloud can be drastically adjusted and
expanded to suit applications and development needs. Scalability allows a
workload that places high demands on the server to operate efﬁciently but only
temporarily or intermittently.
6.When required. Customers can request and access cloud service providers such
as basic infrastructure such as water, electricity and gas. Based on a collection of
visual and auditory resources in the cloud, tasks such as creating, pausing, and
closing are performed without waiting at any time.
7.Savings. The “Pay-As-You-Go” allows private and business customers to work
with the cloud from simple and inexpensive computer nodes. Cloud automation
systems reduce data center repair costs by eliminating basic repair budgets. Lack
of building infrastructure also eliminates the cost of electricity storage, storage,
maintenance, and labor costs.
2.4 Internet of Things
The advent of the Internet of Things (IoT) almost changed the mind of Smart
City. Smart cities are designed to improve inhabitants’ health care, safety, comfort,
and information. In other words, they improve the whole infrastructure. IoT is an
advanced version of a standard network that aims to connect multiple connected
devices. Wireless sensor networks (WSNs) and machine-to-machine communica-
tion (M2M) are examples of advanced technologies that can help the IoT idea
(M2M). High efﬁciency of efﬁcient supply and resources, the development of
IoT-based systems are needed to address the stated problems and big data from
those systems. Efforts have been made to support the establishment of Smart City
to develop a number of facilities, including Smart Homes, Smart Transportation,
Trafﬁc Management, Waste Disposal Systems, Energy Management Systems, and
Healthcare [ 5]. Figure 2shows one of the most effective ways for IoT to improve
the quality of life and well-being of the community, considering both human and
environmental concerns. As a recent concept, IoT collects data through mobile
devices, social media, transportation, and home appliances.
Citizens live and need new services that support a quality of life. These opportu-
nities lead to urban development; however, many challenges affect the resident’s
everyday life [ 20]. As the catalyst for this change, technology has transformed208 P. Bakaraniya et al.
Fig. 2 IoT concept
our world and life in an extraordinary way. The digital world of communications,
people, and devices sets huge limits on how we interact with our work, travel,
community, and environment; It affects a wide range of sectors including urban
planning, environmental monitoring, administrative forums, and health care. Smart
City is gaining growing alternative sorts of urban development in popularity because
it incorporates the concept of how it is deﬁned across the board. Smart City is an
IoT implementation [ 21] and, therefore, gains its core operating system. As in Fig. 3,
IoT introduces key building blocks to Smart City.
3 SmartCity Architecture
Figure 4shows an advanced view of smart city environment which is fully
controlled by IoT initiated by Edge Computing. Smart IoT devices and sensors
linked to a higher quality of life for citizens are affected by these intelligent IoT-5G Enabled Smart City Using Cloud Environment 209
Fig. 3 The role of IOT for smart city
Fig. 4 Brief overview of edge computing enabled smart city
based surroundings. In addition, IoT may be described in a variety of ways, since it
is linked to many technologies and concepts in the literature.
The presence of a worldwide network infrastructure enables a one-of-a-kind
communication system, as well as seamless integration and communication across
different IoT nodes.210 P. Bakaraniya et al.
–The availability of various technologies should be considered in depth.
–Emphasizing an environment based on the assets of the service of the public
good.
–Intelligent communication between objects and people.
Edge Computing is a state-of-the-art computer model and geo-distributed per-
formance, contextual awareness, mobility assistance, and low latency are the
most prominent features. It resources by transferring computer resources such as
computing capabilities, data, and programmes from faraway clouds to network
edges. Three distinct technologies have recently been employed in literature to
highlight the noteworthy qualities of cloud computing: Mobile edge computing,
fog computing, and cloud computing [ 22]. Edge Computing, for example, saves
network capacity by minimising data transit from end-users to distant clouds. Based
on our book reviews, we are proposing Smart City properties here.
Proposed city of smart cities.
The main aim of building a smart city is to make its programmes and applications
more intelligent. Some of the criteria and characteristics are shown below:
Strong and integrated framework that provides secure and open access;
Community-based building;
A large amount of public and private portable and conﬁdential data can be
stored, accessed, shared, and marked, if necessary, by giving citizens access to
information anytime and anywhere;
An app with integrated analytics and features;
IoT controlled Infrastructure enables the transfer of enormous amounts of data as
well as the assistance with complicated and scattered resources and applications.
Smart City has a four-level horizontal conﬁguration as shown in Fig. 5.
The visual component layer acts as a host for all components of various
intelligent systems, services, and applications. In addition, Data collecting is really
crucial in Smart City by controlling all other activities. In addition, data collection
becomes tough when it comes to the collection of a huge diversity of data.
The following are the major physical challenges:
Large number of resources;
Heterogeneity
Low and limited energy resources.
Based on Fig. 5, the layer below is called the data acquisition layer and is in
charge of listening and gathering information. Wireless sensor networks (WSNs),
intelligent components, and data collecting tools are all found in this layer. This
layer also collects data from numerous sensors and devices at all levels. This layer
demonstrates the most efﬁcient data collecting using a number of strategies. Its
sensory network records a wide range of data. Such as humidity, temperature,
pressure, and light.
Data Navigation Background: This includes all network technology options for
ﬁnding connections between objects and applications. Data sources are linked to5G Enabled Smart City Using Cloud Environment 211
Fig. 5 Proposed framework for smart city deployment
managers and that is why this layer is an integral part of smart city architecture. This
layer connects to different communication networks. This layer contains a variety
of technologies. With regard to its installation, this layer can be divided into the
cloud layer and the network layer. Technologies that provide short-term coverage
are considered access networks, while offering broad coverage. It is divided into
5G, 4G Long Term Evolution (LTE), and 3G Transmission classes.
Data Management Layout: The smart city brain may be established by develop-
ing a data processing layer between the collecting and application layers. This layer
contains a variety of data manipulation, editing, analysis, storage, and decision-
making functions. Because the activities of a smart controlled city are dependent
on data management, a good data management platform is a must. The data layer’s
major goal is to keep data capacity up to date by concentrating on data cleansing,
development, participation, and security. This layer can be broken down into several
sub-units, which include data aggregation, processing, analysis, data storage, and,
ﬁnally, oversight of decision-making.
Application layer: As a smart city-level layer, citizens and the data management
layer are linked through the application layer. As it has a direct relationship
with the citizens, its performance has a profound impact on the mental state and
customer satisfaction of Smart City. This layer covers the wholly intelligent and
developmental process. These systems can provide and process large amounts of
data using a data path layer to allow businesses to streamline all of the application
processes. Smart apps are in charge of putting data management layer choices into
action. Each level denotes a set of characteristics and criteria. In this case, various
computer models have been used and tested. Because Big Data can solve intelligent212 P. Bakaraniya et al.
city difﬁculties, relevant challenges must ﬁrst be identiﬁed. Rare studies, on the
other hand, have combined existing operations and established themselves as a
framework for providing an effective approach to handling big data. Some practicalways to solve a big data problem are shown.
4 Smart City Service Cases
In this section, we will focus on the different uses of smart cities in different domains
and how to create smarter spaces within smart cities. Smart City apps have many
features and can handle a variety of challenges that the same cities around the worldface. Overcrowding, population expansion, infrastructural gaps, inadequate service
delivery, marginalization, poverty, lack of competition, low resilience, and natural
and man-made catastrophes are only a few of these issues. The obstacles are signif-icantly larger in other circumstances, particularly in developing countries, and they
are threatening the future of several of these cities. Accelerate initiatives that address
urban waste management, trafﬁc congestion, public safety, affordable housing,water resource management, smart buildings, energy efﬁciency, renewable energy
sources, private car roaming, citizen involvement, and stakeholder consultation with
smart solutions. Some names. Through inexpensive digital solutions, the FourthIndustrial Revolution aids cities in overcoming speciﬁc stages of development.
4.1 Smart Grid
Researchers are encouraged to obtain, evaluate, and apply real-time power gen-
eration and consumption data, as well as environmental data, due to the rising
adoption of smart grids. To make a higher investment in smart grid infrastructure,always higher accuracy expect improvements in energy efﬁciency and sophisticated
services. In the smart grid area, large amounts of data are generated from a variety
of sources, such as consumer power consumption and power consumption datameasured over a wide range of smart meters. The effective use of large amounts of
data collected in intelligent grid environments can help decision-makers make wise
decisions about power supply levels in line with consumer needs. Smart grid dataanalyses are also used to measure the need for future power supply and demand. In
addition, intelligent grid data analysis can help achieve strategic objectives through
speciﬁc pricing systems designed for supply, demand, and production models.5G Enabled Smart City Using Cloud Environment 213
4.2 Smart Healthcare
Over the past decade, the healthcare industry has generated huge amounts of data.
Rapid population growth in the world has made rapid changes in treatment delivery
patterns and many decisions in the medical ﬁeld. Health experts may gather and
evaluate patient data using appropriate diagnostic tools, which can also be utilized
by insurance companies and management companies. In addition, an accurate
study of healthcare data can help to predict disease, treatment, and morbidity and
prevent avoidable deaths. Smart devices that may be added to homes or clinics to
monitor behavior and aid comprehend patient records can boost the amount and
consistency of information collected on speciﬁc patient health concerns. Real-time
health facilities around the world. Smart V oice Therapeutic Framework: Proposed
to introduce V oice Disorders Assessment and Treatment Framework with Edge
Computing and In-depth Reading [ 4]. User speech samples are collected using
intelligent sensors, which are then analyzed at the edges before being transferred
to the main cloud. Automatic testing decisions are sent to specialists by the cloud
manager to issue instructions to patients. The Sorbrooken V oice Disorders (SVD)
website is a website used for training, testing, and certiﬁcation. The utilization of
particular disorders is the fundamental constraint of his work.
4.3 Smart Transport
Samples obtained from vast amounts of trafﬁc data can aid in the improvement of
transportation systems by providing alternative routes to reduce trafﬁc congestion
and reduce the number of collisions. Things like speed. Additionally, large amounts
of data collected on intelligent transport systems can help integrate shipping and
improve navigation by reducing supply chain losses. Smart transport data offers
many beneﬁts, including reducing local impact and improving safety, and improving
user experience to the end [ 10]. Based on 5G speeds and simpliﬁcation of data, cities
could completely change their trafﬁc systems. This is the ﬁrst time this has happened
as shared sharing replaces regular transportation. An MIT study found that sharing a
ride could reduce New York City’s population by 75%. Fully autonomous cars have
also become a viable possibility with the advent of 5G. When all cars in the city
have a Level-5 autonomy (the highest level – no wheels, no pedals, and no driver
control options), there is no delay [ 6].
4.4 Smart Governance
Big Data Analytics are crucial in the implementation of intelligent governance [ 23].
Data analysis may readily identify agencies and companies with similar interests214 P. Bakaraniya et al.
which leads to interaction between them. This alliance has the potential to help
countries grow. Furthermore, since authorities already know what people need
in terms of health care, social services, education, and other sectors, Big Data
Analytics assists governments in designing and executing satisfactory policies.
Apart from this, the unemployment rate can be reduced by analysis. Big data for
different educational institutions.
4.5 Remote Monitoring
Inpatient services, where beds, experts, and 24-hour care are limited, are vulnerable
to high prices, volume, and power restrictions in some health care centers. Some
of these restrictions can be overcome by remotely monitoring the patient. Several
programs may gather and evaluate external patient data in real-time, delivering
warnings when immediate action is required. Intensive care is one of the most costly
types of medical treatment. Furthermore provides more ﬂexibility to better control
supply and demand while also lowering life expectancy by up to 50% [ 24]. By
delaying or totally avoiding the development of illness conditions, a patient’s health
goes down which can be improved by early identiﬁcation and 5G therapies have
the potential to greatly enhance patient outcomes. Combined with the suite of 5G
applications, the use of remote wearables can have signiﬁcant ﬁnancial beneﬁts for
health care, which includes a 16% reduction in hospital costs. The ﬁnancial beneﬁts
of long-term patient monitoring are estimated at 30% of the total ﬁnancial beneﬁts
of 5G [ 6].
4.6 Event Detection
In 5G Smart City, 5G that connects street lights with video cameras or microphone
sensors to detect a shot can alert public safety ofﬁcers so that they can react
swiftly. Video statistics can provide instant information. AI can be added to improve
security. First responders can take advantage of the reduced delay of 5G video
cameras throughout the city to check the situation. By connecting to a network that
links road signs and interacts with a physician at a trauma center, a 5G-connected
ambulance may drive freely around the city [ 7].
4.7 Emergency Response
In a smart 5G city, when an emergency is detected, the system may initiate a variety
of emergency response measures, such as dispatching ﬁrst responders, medical
teams, and ﬁremen to an emergency location right away. Additionally, in the event5G Enabled Smart City Using Cloud Environment 215
of a ﬁre, the system can seal ﬁre doors and activate ﬁre extinguishers as needed.
Building sensors can detect ﬁres and notify the Control System. The Building
Management System gives commands to the building’s actuators. 5G can alsoenable fast response times, when a ﬁre is detected, ﬁre sources can be activated
in 1–2 s.
4.8 Emotional Monitoring
Sensitive networks aim to monitor the condition or behavior of a particular area.
Mostly moisture sensors, humidity sensors, etc. are commonly used to monitor theregion. These sensors are frequently used to create a distributed monitoring system
that uses machine learning techniques to recognize data that is ambiguous. Nervous
networks help with the difﬁcult duty of monitoring the region for possible risks,breakdowns, and malfunctions, among other things. In an emergency, this can serve
to strengthen security by immediately launching a reaction, such as an emergency
machine.
4.9 Crowd Management
Smart Cities can monitor the broadcast of a television circuit (CCTV) by forcinglimited areas throughout the city. Local cameras use motion detection, and when
they detect movement in their position, they broadcast live to a video monitor for
analysis. Remote control analysis may be based on the object and facial recognitionto provide information alerts to city authorities to guide safety responses. 5G
ensures that video streaming has sufﬁcient guaranteed quality service to maintain
high quality, consistent delays and performance to prevent bumps, and that videostreaming is delivered in a way that avoids network video enhancement functions,
which can compress feeds, making analysis even more difﬁcult.
4.10 Flexible Building Materials
The administration of equipment in structures is referred to as construction automa-tion. The similar systems’ mechanization helps to reduce energy consumption,
improves the comfort level of people within the building, and improves themanagement of failures and emergencies. The sensors embedded in the structure
perform environmental measurements and report these measurements to Local
Controllers (LC). LCs, in turn, report these results to the Property ManagementSystem (BMS). The BMS may perform a variety of functions, including storing
information, sending alarms, or sending commands to the actuator.216 P. Bakaraniya et al.
4.11 Environmental Monitoring
With 5G, Smart Cities can enable more efﬁcient recycling, waste management, air
quality monitoring, and water quality assurance. Air quality is a very important
factor. 19 Being able to track and respond to pollution levels, provide clean water,
and create healthy living spaces has the potential to signiﬁcantly improve the quality
of life for urban dwellers. These collected beneﬁts help to contribute to the city’s
healthy personality, investment climate, and potential growth.
4.12 Smart Electrical Power Distribution
The energy sector is currently advancing to renewable energy, with solar power
plants and wind from around the world. These changes lead to bi-directional electri-
cal ﬂow and increase the ﬂexibility of the energy system. New sensors and actuators
used in the power system to better monitor and control grid power, require real-time
information exchange. A smart grid enhances grid understanding as a power net-
work and system. Improved understanding enhances control and forecasting, both
of which enhances improved performance and economic performance and serves as
a prerequisite for the integration of renewable and non-renewable resources down
the grid and transformation into a new grid. The beneﬁts of the Smart Grid are
spread across a broad spectrum that often includes power and quality improvements,
grid durability, energy-saving, performance details, renewable integration, power
consumption, and safety and security. 5G also provides the ability to use grid-scale
batteries efﬁciently, a very ﬂexible load on the grid, leading to efﬁcient use of fossil
fuels.
4.13 Smart Precision Agriculture
IoT-based intelligent infrastructure is recommended for the producer with the aid of
edge nodes to reap the beneﬁts. The basic structure consists of four levels, namely,
a layer of objects, a layer of edges, a layer of mist, and a layer of contact. In
the main stage, the object can be an actuator, controller or sensor, designed to
meet the production environment. Interaction, retention, intelligent analysis, and
proximity are all functions of edge computing. Edge computing is advantageous
because it reduces the time required for control, analytical response, and sensor
monitoring. Short-term storage can be used to improve the performance of an
intelligent framework [ 25]. In [ 25], shared delays and improved power connections
were achieved with the Precision Agriculture (PA) data storage algorithm. The data
storing technique was designed to improve wireless network node sleep/waking
times. Sensor node data requests are cached before transferring. This enhances
sensory health and, as a result, the PA system’s energy efﬁciency.5G Enabled Smart City Using Cloud Environment 217
4.14 Animal Health Monitoring System
Animal welfare in a smart agricultural environment is better [ 22]. An animal-
based system, an environmental strategy, and a farm controller are all part of the
planned framework. The farm operator may be used either locally or remotely. The
Raspberry Pi (R-Pi) is a tiny computer that was used to create an underground
system and an animal-focused application. In system use, local RPi observes the
situation, and R-Pi checks animal health. The workspace using the farm controller
performs centralized system functions. The capacity of the planned structure to
convey alarms to mobile phones is one of its most notable features. These warnings
are fully utilized to determine whether animals are unwell, especially if a cow is
pregnant due to body temperature. The authors failed to evaluate the resilience of
the sensory network employed in data collecting, which might be vulnerable to data
loss [ 4] (Fig. 6).
Fig. 6 Smart city application areas218 P. Bakaraniya et al.
5 Case Study of Smart City
Examples of studies from cities around the world was collected per term, cities that
have successfully adopted programs and expanded them beyond the testing phase
are included. These include undertakings that make use of sophisticated technology
and technology and show cities that have done little to increase the utilization of
resources to satisfy people’s needs, the environment, and the economy. This section
presents examples of smart cities empowered. The section provides an overview of
these courses, detailing the aims, organizations participating, provider status, and
country.
5.1 Barcelona
Barcelona is a smart city with various technical aspects which makes it IoT con-
trolled [ 26]. The project participants focused their efforts on ﬁve application areas
including power/feature control monitoring, access control and cabinet telemetry,
event-based video, trafﬁc management, and required communications. A signiﬁcant
number of powerful IoT devices must be installed in order to create a smart city,
which in turn creates performance and space issues. Regarding this, the number of
panels employed in the purchase Barcelona exceeds 3000. On the other hand, The
fog computer will unquestionably allow for real-time decision making, automatic
power resources, and the use of sophisticated algorithms to facilitate trafﬁc ﬂow.
5.2 Smart Dubai Happiness Meter – Dubai, United Arab
Emirates (UAE)
The Smart Dubai Happiness Meter is a extremely effective instrument for measuring
emotions, which is used to measure happiness in the midst of a city experience
in thousands of touch scenes. The Happiness Meter has been accepted by all
high-ranking city authorities. Records customer satisfaction details at city level in
the integrated dashboard. A authorization team was formed and a policy for the
use and development of the city experience. The technique was used on a large
basis throughout the city, and the to assure enjoyment, the design was purposely
kept simple. Criteria across a range of areas, including public services, mobility,
energy, the environment, and public services. The aim is to transfer to private
companies in a timely manner. Used in stages, the tool was tested in a number
of government agencies and later distributed to 172 public and private companies
across all customer service channels (websites, mobile applications, environmental
service centers) throughout the year, beneﬁting collaboratively to make it a reality.5G Enabled Smart City Using Cloud Environment 219
Users. In two and a half years, more than 22 million joyful votes were gathered from
4400 contact points across 172 organisations [ 14].
5.3 #SmartME
#SmartME is a project designed to develop Messina into a smart city [188].
A team of researchers launched the experiment. The primary goals of the #Sm
SmartME project are to create a smart city infrastructure that will allow all people to
contribute to infrastructure by sharing hardware. The #SmartME framework consists
of three layers: the application layer, the Stack4Things layer and the city layer. The
Stack4Things layer was created by the University of Messina to allow administrators
to control Iota devices independent of their physical location. Visual acuity, network
compression, remote control and customization, and fog music are all key elements
of Stack4Things.
5.4 Urban Area Quality Index – Russian Federation
It is a widely used monitoring instrument. The indicators assess the quality of the
city’s infrastructure, the attractiveness of city parks or pedestrian attractions, and the
identiﬁcation of undeveloped areas of the city. The Russian government a program,
with the goal of increasing Index points by 30% in over 1000 communities and
reducing the number of uncontrolled cities in the country by 2024. System rating
has never been seen. To prioritise projects methodically at the national level and to
guarantee consistent quality of evaluation in project implementation outcomes in
more than 1000 cities, the Index is a necessary tool to support the annual quality
assessment and status of those urban areas. One of the services later adopted the
specialized strategy, which was ﬁrst tried in 20 cities during the test. It soon became
a nationwide instrument for rating each city’s urban quality and has been extensively
utilized by governments, mayors, business people, and ordinary residents [ 14].
We conclude from this part that extremely effective simulation tools for the
deployment and evaluation of applications for intelligently based cities should be
developed. For example, IoTIFY is an online cloud-based network module that
allows for intelligent waste management simulations, intelligent parking spaces,
intelligent trafﬁc lights, intelligent trafﬁc signals, and intelligent transport [ 27].
6 Challenges and Problems
A city that properly organizes and manages its fundamental activities with data
and digital technology is efﬁcient, inventive, inclusive, and adaptable. Integrating220 P. Bakaraniya et al.
Table 1 Challenges of a smart city
Citizens Mobility Environment Governance
Joblessness
Social integration
Population ageing
Urban violence and
insecurity
Health and
emergency
managementSustainable
mobility
Interoperatibility
Trafﬁc congestion
Lack of public
transport
PollutionEnergy saving
Population growth
A comprehensive
approach to
environmental and
energy challenges
Climate change
effectFlexible governance
Citizens and
government are at
odds.
Unbalanced
urbanisation
Sustainable economy
digital technology, particularly artiﬁcial intelligence (AI), into municipal processes
and services provides new and cheap alternatives for the city to address its own
concerns. Next, the major obstacles, problems, and open challenges identiﬁed by
smart cities are summarized. In particular, we have highlighted four important types
of difﬁculties, as illustrated in Table 1[8]: nationality, mobility, governance, and
nature.
Research Challenges
Some of the most critical research issues in business and technology are addressed
in this area. In the ﬁrst section, we look at business issues. In the second section, we
will encounter technological hurdles.
6.1 Business Challenges
Commercial enterprises have shown an interest in the smart city business model in
recent years as the market for future smart technology advances has grown, and the
provision of big data has expanded. Business executives encounter obstacles when
it comes to integrating IoT and big data to better their operations. Here, we discuss
some of the business-related obstacles that business owners must address in order to
fully reap the beneﬁts of large city smart data [ 10].
6.1.1 Planning
Creating integrated intelligence systems and metropolitan data management is a
major challenge for smart city planners. Most of the data that will be shared will
need time and money in order to cover future costs and potential requirements.
Designing a good performance model and guidelines will help authorities develop a
smart city strategy at a minimal cost. as support.5G Enabled Smart City Using Cloud Environment 221
6.1.2 Stability
Participants’ actual involvement and connection with IoT technology and a tremen-
dous amount of information may be used swiftly and reliably, ability to reach
their complete capacity. A smart city may expand its resources by utilizing IoT
capabilities and big data. The issue for cities is to recognise the beneﬁts of using
big data to improve the standard of living of their residents through great decision,
knowledge, and customer service. According to the researchers [ 28], the difﬁculty
of constructing a smart city may be related to a lack of resources to undertake
infrastructure investment, model development, and sustainability.
6.1.3 Market Source and Customer
New technology may help ﬁrms broaden their reach, enhance management choices,
and speed the creation of new goods and services. The multiplicity of smart gadgets
and apps utilized in the urban environment, on the other hand, makes it difﬁcult
for enterprises to discover suitable market resources and clients. Many social
networking applications, for example, can be used on a regular basis by consumers;
tracking such clients may necessitate additional organizational effort to identify the
correct customers. Furthermore, IoT has introduced new hurdles in coping with
noisy and bright items.
6.1.4 Smart City Acquisition Costs
Another challenge to being accepted into smart cities is the cost. Considering smart
cities, it requires a range of components, the government may face signiﬁcant costs
in locating them owing to a scarcity and people. TAs a conclusion, standard open
architecture and technologies will reduce costs in this sector. Efforts to unlock
analogous structures and technologies, on the other hand, should be stepped up.
Strong open standards that are maintained in collaboration and compliance will
promote data interaction and exchange across various devices, apps, goods, or
services in a modern city.
6.1.5 Cloud Computing Integration
While cloud computing technology offers ﬂexibility and cheap cost for large –
scale data management, merging it with IoT to reap the beneﬁts of a smart city
is a signiﬁcant problem. Regardless of the fact that cloud – based services have
lately been widely developed, their incorporation into a smart city raises a number
of security, administrative, and open areas concerns. These problems arise as a
result of the need to move certain data and functions from the gateway to a cloud.
To handle the computing difﬁculties, security, administration, and ﬂexibility of the222 P. Bakaraniya et al.
smart city application platform, a suitable framework for the cloud business must be
built. Furthermore, the capacity to personalize services based on consumer recom-
mendations or demands draws more customers to such communication providers,
resulting in increased income. Furthermore, cloud providers are monetizing data
center integration by hosting their systems in multi-vendor centers, making it easier
for them to deliver services in varied or various geographical settings [ 10].
6.2 Technical Challenges
The increased need for smart cities and big data drives new inventions, and the
creation of new smart apps is critical. However, in order to increase a smart city’s
resources, the data acquired must be carefully handled. This section addresses some
of the technical issues connected with big data and smart cities.
6.2.1 Privacy
In the age of big data, some people’s knowledge in a smart city is vulnerable to
analysis, sharing, and abuse, raising worries about identiﬁcation, theft, and control
failure. For example, a large amount of data identifying persons about residents,
such as public works and locations, is collected on a daily basis. Despite several
attempts to resolve such problems, getting massive volumes of private data acquired
by city-smart technology from thieves and theft becomes a signiﬁcant difﬁculty.
Furthermore, while successful cyber assaults Smart cities are still uncommon, but
they raise a number of internet security risks that must be addressed. Researchers
discovered privacy vulnerabilities that may necessitate more investigation, like
connectivity, graph comparisons, and so on [ 10].
6.2.2 Data Analysis
Data analysis is regarded as a vital source of growth and well-being in each modern
metropolis. This knowledge carries with it the difﬁculties that must be addressed in
order to improve our inhabitants’ quality of life and make their communities more
sustainable. In a smart city, data is collected on a variety of things; data acquisition
and decision-making require new algorithms and methods of observation, which
affect tasks centered on a smart city. For example, the loss of power or water
caused by improper equipment can be reduced by comparing user meters with other
applications. Therefore, faster data processing becomes even more important, while
traditional store processing methods, where each company acquires its data and
stores it for future access, may no longer be eligible.5G Enabled Smart City Using Cloud Environment 223
6.2.3 Data Integration
Smart City Data combines several data types by utilizing a number of smart devices
strategically placed across the city. However, the vision of a wise city is to gather
information from a variety of sources; One of the major difﬁculties that must be
solved is data integration inside a smart city. More and more technologies have
been integrated into smart cities in recent years, lowering technological hurdles to
data management. However, data quality is one of the most challenging issues in
any data collection process, especially if data is incorrect, lost, use incorrect format,
and or incomplete [ 12].
6.2.4 Visualization based on GIS
Visualization based on GIS Geographic information systems (GIS) are commonly
used to locate and analyse local data; GIS has lately acquired prominence for city
planning, environmental planning, trafﬁc monitoring, and transportation acquisition
method. Active GIS-based visualisation is critical for smart city apps since it can
deliver interactive and user-friendly platforms. These platforms, on the other hand,
necessitate the use of 3D and touch screen technologies, as well as intelligent city
applications. Such integration can help policymakers translate data into information,
which is critical for making quick decisions [ 10]. The information gathered from the
data model will be expressed in accordance with the needs of the user. Developing
practical and adaptable gadgets and software programmes based on smart city
technology is a thrilling way to realise the goal of a smart environment.
6.2.5 Quality of Service
A variety of technologies must be added in order to construct a smart city.
Another barrier to smart city adoption is the quality of service provided by diverse
technologies. For example, in order to fulﬁl the aim of a smart city, networks
must be dependable, adaptable, controllable, and tolerant. Similarly, managing high-
performance data and processing systems powered by cloud-based services is an
open challenge. Before a smart city application can be completely integrated, the
QoS supplied by this technology mets. The framework and methods for deﬁning
and implementing QoS parameters are critical.
6.2.6 Computational Intelligence Algorithms for Smart City Big Data
Analytics
CI algorithms, such are effective, efﬁcient, and durable in information engineering.
Data mining, machine learning, and computers However, computational methods’
efﬁciency, efﬁciency, and resilience are restricted to tiny data sets. As a result,224 P. Bakaraniya et al.
these algorithms are ineffective for analysing intelligent city data. Smart city big
data has enabled current computer intelligence algorithms to function in big data
analysis. As data sets expand in size, the efﬁciency, efﬁciency, and durability of
computer-produced algorithms decline, making it inappropriate for testing informa-
tion supplied by a smart city [ 29].
7 Conclusion
The tremendous expansion in gadgets linked in metropolitan areas as a consequence
of rapid data accumulation, which has piqued the interest of many academics from
numerous sectors. This study seeks to offer a high-level understanding of how
various technologies work in a wise city. In this context, we’ve spoken about
the permissive technology utilised in smart infrastructure cities. It is therefore
recommended to construct a smart city for the purpose of managing large city
data and smart city applications in which big data analysis may play a signiﬁcant
role. A number of events too were investigated. Finally, some possible research
problems were given in order to serve as research guides for future studies in the
area. To address the problems, the beneﬁts and limits of cloud computing, edge
computing, and mobile computing in smart city operating systems were examined.
Cloud computing provides an integrated and efﬁcient platform, large infrastructure,
sustainable and green software and software development, as well as security and
acquisition difﬁculties. Edge computing reduces viewing latencies and boosts data
collection efﬁciency, while also boosting data privacy and security, lowering data
transit burden over a computer network, and enabling a more diverse allocation
of machine needs. We found that extremely effective simulation tools for use in
ensuring intelligent computer-based applications should be developed. We should
develop the best acting novels to test computers and other emerging technologies in
smart cities.
References
1.Badidi E, Mahrez Z, Sabir E (2020) Fog computing for smart cities’ big data management and
analytics: a review. Future Internet 12:190. https://doi.org/10.3390/ﬁ12110190
2.Tufail A, Namoun A, Alrehaili A, Ali A (2021) A survey on 5G enabled multi-access edge
computing for smart cities: issues and future prospects. IJCSNS Int J Comp Sci Netw Secur
21(6)
3.Taylor L, Changbo ZHU, Hu T 5G Smart cities white paper, June ©2020. For information,
contact Deloitte China
4.Latif U. Khan, Ibrar Yaqoob, Senior Member, IEEE, Nguyen H. Tran, Senior Member, IEEE,
S. M. Ahsan Kazmi, Tri Nguyen Dang, Choong Seon Hong, Senior Member, IEEE. Edge-
Computing-Enabled Smart Cities: A Comprehensive Survey. [cs.NI], 12th October 2020
5.Marieh Talebkhah, Aduwati Sali, (Senior Member, IEEE), Mohsen Marjani, Meisam Gordan,
Shaiful Jahari Hashim, and Fakhrul Zaman Rokhani, (Member, IEEE) IoT and big data5G Enabled Smart City Using Cloud Environment 225
applications in smart cities: recent advances, challenges, and critical issues. IEEE. Access,
April 16, 2021. Digital Object Identiﬁer. https://doi.org/10.1109/ACCESS.2021.3070905
6.Gohar A, Nencioni G (2021) The role of 5G technologies in a smart city: the case for
intelligent transportation system. https://www.researchgate.net/publication/351372050 .https:/
/doi.org/10.3390/su13095188
7.Sajid Khan Director of Smart Cities Task Force, Internet of Things Group, Intel Corporation
Sameer Sharma, GLOBAL GENERAL MANAGER Iot, INTEL CORPORATION, “Build 5G
Smart Cities & Transportation Systems”. 2021
8.Belli L, Cilfone A, Davoli L, Ferrari G, Adorni P, Di Nocera F, Dall’Olio A, Pellegrini C,
Mordacci M, Bertolotti E (August 2020) IoT-enabled smart sustainable cities: challenges and
approaches. MDPI. Accepted: 13 September 2020; Published: 18 September 2020
9.Szum K (2021) IoT-based smart cities: a bibliometric analysis and literature review. ISMSME
13(2):115–136
10.Hashem IAT, Chang V , Anuar NB, Adewole K, Yaqoob I, Gani A, Ahmed E, Chiroma H The
role of big data in smart city. IJIM
11.Shi W, Goodchild MF, Batty M, Kwan M-P, Zhang A (eds) Urban Informatics. The Urban
Book series, Springer. http://www.springer.com/series/14773
12.Sethi P, Sarangi SR Internet of things: architectures, protocols, and applications. Hindawi J
Elect Comput Eng 2017:9324035., 25 pages. https://doi.org/10.1155/2017/9324035
13.Eleonora Riva Sanseverino, Raffaella Riva Sanseverino, Valentina Vaccaro, Ina Macaione.
Smart Cities: Case Studies. https://www.researchgate.net/publication/310485601 . Chapter
November 2017. https://doi.org/10.1007/978-3-319-47361-1_3
14.Abha Joshi-Ghani, Carlo Ratti, Director, Alice Charles, Global Future Council on Cities and
Urbanization. Smart at Scale: Cities to Watch 25 Case Studies. C O M M U N I T Y P A P E R
A U G U S T 2 0 2 0. © 2020 World Economic Forum
15.Bonomi F, Milito R, Zhu J, Addepalli S (2012) Fog computing and its role in the internet
of things. In: Proceedings of the First Edition of the MCC Workshop on Mobile Cloud
Computing, MCC ‘12, Helsinki, Finland, 13–17 August 2012. ACM, New York, pp 13–16
16.OpenFog Consortium Architecture Working Group. OpenFog Reference Archi-
tecture for Fog Computing. Available online: https://www.iiconsortium.org/pdf/
OpenFog_Reference_Architecture_2_09_17.pdf
17.Roca D, Quiroga JV , Valero M, Nemirovsky M (2017) Fog function virtualization—a ﬂexible
solution for IoT applications. In: Proceedings of the 2017 Second International Conference on
Fog and Mobile Edge Computing (FMEC), Valencia, Spain, 8–11 May 2017, pp 74–80
18.Panwar N, Singh AK (2016) A survey on 5G—the next generation of mobile communication.
Phys Commun 18:64–84
19.Yu S, Li J, Wu J (2019) Emergent LBS: If GNSS fails, how can 5G-enabled vehicles get
locations using fogs? In: Proceedings of the 15th International Wireless Communications &
Mobile Computing Conference (IWCMC), vol 24–28. Tangier, Morocco, pp 597–602
20.Souza J, Francisco A, Piekarski C, Prado G (2019) Data mining and machine learning to
promote smart cities: A systematic review from 2000 to 2018. Sustainability 11(4):1077
21.Silva BN, Khan M, Han K (2020) Integration of big data analytics embedded smart city archi-
tecture with RESTful Web of things for efﬁcient service provision and energy management.
Future Gener Comput Syst 107:975–987
22.Abbas N, Zhang Y , Taherkordi A, Skeie T (2018) Mobile edge computing: a survey. IEEE
Internet Things J 5(1):450–465
23.Meijer A, Bolívar MPR (2015) Governing the smart city: a review of the literature on smart
urban governance. Int Rev Adm Sci:0020852314564308
24.Lilly CM, Cody S, Zhao H, Landry K, Baker SP, McIlwaine J, Chandler MW, Irwin RS
(2011) Hospital mortality, length of stay, and preventable complications among critically ill
patients before and after tele-ICU reengineering of critical care processes. JAMA 305:2175–
2183. https://doi.org/10.1001/jama.2011.697
25.Musaazi KP, Bulega T, Lubega SM (2014) Energy efﬁcient data caching in wireless sensor
networks: A case of precision agriculture. In: Springer International Conference on e-
Infrastructure and e-Services for Developing Countries. Kampala, pp 154–163226 P. Bakaraniya et al.
26.Yannuzzi M, van Lingen F, Jain A, Parellada OL, Flores MM, Carrera D, P’erez JL, Montero
D, Chacin P, Corsaro A et al (2017) A new era for cities with fog computing. IEEE Internet
Comput 21(2):54–67
27.Smart city simulation. [Online]. Available: https://iotify.help/network/smart-city/
simulation.html . Online; Accessed Jan 4 2019
28.Vilajosana I, Llosa J, Martinez B, Domingo-Prieto M, Angles A, Vilajosana X (2013)
Bootstrapping smart cities through a self-sustainable model based on big data ﬂows. Com-
munications Magazine, IEEE 51(6):128–134
29.Singh D, Tripathi G, Jara AJ (2014) A survey of internet-of-things: future vision, architecture,
challenges and services. Paper presented at the Internet of Things (WF-IoT), 2014 IEEE World
Forum onHardware Implementation for Spiking
Neural Networks on Edge Devices
Thao N. N. Nguyen, Bharadwaj Veeravalli, and Xuanyao Fong
1 Introduction
Deep learning algorithms, especially those based on ANN, have gained signiﬁcant
popularity in Internet of Things (IoT) applications due to their proven ability to
deliver human-like performance in artiﬁcial intelligence (AI)-enabled applications
such as visual recognition and speech processing [ 1]. Simultaneously, the size and
complexity of the neural network (NN) models have been expanded rapidly owing
to the push for more competitive accuracy. As the simulation of these complex
NN models demands a large amount of memory and computational resources, it
is usually performed in the high-performance computing (HPC) clusters in data
centers. However, in IoT applications (e.g. surveillance robots, mobile phones,
autonomous vehicles, etc.), data is often gathered on edge devices and the cost
and time delay for data transmission pose a vital problem, since the edge devices
are far from the data center. This challenge motivates the development of edge
computing, in which data is consumed on the intelligent edge devices themselves.
Notwithstanding the beneﬁts on the response time and communication cost, the
potential of deep neural networks (DNNs) on the edge is hindered by the limitation
of memory resources and energy constraint of the device. This led to a drift
towards tiny machine learning (ML), which emphasises low power and memory
consumption on IoT devices along with accuracy. The research on tiny ML
focuses mainly on the popular ANNs that are capable of achieving state-of-the-
art accuracy in many applications [ 1]. However, despite several optimizations that
target the algorithm, hardware architecture, and device-level implementation [ 2],
the performance of ANN on edge devices is still limited due to computationally
T. N. N. Nguyen · B. Veeravalli · X. Fong ( /envelopeback)
Department of Electrical and Computer Engineering, National University of Singapore,
Singapore, Singapore
e-mail: thao@nus.edu.sg ;elebv@nus.edu.sg ;kelvin.xy.fong@nus.edu.sg
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
H. K. Thakkar et al. (eds.), Predictive Analytics in Cloud, Fog, and Edge Computing ,
https://doi.org/10.1007/978-3-031-18034-7_13227228 T. N. N. Nguyen et al.
heavy operations in the inference and network training. A potential direction to
overcome this impediment is to explore the bio-inspired approaches, which incur
less computational cost on edge devices while delivering similar performance as the
ANN.
Advances in neuroscience and ML research have paved the way for the develop-
ment of the bio-inspired SNN, which is an alternative NN model to the ANN and
is expected to be more suitable for the energy-constrained edge-IoT applications.
Designed to emulate the behaviour of biological NNs, information ﬂows through the
SNN in the form of binary events called spikes whereas ANN computes input data
(e.g., images) in a static, frame-based manner. As the computation of SNN is only
performed upon the occurrence of a spike, the dynamic energy consumption of SNN
is deemed to be lower than ANN [ 3]. Despite this, the implementation of SNN on
IoT devices remains limited by tight constraints on power and hardware resources.
Deep and large networks (which are developed to target a competitive accuracy
[4]) consume a large amount of memory and computational cost. The resources and
energy consumption to emulate these SNNs may exceed the capacity of intelligent
devices on the edge. Therefore, optimizations which reduce the network complexity
and energy consumption of SNN implementation (also referred to as neuromorphic
hardware/processor) on edge devices are of interest in the recent years. In this
chapter, we review these optimizations, which are classiﬁed into two categories:
hardware implementation and algorithm design (e.g. pruning and quantization). We
will ﬁrst present an overview of SNN, followed by the optimizations targeting the
hardware implementation of SNN on the edge. Thereafter, the algorithm optimiza-
tions to reduce the network complexity and energy consumption of the SNN are
surveyed. Finally, a comparison between SNN and ANN for edge computing is
presented, followed by the conclusions and directions for future works.
2 The Spiking Neural Network (SNN)
The bio-inspired SNN is designed to more closely emulate the behavior of the
human brain to process and store information than the ANN. Similar to the
biological NN, the information in the SNN is encoded in the form of spike events.
Conversion between the real numbers (e.g., image pixels) and the spike events may
be achieved using either the rate-based coding or time-based coding [ 5]. In the
rate-based coding, the real number is proportional to the spike rate whereas in the
time-based coding scheme, the real number is encoded as the timing of the spikes.
In this section, we will describe the response of the neurons when the spike events
arrive, based on the leaky integrate-and-ﬁre (LIF) model, followed by the learning
algorithms to train the synaptic weights of the SNN.Hardware Implementation for Spiking Neural Networks on Edge Devices 229
n1
n2
n3
n4mw1
w2
w3
w4
Pre-synaptic Spikes Post-synaptic Spikes
(a) (b)
Fig. 1 (a) Overview of the Spiking Neural Networks. ( b) An example to illustrate the membrane
potential accumulation based on the LIF model and the presynaptic spike activities as shown in ( a)
2.1 The Leaky Integrate-and-Fire (LIF) Neuron Model
The LIF model [ 6] is adopted widely in the SNN applications in the edge-IoT
domain in the recent years [ 7–10]. When a postsynaptic neuron, i, receives a spike
from its presynaptic neuron, j, at time step, t, the synaptic current, Ui, and the
membrane potential, Vi, are updated as follows:
Ui(t)=/summationdisplay
jwij∗sj(t) (1)
τidVi(t)
dt=−Vi(t)+Ui(t) (2)
where wijis the strength of the synapse (also called the synaptic weight ) connecting
to neuron ifrom neuron j.sj(t)indicates if there is a spike transmitted from neuron
jat time step t, andτiis the decay factor that characterizes the decay (or leak)o fVi
over time. If Viexceeds the ﬁring threshold, φi, neuron iﬁres a spike and Viis reset
to a default value, γi, as shown in Fig. 1. In addition, the neurons that are located
near neuron iin the network may be inhibited from accumulating the membrane
potential for a period of time. In the simpler integrate-and-ﬁre (IF) model [ 6], the
membrane potential is computed in the same way except that Vi(t)is removed from
the right hand side of Eq. (2).
2.2 The Learning Algorithms
The power of the NN is derived from its ability to be trained (or to learn ) to perform
various cognitive tasks. Ofﬂine learning , in which NNs are trained ofﬂine (often
in the cloud) before they are deployed, is often used as the training process is
computationally and energetically expensive. This is often the approach used to train230 T. N. N. Nguyen et al.
modern ANNs that can achieve excellent accuracy in various cognitive tasks. On the
other hand, online learning , in which the NN can learn and adapt to its deployment
environment, may be useful for IoT applications that require machines to react to
unpredictable events. As we will discuss later, an advantage of SNN over ANN is
the ease of implementing online learning.
While the backward propagation of errors [ 11] is the most commonly used
approach to train an ANN, the learning algorithms for SNN is still an active research
area. An approach for obtaining the synaptic weights of an SNN is to convert them
from a fully-trained ANN. The conversion process involves performing the weight
normalization and adjusting the ﬁring threshold to control the ﬁring rate of the
SNN [ 12,13]. Moreover, the conversion is fast to perform [ 12], helps the SNN
achieve accuracy close to the ANN counterpart, and can be applied in the SNN
applications on the edge devices that do not require the online learning capability
[7].
For the SNN applications that require the learning to be performed online or
on the chip, the synaptic weights can be trained using unsupervised [14,15]o r
supervised [4,16] learning methods. The supervised learning methods, which are
based on the back-propagation algorithm, are able to achieve better accuracy and is
explored actively in the recent years [ 4,17]. In these works, the surrogate gradient
is proposed to solve the problem that the spike function, sj(t),a su s e di nE q .( 1), is
not differentiable. While some works explored deep and large SNN architectures
to improve the state-of-the-art accuracy [ 4], others have developed small-scale
networks and hardware-friendly back-propagation rules that can be implemented
on the embedded systems platforms [ 16,17].
The unsupervised learning (in which training data is unlabeled ) methods are
based on the spike timing-dependent plasticity (STDP) process, which adjusts
the efﬁcacy of the spike transmission among the neurons in the biological neural
networks. In this process, the strength of the synapses are regulated based on the
temporal correlation between the output spikes and the action potentials. If a neuron
consistently ﬁres a spike in response to stimuli from another neuron, the signal
transmission between the two neurons is strengthen. This phenomenon is called
long-term potentiation (LTP). On the other hand, in the long-term depression (LTD)
process, the long-lasting depression of the signal transmission occurs when the
receiving neuron is not excited by the input stimuli. The LTP and LTD phenomena
are thought to be the mechanism of the biological brain to store information and
learn to adapt to the surrounding environment [ 14]. The STDP-based learning
rule for the SNN is inspired by these two phenomena, as shown in Fig. 2.I fa
postsynaptic neuron ﬁres a spike shortly after it receives a spike trigger from a
presynaptic neuron, the connection between the two neurons is enhanced; otherwise,
if a postsynaptic neuron ﬁres a spike shortly before it receives a spike trigger
from a presynaptic neuron the corresponding connection is diminished [ 15]. In the
STDP-based learning algorithm, as proposed in [ 15], the synaptic weight update is
computed as follows:Hardware Implementation for Spiking Neural Networks on Edge Devices 231
Pre1
Pre2
Pre3
Post
LTP LTD
(a) (b)
Fig. 2 (a) The LTP and LTD based on the time order of the presynaptic and the postsynaptic
spikes. ( b) An example to illustrate the synaptic weight update based on the STDP-based learning
algorithm, as shown in Eq. (3)
Δw=/braceleftBigg
A+eΔt/τ + ,i fΔt < 0;
−A−e−Δt/τ −,i fΔt≥0(3)
where Δtis the time difference between the occurrences of the presynaptic and
the postsynaptic spikes, τ+andτ−determine the effective time window of the
LTP and LTD, and A+andA−are the positive numbers which determine the
range of the synaptic weight update. There are modiﬁed versions of the STDP-
based learning algorithm which are hardware-friendly to be emulated on the
neuromorphic accelerators on the edge devices [ 18]. Note that in the STDP-based
learning approach, the synaptic weights are updated using the local information of
the presynaptic and the postsynaptic neurons. Therefore, the complexity of back-
propagating the error signal through the deep networks can be avoided. Hence,
STDP-based learning rule is much more suitable for edge devices in which learning
needs to be done quickly.
Evolutionary approaches have also been explored to train the SNN for the
edge-IoT applications [ 19–21]. The evolutionary training approaches achieve better
accuracy on the small-size networks on the edge devices as compared to the
conventional ways of training the neural network, in which the synaptic weights are
obtained by learning the training samples. However, experimental results showed
that they incur a longer training time due to excessive interaction with the simulated
environment [ 21].
3 Hardware Accelerators for SNNs on the Edge
The edge devices have tighter constraints on the hardware resources and energy con-
sumption as compared to the HPC clusters. In addition, there are ML applications
on the edge that require real-time responses [ 22]. Therefore, energy consumption,232 T. N. N. Nguyen et al.
hardware resources, and response time delay are important metrics for the SNN
hardware implementations on the edge devices. In the following, we will discuss
the approaches to reduce these costs in the existing hardware acceletors for SNN.
Furthermore, we will present a survey of the existing low-power, ﬂexible SNN
processors in the edge-IoT domain, which have been integrated with high-level APIs
to enable fast prototyping of the SNN architectures.
3.1 Optimizations that Exploit the Temporal Sparsity of SNN
The SNN implementations that target the HPC clusters [ 29,30] usually rely on
the massively parallel computation to achieve a fast response time. This design
approach consumes a large amount of energy and is not suitable for the energy-
constrained SNN applications [ 7,31]. Therefore, many existing neuromorphic
hardware [ 3,23,32–37] also exploit the temporal sparsity of the spikes in SNNs to
perform the computation in an event-driven manner. In this design, the membrane
potentials of the postsynaptic neurons are only updated upon the arrival of a spike
from the presynaptic neurons. During the idle time, the hardware implementation
only requires the static power to retain the data in the memory cells [ 7]. The
synaptic weight update may also be performed in an event-driven manner on the
chips that emulate the online learning [ 23,37,38]. Hence, the energy consumption
of the hardware implementation for simulating the SNN can be lower than those that
simulate the ANN [ 1].
The energy consumption of the hardware implementation for SNN on the
edge devices can also be optimized by eliminating the power-hungry global clock
signal [ 7,31]. A small-scale, low-power neuromorphic processor, referred to as
μBrain [ 7], was proposed for the applications in the edge-IoT domain, as shown
in Table 1. The hardware implementation of μBrain utilizes the delay cell in a
multi-phase oscillator to perform the asynchronous spike communication and event-
driven neuron computation. When a spike arrives at the neuron core, an oscillation
cycle of the multi-phase oscillator is set. The spike receiver unit of the neuron core
receives the signal and triggers the event-driven membrane potential accumulation.
The spike transmission of the neuron core follows the AER protocol [ 39], which was
proposed in earlier works [ 10,23,31]. In the AER communication scheme (refer to
Fig.3), the spike events are encoded into the lightweight packets containing the
neuron addresses and are transmitted on the asynchronous buses to the destination
neuron cores. Since the spike transmission is asynchronous, lightweight, and event-
driven, it incurs low power and communication delay on the multi-core hardware
architectures for SNN. Therefore, the AER protocol has been employed in the
large-scale neuromorphic chips [ 31] as well as the hardware implementations that
consume low power, small footprint for the SNN applications on the edge [ 7,10,23].Hardware Implementation for Spiking Neural Networks on Edge Devices 233
Table 1 Selected hardware implementations for SNN that consume low power and small footprint
Work [9] [23] [24] [25] [7]
Technology (nm) 40 28 FDSOI 65 65 40
Neuron model LIF Izhikevich [ 26][24] IF IF
Total # neurons NA 256 410 1280 336
Total # synapses NA 64 k 199 k 163 k 37 k
Time-
multiplexedNo Yes No No No
Coding scheme Rate-based Rank order Rate-based Rate-based Time-
based,
Rate-
based
On-chip learning Back-
propagation
(supervised)SDSP [ 27]
(supervised)Segregated
dendrites [ 28]
(supervised)No No
Synaptic weight
precision (bits)7 4 8 1 4
Frequency 163 MHz 75 MHz 20 MHz 70 KHz No clock
Area (mm2) 1.65 0.086 10.08 1.99 2.68
Power 70.4 mW 35–447 μW 23.6 mW 305 nW 73μW
Vo l t a g e ( V ) 0.9 0.55 0.8 0.5 1.1
SNN architecture FC* FC FC FC FC,
recurrent
# Neurons
emulated522 10 410 650 74
# Synapses
emulated269 k 2.5 k 199 k 67 k 17 k
Energy/inference
(nJ)48.4 15 236.5 195 308
Accuracy
(MNIST)98% 84.5% 97.83% 97.6% 91.7%
∗Fully connectedEncodern1
n2
n3
n4
time 
stept1t22t33t4
Decoder3 2
t54Sender
2 1
REQ
ACKn1
n2
n3
n4
time 
stept1t22t33t4t5Receiver
Fig. 3 The AER communication protocol. Figure adapted from [ 39,40]234 T. N. N. Nguyen et al.
Controller
PE 1
Mem 1PE 2 PE n
Mem 2 Mem nI/OSensor Data
…
…Membrane potential,
synaptic weights,
spike time, etc.Off-Chip Mem
(if supported)Output
Fig. 4 Basic building blocks of the hardware architecture for SNN, following the non-V on
Neumann paradigm. The memory units are distributed near the processing elements to reduce
the data communication delay
3.2 Data and Memory-Centric Architectures
The traditional computing systems are developed based on the von Neumann
architecture, which separates the memory storage elements and the computing units.
However, the cost (latency and energy) of data movement has become a signiﬁcant
bottleneck for the data-centric applications using this architecture. This motivates a
shift towards the non-von Neumann architecture (i.e., near-memory computing and
in-memory computing) for the neuromorphic hardware [ 41]. In the near-memory
computing paradigm, the memory units are distributed near the processing elements
where the data is consumed [ 3,33,38] as shown in Fig. 4. However, the in-memory
computing architecture utilizes the physical characteristics of the memory devices
to perform the neuron computations within the memory. In this subsection, we will
discuss the techniques to reduce the data movement in the near-memory computing
architecture, followed by the in-memory computing designs that can be utilized for
the energy-constrained edge computing applications.
In the near-memory computing architecture, the data movement on the neu-
romorphic hardware can be optimized by spike bundling or batch processing
the membrane potential accumulation and synaptic weight update [ 3,32–35]. By
delaying and dynamically grouping the spike events to be transmitted from the
presynaptic to the postsynaptic neurons over multiple time steps [ 32,35], the
communication time, number of operations, and memory access on the hardware
implementation may be reduced. However, the batch size and maximum time delay
for the spike transmission need to be determined based on the spike activities of
the network in order to achieve a reasonable energy-accuracy trade-off [ 35]. The
computation of the leak current in the LIF neurons can also be deferred until the next
membrane potential accumulation instead of being performed in every time step [ 3].
This reduces the number of operations and also saves on the time and energy to
access the potential memory, especially when it needs to be loaded from an off-chip
storage. Furthermore, the computation and memory access can be saved by reducing
the number of synaptic weight updates during the on-chip learning. The synaptic
weight modiﬁcation can be computed once every few time steps [ 33,42]o rw h e n
there is a new incoming spike [ 34]. Approximations that are proposed to reduce theHardware Implementation for Spiking Neural Networks on Edge Devices 235
Vin1
Vin2
Vin3
Vin4
Iout1Iout2Iout3Iout4n1
n2
n3
n4n1
n2
n3
n4m1
m2
m3
m4
Fig. 5 The crossbar array architecture to perform the membrane potential accumulation of the
SNN
number of computationally heavy operations (e.g. convolution and multiplication)
and communication cost of DNN on mobile devices [ 43–45] may also be considered
for SNN.
The in-memory computing approach leverages the physical characteristics and
organization of the memory devices to perform the membrane potential accumula-
tion and synaptic weight update within the data storage. As data movement between
the memory and the computing units is minimized, the energy and hardware
dedicated for the data transmission is reduced, albeit at the cost of possibly more
complicated memory design. The memristive devices are organised in a crossbar
array architecture, as shown in Fig. 5, and have their conductance set based on the
synaptic weight values. The input data is presented as the voltage at the rows while
the output data is measured based on the resulting current at the columns of the
crossbar array. The multiplication of the inputs and the synaptic weights of the ANN
and SNN is performed based on the Ohm’s Law, which speciﬁes the relationship
between the voltage, current, and conductance [ 41]. Emerging non-volatile memory
devices, such as: phase-change memory (PCM) [ 46], resistive random access
memory (RRAM) [ 47], spin-transfer torque magnetic RAM (STT-RAM) [ 48], and
magnetic skyrmions [ 49] were utilized to perform the in-memory computing for
SNN, which results in low energy consumption, fast response time, and small
footprint. These in-memory computing designs can be employed as the back-end
of an SNN development stack, which includes the components from the high level
API to the device-level implementation, as we will discuss next.
3.3 Flexible Hardware Architectures for SNN on the Edge
Although the neuromorphic hardware targeting the edge-IoT domain consume a
small amount of power and area as compared to the SNN implementation on the
HPC clusters, most of them lack the ﬂexibility and ease of use for the neuroscientists
who are not familiar with the hardware development process. In contrast to the SNN
simulators on GPU [ 29,30], which are capable of emulating a wide range of SNN236 T. N. N. Nguyen et al.
architecture and neuron models, most of the neuromorphic hardware emulate the
SNNs that have ﬁxed conﬁguration [ 51]. Therefore, there have been efforts to bridge
the gap between the hardware development and the high-level prototyping to enable
fast development and evaluating of the SNN models on the edge devices [ 8,51,55–
61].
Caspian is a neuromorphic development platform consisting of a low-power
neuromorphic processor integrated with a high-level API that enables the simulation
of a small-scale SNN for the edge-IoT applications [ 51]. The network size, connec-
tivity, and the hyper-parameters such as the ﬁring threshold, leak time constant,
synaptic delay, and axonal delay are deﬁned in software using the Python API.
Moreover, a neuromorphic computing framework, TeNNLab [ 55], was integrated
with Caspian to bridge the gap between the software application and the underlying
hardware implementation on the edge devices.
Similar to TeNNLab, there are several neurocomputing frameworks that offer
high-level interfaces to conﬁgure the SNN architecture and hyper-parameters to be
emulated on various neuromorphic hardware on the edge [ 57–61]. Energy-efﬁcient
hardware implementations that serve as a back-end of several high-level interfaces
on the edge devices have also been proposed [ 8,50,62]. An SNN co-processor,
which executes a set of custom-made commands issued from a main processor
on the board, was developed to enable programming the SNN architecture with
an arbitrary number of neurons, synapses, and layers. The hyper-parameters of
the SNN can also be conﬁgured using the custom-made commands without re-
programming the hardware implementation [ 38]. Table 2presents selected ﬂexible
Table 2 Selected ﬂexible neuromorphic hardware that emulate the SNN architecture deﬁned in
software for edge-IoT applications
Work [50] [8] [51] [38]
Platform Nallatech 385A Pynq-Z1 Zynq Lattice iCE40
UP5kZynq-7000
Zedboard
High-level API N2A Nengo TeNNLab custom-made API
Neuron model LIF NEF [ 52] LIF IF
# Neurons 2048 ∼32,000 256 32,768
On-chip learning No PES [ 53]
(supervised)SLAYER [ 54],
EONS [ 19]
(evolutionary)STDP-based
(unsupervised)
Synaptic weight
precision (bits)18 1–64 8 24
Memory
expansion*Yes No No Yes
Hardware
resourcesNA ∼28,000 LUTs;
220 DSPs**;
∼100 BRAMs
(450 kB)ﬁts 5280 LUTs;
120 kB BRAM23,375 LUTs;
29,526 FFs; 22
DSPs; 97.5 BRAMs
(438.75 kB)
Total power 25.424 W 3W (8 k neurons) in mW range 2.179 W
∗Use of off-chip memory
∗∗Estimated from Figure 9 in [ 8]Hardware Implementation for Spiking Neural Networks on Edge Devices 237
neuromorphic hardware that target the edge-IoT applications. These hardware
implementations are capable of performing the on-chip learning using various
learning algorithms and consume a small amount of on-chip memory.
4 Algorithm Design
Due to limited memory resources and tight constraints on the energy consumption,
the SNN accelerators on the edge devices usually emulate small-scale networks that
do not require a large amount of memory storage and intensive computations [ 51].
However, in the research on neuroscience-based AI, deep and large SNNs were
proposed to achieve a competitive accuracy on many applications [ 4,13,63]. These
deep and large networks consume a large amount of memory resources, which are
not available on the edge devices. Therefore, there have been efforts reduce the
network complexity and memory storage consumption, such as synapse pruning [ 13,
64–67] and reducing the bit length of the synaptic weights [ 48,67–73]. In addition,
there are methods to optimize the number of time steps to present the input to the
SNN [ 63,64,73]. These approach aim to ﬁnd a balance between the classiﬁcation
accuracy and the cost-efﬁciency to implement the SNN on the platforms that have
limited hardware resources, such as the edge-IoT devices. We will discuss these
approaches in the following.
4.1 Synapse Pruning
The contribution of synapses with small weights to the overall SNN operation may
be insigniﬁcant. In synapse pruning, synapses that are insigniﬁcant to the SNN
operation are removed to save on memory storage and energy consumption. Note
that the synapses may be pruned either after the network has ﬁnished learning
[13,64] or during the network learning itself [ 65–67,74].
The former method can be performed off-chip on a well-trained network before
loading the synaptic weights to the on-chip memory storage [ 13]. It may also
be applied during the inference of the SNN on the chip to utilize the dynamic
parameters (e.g., the spike activities, the membrane potentials of the neurons, the
sparsity of the network, etc.) in every pruning iteration [ 64]. The synapse pruning
techniques have been shown to eliminate as much as 89% of synapses [ 13]. As
a result, the memory resources to implement the compressed SNN on the edge
devices is reduced drastically as compared to the SNN before pruning and without
incurring signiﬁcant loss in the classiﬁcation accuracy. Moreover, the number of
operations is reduced by 85–90% [ 13,64]. Therefore, synapse pruning can be an
attractive approach to improving the response time, hardware resources, and energy
consumption in the inference of the SNN on the edge devices.238 T. N. N. Nguyen et al.
When synapse pruning is performed during the learning process [ 65–67,74],
the runtime and energy consumption of the learning process can be reduced.
However, care must be taken to ensure that inference accuracy of the SNN is not
degraded. In the proposed techniques [ 65–67,74], the synapse pruning were applied
every kiterations during the network learning (which may have accuracy loss),
followed by the training iterations to re-gain the accuracy. As the synapse pruning
is performed during the network learning, it may affect the learning activities and
cause signiﬁcant accuracy degradation. For example, if the synapses were pruned
aggressively during the network learning, the neurons may not accumulate sufﬁcient
membrane potential to ﬁre a spike. Consequently, the spike activities of the network
are corrupted, which leads to erroneous weight updates. Moreover, if the network
learning is event-based, the insufﬁciency of spike events may cause the learning to
stop prematurely [ 65,66]. Despite these challenges, the synapse pruning approaches
are helpful for the SNN applications on the edge devices that require the online
learning capability.
4.2 Hybrid On/Off Chip Training
Modern IoT applications desire the on-chip learning capability to “learn” new
patterns that were not presented in the initial training set. However, computational
complexity and excessive memory access pose major challenges to implement
the state-of-the-art learning algorithms, which achieve competitive accuracy, on
the resource and energy-constrained edge devices. Hardware-friendly learning
algorithms were developed to train SNNs on edge devices but they are yet to achieve
performance on-par with the computationally expensive algorithms [ 4,18,75]. This
limitation spurred the development of hybrid on/off chip learning algorithm, in
which the SNNs are trained off-chip without constraints on hardware resources and
energy consumption using a complex algorithm, and ﬁne-tuned on-chip using an
energy-efﬁcient algorithm [ 37,75].
The hybrid on/off chip training may also be applied in a transfer learning scheme
to achieve low energy consumption on edge devices, as shown in Fig. 6[37]. From
the ML perspective, transfer learning is a technique in which the model leverages
on the knowledge it has acquired from previous tasks (e.g. object recognition) as
a starting point to quickly adapt to a similar task (e.g. digit recognition). In most
of the transfer learning schemes, the online adaptation is performed only at the
last few layers, which learn task-speciﬁc features. The parameters of the preceding
hidden layers, which serve as high-level feature extractors (e.g. edge detectors),
may be reused across similar tasks. From the hardware implementation perspective,
transfer learning has several beneﬁts in the terms of response time delay and energy
consumption on edge devices. First, as network learning leverages on the pre-
trained parameters, it converges in fewer epoches, which results in faster training
time and less computational cost. Second, most of the SNN parameters may be
pre-trained off-chip to achieve top-notch performance. As the on-chip learning isHardware Implementation for Spiking Neural Networks on Edge Devices 239
… …On-Chip
AdaptationOff-Chip Pre-Training
Conv-Pooling Layers Fully-Connected Layers Input Layer
Fig. 6 Overview of an SNN-based transfer learning scheme [ 37]
performed only at the last few layers using a hardware-friendly algorithm, the
energy consumption is signiﬁcantly less than performing the network learning for
all of the layers on the chip. This transfer learning technique was reported to enable
fast, energy-efﬁcient online adaptation for SNN on a neuromorphic processor [ 37].
4.3 Quantization and Binarization
Quantization techniques may also be applied to reduce the memory footprint of
the SNN [ 48,67–73]. It was found that the SNN accuracy is more prone to
degradation in synapse pruning than in weight quantization [ 67]. Generally, the
synapse binarization can be performed together with the conversion from the ANN
to the SNN [ 71,73], or during the learning process [ 48,67–70,72,73]. The
compressed networks achieved after the quantization and synapse pruning consume
a small amount of memory, area, and energy consumption, as shown in Tables 3
and4. Therefore, they are suitable to be implemented on the edge-IoT devices. Note
that in these tables, the memory compression and energy reduction results were
measured relative to hardware/software implementation baseline in the respective
work, using the full-precision (32-bit) synaptic weights without optimizations. As
the object recognition task on the CIFAR-10 and CIFAR-100 datasets [ 76]i sm o r e
challenging than the digit recognition task on the MNIST dataset [ 77], the SNN
architectures presented in Table 4are relatively larger and deeper than Table 3.
Therefore, the memory consumption of the SNN implementations presented in
Table 4are generally larger than Table 3. However, as the works in Table 4reported
a high rate of memory compression, they can be promising for reducing the memory
and energy consumption of the SNN implementation on the edge devices. Moreover,
t h ew o r ki n[ 48], as shown in Table 4, consumed low power and is deemed to be
suitable for the edge-IoT applications.240 T. N. N. Nguyen et al.
Table 3 Accuracy and beneﬁts on the memory, computational, and energy cost to implement the
binary SNN proposed in selected works, evaluated on the MNIST dataset
Work [68] [69] [70] [71] [72] [67]
SNN
architec-
ture576×400 784×400×1028×28-
16C3-16C3-
16C3-16C3-6C3*28×28-
64C3-
MP2-
64C3-
2MP-
128FC-
10*784×600
×10LeNet-5
Learning
methodUnsupervised Unsupervised
+Adam
optimizerSupervised Converted
from
CNNSupervised
+Adam
optimizerSupervised
Spike
codingRate-based Rate-based Rate-based Rate-
basedTime to
ﬁrst spikeRate-
based
Platform 65 nm CMOS 90 nm
CMOS90 nm CMOS GPU GPU GPU
Learning
on Hard-
wareYes Yes No No No No
Accuracy 87.4% 92.30% 98.73% 99.43% 97% 97.16%
Area and
memory
con-
sumptionarea 0.39 mm2,
mem∼28 KBmem saved
96.88%Area 2.07 mm2mem
2.15 MB,
saved
69.85%mem
saved
∼68%mem
saved
99.22%
Energy
con-
sumption104 pJ/spike,
0.31μJ/inference,
saved 87.68%8.4 pJ/spike 24.82μJ/inference NA NA Saved
99.59%
opera-
tions
∗Cconvolutional layer, MPmax pooling layer
4.4 Time Step Reduction
In the ANN, the inference result is obtained in a single pass through all layers in
the NN after the input data has been given to the network. In contrast, due to the
temporal nature of the network, the SNN may require several passes through all
layers before the inference result is obtained. Consequently, reducing the number of
time steps required in the SNN simulation not only reduces the response time but can
also reduce the number of computations and memory accesses and hence, reduce
the energy consumption of the hardware implementation. Signiﬁcant improvements
can also be achieved when time step reduction is combined with techniques such
as network pruning and quantization. It was shown that when the synapse pruning
is applied to reduce the network complexity, the number of time steps needed
by the SNN to accurately classify the input may also be reduced [ 64]. Thus, the
classiﬁcation of the input may be terminated before the maximum number of timeHardware Implementation for Spiking Neural Networks on Edge Devices 241
Table 4 Accuracy and beneﬁts on the memory, computational, and energy cost to implement the
binary SNN proposed in selected works, evaluated on the CIFAR-10 and CIFAR-100 datasets. In
the works presented in this table, the SNN uses the rate-based coding scheme
Dataset CIFAR-10 CIFAR-100
Work [71] [67] [48] [71] [73]
SNN archi-
tecture*32×32-
128C3-
128C3-MP2-
256C3-
256C3-MP2-
512C3-
512C3-MP2-
1024-1024-1032×32-
128C3-
256C3-
256C3-
512C3-
512C3-
1024C3-
1024C3-
2048C3-
1024-
512-1032C3-32C3-
32C3-32C3-
32C3-32C3-
256C3-AP4-
512-10
(residual)32×32-
128C3-
128C3-MP2-
256C3-
256C3-
MP2-512C3-
512C3-MP2-
512-100VGG-15
Platform GPU GPU 65 nm CMOS GPU GPU
Learning
methodCNN to SNN
conversionSupervised Supervised CNN to SNN
conversionCNN to SNN
conversion
Accuracy 90.19% 86.75% 83.85% 62.02% 62.07%
Area and
memory
consump-
tionmem 36.45
MB, saved
83.8%mem saved
99.22%area
115μm2/neuronmem 36.35
MB, saved
83.83%mem saved
∼68%**
Energy con-
sumptionNA saved 99.61%
operations1.63 pJ/spike,
176.6 TOPS/WNA NA
∗Cconvolutional layer, MPmax pooling layer, APaverage pooling layer
∗∗Estimated using the method similar to [ 72]
steps has elapsed to save on energy consumption as well. It has been shown that
if all but one of the neurons at the output layer have all of their synapses pruned
during inference, the inference may be immediately terminated since the remaining
output neuron that is still connected in the network gives the result [ 64]. In another
approach, the classiﬁcation of the image is terminated early if the membrane
potential of any neuron in the output layer exceeds a threshold [ 73].
Also, it was found that for the SNN converted from the ANN, the network may
require thousands of time steps to classify one input image whereas the SNNs
trained using other methods may require less than 500 time steps [ 16]. Motivated by
this, a method to reduce the number of time steps followed by supervised learning
to re-gain the accuracy was proposed [ 63]. The results showed that the ﬁnal SNN
was able to achieve a accuracy close to the SNN that was converted from the ANN,
while achieving signiﬁcant speedup in inference delay.242 T. N. N. Nguyen et al.
5 SNN versus ANN for Edge Computing
The event-driven nature and temporal sparsity of SNN are expected to beneﬁt
IoT applications in the terms of low energy consumption and computational cost.
Nonetheless, there are ongoing debates regarding the advantages of SNN over ANN
and its suitability to be implemented on edge devices [ 1,78,79]. In this section,
we will discuss the beneﬁts and limitations of SNN relative to ANN in the edge
computing domain. Our discussion will focus on two major considerations: memory
and energy consumption.
5.1 Memory Consumption
Memory consumption is one of the key metrics to evaluate hardware implemen-
tations on resource-constrained edge computing platforms. Essential data, which
consume a large percentage of memory resources to emulate an SNN, include: (i)
synaptic weight values, (ii) accumulated membrane potentials (which is analogous
to the activation values of ANN), and (iii) spike data (e.g. pre/postsynaptic neuron
ID, spike time, etc.). The ﬁrst two components, synaptic weight values and
accumulated membrane potentials, consume a similar amount of memory if the SNN
and ANN are of the same size. Consequently, the difference in memory consumption
of SNN and ANN, without network compression techniques, mainly arises from
the demand for a buffer to store spike data (i.e. the spike queue). The size of the
buffer is determined by various factors such as spike sparsity, network size, neuron
model, and spike information (e.g. neuron ID with/without spike time). In short,
the hardware implementation of SNN may incur an overhead in memory resources
as compared to ANN due to the need to store the spike information, which may
be a disadvantage in the edge computing domain. Nevertheless, signiﬁcance of this
difference on memory consumption depends on the SNN/ANN model and design of
the hardware implementation.
5.2 Energy Consumption
Energy consumption is another essential factor that determines the serviceability of
an NN implementation on edge devices. The SNN has potential advantages over
the ANN in energy consumption because of (i) binary spikes, and (ii) the event-
driven characteristics. First, since the inputs of SNN are binary, the membrane
potential accumulation involves only the addition operation (for the IF neuron
model) as opposed to the multiply-accumulation (MAC) operation in ANN. The
elimination of the computationally intensive multiplication signiﬁcantly reduces theHardware Implementation for Spiking Neural Networks on Edge Devices 243
implementation cost and energy consumption on edge devices, which contributes
to the beneﬁts of SNN over ANN. Second, the event-driven characteristics is
also a crucial component, which is exploited in the push for energy efﬁciency of
SNN. The event-driven computing approach is expected to result in a lower energy
consumption of SNN compared to ANN, in which the computation is performed in
the frame-based manner. However, it is argued that the energy consumption of the
SNNs that have dense activities (i.e. the neurons ﬁre a spike too often over the time
steps during inference, which results in intensive memory access and computation
cost) may exceed ANN [ 1]. In essence, the advantages of SNN over ANN in the
terms of energy consumption depends on the sparsity of the spike events. Encoding
schemes which results in one or few spikes per neuron are desirable, even though
they may have yet delivered the state-of-the-art accuracy [ 16,72].
6 Conclusions
In this chapter, we have reviewed the hardware implementation and algorithm
design techniques to improve the performance of the SNN implementations in
the edge computing domain. At the hardware architecture level, the proposed
methods utilise the temporal sparsity, event-driven characteristics, and data locality
to reduce the cost of the memory access, data transmission, and neuron computation.
The asynchronous communication protocol, data bundling, and non-V on Neumann
architectures are shown to considerably reduce the power consumption, response
time delay, and hardware cost of the existing hardware implementations for SNN.
In addition, we discussed the neuromorphic back-ends and software development
frameworks, which support the emulation of the SNN with ﬂexible network archi-
tecture and conﬁguration. Moreover, on the algorithm level, the synapse pruning,
quantization, binarization, and early termination techniques were discussed. The
algorithm-level and hardware-level optimizations can be combined to achieve the
most optimized performance without incurring signiﬁcant accuracy degradation.
While hardware architecture-level and algorithm-level optimizations have signif-
icantly reduced the energy consumption, hardware resources, and response time
of the SNN on the edge devices, there are opportunities for future improvements.
For example, there is a need for new hardware architectures and/or communication
protocols to further improve the power efﬁciency. At the algorithm-level, the search
for the new SNN model, which is compact but yet can achieve a competitive
accuracy on the complex tasks, and effective network compression approaches, is
still ongoing.244 T. N. N. Nguyen et al.
References
1.Davidson S, Furber SB (2021) Comparison of artiﬁcial and spiking neural networks on digital
hardware. Front Neurosci 15:345. https://doi.org/10.3389/fnins.2021.651141
2.Véstias MP (2019) A survey of convolutional neural networks on edge with reconﬁgurable
computing. Algorithms 12(8):154. https://doi.org/10.3390/a12080154
3.Roy A, Venkataramani S, Gala N, Sen S, Veezhinathan K, Raghunathan A (2017) A
programmable event-driven architecture for evaluating spiking neural networks. In: ISLPED,
IEEE, Piscataway, pp 1–6. https://doi.org/10.1109/ISLPED.2017.8009176
4.Lee C, Sarwar SS, Panda P, Srinivasan G, Roy K (2020) Enabling spike-based backpropagation
for training deep neural network architectures. Front Neurosci. https://doi.org/10.3389/fnins.
2020.00119
5.Guo W, Fouda ME, Eltawil AM, Salama KN (2021) Neural coding in spiking neural networks:
a comparative study for robust neuromorphic systems. Front Neurosci 15:212. https://doi.org/
10.3389/fnins.2021.638474
6.Dayan P, Abbott LF et al. (2001) Theoretical neuroscience, vol 806. MIT Press, Cambridge,
MA. https://doi.org/10.1086/421681
7.Stuijt J, Sifalakis M, Yousefzadeh A, Corradi F (2021) μBrain: An event-driven and fully
synthesizable architecture for spiking neural networks. Front Neurosci 15:538. https://doi.org/
10.3389/fnins.2021.664208
8.Morcos B, Stewart TC, Eliasmith C, Kapre N (2018) Implementing NEF neural networks on
embedded FPGAs. In: FPT, IEEE, Piscataway, pp 22–29. https://doi.org/10.1109/FPT.2018.
00015
9.Yin S, Venkataramanaiah SK, Chen GK, Krishnamurthy R, Cao Y , Chakrabarti C, Seo JS
(2017) Algorithm and hardware design of discrete-time spiking neural networks based on back
propagation with binary activations. In: BioCAS, IEEE, Piscataway, pp 1–5. https://doi.org/10.
1109/BIOCAS.2017.8325230
10.Buhler FN, Brown P, Li J, Chen T, Zhang Z, Flynn MP (2017) A 3.43 TOPS/W 48.9 pj/pixel
50.1 nj/classiﬁcation 512 analog neuron sparse coding neural network with on-chip learning
and classiﬁcation in 40 nm CMOS. In: IEEE Symp. VLSI Circuits, IEEE, pp C30–C31. https://
doi.org/10.23919/VLSIC.2017.8008536
11.Hecht-Nielsen R (1992) Theory of the backpropagation neural network. In: Neural networks
for perception. Elsevier, Amsterdam, pp 65–93. https://doi.org/10.1016/B978-0-12-741252-8.
50010-8
12.Diehl PU, Neil D, Binas J, Cook M, Liu SC, Pfeiffer M (2015) Fast-classifying, high-accuracy
spiking deep networks through weight and threshold balancing. In: IJCNN, IEEE, Piscataway,
pp 1–8. https://doi.org/10.1109/IJCNN.2015.7280696
13.Chen R, Ma H, Xie S, Guo P, Li P, Wang D (2018) Fast and efﬁcient deep sparse multi-strength
spiking neural networks with dynamic pruning. In: IJCNN, IEEE, Piscataway, pp 1–8. https://
doi.org/10.1109/IJCNN.2018.8489339
14.Hebb DO (2005) The organization of behavior: a neuropsychological theory. Psychology Press.
https://doi.org/10.4324/9781410612403
15.Song S, Miller KD, Abbott LF (2000) Competitive Hebbian learning through spike-timing-
dependent synaptic plasticity. Nat Neurosci 3(9):919–926. https://doi.org/10.1038/78829
16.Kheradpisheh SR, Masquelier T (2020) Temporal backpropagation for spiking neural net-
works with one spike per neuron. Int J Neural Syst 30(06):2050027. https://doi.org/10.1142/
S0129065720500276
17.K i mJ ,K w o nD ,W o oS Y ,K a n gW M ,L e eS ,O hS ,K i mC H ,B a eJ H ,P a r kB G ,L e eJ H
(2021) Hardware-based spiking neural network architecture using simpliﬁed backpropagation
algorithm and homeostasis functionality. Neurocomputing 428:153–165. https://doi.org/10.
1016/j.neucom.2020.11.016
18.Thiele JC, Bichler O, Dupret A (2018) Event-based, timescale invariant unsupervised online
deep learning with STDP. Front Comput Neurosci 12:46. https://doi.org/10.3389/fncom.2018.
00046Hardware Implementation for Spiking Neural Networks on Edge Devices 245
19.Schuman CD, Mitchell PJ, Patton RM, Potok TE, Plank JS (2020) Evolutionary optimization
for neuromorphic systems. In: NICE Workshop, pp 1–9. https://doi.org/10.1145/3381755.
3381758
20.Schuman CD, Young SR, Maldonado BP, Kaul BC (2021) Real-time evolution and deployment
of neuromorphic computing at the edge. In: IGSC. IEEE, Piscataway, pp 1–8. https://doi.org/
10.1109/IGSC54211.2021.9651607
21.Schuman C, Patton R, Kulkarni S, Parsa M, Stahl C, Haas NQ, Mitchell JP, Snyder S, Nagle
A, Shanaﬁeld A et al. (2022) Evolutionary vs imitation learning for neuromorphic control at
the edge. Neuromorph Comput Eng 2(1):014002. https://doi.org/10.1088/2634-4386/ac45e7
22.Fra V , Forno E, Pignari R, Stewart T, Macii E, Urgese G (2022) Human activity recognition:
suitability of a neuromorphic approach for on-edge IoT applications. Neuromorph Comput
Eng. https://doi.org/10.1088/2634-4386/ac4c38
23.Frenkel C, Lefebvre M, Legat JD, Bol D (2018) A 0.086-mm212.7-pJ/SOP 64k-synapse 256-
neuron online-learning digital spiking neuromorphic processor in 28-nm CMOS. IEEE Trans
Biomed Circuits Syst 13(1):145–158. https://doi.org/10.1109/TBCAS.2018.2880425
24.Park J, Lee J, Jeon D (2019) A 65-nm neuromorphic image classiﬁcation processor with
energy-efﬁcient training through direct spike-only feedback. IEEE J Solid-State Circuits
55(1):108–119. https://doi.org/10.1109/JSSC.2019.2942367
25.Wang D, Chundi PK, Kim SJ, Yang M, Cerqueira JP, Kang J, Jung S, Kim S, Seok M (2020)
Always-on, sub-300-nw, event-driven spiking neural network based on spike-driven clock-
generation and clock-and power-gating for an ultra-low-power intelligent device. In: A-SSCC.
IEEE, Piscataway, pp 1–4
26.Izhikevich EM (2003) Simple model of spiking neurons. IEEE Trans Neural Netw 14(6):1569–
1572. https://doi.org/10.1109/TNN.2003.820440
27.Brader JM, Senn W, Fusi S (2007) Learning real-world stimuli in a neural network with spike-
driven synaptic dynamics. Neural Comput 19:288–2912. https://doi.org/10.1162/neco.2007.19.
11.2881
28.Guerguiev J, Lillicrap TP, Richards BA (2017) Towards deep learning with segregated
dendrites. ELife 6:e22901. https://doi.org/10.7554/eLife.22901.001
29.Stimberg M, Brette R, Goodman DF (2019) Brian 2, an intuitive and efﬁcient neural simulator.
Elife 8:e47314. https://doi.org/10.7554/eLife.47314
30.Knight JC, Nowotny T (2021) Larger GPU-accelerated brain simulations with procedural
connectivity. Nat Comput Sci 1(2):136–142. https://doi.org/10.1038/s43588-020-00022-7
31.Akopyan F, Sawada J, Cassidy A, Alvarez-Icaza R, Arthur J, Merolla P, Imam N, Nakamura Y ,
Datta P, Nam GJ et al. (2015) TrueNorth: Design and tool ﬂow of a 65 mW 1 million neuron
programmable neurosynaptic chip. IEEE TCAD 34(10):1537–1557. https://doi.org/10.1109/
TCAD.2015.2474396
32.Rast A, Jin X, Khan M, Furber S (2008) The deferred event model for hardware-oriented
spiking neural networks. In: ICONIP. Springer, Berlin, pp 1057–1064. https://doi.org/10.1007/
978-3-642-03040-6_128
33.Cheung K, Schultz SR, Luk W (2016) NeuroFlow: a general purpose spiking neural network
simulation platform using customizable processors. Front Neurosci 9:516. https://doi.org/10.
3389/fnins.2015.00516
34.Zheng N, Mazumder P (2018) A low-power hardware architecture for on-line supervised
learning in multi-layer spiking neural networks. In: ISCAS. IEEE, Piscataway, pp 1–5. https://
doi.org/10.1109/ISCAS.2018.8351516
35.Krithivasan S, Sen S, Venkataramani S, Raghunathan A (2019) Dynamic spike bundling for
energy-efﬁcient spiking neural networks. In: ISLPED. IEEE, Piscataway, pp 1–6. https://doi.
org/10.1109/ISLPED.2019.8824897
36.Fang H, Shrestha A, Zhao Z, Li Y , Qiu Q (2019) An event-driven neuromorphic system with
biologically plausible temporal dynamics. In: ICCAD. IEEE, Piscataway, pp 1–8. https://doi.
org/10.1109/ICCAD45719.2019.8942083246 T. N. N. Nguyen et al.
37.Stewart K, Orchard G, Shrestha SB, Neftci E (2020) Online few-shot gesture learning on a
neuromorphic processor. IEEE J Emerg Sel Top Circuits Syst 10(4):512–521. https://doi.org/
10.1109/JETCAS.2020.3032058
38.Nguyen TNN, Veeravalli B, Fong X (2022) An FPGA-based co-processor for spiking neural
networks with on-chip stdp-based learning. In: ISCAS. IEEE, Piscataway
39.Boahen KA (1998) Communicating neuronal ensembles between neuromorphic chips. In:
Neuromorph. Syst. Eng. Springer, Berlin, pp 229–259. https://doi.org/10.1007/978-0-585-
28001-1_11
40.James MD (2020) Address-event representation for spiking neural networks. https://
jamesmccaffrey.wordpress.com/2020/01/03/address-event-representation-for-spiking-neural-
networks/ . Accessed 7 Apr 2022
41.Sebastian A, Le Gallo M, Khaddam-Aljameh R, Eleftheriou E (2020) Memory devices and
applications for in-memory computing. Nat Nanotechnol 15(7):529–544. https://doi.org/10.
1037/s41565-020-0655-z
42.Davies M, Srinivasa N, Lin TH, Chinya G, Cao Y , Choday SH, Dimou G, Joshi P, Imam N,
Jain S, et al. (2018) Loihi: A neuromorphic manycore processor with on-chip learning. Micro
38(1):82–99. https://doi.org/10.1109/MM.2018.112130359
43.Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam
H (2017) Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications.
arXiv preprint arXiv:170404861. https://doi.org/10.48550/arXiv.1704.04861
44.Zhang X, Zhou X, Lin M, Sun J (2018) Shufﬂenet: an extremely efﬁcient convolutional neural
network for mobile devices. In: CVPR, pp 6848–6856. https://doi.org/10.1109/CVPR.2018.
00716
45.Boroumand A, Ghose S, Akin B, Narayanaswami R, Oliveira GF, Ma X, Shiu E, Mutlu O
(2021) Google neural network models for edge devices: analyzing and mitigating machine
learning inference bottlenecks. In: PACT. IEEE, Piscataway, pp 159–172. https://doi.org/10.
1109/PACT52795.2021.00019
46.Miriyala VPK, Ishii M (2020) Ultra-low power on-chip learning of speech commands
with phase-change memories. arXiv preprint arXiv:201011741. https://doi.org/10.48550/arXiv.
2010.11741
47.She X, Long Y , Mukhopadhyay S (2019) Improving robustness of reram-based spiking neural
network accelerator with stochastic spike-timing-dependent-plasticity. In: IJCNN. IEEE,
Piscataway, pp 1–8. https://doi.org/10.1109/IJCNN.2019.8851825
48.Nguyen VT, Trinh QK, Zhang R, Nakashima Y (2021) STT-BSNN: an in-memory deep binary
spiking neural network based on STT-MRAM. IEEE Access 9:151373–151385. https://doi.
org/10.1109/ACCESS.2021.3125685
49.Das D, Cen Y , Wang J, Fong X (2022) Bilayer-Skyrmion based design of neuron and synapse
for spiking neural network. arXiv preprint arXiv:220302171. https://doi.org/10.48550/arXiv.
2203.02171
50.Hill AJ, Donaldson JW, Rothganger FH, Vineyard CM, Follett DR, Follett PL, Smith MR,
Verzi SJ, Severa W, Wang F, et al. (2017) A spike-timing neuromorphic architecture. In: ICRC.
IEEE, Piscataway, pp 1–8. https://doi.org/10.1109/ICRC.2017.8123631
51.Mitchell JP, Schuman CD, Patton RM, Potok TE (2020) Caspian: a neuromorphic development
platform. In: NICE Workshop, pp 1–6. https://doi.org/10.1145/3381755.3381764
52.Eliasmith C, Anderson CH (2003) Neural engineering: computation, representation, and
dynamics in neurobiological systems. MIT Press, Cambridge, MA. https://doi.org/10.1086/
425829
53.Bekolay T, Kolbeck C, Eliasmith C (2013) Simultaneous unsupervised and supervised learning
of cognitive functions in biologically plausible spiking neural networks. In: CogSci, vol 35.
https://cogsci.mindmodeling.org/2013/papers/0058/paper0058.pdf
54.Shrestha SB, Orchard G (2018) Slayer: spike layer error reassignment in time. In: NIPS 31.
https://doi.org/arXiv:1810.08646Hardware Implementation for Spiking Neural Networks on Edge Devices 247
55.Plank JS, Schuman CD, Bruer G, Dean ME, Rose GS (2018) The TENNLab exploratory
neuromorphic computing framework. IEEE Lett Comput Soc 1(2):17–20. https://doi.org/10.
1109/LOCS.2018.2885976
56.Mitchell JP, Schuman C (2021) Low power hardware-in-the-loop neuromorphic training
accelerator. In: ICONS, pp 1–4. https://doi.org/10.1145/3477145.3477150
57.Bekolay T, Bergstra J, Hunsberger E, DeWolf T, Stewart TC, Rasmussen D, Choo X, V oelker
A, Eliasmith C (2014) Nengo: a python tool for building large-scale functional brain models.
Front Neuroinform 7:48. https://doi.org/10.3389/fninf.2013.00048
58.Blackstock M, Lea R (2014) Toward a distributed data ﬂow platform for the web of things
(distributed NODE-RED). In: WoT, pp 34–39. https://doi.org/10.1145/2684432.2684439
59.Rothganger F, Warrender CE, Trumbo D, Aimone JB (2014) N2a: a computational tool for
modeling from neurons to algorithms. Front Neural Circuits 8:1. https://doi.org/10.3389/fncir.
2014.00001
60.Kim S, Jeong J, Kim J, Yun YS, Kim B, Jung J (2020) Developing IoT applications using
spiking neural networks framework. In: RACS, pp 196–200. https://doi.org/10.1145/3400286.
3418271
61.DeWolf T, Jaworski P, Eliasmith C (2020) Nengo and low-power ai hardware for robust,
embedded neurorobotics. Front Neurorobot. https://doi.org/10.3389/fnbot.2020.568359
62.Morcos B (2019) NengoFPGA: an FPGA backend for the nengo neural simulator. Master’s
Thesis, University of Waterloo
63.Rathi N, Srinivasan G, Panda P, Roy K (2020) Enabling deep spiking neural networks with
hybrid conversion and spike timing dependent backpropagation. In: ICLR. https://doi.org/2005.
01807
64.Sen S, Venkataramani S, Raghunathan A (2017) Approximate computing for spiking neural
networks. In: DATE. IEEE, Piscataway, pp 193–198. https://doi.org/10.23919/DATE.2017.
7926981
65.Shi Y , Nguyen L, Oh S, Liu X, Kuzum D (2019) A soft-pruning method applied during training
of spiking neural networks for in-memory computing applications. Front Neurosci 13:405.
https://doi.org/10.3389/fnins.2019.00405
66.Nguyen TNN, Veeravalli B, Fong X (2021) Connection pruning for deep spiking neural
networks with on-chip learning. In: ICONS, pp 1–8. https://doi.org/10.1145/3477145.3477157
67.Deng L, Wu Y , Hu Y , Liang L, Li G, Hu X, Ding Y , Li P, Xie Y (2021) Comprehensive SNN
compression using ADMM optimization and activity regularization. IEEE Trans Neural Netw
Learn Syst. https://doi.org/10.1109/TNNLS.2021.3109064
68.Tang H, Kim H, Kim H, Park J (2019) Spike counts based low complexity SNN architecture
with binary synapse. IEEE Trans Biomed Circuits Syst 13(6):1664–1677. https://doi.org/10.
1109/TBCAS.2019.2945406
69.Koo M, Srinivasan G, Shim Y , Roy K (2020) SBSNN: Stochastic-bits enabled binary spiking
neural network with on-chip learning for energy efﬁcient neuromorphic computing at the edge.
IEEE Trans Circuits Syst I Regul Pap 67(8):2546–2555. https://doi.org/10.1109/TCSI.2020.
2979826
70.Chuang PY , Tan PY , Wu CW, Lu JM (2020) A 90 nm 103.14 TOPS/W binary-weight spiking
neural network CMOS ASIC for real-time object classiﬁcation. In: DAC, IEEE, Piscataway,
pp 1–6. https://doi.org/10.1109/DAC18072.2020.9218714
71.Wang Y , Xu Y , Yan R, Tang H (2020) Deep spiking neural networks with binary weights
for object recognition. IEEE Trans Cogn and Develop Syst 13(3):514–523. https://doi.org/10.
1109/TCDS.2020.2971655
72.Kheradpisheh SR, Mirsadeghi M, Masquelier T (2021) BS4NN: binarized spiking neural
networks with temporal coding and learning. Neural Process Lett. https://doi.org/10.1007/
s11063-021-10680-x
73.Lu S, Sengupta A (2020) Exploring the connection between binary and spiking neural
networks. Front Neurosci 14:535. https://doi.org/10.3389/fnins.2020.00535248 T. N. N. Nguyen et al.
74.Chen Y , Yu Z, Fang W, Huang T, Tian Y (2021) Pruning of deep spiking neural networks
through gradient rewiring. In: Zhou ZH (ed) International joint conferences on artiﬁcial
intelligence organization, IJCAI, pp 1713–1721. https://doi.org/10.24963/ijcai.2021/236
75.Furuya K, Ohkubo J (2021) Semi-supervised learning combining backpropagation and STDP:
STDP enhances learning by backpropagation with a small amount of labeled data in a spiking
neural network. J Phys Soc Jpn 90(7):074802. https://doi.org/10.7566/JPSJ.90.074802
76.Krizhevsky A, Hinton G et al. (2009) Learning multiple layers of features from tiny images.
Master’s Thesis, Department of Computer Science, University of Toronto
77.Deng L (2012) The MNIST database of handwritten digit images for machine learn-
ing research. IEEE Signal Process Mag 29(6):141–142. https://doi.org/10.1109/MSP.2012.
2211477
78.Deng L, Wu Y , Hu X, Liang L, Ding Y , Li G, Zhao G, Li P, Xie Y (2020) Rethinking the
performance comparison between SNNs and ANNs. Neural Netw 121:294–307. https://doi.
org/10.1016/j.neunet.2019.09.005
79.He W, Wu Y , Deng L, Li G, Wang H, Tian Y , Ding W, Wang W, Xie Y (2020) Comparing
SNNs and RNNs on neuromorphic vision datasets: similarities and differences. Neural Netw
132:108–120. https://doi.org/10.1016/j.neunet.2020.08.001Georgios/uni00A0Karakonstantis
Charles/uni00A0J./uni00A0Gillan/uni00A0 /uni00A0/uni00A0Editors
Computing 
at/uni00A0the/uni00A0EDGE
New Challenges for/uni00A0Service ProvisionComputing at the EDGEGeorgios Karakonstantis • Charles J. Gillan
Editors
Computing at the EDGE
New Challenges for Service ProvisionISBN 978-3-030-74535-6    ISBN 978-3-030-74536-3 (eBook)
https://doi.org/10.1007/978-3-030-74536-3
© Springer Nature Switzerland AG 2022
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of 
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, 
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information 
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology 
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the 
editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandEditors
Georgios Karakonstantis
BELFAST, UKCharles J. Gillan
Queen’s University Belfast
BELFAST, UKvPreface
It is widely accepted that innovation in the field of information technology moves at 
a rapid pace, perhaps even more rapidly than in any other academic discipline. Edge 
computing is one such example of an area that is still a relatively new field of tech -
nology, with the roots of the field arguably lying in the content delivery networks of 
the 1990s. The generally accepted definition of edge computing today is that it is 
those computations taking place at the edge of the cloud and in particular computing 
for applications where the processing of the data takes place in near real time. Stated 
this way, edge computing is strongly linked to the emergence of the Internet of 
Things (IoT). The existence globally of many funded research projects, leading to 
many publications in academic journals, bears witness to the fact that we are still in 
the early days of the field of edge computing.
In the final days (late September 2019) of the UniServer project, which received 
funding from the European Commission under its Horizon 2020 Programme for 
research and technical development, we came up with the idea of creating a book 
aimed at summarizing the state of the art. Our aim is to reflect the output from 
3 years of UniServer research and its position in the wider research field at the time. 
The individual book chapters are the output of many different members of the 
UniServer project, and we have undertaken the task to organize and edit these into 
a coherent book. It is our hope that the style of presentation in the book makes the 
material accessible, on the one hand, to early stage academic researchers including 
PhD students while, on the other hand, being useful to managers in businesses that 
are deploying, or considering deployment of, their solutions in an edge computing 
environment for the first time. Various parts of the book will appeal more to one or 
other of these different audiences.
We are grateful to the publication team at Springer for bearing with us during the 
familiar delays in the writing process.
Belfast, Northern Ireland, UK Georgios Karakonstantis  
  Charles J. Gillan   January 2021viiContents
  Introduction  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    1
Charles J. Gillan and George Karakonstantis
  Challenges on Unveiling Voltage Margins from the Node  
to the Datacentre Level  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13
George Papadimitriou and Dimitris Gizopoulos
  Harnessing Voltage margins for Balanced Energy and Performance  . . . .   51
George Papadimitriou and Dimitris Gizopoulos
  Exploiting Reduced Voltage Margins: From Node- to the Datacenter-
level  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   91
Panos Koutsovasilis, Christos Kalogirou, Konstantinos Parasyris, 
Christos D. Antonopoulos, Nikolaos Bellas, and Spyros Lalis
  Improving DRAM Energy-efficiency  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  123
Lev Mukhanov and Georgios Karakonstantis
  Total Cost of Ownership Perspective of Cloud vs Edge Deployments  
of IoT Applications  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141
Panagiota Nikolaou, Yiannakis Sazeides, Alejandro Lampropulos,  
Denis Guilhot, Andrea Bartoli, George Papadimitriou,  
Athanasios Chatzidimitriou, Dimitris Gizopoulos,  
Konstantinos Tovletoglou, Lev Mukhanov, Georgios Karakonstantis, 
Marios Kleanthous, and Arnau Prat
  Software Engineering for Edge Computing  . . . . . . . . . . . . . . . . . . . . . . . . .  163
Dionysis Athanasopoulos
  Overcoming Wifi Jamming and other security challenges at the Edge  . . .  183
Charles J. Gillan and Denis Guilhot
  Index  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2131Introduction
Charles J. Gillan and George Karakonstantis
1  The Internet of Things, Edge Computing 
and Its Architectures
The Internet is in the early stages of a new operating model known as the Internet of 
Things (IoT) due to the ever-increasing number of Internet-connected intelligent 
devices. Each intelligent device is pushing a small amount of data to the Internet, 
and these small amounts multiplied by billions of devices aggregate to become Big 
Data [ 1]. In one of their white papers, the manufacturer Cisco [ 2] suggested that the 
IoT era began in late 2008 or early 2009 at the point when the number of devices 
connected to the Internet exceeded the human population of earth. By 2012, McAfee 
and co-authors [ 3] reported that around 2.5 exabytes of new data appeared on the 
Internet each day, a concept that resonates with the term Big Data. McAfee and 
co-authors distinguished this new IoT data environment from the previous data 
environment in terms of the characteristics, summarized as the three Vs: velocity, 
variety and volume. The trend has continued as expected since 2012, driven by 
applications such as smart homes [ 4] where multiple devices now collect data.
The traditional cloud architecture as defined by the US National Institute of 
Standards and Technology (NIST) [ 5] on its own cannot handle the volume and 
velocity of this new level information. Certainly, the cloud enables access to 
compute, storage and connectivity, the fact that these resources are ultimately 
centralized in the data centre creates network latency and therefore performance 
issues for devices and data that are geographically remote.
Innovation driven by Big Data-driven innovation forms a key pillar in twenty- 
first- century sources of growth. These large data sets are becoming a core asset in 
the economy, fostering new industries, processes and products and creating 
C. J. Gillan ( *) · G. Karakonstantis 
The School of Electrical and Electronic Engineering and Computer Science (EEECS), 
Queen’s University Belfast, Belfast, Northern Ireland
e-mail:  c.gillan@qub.ac.uk
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_12
significant competitive advantages. Analysis of the market by the company 
McKinsey suggests that the field of IoT has the potential to create economic impact 
up to $6 trillion annually by 2025 with some of the most promising uses arising in 
the field of health care, infrastructure, and public-sector services [ 6]. For example, 
in the healthcare field, McKinsey points out that IoT can assist health care by the 
creation of a 10–20% cost reduction by 2025 in the management of chronic diseases. 
This is made possible, in part, by enabling significantly more remote monitoring of 
patient state. Patients may therefore remain in their home rather than needing 
hospital visits and admissions.
Given the relative geographical remoteness of the traditional cloud data centre in 
the IoT environment, the seemingly obvious first step is to try to move the comput -
ing closer to the data source in order to overcome issues of latency. This is known 
as edge computing, meaning that significant amounts of processing, but not neces -
sarily all of it, take place close to where the data is collected. Edge computing is in 
essence a model or a concept. There are potentially many ways to implement this 
concept in practice. Fog computing is an architectural model for the implementation 
of edge computing with its roots in the work of Bar-Magen et al. [ 7–9]. Cisco was 
one of the early pioneers of fog computing [ 10] and the field has gained significant 
traction in the market since the creation of the OpenFog consortium in 2015 [ 29] 
with leading members including Cisco, ARM, Dell, Intel, Microsoft and Princeton 
University.
Mouradian and co-workers [ 11] surveyed the diverse research literature for the 
period 2013–2017 finding sixty-eight papers (excluding papers of security issues) 
addressing the field of fog computing. Other reviewers have reviewed the literature 
for security-related publications for different time periods [ 12–14]. Following [ 13, 
15] we can define the characteristics of fog computing system as including the 
properties that it:
• is located at the edge of network with rich and heterogeneous end-user support;
• provides support to one from a broad range of industrial applications due to 
instant response capability;
• has its own local computing, storage, and networking services; [ 28]
• operates on data gathered locally;
• is a virtualized platform offering relatively inexpensive, flexible and portable 
deployment in terms of both hardware and software.
There are competing architectures for edge computing distinct from fog comput -
ing. These include Mobile Cloud Computing (MCC) [ 16], Mobile Edge Computing 
(MEC) [ 12, 30] and Multi-access Edge Computing [ 13, 31]. The cloudlet concept 
[17] was proposed a few years before fog computing was first discussed; however, 
the two concepts overlap significantly. A cloudlet has the properties of a cloud but 
has limited capacity to scale resources.
Mist Computing is an approach that goes beyond fog computing embedding sig -
nificant amounts of computing in the sensor devices at the very edge of the network 
[18]. While this reduces data transfer latency significantly, it places a load on these C. J. Gillan and G. Karakonstantis3
small and resource-constrained devices although it also decouples the devices more 
from each other. In this model, the self-awareness of every device is critical. By 
definition, centralized management would mitigate against this distribution of work, 
a consequence if that network interaction between devices needs to be managed by 
the devices themselves.
All of the architectures for computing at the edge are dependent on improving 
the performance of servers that run Internet/cloud-based services, while reducing 
their design and implementation cost as well as power consumption. This is very 
important for reducing the running costs in a server farm that supports data centres 
and cloud providers, while at the same time it enables the placement of servers 
co-located with the origin of the data (e.g., sensors, cameras) where electrical power 
is generally limited. In addition, all these new efficient servers need to be able to 
support useful attributes of software stacks in common use by cloud service 
providers that facilitate migration and programmability. What is more, there is a 
need to re-think continually the architecture model of Internet in terms of 
sustainability and security. This book presents some of the latest work in these fields.
A key advantage of edge computing is that it makes it possible to run a service 
close to the data sources that it processes. It follows that this presents an oppor -
tunity to improve energy efficiency by significantly reducing the latency to com -
municate through the public network to a cloud located in a remote data centre. 
By exploiting this attribute, one can run a compute service either using signifi -
cantly less energy or alternatively for the same energy spend could offer more 
functionality within the same power envelope. Typical figures today show that 
the overall latency targeted for interactive cloud services ranges up to several 
hundred milliseconds. On paper then, some IoT service with a target end-to-end 
latency of 200ms, for a roundtrip to the cloud, might expect to spend half of its 
energy budget in the network. Using edge computing to remove most of the com -
munication latency can permit the execution of the server edge CPU at 50% of 
the peak frequency with 30% less voltage. This means that the energy cost can be 
reduced by up to 50%.
2  Challenges for the Operation at the Edge of the Cloud
The previous section has discussed some of the generic challenges facing operation 
at the edge of the cloud today. In this section, we look at the technical challenges at 
each edge node. Many of the chapters in this book are based on research carried out 
in the UniServer project funded by the European Commission under its research and 
technical development programme known as Horizon 2020. The UniServer 
approach overlaps with the strategy followed by other research groups around the 
world and we base our discussion on the UniServer approach here.
The project adopted a cross-layer approach, shown in Fig.  1, where the layers 
rang from the hardware levels up to the system software layers. Optimizations Introduction4
Hardware (Cores, Memory, Buses)Hardware
CharacterizationHypervisor–Guest OSOpenStack and Resource ManagementApplicationsExploitation of Cloud
and Fog/Edge
Processing
Utilization of Design
and Intrinsic
Heterogeneity
Error Resilient KVM
Dynamic Health
Diagnostics and
Characterization
(HealthLog, StressLog)
HW CharacterizationSoftware
Characterization
(V, F)(V, F, Er)(R, En, P)Resilience
Energy
Performance
Firmwarelow-level Error
HandlersOS Error
HandlersRe-configure
(V, F)Task assignmentTasks
Error Handling
Errors
Fig. 1  A layered view of the operation of an edge server. The boxes on the right-hand side show 
the different types of work that needs to be undertaken to research the optimization of the system. 
These are explored in later chapters of the book
were performed at the circuit, micro-architecture and architecture layers of the 
system architecture by automatically revealing the worst possible operating 
points, for example, voltage and frequency, of each hardware component. The 
operating point chosen can help to boost performance or energy efficiency at 
levels closer to the Pareto front maximizing the returns from technology scaling. 
UniServer achieved this at the firmware layer using low-level software handlers 
to monitor and control the operating status of the underlying hardware compo -
nents. Expanding on the detail in Fig.  1 the interaction of one of the key handlers, 
named HealthLog, with other components in the system is shown in Fig.  2. To 
enable additional functionality, the UniServer team ported state-of-the-art soft -
ware packages for virtualization (i.e., KVM) and resource management (i.e., 
OpenStack) onto the micro-server further strengthening its advantages with min -
imum intrusion and easy adoption.C. J. Gillan and G. Karakonstantis5
Fig.  2  The interaction of the HealthLog component with other parts of the system
Fig.  3  A block diagram view of the physical architecture of the XGene2 server
The initially chosen hardware platform for the edge server used by UniServer 
was one of the first ARM 64-bit Server-on-a-Chip solutions (X-Gene2). This 
includes eight ARMv8 cores. Later in the project the X-Gene 3 CPU became avail -
able, a platform which has a 32-core ARMv8 chip. The CPU features hardware 
virtualization acceleration, MMU virtualization, advanced SIMD instructions and a 
floating-point unit. In addition, the platform comes equipped with network interface 
accelerators and high-speed communicators to support node-to-node communica -
tion required within server racks but also from the cloud edge to the cloud data 
centre (Fig.  3).
Any semiconductor vendor that ships designs in scaled technologies has to cope 
with process variations by performing extensive statistical analysis at the design 
phase of their products. Note that the vendor of the XGene product changed from 
Applied Micro to Ampere. The objective of the vendor is to try to limit as much as 
possible the pessimistic design margins in timing and voltage and the resulting 
power and performance penalties.Introduction6
2.1  Challenges for the operation of CPUs at the Edge 
of the Cloud
Rather than trying to predict the operational margins at design time, an alternative 
approach is to reveal these and to them effectively at the run-time on the actual 
boards shipped to users. Figure  4 illustrates that this method takes account of 
different types of operational changes inherent in CPU chips. The graph on the left- 
hand side of the figure illustrates the distribution of operational frequency of chips 
at the fabrication stage. Typically, the vendor will discard chips to the left or right of 
the blue peak. The variation arises during chip fabrication due to small variances in 
transistor dimensions (length, width, oxide thickness). These in turn have a direct 
impact on the threshold voltage for the device. Other variations exist, some of which 
can be attributed to ageing when deployed. The right-hand side of the figure shows 
that using technologies mentioned above and described in later chapters of this 
book, the CPU chips labelled red and green can be deployed in products.
2.1.1  Stagnant Power Scaling.
For over four decades Moore ʼs law, coupled with Dennard scaling [ 19], ensured the 
exponential performance increase in every process generation through device, 
circuit, and architectural advances. Up to 2005, Dennard scaling meant increased 
transistor density with constant power density. If Dennard scaling would have 
continued, according to Kumey [ 20], by the year 2020 we would have approximately 
40 times increase in energy efficiency compared to 2013. Unfortunately, Dennard 
scaling has ended because of the slowdown of voltage scaling due to slower scaling 
of leakage current as compared to area scaling. The scale of the issue is depicted in 
Fig. 5, based on collected data [ 21, 22].
Fig. 4  Schematic illustration of the variation in operational parameters of the CPU chipsC. J. Gillan and G. Karakonstantis7
Fig. 5  Comparison of energy efficiency relation to 2013 (y-axis) for three cases. The grey line is 
Dennard Scaling, the blue line is from the ITRS roadmap and the organ line is a conservative 
estimate
The increasing gap between the energy efficiency gains that could be achieved 
according to the ideal Dennard scaling is what actually achieved based on the ITRS 
roadmap [ 22] and the actual conservative voltage scaling. The end of Dennard 
scaling has changed the semiconductor industry dramatically. To continue the 
proportional scaling of performance and exploit Moore ʼs law scaling, processor 
designers have focused on building multicore systems and servicing multiple tasks 
in parallel instead of building faster single cores. Even so, limited voltage scaling 
increasingly results in having a larger fraction of a chip unusable, commonly 
referred to as Dark Silicon [ 21]. Some industrial technologists have previously 
warned in a number of talks that meeting very tight power budgets may bring the 
limitation of activating only nine percent of available transistors at any point in 
time [ 21].
2.1.2  Variations and Pessimistic Margins
The variability in device and circuit parameters whether on a processor core within 
a system on chip (SoC) or on a CPU in an enterprise-level server adversely impacts 
both energy efficiency and the performance of the system. V oltage values will vary 
in time during the microprocessor operation because of workload changes on the 
system and furthermore due to changes in the environment whether the system is 
located. V oltage safety margins are added therefore to ensure correct operation. Introduction8
Table  1 summarizes some of the main causes for safety margins and provides their 
relative contribution to the up-scaling of the supply voltage Vdd.
The added safety voltage margins increase energy consumption and force opera -
tion at a higher voltage or lower frequency. They may also result in lower yield or 
field returns if a part operates at higher power than its specification allows. The 
voltage margins are becoming more prominent with area scaling and the use of 
more cores per chip large voltage droops [ 23, 24] reliability issues at low voltages 
(Vmin) [ 25], and core to core variations [ 26]. The scale of pessimism is also 
observed on recently measured ARM processors revealing more than 30% timing 
and voltage margins in 28nm [ 24, 27]. Note that these margins are only due to the 
characterized voltage droops and have not considered the joint effect of other 
variability sources.
Combined leakage and variations have elevated power as a prime design param -
eter. If we need to go faster, we need to find ways to become more power efficient. 
All other things being equal, if one design uses less power than another, then it has 
headroom to improve performance by using more resources or operating at a higher 
frequency. Simply put, the more energy efficient a chip is, the more functionality 
with higher utilization occurs and, naturally, it will service more tasks.
3  Summary of Chapters in the Book
Each subsection below presents a short summary of the information presented in 
each chapter of the book.
3.1  Introduction
This, the present chapter, introduces the general ideas presented in more detail in 
each chapter that follows.
Table 1  Reasons for addition 
of safety marginsReasons for margins Vdd Up-scaling
V oltage droops ~20%
Vmin ~15%
Core-to-core variations ~5%C. J. Gillan and G. Karakonstantis9
3.2  Challenges on Unveiling Pessimistic Voltage Margins 
at the System Level
This chapter starts by briefly reviewing the currently established techniques, which 
contribute to either unveil the pessimistic voltage margins or propose mitigation 
techniques to make the microprocessors more tolerant to low-voltage conditions. 
Following that, the chapter discusses the challenges faced in characterizing 
microprocessor chips and present comprehensive solutions that overcome these 
challenges and can reveal the pessimistic voltage margins to unlock the full potential 
energy savings.
3.3  Harnessing Voltage Margins for Balanced Energy 
and Performance
Understanding the behaviour in non-nominal conditions is very important for mak -
ing software and hardware design decisions for improved energy efficiency while at 
the same time preserving the correctness of operation. The chapter discusses how 
characterization modelling supports design and system software decisions to har -
ness voltage margins and thus improve energy efficiency while preserving operation 
correctness.
3.4  Exploiting Reduced Voltage Margins
Dynamic hardware configuration in non-nominal conditions is a challenging under -
taking, as it requires real-time characterization of hardware-software interaction. 
This chapter discusses mechanisms to achieve dynamic operation at reduced CPU 
voltage margins. It then evaluates the trade-off between improved energy efficiency, 
on the one hand, and the cost of software protection and potential SLA penalties in 
large-scale cloud deployments, on the other hand.
3.5 Improving DRAM Energy-efficiency
The organization of a DRAM device and the operating parameters that are set for 
the device can have a strong impact on the energy efficiency of the memory. This 
chapter demonstrates a machine learning approach that enables relaxation of operat -
ing parameters without compromising the reliability of the memory.Introduction10
3.6  Adoption of New Business Models: Total Cost 
of Ownership Analysis
Dynamic adaption to operational hardware parameters lays the foundation for pur -
pose-built cloud and enterprise server deployments specifically focusing on 
increased density and field serviceability resulting in a lower total cost of ownership 
(TCO). End-to-end TCO in edge computing, which is a new concept, aims to 
estimate the entire eco-system lifetime capital and operating expenses including the 
costs of data source nodes (i.e. IoT nodes). There is, therefore, an opportunity to 
develop a new business model of owning your own server to establish a private fog.
Chapter 5 is dedicated to analysis and modelling of end-to-end TCO model to 
identify the benefits of a private fog versus a mix fog/cloud model. It studies two 
applications with distinctly different characteristics. One is a financial application 
and the other is a social customer relationship management application. The chapter 
shows that by making edge and cloud computing more power efficient, one can 
achieve in many situations considerable gains in the TCO metric, an attribute that 
can lead to enhanced profitability of the business providing the service.
3.7  The Role of Software Engineering
The description in the previous paragraphs highlights the interaction between the 
hardware and the system software. Clearly, it is therefore critical to consider the 
relevant software engineering principles. Chapter 6 considers these objectives. It 
starts by specifying the core concepts of the general-purpose software-engineering 
process before proceeding to present the multi-tier architecture of edge infrastructure, 
and how software applications are deployed to such an infrastructure. The chapter 
concludes with a description of the view and the role of a software-engineering 
process for edge computing, along with research challenges in this process.
3.8  Security at the Edge
The extensive use of WiFi links at the edge of the cloud, for example, to connect to 
sensors, implies that particular attention needs to be paid to the security of the WiFI 
infrastructure. The chapter looks at the role of jamming attacks at the edge and 
proposed solutions to defend against these. Of course, such attacks be targeted 
against any WiFi network and are not limited to edge networks.
If an attacker manages to join the WiFi network and access an edge system, they 
gain an enhanced ability to tamper with the system. There are new many attack 
vectors, generally called side-channel attacks, which become possible because the 
system is operating outside normal margins. Chapter 7 explains both jamming and 
side-channel attacks, and presents viable counter measures that may be deployed to 
defend against these.C. J. Gillan and G. Karakonstantis11
4  Conclusion
The editors of the book, and the authors of each chapter, trust that you will find this 
book interesting and relevant. In addition to reporting research results by the authors, 
each chapter references other relevant work.
We hope that the material will be well suited to early-stage PhDs entering the 
field but also that the material on the total cost of ownership modelling will be 
relevant to business and operational managers in the IT field considering deployment 
of edge solutions.
References
 1. A. Yousefpour, C. Fung, T. Nguyen, K. Kadiyala, F. Jalali, A. Niakanlahiji, J. Kong, J.P. Jue, 
J. Syst. Archit. 98, 289–330 (2019)
 2. D. Evans, The Internet of Things: how the next evolution of the Internet is changing every -
thing, CISCO white paper 1 (2011) (2011) 1–11. Available on the web at.: https://www.cisco.
com/c/dam/en_us/about/ac79/docs/innov/IoT_IBSG_0411FINAL.pdf
 3. A. McAfee, E. Brynjolfsson, T.H. Davenport, D. Patil, D. Barton, Big data: the management 
revolution. Harv. Bus. Rev. 90(10), 60–68 (2012)
 4. A. Yassinea, S. Singh, M.S. Hossain, G. Muhammad, IoT big data analytics for smart homes 
with fog and cloud computing. Futur. Gener. Comput. Syst. 91, 563–573 (2019). https://doi.
org/10.1016/j.future.2018.08.040
 5. P. Mell, T. Grance, The NIST definition of cloud computing, US National Institute of Standards 
and Technology (NIST) Special Publication 800-145, 2011, available on the web at: https://
nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800- 145.pdf
 6. J.  Manyika, M.  Chui, J.  Bughin, R.  Dobbs, P.  Bisson, A.  Marrs, Disruptive technologies: 
advances that will transform life, business, and the global economy, McKinsey Global 
Institute, May 2013, available on the web at: https://www.mckinsey.com/~/media/McKinsey/
Business%20Functions/McKinsey%20Digital/Our%20Insights/Disruptive%20technologies/
MGI_Disruptive_technologies_Full_report_May2013
 7. J. Bar-Magen, A. Garcia-Cabot, E. Garcia, L. de-Marcos, J.A. Gutierrez de Mesa, Collaborative 
network development for an embedded framework, in 7th international conference on knowl -
edge management in organizations: service and cloud computing , ed. by L. Uden, F. Herrera, 
J. B. Pérez, J. M. Corchado Rodríguez, (Springer, Berlin/Heidelberg, 2013), pp. 443–453
 8. J. Bar-Magen, Fog computing- introduction to a new cloud evolution, in Escrituras Silenciadas: 
El paisaje como Historiografia , ed. by F. Jose, F. Casals, P. Numhauser, 1st edn., (UAH, Alcala 
de Henares, 2013), pp. 111–126
 9. J.B.-M.  Numhauser, J.A.G. de Mesa, XMPP distributed topology as a potential solution 
for fog computing, in MESH 2013 the sixth international conference on advances in mesh 
networks , ed. by E.  Borcoci, S.  S. Compte, (Pub: IARIA, Barcelona), pp.  26–32. ISBN 
978-1-61208-299-8
 10. M.S.V . Janakiram, Is fog computing the next big thing in the internet of things. Forbes Magazine. 
18 April 2016. Available on the web at: https://www.forbes.com/sites/janakirammsv/2016/04/18/
is- fog- computing- the- next- big- thing- in- internet- of- things/#1d77ebcc608d
 11. C. Mouradian, D. Naboulsi, S. Yangui, R.H. Glitho, M.J. Morrow, P.A. Polakos, A comprehen -
sive survey on fog computing: state-of-the-art and research challenges. IEEE Commun. Surv. 
Tutor. 20(1), 416–464 (2018)Introduction12
 12. A. Al-Fuqaha, M. Guizani, M. Mohammadi, M. Aledhari, M. Ayyash, Internet of things: a sur -
vey on enabling technologies, protocols, and applications. IEEE Commun. Surv. Tuts 17(4), 
2347–2376., 4th Quart (2015)
 13. S. Khan, S. Parkinson, Y . Qin, Fog computing security: a review of current applications and 
security solutions. J. Cloud. Comp. 6, 19 (2017). https://doi.org/10.1186/s13677- 017- 0090- 3
 14. J. Yakubu, S.M. Abdulhamid, H.A. Christopher, et al., Security challenges in fog-computing 
environment: a systematic appraisal of current developments. J.  Reliab. Intell. Environ. 5, 
209–233 (2019). https://doi.org/10.1007/s40860- 019- 00081- 2
 15. F. Bonomi, R. Milito, J Zhu, S Addepalli, Fog computing and its role in the internet of things, 
in Proceedings of the first edition of the MCC workshop on Mobile Cloud Computing (ACM, 
2012), pp. 13–16
 16. H.T. Dinh, C. Lee, D. Niyato, P. Wang, A survey of mobile cloud computing: Architecture, 
applications, and approaches. Wireless Commun. Mobile Comput. 13(18), 1587–1611 (2013)
 17. M. Satyanarayanan, P. Bahl, R. Caceres, N. Davies, The case for VM-based cloudlets in mobile 
computing. IEEE Pervasive Comput. 8(4), 14–23 (2009)
 18. S. Jürgo, K.T. Preden, A. Jantsch, M. Leier, A. Riid, E. Calis, The benefits of self-awareness 
and attention in fog and mist computing. Computer 48(7), 37–45 (Jul 2015)
 19. G.E. Moore, Cramming more components onto integrated circuits. Proc IEEE 86(1), 78 (1998)
 20. J. Koomey, S. Berard, M. Sanchez, H. Wong, Implications of historical trends in the elec -
trical efficiency of computing. IEEE Ann. Hist. Comput. 33(3), 46–54 (2011). https://doi.
org/10.1109/MAHC.2010.28
 21. H. Esmaeilzadeh, E. Blem, R.S. Amant, K. Sankaralingam, D. Burger, Dark silicon and the end 
of multicore scaling, in 2011 38th annual International Symposium on Computer Architecture 
(ISCA), San Jose, CA, 2011, pp. 365-376.
 22. The International Technology Roadmap for Semiconductors (ITRS), 2013 tables available on-
line at ITRS http://www.itrs.net//2013ITRS/2013TableSummaries
 23. Y . Kim et al., AUDIT: stress testing the automatic way, in 2012 45th annual IEEE/ACM inter -
national symposium on microarchitecture, Vancouver, BC (2012), pp. 212–223, https://doi.
org/10.1109/MICRO.2012.28 .
 24. P.N. Whatmough, S. Das, Z. Hadjilambrou, D.M. Bull, An all-digital power-delivery monitor 
for analysis of a 28nm dual-core ARM Cortex-A57 cluster, 2015 IEEE International Solid- 
State Circuits Conference – (ISSCC) Digest of Technical Papers, San Francisco, CA, 2015, 
pp. 1-3, https://doi.org/10.1109/ISSCC.2015.7063026
 25. V .J. Reddi et al., V oltage smoothing: characterizing and mitigating voltage noise in produc -
tion processors via software-guided thread scheduling, in 2010 43rd annual IEEE/ACM 
International Symposium on Microarchitecture, Atlanta, GA, 2010, pp.  77-88, https://doi.
org/10.1109/MICRO.2010.35 .
 26. A. Bacha, R. Teodorescu, Dynamic reduction of voltage margins by leveraging on-chip ECC 
in Itanium II processors, in Proc. of International Symposium on Computer Architecture 
(ISCA), June 2013, pp. 297–307 https://doi.org/10.1145/2485922.2485948
 27. K.A.  Bowman et  al., A 45  nm resilient microprocessor core for dynamic variation toler -
ance. IEEE J.  Solid-State Circuits 46(1), 194–208 (Jan. 2011). https://doi.org/10.1109/
JSSC.2010.2089657
 28. A.C. Baktir , A. Ozgovde , C. Ersoy, How can edge computing benefit from software-defined 
networking: a survey, use cases, and future directions, IEEE Commun. Surv. Tutor. 19 (4) 
(2017) 2359–2391.
 29. OpenFogConsortium, Openfog reference architecture for fog computing, 2017. Available on 
line: https://www.openfogconsortium.org/ra/ , February 2017
 30. European Telecommunications Standards Institute, MobileEdge Computing (MEC) Terminology. 
Available on-line. http://www.etsi.org/deliver/etsi_gs/MEC/001_099/001/01.01.01_60/gs_
MEC001v010101p.pdf
 31. European Telecommunications Standards Institute. Multi-Access Edge Computing. Accessed 
on May 2017. Available on-line: http://www.etsi.org/technologies- clusters/technologies/
multi- accessedge- computingC. J. Gillan and G. Karakonstantis13Challenges on Unveiling Voltage Margins 
from the Node to the Datacentre Level
George Papadimitriou and Dimitris Gizopoulos
1  Introduction
Technology scaling has enabled improvements in the three major design optimiza -
tion objectives: performance increase, power consumption reduction, and die cost 
reduction, while system design has focused on bringing more functionality into 
products at a lower cost. While today’s microprocessors are much faster and much 
more versatile than their predecessors, they also consume significantly more 
power [ 1].
To date, the approach has been to attempt to lower the voltage with each process 
generation. But as the voltage is lowered, leakage current and energy increase, con -
tributing to a higher power. These high-power densities impair the reliability of 
chips and life expectancy, increase cooling costs, and even raise environmental con -
cerns primarily due to the heavy deployment and use of large data centers. Power 
problems also pose issues for smaller mobile devices with limited battery capacity. 
While these devices could be implemented using faster microprocessors and larger 
memories, their battery life would be further diminished. Improvements in micro -
processor technology will eventually come to a standstill without cost-effective 
solutions to the power problem.
Power and energy are commonly defined as the work performed by a system. 
Energy is the total amount of work performed by a system over some time, whereas 
power is the rate at which the system performs the work. In formal terms,
 PW
T= (1)
G. Papadimitriou ( *) · D. Gizopoulos 
Department of Informatics and Telecommunications, National and Kapodistrian  
University of Athens, Athens, Greece
e-mail:  georgepap@di.uoa.gr
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_214
 EPT=∗  (2)
where P is power, E is energy, T is a specific time interval, and W is the total work 
performed in that interval. Energy is measured in joules, while power is measured 
in watts [ 1].
The relation of the power and energy of a microprocessor can be described by a 
simple example: by halving the rate of the input clock, the power consumed by a 
microprocessor can be reduced. If the microprocessor, however, takes twice as long 
to run the same programs, the total energy consumed is the same. Whether power or 
energy should be reduced depends on the context. Reducing energy is often more 
critical in data centers because they occupy an area of a few football fields, contain 
tens of thousands of servers, consume electricity of small cities, and utilize expen -
sive cooling mechanisms.
There are two forms of power consumption: dynamic power consumption and 
static power consumption. Dynamic power consumption is caused by circuit activ -
ity such as input changes in an adder or values in a register. As the following equa -
tion shows, the dynamic power ( Pdynamic ) depends on four parameters namely, supply 
voltage (Vdd), clock frequency ( f), physical capacitance ( C), and an activity factor 
(a) that relate to how many transitions occur in a chip:
 Pa CVfdynami c=2
 (3)
Both static and dynamic variations lead microprocessor architects to apply con -
servative guardbands (operating voltage and frequency settings) to avoid timing 
failures and guarantee correct operation, even in the worst-case conditions excited 
by unknown workloads or the operating environment. Revealing and harnessing 
the pessimistic design-time voltage margins offers a significant opportunity for 
energy- efficient computing in multicore CPUs. The full energy savings potential 
can be exposed only when accurate core-to-core, chip-to-chip, and workload-to-
workload voltage scaling variation is measured. When all these levels of variation 
are identified, system software can effectively allocate hardware resources to soft -
ware tasks matching the capabilities of the former (undervolting potential of the 
CPU cores) and the requirements of the latter (for reduced energy or increased 
performance).
In this chapter, we begin by briefly reviewing the currently established tech -
niques, which contribute to either unveil the pessimistic voltage margins or pro -
pose mitigation techniques to make the microprocessors more tolerant to 
low-voltage conditions. Later, we describe the challenges in characterizing micro -
processor chips and present comprehensive solutions that overcome these chal -
lenges and can reveal the pessimistic voltage margins to unlock the full potential 
energy savings.G. Papadimitriou and D. Gizopoulos15
2  Supply Voltage Scaling: Challenges 
and Established Techniques
2.1  Established Techniques
During the last years, the goal for improving microprocessors’ energy efficiency, 
while reducing their power supply voltage, is a major concern of many scientific 
studies that investigate the chips’ operation limits in nominal and off-nominal con -
ditions [ 2, 3]. In this section, we briefly summarize the existing studies and findings 
concerning low-voltage operation and characterization studies.
Wilkerson et al. [ 4] go through the physical effects of low-voltage supply on 
SRAM cells and the types of failures that may occur. After describing how each cell 
has a minimum operating voltage, they demonstrate how typical error protection 
solutions start failing far earlier than a low-voltage target (set to 500 mV) and pro -
pose two architectural schemes for cache memories that allow operation below 
500 mV . The word-disable and bit-fix schemes sacrifice cache capacity to tolerate 
the high failure rates of low voltage operation. While both schemes use the entire 
cache on high voltage, they sacrifice 50% and 25% accordingly in 500 mV . Compared 
to existing techniques, the two schemes allow a 40% voltage reduction with power 
savings of 85%.
Chishti et al. [ 5] propose an adaptive technique to increase the reliability of cache 
memories, allowing high tolerance on multi-bit failures that appear on the low- 
voltage operation. The technique sacrifices memory capacity to increase the error- 
correction capabilities, but unlike previously proposed techniques, it also offers soft 
and non-persistent error tolerance. Additionally, it does not require self-testing to 
identify erratic cells in order to isolate them. The MS-ECC design can achieve a 
30% supply voltage reduction with 71% power savings and allows configurable 
ECC capacity by the operating system based on the desired reliability level.
Bacha et al. [ 6] present a new mechanism for the dynamic reduction of voltage 
margins without reducing the operating frequency. The proposed mechanism does 
not require additional hardware as it uses existing error correction mechanisms on the 
chip. By reading their error correction reports, it manages to reduce the operating 
voltage while keeping the system in safe operation conditions. It covers both core-to-
core and dynamic variability caused by the running workload. The proposed solution 
was prototyped on an Intel Itanium 9560 processor and was tested using SPECjbb2005 
and SPEC CPU2000-based workloads. The results report promising power savings 
that range between 18% and 23%, with marginal performance overheads.
Bacha et al. [ 7] again rely on error correction mechanisms to reduce operating 
voltage. Based on the observation that low-voltage errors are deterministic, the 
paper proposes a hardware mechanism that continuously probes weak cache lines to 
fine-tune the system’s supply voltage. Following an initial calibration test that 
reveals the weak lines, the mechanism generates simple write-read requests to trig -
ger error correction and is capable to adapt to voltage noise as well. The proposed 
mechanism was implemented as a proof-of-concept using dedicated firmware that Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level16
resembles the hardware operation on an Itanium-based server. The solution reports 
an average of 18% supply voltage reduction and an average of 33% power con -
sumption savings, using a mixed set of applications.
Bacha et al. [ 8] exploit the observation of deterministic error distribution to pro -
vide physically unclonable functions (PUF) to support security applications. They 
use the error distribution of the lowest save voltage supply as an unclonable finger -
print, without the typical requirement of additional dedicated hardware for this pur -
pose. The proposed PUF design offers a low-cost solution for existing processors. 
The design is reported to be highly tolerant to environmental noise (up to 142%) 
while maintaining very small misidentification rates (below 1 ppm). The design was 
tested on a real system using an Itanium processor as well as on simulations. While 
this study serves a different domain, it highlights the deterministic error behavior on 
SRAM cells.
Duwe et al. [ 9] propose an error-pattern transformation scheme that re-arranges 
erratic bit cells that correspond to uncorrectable error patterns (e.g., beyond the cor -
rectable capacity) to correctable error patterns. The proposed method is low-latency 
and allows the supply voltage to be scaled further than it was previously possible. 
The adaptive rearranging is guided using the fault patterns detected by the self-test. 
The proposed methodology can reduce the power consumption up to 25.7%, based 
on simulated modeling that relies on literature SRAM failure probabilities.
There are several papers that explore methods to eliminate the effects of voltage 
noise. V oltage noise can significantly increase the pessimistic voltage margins of the 
microprocessor. Gupta et al. [ 10] and Reddi et al. [ 11] focus on the prediction of 
critical parts of benchmarks, in which large voltage noise glitches are likely to 
occur, leading to malfunctions. In the same context, several studies were presented 
to mitigate the effects of voltage noise [ 12–14] [15, 16] or to recover from them 
after their occurrence [ 17]. For example, in [ 18–20] the authors propose methods to 
maximize voltage droops in single-core and multicore chips in order to investigate 
their worst-case behavior due to the generated voltage noise effects.
Similarly, authors in [ 21, 22] proposed a novel methodology for generating di/dt 
viruses that is based on maximizing the CPU emitted electromagnetic (EM) emana -
tions. Particularly, they have shown that a genetic algorithm (GA) optimization 
search for instruction sequences that maximize EM emanations and generates a di/
dt virus that maximizes voltage noise. They have also successfully applied this 
approach on 3 different CPUs: two ARM-based mobile CPUs and one AMD 
Desktop CPU [ 23, 24].
Lefurgy et al. [ 25] propose the adaptive guardbanding in IBM Power 7 CPU. It 
relies on the critical path monitor (CPM) to detect the timing margin. It uses a fast 
CPM-DPLL (digital phase lock loop) control loop to avoid possible timing failures: 
when the detected margin is low, the fast loop quickly stretches the clock. To miti -
gate the possible frequency loss, adaptive guardbanding also uses a slow loop to 
boost the voltage when the averaged clock frequency is below the target. Leng et al. 
[26] study the voltage guardband on the real GPU and show the majority of GPU 
voltage margin protects against voltage noise. To fulfill the energy saving in the 
guardband, the authors propose to manage the GPU voltage margin at the kernel G. Papadimitriou and D. Gizopoulos17
granularity. They study the feasibility of using a kernel’s performance counters to 
predict the Vmin, which enables a simpler predictive guardbanding design for GPU- 
like co-processors.
Aggressive voltage underscaling has been recently applied in part to FPGAs, as 
well. Ahmed et al. [ 27] extend a previously proposed offline calibration-based DVS 
approach to enable DVS for FPGAs with BRAMs using a testing circuitry to ensure 
that all used BRAM cells operate safely while scaling the supply voltage. L. Shen 
et al. [ 28] propose a DVS technique for FPGAs with Fmax; however, voltage under -
scaling below the safe level is not thoroughly investigated. Ahmed et al. [ 29] evalu -
ate and compare the voltage behavior of different FPGA components such as LUTs 
and routing resources and design FPGA circuitry that is better suited for voltage 
scaling. Salamat et al. [ 30] evaluate at simulation level a couple of FPGA-based 
DNN accelerators with low-voltage operations.
As we can see, several microarchitectural techniques have been proposed that 
eliminate a subset of these guardbands for efficiency gains over and above what is 
dictated by the design conservative guardbands. However, all of these techniques 
are associated with significant design, test, and measurement overheads that limit its 
application in the general case. Another example is the Razor technique [ 31], sup -
port for timing-error detection and correction has to be explicitly designed into the 
processor microarchitecture which comes with significant verification overheads 
and circuit costs. Similarly, in adaptive-clocking approaches [ 32], extensive test and 
verification effort is required until the microprocessor is released to the market. 
Ensuring the eventual success of these techniques requires a deep understanding of 
dynamic margins and their manifestation during normal code execution.
2.2  Supply Voltage Scaling
Reducing supply voltage is one of the most efficient techniques to reduce the 
dynamic power consumption of the microprocessor, because dynamic power is qua -
dratic in voltage (as Eq.  3 shows). However, supply voltage scaling increases sub -
threshold leakage currents, increases leakage power, and also poses numerous 
circuit design challenges. Process variations and temperature parameters (dynamic 
variations), caused by different workload interactions, are also major factors that 
affect microprocessor’s energy efficiency. Furthermore, during microprocessor chip 
fabrication, process variations can affect transistor dimensions (length, width, oxide 
thickness, etc. [ 33]) which have direct impact on the threshold voltage of a MOS 
device [ 34].
As technology scales further down, the percentage of these variations compared 
to the overall transistor size increases and raises major concerns for designers, who 
aim to improve energy efficiency. This variation is classified as static variation and 
remains constant after fabrication. Both static and dynamic variations lead micro -
processor architects to apply conservative guardbands (operating voltage and fre -
quency settings), as shown in Fig.  1a to avoid timing failures and guarantee correct Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level18
operation, even in the worst-case conditions excited by unknown workloads, envi -
ronmental conditions, and aging [ 35, 36]. The guardband results in faster circuit 
operation under typical workloads than required at the target frequency, resulting in 
additional cycle time, as shown in Fig.  1b. In case of a timing emergency caused by 
voltage droops, the extra margin prevents timing violations and failures by tolerat -
ing circuit slowdown. While static guardbanding ensures robust execution, it tends 
to be severely overestimated as timing emergencies rarely occur, making it less 
energy-efficient [ 32]. These pessimistic guardbands impede power consumption 
and performance, and block the savings that can be derived by reducing the supply 
voltage (Fig.  1c) and increasing the operation frequency, respectively, when condi -
tions permit.
2.3  System-Level Characterization Challenges
To bridge the gap between energy efficiency and performance improvements, sev -
eral hardware and software techniques have been proposed, such as Dynamic 
V oltage and Frequency Scaling (DVFS) [ 37]. The premise of DVFS is that a micro -
processor’s workloads as well as the cores’ activity vary, so when one or more cores 
have less or no work to perform, the frequency, and thus, the voltage can be slowed 
down without affecting performance adversely. However, to further reduce the 
power consumption by keeping the frequency high when it is necessary, recent stud -
ies aim to uncover the conservative operational limits, by performing an extensive 
system-level voltage scaling characterization of commercial microprocessors’ oper -
ation beyond nominal conditions [ 38] [39] [40–42]. These studies leverage the 
Reliability, Accessibility, and Serviceability (RAS) features, provided by the hard -
ware (such as ECC), in order to expose reduced but safe operating margins.
A major challenge, however, in voltage scaling characterization at the system 
level is the time-consuming large population of experiments due to: (i) different 
voltage and frequency levels, (ii) different characterization setups (e.g., for a 
Cycle TimeTiming Margin
Nomina l 
VoltageGuardband
Actual 
Needed  
VoltageNominal 
Static 
Margin
Reduced 
Voltage 
Margin
(a) Guardban d( b) Static Margin (c) Reduced Voltage Margin
Fig. 1  V oltage guardband ensures reliability by inserting an extra timing margin. Reduced voltage 
margins improve total system efficiency without affecting the reliability of the microprocessorG. Papadimitriou and D. Gizopoulos19
multicore chip both the cases of running a benchmark in each individual core and 
simultaneously in all cores should be examined), and (iii) diverse-behavior work -
loads. In addition, due to the non-deterministic behavior of the experiments, caused 
by different microarchitectural events that occur in a system-level characterization 
and to ensure the statistical significance of the observations, the same experiments 
should be repeated multiple times at the same voltage level, which further increases 
the characterization time. Moreover, when the system operates in voltage levels that 
are significantly lower than its nominal value, system crashes are frequent and 
unavoidable and the recovery from these cases constitutes a significant portion of 
the overall experiment time.
To this end, there are numerous challenges that arise for a comprehensive voltage 
scaling characterization at the system level. Below, we discuss several challenges 
that must be taken into consideration.
Safe Data Collection  During the characterization, given that a system operating 
beyond nominal conditions often has unexpected behaviors (e.g., file system driver 
failures), there is the need to correctly identify and store all the essential informa -
tion in log files (to be subsequently parsed and analyzed). Characterization process 
should be performed in such a way to collect and store safely all the necessary 
information about the experiments in order to be able to provide correct results.
Failure Recognition  Another challenge is to recognize and distinguish the system 
and program crashes or hangs. During underscaled voltage conditions, the running 
application and/or the whole system can be crashed. Therefore, characterization 
process should take this into account in order to be able to easily identify and clas -
sify the final results in a correct way, with the most possible distinct information 
concerning the characterization.
Microprocessor Cores Isolation  Another major challenge is that the characteriza -
tion of a system is performed primarily by using properly chosen programs in order 
to provide diverse behaviors and expose all the potential deviations from nominal 
conditions. For characterization of each individual microprocessor core, it is impor -
tant to run the selected benchmarks in the desired cores by isolating the other avail -
able ones. This means that the core(s), where the benchmark runs, must be isolated 
and unaffected from the other active processes of the kernel in order to capture only 
the effects of the desired benchmark.
Iterative Execution  Since the characterization process is performed on real micro -
processor chips, it is guaranteed that the microprocessor’s behavior during under -
scaled voltage conditions will be non-deterministic. The non-deterministic behavior 
of the characterization results due to several microarchitectural features makes it 
necessary to repeat the same experiments multiple times with the same configura -
tions to increase the statistical significance of the results.
For all these reasons, manually controlled voltage scaling characterization is 
infeasible; a generic and automated experimental framework that can be easily rep -
licated in different machines is required. Furthermore, such a framework has to Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level20
ensure the credibility of the delivered results because when a system operates 
beyond nominal conditions it can fall into unstable states. In the next section, we 
describe a fully automated characterization framework [ 43, 44], which can over -
come the above challenges and result in correct and reliable findings, which may be 
used as a basis for any further energy-efficient technique.
3  Automated Characterization Framework
The primary goals of the described framework are: (1) to identify the target system’s 
limits when it operates at underscaled voltage and frequency conditions, and (2) to 
record/log the effects of a program’s execution under these conditions. The frame -
work should provide at least the following features:
• Comparing the outcome of the program with the correct output of the program 
when the system operates in nominal conditions to record Silent Data 
Corruptions (SDCs).
• Monitoring the exposed corrected and uncorrected errors from the hardware plat -
form’s error reporting mechanisms.
• Recognizing when the system is unresponsive to restore it automatically.
• Monitoring system failures (crash reports, kernel hangs, etc.).
• Determining the safe, unsafe, and non-operating voltage regions for each appli -
cation for all available clock frequencies.
• Performing massive repeated executions of the same configuration.
The automated framework (outlined in Fig.  2) is easily configurable by the user, 
can be embedded to any Linux-based system, with similar voltage and frequency 
regulation capabilities, and can be used for any voltage and frequency scaling char -
acterization study.
To completely automate the characterization process, and due to the frequent and 
unavoidable system crashes that occur when the system operates in reduced voltage 
levels, a Raspberry Pi board is connected externally to the system board, which 
behaves as a watchdog. The Raspberry is physically connected to both the Serial 
Port and the Power and Reset buttons of the system board to enable physical access 
to the system.
3.1  Initialization Phase
During the initialization phase, a user can define a list of benchmarks with any input 
dataset to run in any desirable characterization setup. The characterization setup 
includes the voltage and frequency (V/F) values under which the experiment will 
take place and the cores where the benchmark will be run; this can be an individual 
core, a pair of cores, or all of the available eight cores in the microprocessor. The G. Papadimitriou and D. Gizopoulos21
Results
Voltage / 
Frequency 
RegulationSerial
Network
Results 
ParsingExecution LoopReset Switch
Power Switch
Watchdog
Monitor
Raw Data
Final csv/json ResultsInitialization
Nominal 
VoltageBenchmarks ConfigurationInitialization Phase
Execution Phase
Parsing Phase
Fig. 2  Margins characterization framework layout
characterization setup depends on the power domains supported by the chip, but the 
framework is easily extensible to support the power domain features of different 
CPU chips.
This phase is in charge of setting the voltage and frequency ranges, the initial 
voltage and frequency values, with which the characterization begins, and to pre -
pare the benchmarks: their required files, inputs, outputs, as well as the directory 
tree where the necessary logs will be stored. This phase is performed at the begin -
ning of the characterization and each time the system is restored by the Raspberry 
or other external means (e.g., after a system crash) in order to proceed to the next 
run until the entire Execution Phase finishes. Each time the system is restored, this 
phase restores the initial user’s desired setup and recognizes where and when the 
characterization has been previously stopped. This step is essential for the charac -
terization to proceed sequentially according to the user’s choice, and to complete 
the whole Execution Phase.
This phase is also responsible to overcome the challenge of cores’ isolation 
which is important to ensure the correctness and integrity of the characterization 
results. The benchmark must run in an “as bare as possible” system without the 
interference of any other running process. Therefore, cores isolation setup is Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level22
twofold: first, it recognizes these cores or group of cores that are not currently under 
characterization, and migrates all currently running processes (except for the bench -
mark) to a completely different core. The migration of system processes is required 
to isolate the execution of the desired benchmark from all other active processes.
Second, given that more than one cores in the majority of current microproces -
sors are in the same power domain, they always have the same voltage value (in case 
this does not hold in a different microarchitecture the described framework can be 
adapted). This means that even though there are several processes run on different 
cores (not in the core(s) under characterization), they have the same probability to 
affect an unreliable operation while reducing the voltage. On the other hand, each 
individual core (or pair of cores) can have a different clock frequency, so we lever -
age the combination of V/F states in order to set the core under characterization to 
the desired frequency, and all other cores to the minimum available frequency in 
order to ensure that an unreliable operation is due to the benchmark’s execution 
only. When for example the characterization takes place in the cores 0 and 1, they 
set to the pre-defined by the user frequency (e.g., the maximum frequency), and all 
the other available cores are set to the minimum available frequency. Thus, all the 
running processes, except for the benchmark, are executed isolated.
3.2  Execution Phase
After the characterization setup is defined, the automated Execution Phase begins. 
The Execution Phase consists of multiple runs of the same benchmark, each one 
representing the execution of the benchmark with a pre-defined characterization 
setup. The set of all the characterization runs running the same benchmark with dif -
ferent characterization setups represents a campaign. After the initialization phase, 
the framework enters the Execution Phase, in which all runs take place. The runs are 
executed according to the user’s configuration, while the framework reduces the 
voltage with a step defined by the user in the initialization phase. For each run, the 
framework collects and stores the necessary logs at a safe place externally to the 
system under characterization, which will be then used by the parsing phase.
The logged information includes: the output of the benchmark at each execution, 
the corrected and uncorrected errors (if any) collected by the Linux EDAC Driver 
[45], as well as the errors’ localization (L1, L2, L3 cache, DRAM, etc.), and several 
failures, such as benchmark crash, kernel hangs, and system unresponsiveness. The 
framework can distinguish these types of failures and keep logging about them to be 
parsed later by the parsing phase. Benchmark crashes can be distinguished by moni -
toring the benchmark’s exit status. On the other hand, to identify the kernel hangs 
and system unresponsiveness, during this phase the framework notifies the Raspberry 
when the execution is about to start and also when the execution finishes.
In the meantime, the Raspberry starts pinging the system to check its responsive -
ness. If the Raspberry does not receive a completion notification (hang) in the given 
time (we defined as timeout condition 2 times the normal execution time of the G. Papadimitriou and D. Gizopoulos23
benchmark) or the system turns completely unresponsive (ping is not responding), 
the Raspberry sends a signal to the Power Off button on the board, and the system 
resets. After that, the Raspberry is also responsible to check when the system is up 
again, and sends a signal to restart the experiments. These decisions contribute to 
the Failure Recognition challenge.
During the experiments, some Linux tasks or the kernel may hang. To identify 
these cases, we use an inherent feature of the Linux kernel to periodically detect 
these tasks by enabling the flag “hung_task_panic” [ 45]. Therefore, if the kernel 
itself recognizes a process hang, it will immediately reset the system, so there is no 
need for the Raspberry to wait until the timeout. In this way, we also contribute to 
the Failure Recognition challenge and accelerate the reset procedure and the entire 
characterization.
Note that, in order to isolate the framework’s execution from the core(s) under 
characterization, the operations of the framework are also performed in isolation (as 
described previously). However, when there are operations of the framework, such 
as the organization of log files during the benchmark’s execution that is an integral 
part of the framework, and thus, they must run in the core(s) under characterization, 
these operations are performed after the benchmark’s execution in the nominal con -
ditions. This is the way to ensure that any logging information will be stored cor -
rectly and no information will be lost or changed due to the unstable system 
conditions, and thus, to overcome the Safe Data Collection challenge.
3.3  Parsing Phase
In the last step of the framework, all the log files that are stored during the Execution 
Phase are parsed in order to provide a fine-grained classification of the effects 
observed for each characterization run. Note that, each run is correlated to a specific 
benchmark and characterization setup. The categories that are used for our classifi -
cation are summarized in Table  1, but the parser can be easily extended according to 
the user’s needs. For instance, the parser can also report the exact location that the 
correctable errors occurred (e.g., the cache level, the memory, etc.) using the log -
ging information provided by the Execution Phase.
Note that each characterization run can manifest multiple effects. For instance, 
in a run both SDC and CE can be observed; thus, both of them should be reported 
by the parser for this run. Furthermore, the parser can report all the information col -
lected during multiple campaigns of the same benchmark. The characterization runs 
with the same configuration setup of different campaigns may also have different 
effects with different severity. For instance, let us assume two runs with the same 
characterization setup of two different campaigns. After the parsing, the first run 
finally revealed some CEs, and the second run was classified as SDC. At the end of 
the parsing step, all the collected results concerning the characterization (according 
to Table  1) are reported in .csv and .json files.Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level24
Table 1  Experimental effect categorization
Effect Description
ΝΟ
(Normal Operation)The benchmark was successfully completed without any 
indications of failure.
SDC
(Silent Data Corruption)The benchmark was successfully completed, but a mismatch 
between the program output and the correct output was observed.
CE
(Corrected Error)Errors were detected and corrected by the hardware.
UE
(Uncorrected Error)Errors were detected, but not corrected by the hardware.
AC
(Application Crash)The application process was not terminated normally (the exit 
value of the process was different than zero).
TO
(Application Timeout)The application process cannot finish and exceeds its normal 
execution time (e.g., infinite loop).
SC
(System Crash)The system was unresponsive; meaning that the X-Gene 2 is not 
responding to pings or the timeout limit was reached.
4  Fast System-Level Voltage Margins Characterization
Apart from the automated characterizing framework, which overcomes the previ -
ously described challenges, there is also one more important challenge when char -
acterizing the pessimistic voltage margins. The characterization procedure to 
identify these margins becomes more and more difficult and time-consuming in 
modern multicore microprocessor chips, as the systems become more complex and 
non-deterministic and the number of cores is rapidly increasing [ 46–54]. In a mul -
ticore CPU design, there are significant opportunities for energy savings, because 
the variability of the safe margins is large among the cores of a chip, among the 
different workloads that can be executed on different cores of the same chip and 
among the chips of the same type.
The accurate identification of these limits in a real multicore system requires 
massive execution of a large number of real workloads (as we have seen in the pre -
vious sections) in all the cores of the chip (and all different chips of a system), for 
different voltage and frequency values. The excessively long time that SPEC- based 
or similar characterization takes forces manufacturers to introduce the same pessi -
mistic guardband for all the cores of the same multicore chips. Clearly, if shorter 
benchmarks are able to reveal the Vmin of each core of a multicore chip (or the Vmin 
of different chips) faster than exhaustive campaigns, finer-grained exploitation of 
the operational limits of the chips and their cores can be effectively employed for 
energy-efficient execution of the workloads.
In this section, we introduce the development of dedicated programs (diagnostic 
micro-viruses), which are presented in [ 55]. Micro-viruses aim to stress the funda -
mental hardware components of a microprocessor and unveil the pessimistic volt -
age margins significantly faster rather than running extensive campaigns using 
long-time and diverse benchmarks.G. Papadimitriou and D. Gizopoulos25
With diagnostic micro-viruses, one can effectively stress (individually or simul -
taneously) all the main components of the microprocessor chip:
 (a) The caches (the L1 data and instruction caches, the unified L2 caches, and the 
last level L3 cache of the chips).
 (b) The two main functional components of the pipeline (the ALU and the FPU).
These diagnostic micro-viruses are executed in a very short time (~3 days for the 
entire massive characterization campaign for each individual core for each one 
microprocessor chip) compared to normal benchmarks such as those of the SPEC 
CPU2006 suite which need 2 months as Fig.  3a shows.
The micro-viruses’ purpose is to reveal the variation of the safe voltage margins 
across cores of the multicore chip and also to contribute to diagnosis by exposing 
and classifying the abnormal behavior of each CPU unit (silent data corruptions, 
bit-cell errors, and timing failures).
There have been many efforts toward writing power viruses and stress bench -
marks. For example, SYMPO [ 56], an automatic system-level max power virus gen -
eration framework, which maximizes the power consumption of the CPU and the 
memory system, MAMPO [ 57], as well as the MPrime [ 58] and stress-ng [ 59] are 
the most popular benchmarks, which aim to increase the power consumption of the 
microprocessor by torturing it; they have been used for testing the stability of the 
microprocessor during overclocking. However, power viruses are not capable to 
reveal pessimistic voltage margins.
Figure 3b shows that the power consumption of a workload is not correlated to 
the safe Vmin (and thus to voltage guardbands) of a core. As we can see, libquantum  
is the most power-hungry benchmark among the 12 SPEC CPU2006 benchmarks 
we used. However, libquantum’s  safe Vmin is significantly lower (20 mV) than the 
namd  benchmark, which has lower power consumption.
The purpose of the micro-viruses is to stress individually the fundamental micro -
processor units (caches, ALU, FPU) that define the voltage margins variability of 
the microprocessor. Micro-viruses do not aim to reveal the absolute Vmin (which can 
be identified by worst-case voltage noise stress programs). However, we provide 
37.6
20.7
1.5 1.9
010203040506070
1T 8TDays(a)
SPEC Micro-Viruses
870880890900910920
0481216
libquantum namd Viruses
Vmin(mV)Power(Watt)(b)
PowerV min
Fig. 3  (a) Time needed for a complete system-level characterization to reveal the pessimistic 
margins for one chip. Programs are executed on individual cores (1 T) and on all 8 cores concur -
rently (8 T). ( b) Safe Vmin values and their independence on power consumptionChallenges on Unveiling Voltage Margins from the Node to the Datacentre Level26
strong evidence (IPC and power measurements) that the micro-viruses stress the 
chips more intensively than the SPEC CPU2006 benchmarks.
4.1  System Architecture
For the study described in this chapter, we use Applied Micro’s (APM  – now 
Ampere Computing) X-Gene 2 microprocessor for all of our experiments and 
results. The X-Gene 2 microprocessor chip consists of eight 64-bit ARMv8 cores. It 
also includes the Power Management processor (PMpro) and Scalable Lightweight 
Intelligent Management processor (SLIMpro) to enable breakthrough flexibility in 
power management, resiliency, and end-to-end security for a wide range of applica -
tions. The PMpro, a 32-bit dedicated processor, provides advanced power manage -
ment capabilities such as multiple power planes and clock gating, thermal protection 
circuits, Advanced Configuration Power Interface (ACPI) power management 
states, and external power throttling support. The SLIMpro, 32-bit dedicated pro -
cessor, monitors system sensors, configures system attributes (e.g., regulate supply 
voltage, change DRAM refresh rate, etc.), and accesses all error reporting infra -
structure, using an integrated I2C controller as the instrumentation interface between 
the X-Gene 2 cores and this dedicated processor. SLIMpro can be accessed by the 
system’s running Linux Kernel.
X-Gene 2 has three independently regulated power domains (as shown in Fig.  4):
PMD (Processor Module) – Red Hashed Line  Each PMD contains two ARMv8 
cores. Each of the two cores has separate instruction and data caches, while they 
share a unified L2 cache. The operating voltage of all four PMDs together can 
change with a granularity of 5 mV beginning from 980 mV . While PMDs operate at 
the same voltage, each PMD can operate in a different frequency. The frequency can 
range from 300 MHz up to 2.4GHz at 300 MHz steps.
PCP (Processor Complex)/SoC – Green Hashed Line  It contains the L3 cache, 
the DRAM controllers, the central switch, and the I/O bridge. The PMDs do not 
belong to the PCP/SoC power domain. The voltage of the PCP/SoC domain can be 
independently scaled downwards with a granularity of 5 mV beginning from 950 mV .
Standby Power Domain – Golden Hashed Line  This includes the SLIMpro and 
PMpro microcontrollers and interfaces for I2C buses.
Table 2 summarizes the most important architectural and microarchitectural 
parameters of the APM X-Gene 2 micro-server that is used in our study.G. Papadimitriou and D. Gizopoulos27
PMD
L1I
ARMv8 
Core
L2 Cache 
256K BL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 
256K BL1DL1I
ARMv8 
Core
L1D
L1I
ARMv8 
Core
L2 Cache 
256K BL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 
256K BL1DL1I
ARMv8 
Core
L1D
Shared 8MB L3 Cach eDDR3 DDR3 DDR3 DDR38 x ARMv8 Cores @ 2.4GHz4 x DDR3 @ 1866M HzPCP
Central Switch (CSW)
PMpro SLIMproStan dby 
Power 
Domain
Fig. 4  X-Gene 2 micro-server power domains block diagram. The outlines with dashed lines pres -
ent the independent power domains of the chip
Table 2  Basic characteristics of X-Gene 2
Parameter Configuration
ISA ARMv8 (AArch64, AArch32, Thumb)
Pipeline 64-bit OoO (4-issue)
CPU 8 Cores, 2.4GHz
L1 Instruction Cache 32 KB per core (Parity Protected)
L1 Data Cache 32 KB per core (Parity Protected)
L2 Cache 256 KB per PMD (SECDED Protected)
L3 Cache 8 MB (SECDED Protected)
4.2  Micro-viruses Description
For the construction of the diagnostic micro-viruses we followed two different prin -
ciples for the tests that target the caches and the pipeline, respectively. All micro- 
viruses are small self-checking  pieces of code. This means that the micro-viruses Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level28
Table 3  The X-Gene 2 cache specifications
L1I L1D L2 L3
Size 32 KB 32 KB 256 KB 8 MB
# of Ways 8 8 32 32
Block Size 64 B 64 B 64 B 64 B
# of Blocks 512 512 4096 131,072
# of Sets 64 64 128 4096
Write Policy – Write-through Write-Back –
Write Miss 
PolicyNo-write allocate No-write allocate Write allocate –
Organization Physically 
Indexed 
Physically Tagged 
(PIPT)Physically 
Indexed 
Physically Tagged 
(PIPT)Physically 
Indexed 
Physically Tagged 
(PIPT)Physically 
Indexed 
Physically Tagged 
(PIPT)
Prefetcher YES YES YES NO
Scope Per Core Per Core Per PMD Shared
Protection Parity Protected Parity Protected ECC Protected ECC Protected
check if a read value is the expected one or not. There are previous studies (e.g., [ 60, 
61]) for the construction of such tests, but they focus only on error detection (mainly 
for permanent errors), and to our knowledge this is the first study that is performed 
on actual microprocessor chips; not in simulators or RTL level, which have no inter -
ference with the operating system and the corresponding challenges.
In this section, we first present the details of the caches of the X-Gene 2  in 
Table  3 (the rest of important X-Gene 2’s specifications were discussed previously) 
and then a brief overview of the challenges for the development of such system- 
level micro-viruses in a real hardware and the decisions we made in order to develop 
accurate self-checking tests for the caches and the pipeline.
Caches  For all levels of caches the first goal of the developed micro-viruses is to 
flip all the bits of each cache block from zero to one and vice versa. When the cache 
array is completely filled with the desired data, the micro-virus reads iteratively all 
the cache blocks while the chip operates in reduced voltage conditions and identifies 
any corruptions of the written values, which cannot be detected by dedicated hard -
ware mechanisms of the cache, such as the parity protection that can detect only odd 
number of flips.
All caches in X-Gene 2 have pseudo-LRU replacement policy. All our micro- 
viruses focusing on any cache level need to “warm-up” the cache before the test 
begins, by iteratively accessing the desired data in order to ensure that all the ways 
of the cache are completely filled and accessed with the micro-viruses’ desired pat -
terns. We experimentally observed through the performance monitoring counters 
that the safe number of iterations that “warm-up” the cache with the desired data, 
before the checking phase begins, is log 2(number of ways)  to guarantee that the 
cache is filled only with the data of the diagnostic micro-virus.G. Papadimitriou and D. Gizopoulos29
In order to validate the operation of the entire cache array, it is important to per -
form write/read operations in all bit cells. For every cache level, we allocate a mem -
ory chunk equal to the targeted cache size. As the storing of data is performed in 
cache block granularity, we need to make sure that our data storage is block-aligned, 
otherwise we will encounter undesirable block replacements that will break the 
requirement for complete utilization of the cache array.
Assume for example that the first word of the physical frame will be placed at 
the middle of the cache block. This means that when the micro-virus fills the 
cache, practically, there will be half-block size words that will replace a desired 
previously fetched block in the cache. Thus, if the cache blocks are N, the number 
of blocks that will be written in the cache will be N+1 (which means that one 
cache block will get replaced), and thus, the self-checking property may be jeop -
ardized. To this end, for all cache-related micro-viruses we perform a check at the 
beginning of the test to guarantee that the allocated array is cache aligned (to be 
block aligned afterward).
Another factor that has to be considered in order to achieve full coverage of the 
cache array is the cache coloring  [62]. Unless the memory is a fully associative one 
(which is not the case of the ARMv8 microprocessors), every store operation is 
indexed at one cache block depending on its address. For physically indexed memo -
ries, the physical address of the datum or instruction is used. However, because the 
physical addresses are not known or accessible from the software layer, special 
precautions need to be taken in order to avoid unnecessary replacements. To address 
this issue, we exploit a technique that is used to improve cache performance, known 
as cache coloring  [62]. If the indexing range of the memory is larger than the virtual 
page, two addresses with the same offset on different virtual pages are likely to 
conflict on the same cache block (due to the 32 KB size of the L1 caches the bits that 
index the cache occur in page offset, and thus, there is no conflict; this is the case for 
L2 and L3 caches in our system). To avoid this situation, the indexing range is sepa -
rated in regions equal to the page size, known as colors . It is then enough to use an 
equal number of pages in each color to avoid conflicts. The easiest way to achieve 
this is to allocate contiguous physical address range, which is possible at the kernel 
level using the kmalloc () call. The contiguous physical range will guarantee that all 
the data will be placed and fully occupy the cache, without replacements or unoc -
cupied blocks.
Another challenge that the micro-viruses need to take into consideration is the 
interference of the branch predictors and the cache prefetchers. In our micro-viruses, 
the branch prediction mechanism (in particular the branch mispredictions that can 
flush the entire pipeline) may ruin the self-checking property of the micro-virus, by 
replacing or invalidating the necessary data or instruction patterns. Moreover, 
prefetching requests can modify the pre-defined access patterns of the micro-virus 
execution.
To eliminate these effects, the memory access patterns of the micro-viruses are 
modeled using the stride-based model for each of the static loads and stores of the Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level30
micro-virus. Each of the static loads and stores in the workload walk a bounded 
array of memory references with a constant stride, larger than the X-Gene 2’s 
prefetcher stride. In that way, the cache-related micro-viruses are executed without 
the interference  of the branch predictor or the prefetcher. We validate this by lever -
aging the performance counters that measure the prefetch requests for the L1 and L2 
caches and the mispredictions, and no micro-virus counts any event in the related 
counters.
Pipeline  For the pipeline, we developed dedicated benchmarks that stress: (i) the 
Floating-Point Unit (FPU), (ii) the integer Arithmetic Logical Units (ALUs), and 
(iii) the entire pipeline using a combination of loads, stores, branches, arithmetic, 
and floating-point unit operations. The goal is to trigger the critical paths that could 
possibly lead to an error during off-nominal operation voltage conditions.
Generally, for all micro-viruses, one primary aspect that we need to take into 
consideration is that due to the micro-viruses’ execution in the real hardware with 
the operating system, we need to isolate all the system’s tasks to a single core. 
Assume for example that we run the L1 data or instruction micro-virus in Core 0. 
Each core has its own L1 cache, so we isolate all the system processes and interrupts 
in the Core 7, and we assign the micro-virus to Core 0. To realize this, we use the 
sched_setaffinity()  call of the Linux kernel to set the process’ affinity (execution in 
particular cores). In such a way, we ensure that only the micro-virus is executed in 
the desired core each time. We follow the same concept for all micro-viruses, except 
for L3 cache, because L3 is shared among all cores, so a small noise from system 
processes is unavoidable.
We developed all diagnostic micro-viruses in C language (except for L1 
Instruction cache micro-virus, which is ISA-dependent and is developed with a mix 
of C and ARMv8 assembly instructions). Moreover, the micro-viruses (except for 
L1 instruction cache’s) check the microprocessor’s parameters (cache size, #ways, 
existence of prefetcher, page size, etc.) and adjust the micro-viruses code to the 
specific CPU. This way, the micro-viruses can be executed in any microarchitecture 
and can easily adapted to different ISAs.
4.2.1  L1 Data Cache Micro-virus
For the first level data cache of each core, we defined statically an array in memory 
with the same size as the L1 data cache. As the L1 data cache is no-write allocate , 
after the first write of the desired pattern in all the words of the structure we need to 
read them first, in order to bring all the blocks in the first level of data cache. 
Otherwise, the blocks would remain in the L2 cache and we would have only write 
misses in the L2 cache.
Moreover, due to the pseudo-LRU policy that is used in the L1 data cache, we 
read all the words of the cache: log 2(number of ways of L1D cache) = log 2(8) = 3 G. Papadimitriou and D. Gizopoulos31
(three consecutive times) before the test begins, in order to ensure that all the blocks 
with the desired patterns are allocated in the first level data cache. With these steps, 
we achieve 100% read hit in the L1 data cache during the execution of the L1D 
micro-virus in undervolted conditions. The L1 data micro-virus fills the L1 data 
cache with three different patterns, each of which corresponds to a different micro- 
virus test. These tests are the all-zeros, the all-ones, and the checkerboard pattern. 
To enable the self-checking property of the micro-virus (correctness of execution is 
determined by the micro-virus itself and not externally), at the end of the test we 
check if each fetched word is equal to the expected value (the one stored before the 
test begins).
4.2.2  L1 Instruction Cache Micro-virus
The concept behind the L1 instruction cache micro-virus is to flip all the bits of the 
instruction encoding in the cache block from zero to one and vice versa. In the 
ARMv8 ISA there is no single pair of instructions that can be employed to invert all 
32 bits of an instruction word in the cache, so to achieve this we had to employ 
multiple instructions. The instructions listed in Table  4 are able to flip all the bits in 
the instruction cache from 0 to 1 and vice versa according to the Instruction 
Encoding Section of the ARMv8 Manual [ 63].
Each cache block of the L1 instruction cache holds 16 instructions because each 
instruction is 32-bit in ARMv8 and the L1 Instruction cache block size is 64 bytes. 
The size of each way of the L1 instruction cache is 32 KB/8 = 4 KB, and thus, it is 
equal to the page size which is 4 KB. As a result, there should be no conflict misses 
when accessing a code segment (see cache coloring previously discussed) with size 
equal to the L1 instruction cache (the same argument holds also for the L1 
data cache).
The method that guarantees the self-checking property in the L1 Instruction 
cache micro-virus is the following: The L1 instruction cache array holds 8192 
instructions (64 sets x 8 ways x 16 instructions in each cache block = 8192). We use 
8177 instructions to hold the instructions of our diagnostic micro-virus, and the 
remaining 15 instructions (8177 + 15 = 8192) to compose the control logic of the 
self-checking property and the loop control.
More specifically, we execute iteratively 8177 instructions and at the end of this 
block of code, we expect the destination registers to hold a specific “ signature ” (the 
signature is the same for each iteration of the same group of instructions, but differ -
ent among different executed instructions). If this “signature” is distorted, then the 
micro-virus detects that an error occurred (for instance a bit flip in an immediate 
instruction resulted in the addition of a different value) and records the location of 
the faulty instruction as well as the expected and the faulty signature for further 
diagnosis. We iterate this code multiple times and after that we continue with the 
next block of code.Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level32
Table 4  ARMv8 instructions used in the L1I micro-virus. The right column presents the encoding 
of each instruction to demonstrate that all cache block bits get flipped
Instruction Encoding
add x28, x28, #0x1 1001 0001 0000 0000 0000 0111 1001 1100
sub x3, x3, #0xffe 1101 0001 0011 1111 1111 1000 0110 0011
madd x28, x28, x27, x27 1001 1011 0001 1011 0110 1111 1001 1100
add x28, x28, x27, asr #2 1000 1011 1001 1011 0000 1011 1001 1100
add w28, w28,w27,lsr #2 0000 1011 0101 1011 0000 1011 1001 1100
nop 1101 0101 0000 0011 0010 0000 0001 1111
bics x28, x28, x27 1110 1010 0011 1011 0000 0011 1001 1100
As in the L1 data cache micro-virus, due to the pseudo-LRU policy that is used 
also in the L1 Instruction cache, we fetch all the instructions.
 logl og .22 18 3 number of ways of LIcache () = ()= 
(three consecutive times) before the test begins, to ensure that all blocks with the 
desired instruction patterns are allocated in the L1 instruction cache. With these 
steps, we achieve 100% cache read hit (and thus cache stressing) during undervolt -
ing campaigns.
4.2.3  L2 Cache Micro-virus
The L2 cache is a 32-way associative PIPT cache with 128 sets; thus, the bits of 
the physical address that determine the block placement in the L2 cache are bits 
[12:6] (as shown in Fig.  5). Moreover, the page size we rely on is 4 KB and con -
sequently the page offset consists of the 12 least significant bits of the physical 
address. Accordingly, the most significant bit (bit 12) of the set index (the dotted 
square in Fig.  5) is not a part of the page offset. If this bit is equal to 1, then the 
block is placed in any set of the upper half of the cache, and in the same manner, 
if this bit is equal to 0, the block is placed in a set of the lower half of the cache. 
Bits [11:6] which are part of page/frame offset determine all the available sets for 
each individual half.
In order to guarantee the maximum block coverage (e.g., to completely fill the 
L2 cache array), and thus to fully stress the cache array, the L2 micro-virus should 
not depend on the MMU translations that may result in increased conflict misses. 
The way to achieve this is by allocating memory that is not only virtually contigu -
ous (as with the standard C memory allocation functions used in user space), but 
also physically contiguous by using the kmalloc () function. The kmalloc()  function G. Papadimitriou and D. Gizopoulos33
Tag Set(=index ) W B
38 1312 65
 21 0
Tag V Data Line 0 D765… 210
Cach eLine4 7 26
Tag V Data Line 1 D
Tag V Data Line 2 D
Tag V Data Line 126 D
Tag V Data Line 127 D
Fig. 5  A 256KB 32-way set associative L2 cache
operates similarly to that of user-space’s familiar memory allocation functions, with 
the main difference that the region of physical memory allocated by kmalloc () is 
physically contiguous . This guarantees that in one half of the allocated physical 
pages, the most significant bits of their set index are equal to one and the other half 
are equal to zero.1
Given that the replacement policy of the L2 cache is also pseudo-LRU, the L2 
micro-virus needs to iteratively access.
 logl og .22 32 5 number of ways of L2 cach e () = () = 
(five times) the allocated data array, to ensure that all the ways of each set contain 
the correct pattern. Furthermore, due to the fact that the L1 data cache has write- 
through policy and the L2 cache has write-allocate policy, the stored data will reside 
in the L2 cache right after the initial writes (no write backs).
Another requirement for the L2 micro-virus is that it should access the data only 
from the L2 cache during the test and not from the L1 data cache, to completely 
stress the former one. We meet this requirement using a stride access scheme for the 
array with a one-block (8 words) stride. Therefore, in the first iteration the L2 
1 The Linux kernel was built with the commonly used page size of 4 KB; if the page size is 64 KB 
in another CPU, the micro-virus uses standard C memory allocation functions in user space instead 
of kmalloc (), because the most significant bit of the set index would be part of the page offset like 
the rest of the set index bits.Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level34
micro- virus accesses the first word of each block, in the second iteration it accesses 
the second word of each block, and so on. Thus, it always misses the L1 data cache. 
By accessing the data using these strides, the L2 micro-virus also overcomes the 
prefetching requests. Note that the L1 instruction cache can completely hold all the 
L2 diagnostic micro-virus instructions, so the L2 cache holds only the data of 
our test.
To validate the above, we isolated all the system processes by forcing them to run 
on different cores from the one that executes the L2 diagnostic micro-virus, by set -
ting the system processes’ CPU affinity and interrupts to a different core, and we 
measured the L1 and L2 accesses and misses after we have already “trained” the 
pseudo-LRU with the initial accesses. We measure these micro-architectural events 
by leveraging the built-in performance counters of the CPU.
The performance counters show that the L2 diagnostic micro-virus always 
misses the L1 data cache and always hits the L1 Instruction cache, while it hits the 
L2 cache in the majority of the accesses. Specifically, the L2 cache has 4096 blocks 
and the maximum number of block-misses we observed was 32 at most for each 
execution of the test (meaning 99.2% coverage). In that way, we verify that the L2 
micro-virus completely fills the L2 cache.
The L2 micro-virus fills the L2 cache with three different patterns, each of which 
corresponds to a different micro-virus test. These tests are the all-zeros, the all-ones, 
and the checkerboard pattern. To enable the self-checking property into this micro- 
virus, at the end of the test we check if each fetched word is equal to the expected 
value (the one stored before the test begins).
4.2.4  L3 Cache Micro-virus
The L3 cache is a 32-way associative PIPT cache with 4096 sets and is organized in 
32 banks; so, each bank has 128 sets and 32 ways. Moreover, the bits of the physical 
address that determine the block placement in the L3 cache are the bits [12:6] (for 
choosing the set in a particular bank) and the bits [19:15] for choosing the correct 
bank. Based on the above, in order to fill the L3 cache, we allocate physically con -
tiguous memory with kmalloc ().
However, kmalloc () has an upper limit of 128 KB in older Linux kernels and 
4 MB in newer kernels (like the one we are using; we use CentOS 7.3 with Linux 
kernel 4.3). This upper limit is a function of the page size and the number of buddy 
system free lists (MAX_ORDER). The workaround to this constraint is to allocate 
two arrays with two calls to kmalloc () and each array’s size should be half the size 
of the 8 M L3 cache. The reason that this approach will result in full block coverage 
in the L3 cache is that 4 MB chunks of physically contiguous memory gives us 
contiguously the 22 least significant bits, while we need contiguously only the 20 
least significant (for the set index and the bank index). Moreover, we should high -
light that the L3 cache is as a non-inclusive victim cache.
In response to an L2 cache miss from one of the PMDs, agents forward data 
directly to the L2 cache of the requestor, bypassing the L3 cache. Afterward, if the G. Papadimitriou and D. Gizopoulos35
corresponding fill replaces a block in the L2 cache, a write-back request is issued, 
and the evicted block is allocated into the L3 cache. On a request that hits the L3 
cache, the L3 cache forwards the data and invalidates its copy, freeing up space for 
future evictions. Since data may be forwarded directly from any L2 cache, without 
passing through the L3 cache, the behavior of the L3 cache increases the effective 
caching capacity in the system.
Due to the pseudo-LRU policy, similar to the L2, the L3 micro-virus is designed 
accordingly to perform.
 logl og .22 32 5 number of ways of L2 cach e () = () = 
(five) sequential writes to cover all the ways before the test begins, and the read 
operations afterward are performed by stride of one block (to bypass the L2 cache 
and the prefetcher, so the micro-virus only hits the L3 cache and always misses the 
L1 and L2 caches).
The L3 diagnostic micro-virus fills the L3 cache with three different patterns, 
each of which corresponds to a different micro-virus test. These tests are again the 
all-zeros, the all-ones, and the checkerboard pattern. To enable the self-checking 
property, at the end of the test we check if each fetched word is equal to the expected 
value (the one stored before the test begins).
However, in contrast to the L2 diagnostic micro-virus, in the L3 micro-virus 
there is no way to prove the complete coverage of the L3 cache in the system-level 
because that there are no built-in performance counters in X-Gene 2 that report the 
L3 accesses and misses. However, by using the events that correspond to the L1 and 
L2 accesses, misses and write backs, we check that all the requests from the L3 
micro-virus miss the L1 and L2 caches, and thus only hit the L3 cache. Finally, we 
should highlight that the shared nature of the L3 cache forced us to try to minimize 
the number of the running daemons in the system in order to reduce the noise in the 
L3 cache from their access to it.
4.2.5  Arithmetic and Logic Unit (ALU) Micro-virus
X-Gene 2 features a 4-wide out-of-order superscalar microarchitecture. It has one 
integer scheduler and two different integer pipelines:
• a Simple Integer pipeline, and,
• a Simple+Complex Integer pipeline.
The integer scheduler can issue two integer operations per cycle; each of the 
other schedulers can issue one operation per cycle (the integer scheduler can issue 
2 simple integer operations per cycles; for instance, 2 additions, or 1 simple and 1 
complex integer operation; for instance, 1 multiplication and 1 addition).
The execution units are fully pipelined for all operations, including multiplica -
tions and multiply-add instructions. ALU operations are single-cycle. The fetch 
stage can bring up to 16 instructions (same size as a cache block) per cycle from the Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level36
same cache block or by two adjacent cache blocks. If the fetch begins in the middle 
of a cache block (unaligned), the next cache block will also be fetched in order to 
have 16 instructions available for further processing, and thus there will be a block 
replacement on the Instruction Buffer.
To this end, we use NOP instructions to ensure that the first instruction of the 
execution block is block aligned, so that the whole cache block is located to the 
instruction buffer each time. For the above microarchitecture, we developed the 
ALU self-testing micro-virus, which avoids data and control hazards and iterates 
1000 times over a block of 16 instructions (that resides in the Instruction buffer, and 
thus the L1 instruction and data cache are not involved in the stress testing process). 
After completing 1000 iterations, it checks the value of the registers involved in the 
calculations by comparing them with the expected values.
After re-initializing the values of the registers, we repeat the same test 70 M 
times, which is approximately 60 seconds of total execution (of course, the number 
of executions and the overall time can be adjusted). Therefore, we execute code that 
resides in the instruction buffer for 1000 iterations of our loop and then we execute 
code that resides in 1 block of the cache after the end of these 1000 iterations. As the 
instructions are issued and categorized in groups of 4 (X-Gene 2 issues 4 instruc -
tions) and the integer scheduler can issue 2 of them per cycle, we can’t achieve the 
theoretical optimal IPC of 4 Instructions per Cycle only with Integer Operations. 
Furthermore, we try to have in each group of 4 instructions, instructions that stress 
all the units of all the issue queues like the adder, the shifter, and multiplier. 
Specifically, the ALU micro-virus consists of 94% integer operations and 6% 
branches.
4.2.6  Floating-Point Unit (FPU) Micro-virus
Aiming to heavily stress and diagnose the FPU, we perform a mix of diverse 
floating- point operations by avoiding data hazards (thus stalls) among the instruc -
tions and using different inputs to test as many bits and combinations as possible. To 
implement the self-checking property of the micro-virus, we execute the floating- 
point operations twice, with the same input registers and different result registers. If 
the destination registers of these two same operations have different results, our 
self-test notifies that an error occurred during the computations.
For every iteration, the values of the registers (for all of the FPU operations) are 
increased by a non-fixed stride that is based on the calculations that take place. The 
values in the registers of each loop are distinct between them and between every 
loop. Moreover, we ensure that the first instruction of the execution block is cache 
aligned (as in the ALU micro-virus), so the whole cache block is located to the 
instruction buffer each time.G. Papadimitriou and D. Gizopoulos37
4.2.7  Pipeline Micro-virus
Apart from the dedicated benchmarks that stress independently the ALU and the 
FPU, we have also constructed a micro-virus to stress simultaneously all the issue 
queues of the pipeline. Between two consecutive “heavy” (high activity) floating- 
point instructions of the FPU test (like the consecutive multiply add , or the fsqrt 
which follows the fdiv) we add a small iteration over 24 array elements of an integer 
array and a floating-point array.
To this end, during these iterations, the “costly” instructions such as multiply add  
have more than enough cycles to calculate their result, while at the same time we 
perform load, store, integer multiplication, exclusive or, subtractions and branches. 
All instructions and data of this micro-virus are located in L1 cache in order to fetch 
them at the same cycle to avoid high cache access latency. As a result, the “pipeline” 
micro-virus has a large variety of instructions which stress in parallel all integer and 
FP units. This micro-virus consists of 65% integer operations and 23.1% floating 
point operations, while the rest 11.9% are branches.
4.3  Experimental Evaluation
In the previous section, we described the challenges and our solutions to the com -
plex development process of the micro-viruses and how we verified their coverage 
using the machine performance monitoring counters. However, it is essential to 
validate the stress and utilization of the micro-viruses on the microprocessor. To this 
end, we measure the IPC and power consumption for both micro-viruses and SPEC 
CPU2006 benchmarks. Note that the micro-viruses were neither developed to pro -
vide power measurements nor performance measurements.
We present the IPC and power consumption measurements of the micro-viruses 
only to verify that they sufficiently stress the targeted units. IPC and power con -
sumption along with the data footprints of the micro-viruses (complete coverage of 
the caches bit arrays; see the previous section) are highly accurate indicators of the 
activity and utilization of a workload on a microprocessor. Figure  6 presents the 
IPC, and Figs.  7 and 8 present the power consumption measurements for both the 
micro-viruses and the SPEC CPU2006 benchmarks.
As shown in Fig.  6, the micro-viruses for fast voltage margins variability identi -
fication provide very high IPC compared to most SPEC benchmarks on the target 
X-Gene 2 CPU. In addition, we assessed the power consumption using the dedi -
cated power sensors of the X-Gene 2 microprocessor (located in the standby power 
domain), to take accurate results for each workload. We performed measurements 
for two different voltage values: at the nominal voltage (980 mV) and at 920 mV , 
which is a voltage step that all of the micro-viruses and benchmarks can be reliably 
executed (without Silent Data Corruptions (SDCs), detected/corrected errors, or 
crashes). Figures  7 and 8 show that the maximum and average power consumptions 
of the micro-viruses are comparable to the SPEC CPU2006. In the same figure, we 
can also see the differences concerning the energy efficiency when operating below Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level38
2.14
1.95
1.67
1.08
.86
.61
.121.20
00.511.522.5
L1D
ALUL2L3
PipelineL1I
FPUALLIPCMicro- Viruses
1.65
1.27
.98.98.95.89.87.87.83
.50
.34.30
00.511.522.5
hmmer
libquantumbzip
sjeng
dealII
gobmk
namd
povraygcc
astar
soplex
omnetppIPCSPEC CPU200 6
Fig. 6  IPC measurements for both micro-viruses (top) and SPEC CPU2006 benchmarks (bottom)
13.812.8 12.2 12.1 12.2 12.0 12.0 11.911.811.711.711.711.511.511.5
0.05.010.015.020.025.030.035.0
Virus(max)
libquantumbzip
povray
Virus(avg)
soplex
hmmer
gobmkgcc
dealII
sjeng
astar
namd
omnetpp
Virus(min)Power (Watts)Single Core(980 mV)
12.911.411.410.5 10.5 10.4 10.4 10.4 10.4 10.2 10.0 10.1 10.0 9.99.7
0.05.010.015.020.025.030.035.0
Virus(max)
libquantum
Virus(avg)
gcc
dealII
bzip
gobmk
povray
hmmer
sjeng
Virus(min)
soplex
namd
astar
omnetppPower (Watts)SingleCore(920 mV)
Fig. 7  Power consumption measurements for both the micro-viruses and the SPEC CPU2006 
benchmarks. The upper graph shows the power consumption at nominal voltage (980 mV). The 
lower graph shows the power measurements when the microprocessor operates at 920mVG. Papadimitriou and D. Gizopoulos39
27.0
23.5 23.3
18.9
15.314.9 14.6 14.5 14.5 14.1 14.0 13.9 13.7 13.5 13.3
0.05.010.015.020.025.030.035.0
Virus(max)
Virus(avg)
bzip
Virus(min)
libquantum
hmmer
astar
povray
omnetpp
dealII
gobmk
soplex
sjeng
namdgccPowe r (Watts)8Cores (920 mV)31.430.4 30.429.0 28.6 28.4 28.427.2 26.8 26.025.924.9 24.822.820.6
0.05.010.015.020.025.030.035.0
hmmer
Virus(max)
povray
gobmk
sjeng
dealII
libquantum
namd
astar
bzip
Virus(avg)
soplexgcc
omnetpp
Virus(min)Powe r (Watts)8Cores (980 mV)
Fig. 8  Power consumption measurements for both the micro-viruses and the SPEC CPU2006 
benchmarks. The upper graph shows the power consumption at nominal voltage (980 mV). The 
lower graph shows the power measurements when the microprocessor operates at 920mV
nominal voltage conditions, which emphasizes the need to identify the pessimistic 
voltage margins of a microprocessor. As we can see, in the multi-core execution we 
can achieve 12.6% energy savings (considering that the maximum TDP of X-Gene 
2 is 35 W), by reducing the voltage 6.2% below nominal, where all of the three 
chips operate reliably.
4.4  Experimental Evaluation
For the evaluation of the micro-viruses’ ability to reveal the Vmin of X-Gene 2 CPU 
chips and their cores, we used three different chips: TTT, TFF, and TSS from 
Applied Micro’s X-Gene 2 micro-server family. The TTT part is the nominal Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level40
(typical) part. The TFF is the fast-corner part, which has high leakage but at the 
same time can operate at a higher frequency (fast chip). The TSS part is also a cor -
ner part which has low leakage and works at a lower frequency. The faster parts 
(TFF) are rated for higher frequency and usually sold for more money, while slower 
parts (TSS) are rated for lower frequency. In any event, the parts must still work in 
the slowest environment, and thus, all chips (TTT, TSS, TFF) operate reliably with 
nominal frequency at 2.4GHz.
Using the I2C controller we decrease the voltage of the domains of the PMDs 
and the SoC at 5 mV steps, until the lowest voltage point (safe Vmin) before the 
occurrence of any error (corrected and uncorrected–reported by the hardware ECC), 
SDC (Silent Data Corruption–output mismatch) or Crash. To account for the non- 
deterministic behavior of a real machine (all of our experiments were performed on 
the actual X-Gene 2 chip), we repeat each experiment 10 times and we select the 
execution with the highest safe V min (the worst-case scenario) to compare with the 
micro-viruses.
We experimentally obtained also the safe Vmin values of the 12 SPEC CPU2006 
benchmarks on three X-Gene 2 chips (TTT, TFF, TSS), running the entire time- 
consuming undervolting experiment 10 times for each benchmark. These experi -
ments were performed during a period of 2 months on a single X-Gene 2 machine, 
that is 6 months for all 3 chips. We also ran our diagnostic micro-viruses, with the 
same setup for the 3 different chips, as for the SPEC CPU2006 benchmarks. This 
part of our study focuses on:
 1. The quantitative analysis of the safe Vmin for three significantly different chips of 
the same architecture to expose the potential guard-bands of each chip.
 2. The demonstration of the value of our diagnostic micro-viruses which can stress 
the individual components, and reveal virtually the same voltage guard-bands 
compared to benchmarks.
The voltage guardband for each program (benchmark or micro-virus) is defined 
as the safest voltage margin between the nominal voltage of the microprocessor and 
its safe Vmin (where no ECC errors or any other abnormal behavior occur).
4.4.1  SPEC Benchmarks vs. Micro-viruses
As we discussed earlier, to expose these voltage margins variability among cores in 
the same chip and among the three different chips by using the 12 SPEC CPU2006 
benchmarks, we needed ~2 months for each chip. On the contrary, the same experi -
mentation by using the micro-viruses needs ~3 days, which can expose the corre -
sponding safe Vmin for each core. In Figs.  9, 10, and 11 we notice that the micro-viruses 
provide the same or higher Vmin than the benchmarks for 19 of the 24 cores (3 chips 
x 8 cores). There are a few cases that benchmarks have higher Vmin in 5 cores (the 
difference between them is at most 5  mV  – 0.5%) but in orders of magnitude 
shorter time.G. Papadimitriou and D. Gizopoulos41
860870880890900910920
Virus
bzip
namd
gobmk
dealII
povray
hmmer
omnetpp
sjeng
astar
gcc
soplex
libqua ntum
Virus
astar
namd
dealII
povray
hmmer
sjeng
bzip
gobmk
omnetpp
gcc
soplex
libqua ntum
Virus
namd
bzip
dealII
hmmer
gcc
gobmk
soplex
povray
sjeng
omnetpp
astar
libqua ntum
Virus
dealII
povray
sjeng
namd
bzip
gcc
gobmk
hmmer
omnetpp
soplex
astar
libqua ntum
Core0C ore1 Core2C ore3Vmin(mV)
850860870880890900910920
Virus
bzip
namd
dealII
hmmer
omnetpp
povray
gobmk
gcc
soplex
sjeng
astar
libquantum
Virus
bzip
namd
omnetpp
dealII
povray
hmmer
gobmk
gcc
soplex
sjeng
astar
libquantum
Virus
namd
dealII
bzip
gcc
gobmk
povray
hmmer
libquantum
omnetpp
astar
sjeng
soplex
Virus
bzip
dealII
gcc
namd
povray
libqua ntum
omnetpp
gobmk
soplex
hmmer
sjeng
astar
Core4C ore5 Core6C ore7Vmin (mV)
Vmin Average
Fig. 9  Detailed comparison of V min between the 12 SPEC CPU2006 benchmarks and micro- 
viruses for the TSS chip
Such differences (5 mV or even higher) can occur even among consecutive runs 
of the same program, in the same voltage due to the non-deterministic behavior of 
the actual hardware chip. This is why we run the benchmarks 10 times and present 
only the maximum safest Vmin. For a significant number of programs (benchmarks 
and micro-viruses), we can see variations among different cores and different chips. 
Figure  9 presents the detailed comparison of the safe Vmin between the 12 SPEC 
CPU2006 benchmarks and the micro-viruses for the TSS chip, while Figs.  10 and 
11 represent the maximum safe Vmin for each core and chip among all the bench -
marks (blue line) and all micro-viruses (orange line). Considering that the nominal 
voltage in the PMD voltage domain (where these experiments are executed) is 
980 mV , we can observe that the Vmin values of the micro-viruses are very close to 
the corresponding safe Vmin provided by benchmarks, but in most cases higher.
The core-to-core and chip-to-chip relative variation among the three chips are 
also revealed with the micro-viruses. Both the SPEC CPU2006 benchmarks and the 
micro-viruses provide similar observations for core-to-core and chip-to-chip varia -
tion. For instance, in TTT and TFF chip, cores 4 and 5 are the most robust cores. 
This property holds in the majority of programs but can be revealed by the micro- 
viruses in several orders of magnitude shorter characterization time.
At the bottom-right diagram of Fig.  11, we show the undervolting campaign in 
the SoC voltage domain (which is the focus of the L3 cache micro-virus). As shown 
in Sect. 3.1, in X-Gene 2 there are 2 different voltage domains: the PMD and the Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level42
900905 905
905 885 885
895890900910
900895
820840860880900920940960980
01234567PMD Voltage (mV)SPEC vs.Micro-Viruses (TTT )
8908 90895
8958858 85900900
890900
8808 80905
900
820840860880900920940960980
01234567PMD Voltage (mV)SPEC vs.Micro-Viruses (TFF )
SPEC Micro-Viruses
Fig. 10  Maximum Vmin among 12 SPEC CPU2006 benchmarks and the micro-viruses for TTT 
and TFF in the PMD domain
SoC. The SoC voltage domain includes the L3 cache. Therefore, this graph presents 
the comparison of the L3 diagnostic micro-virus with the 12 SPEC CPU 2006 
benchmarks that were executed simultaneously in all 8 cores (8 copies of the same 
benchmark) by reducing the voltage only in the SoC voltage domain. In this figure, 
we also notice that in TTT/TFF the difference of Vmin between the benchmark with 
the maximum Vmin and the self-test is only 5 mV , while in TSS the micro-viruses 
reveal the Vmin at 20 mV higher than the benchmarks. Note that the nominal voltage 
for the SoC domain is 950 mV (while in the PMD domain it is 980 mV).G. Papadimitriou and D. Gizopoulos43
900910910 910
900895910
9109159 159109 10915
820840860880900920940960980
01234567PMDVoltage (mV)SPEC vs.Micro-Viruses (TSS )
8858 85
890
8808 80910
830850870890910930950
TTTT FF TSSSoC Voltage (mV)SPEC CPU2 006vs.L3Micro-Virus
SPEC Micro-Virus
Fig. 11  Maximum V min among 12 SPEC CPU2006 benchmarks and the micro-viruses for TSS in 
PMD domain (top graph). The bottom graph shows the maximum V min of 12 SPEC CPU2006 
benchmarks and the L3 micro-virus in the SoC domain
4.5  Observations
By using the micro-viruses, we can detect very accurately (divergences have a short 
range, at most 5 mV) the safe voltage margins for each chip and core, instead of 
running time-consuming benchmarks. According to our experimental study, the 
micro-viruses reveal higher Vmin (meaning lower voltage margin) in the majority of 
cores in the three chips we used. Specifically, in 19 out of 24 cores in total, the 
micro-viruses expose higher or the same safe Vmin compared to the SPEC CPU2006 
benchmarks. For the specific ARMv8 design, we point and discuss the core-to-core Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level44
and chip-to-chip variation, which are important to reduce the power consumption of 
the microprocessor.
Core-to-Core Variation  There are significant divergences among the cores due to 
process variation. Process variation can affect transistor dimensions (length, width, 
oxide thickness, etc.) which have a direct impact on the threshold voltage of a MOS 
device, and thus, on the guardband of each core. We demonstrate that although 
micro-viruses can reveal similar divergences as the benchmarks among the different 
cores and chips, however, in most of the cases, micro-viruses expose lower diver -
gences among cores in contrast to time-consuming SPEC CPU2006 benchmarks. 
As shown in Figs.  10 and 11, the micro-viruses reveal higher safe Vmin for all the 
cores than the benchmarks, and also, we notice that the workload-to-workload dif -
ferences are up to 30  mV .  Therefore, due to the diversity of code execution of 
benchmarks, it is difficult to choose one benchmark that provides the highest Vmin. 
Different benchmarks provide significantly different Vmin at different cores in 
 different chips. Therefore, a large number of different benchmarks are required to 
reach a safe result concerning the voltage margins variability identification. Using 
our micro-viruses, which fully stress the fundamental units of the microprocessor, 
the cores guardbands can be safely determined (regarding the safe Vmin) at a very 
short time, and guide energy efficiency when running typical applications.
Chip-to-Chip Variation  As Figs.  10 and 11 present for the TTT and TFF chips, 
PMD 2 (cores 4 and 5) is the most robust PMD for all three chips (it can tolerate up 
to 3.6% more undervolting compared to the most sensitive cores). We can notice 
that (on average among all cores of the same chip) the TFF chip has lower Vmin 
points than the TTT chip, in contrast to the TSS chip, which has higher Vmin points 
than the other two chips, and thus, can deliver smaller power savings.
Diagnosis  By using the diagnostic micro-viruses we can also determine if and 
where an error or a silent data corruption (SDC) occurred. Through this component- 
focused stress process we have observed the following:
 (a) SDCs occur when the pipeline gets stressed (ALU, FPU, and Pipeline tests).
 (b) The cache bit-cells operate safely at higher voltages (the caches tests crash 
lower than the ALU and FPU tests).
Both observations show that the X-Gene 2 is more susceptible to timing-path 
failures than to SRAM array failures. A major finding of our analysis using the 
micro-viruses for ARMv8-compliant multicore CPUs is that SDCs (derived from 
pipeline stressing using the ALU, FPU, and Pipeline micro-viruses) appear at higher 
voltage levels than corrected errors when cache arrays get stressed by cache-related 
micro-viruses. We believe that the reason is that unlike other server-based CPUs 
(like Itanium), X-Gene 2 does not deploy circuit-level techniques (Itanium performs 
continuous clock-path de-skewing during dynamic operation) [ 64], and thereby, 
when the pipeline gets stressed, X-Gene 2 produces SDCs due to timing-path 
failures.G. Papadimitriou and D. Gizopoulos45
5  Conclusions
In this chapter, we presented a versatile framework for system-level voltage and 
frequency scaling characterization built on top of the ARMv8-compliant APM’s 
X-Gene 2 micro-server family. The framework is fully automated and reports infor -
mation supported by the hardware itself, such as cache ECC errors, as well as SDCs, 
system or process crashes, and hangs. We present the challenges for the develop -
ment of such a framework, and describe how we overcame these challenges, by 
using and combining several hardware, software, and system engineering techniques.
We also discussed about the employment of fast diagnostic micro-viruses that 
aim to stress individually the fundamental hardware components of a multicore 
CPU architecture. We described the complex development process of the diagnostic 
micro-viruses which target the three different cache memory levels and the main 
processing components, the integer, and the floating-point units as well as the pipe -
line queues. The combined execution of the micro-viruses takes a very short time to 
reveal their voltage limits when they operate below the nominal levels. We demon -
strated the effectiveness of the synthetic micro-viruses-based programs by compar -
ing the V min values they report to the corresponding V min values that an excessively 
long characterization campaign with SPEC CPU2006 benchmarks reports. The 
micro-viruses-based characterization flow requires orders of magnitude shorter 
time but delivers the same V min values for the different CPU chips and cores within 
a chip. We demonstrate it by evaluating the diagnostic micro-viruses-based charac -
terization flow (and compare it to the SPEC-based flow) on three different chips of 
AppliedMicro’s X-Gene 2.
References
 1. V . Venkatachalam, M. Franz, Power reduction techniques for microprocessor systems. ACM 
Comput. Surv. 37(3), 195–237 (2005)
 2. G.  Papadimitriou, A.  Chatzidimitriou, D.  Gizopoulos, V .J.  Reddi, J.  Leng, B.  Salami, 
O.S. Unsal, A.C. Kestelman, Exceeding conservative limits: a consolidated analysis on mod -
ern hardware margins. IEEE Trans. Device Mater. Reliab. 20(2), 341–250 (April 2020)
 3. D.  Gizopoulos, G.  Papadimitriou, A.  Chatzidimitriou, V .J.  Reddi, J.  Leng, B.  Salami, 
O.S.  Unsal, A.C.  Kestelman, Modern hardware margins: CPUs, GPUs, FPGAs, in IEEE 
International Symposium on On-Line Testing and Robust System Design (IOLTS), Rhodes, 
Greece, July 1–3, 2019
 4. C. Wilkerson, H. Gao, A.R. Alameldeen, Z. Chishti, M. Khellah, S.-L. Lu, Trading off cache 
capacity for reliability to enable low voltage operation, in 2008 International Symposium on 
Computer Architecture , (2008)
 5. Z. Chishti, A.R. Alameldeen, C. Wilkerson, W. Wu, S.-L. Lu, Improving cache lifetime reli -
ability at ultra-low voltages, in Proceedings of the 42nd Annual IEEE/ACM International 
Symposium on Microarchitecture – Micro-42 , (2009)
 6. A. Bacha, R. Teodorescu, Dynamic reduction of voltage margins by leveraging on-chip ECC 
in Itanium II processors, in ACM/IEEE International Symposium on Computer Architecture 
(ISCA) , (2013)Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level46
 7. A. Bacha, R. Teodorescu, Using ECC feedback to guide voltage speculation in low-voltage 
processors, in IEEE/ACM International Symposium on Microarchitecture (MICRO) , (2014)
 8. A. Bacha, R. Teodorescu, Authenticache: harnessing cache ECC for system authentication, in 
Proceedings of the 48th International Symposium on Microarchitecture – MICRO-48, 2015
 9. H. Duwe, X. Jian, D. Petrisko, R. Kumar, Rescuing uncorrectable fault patterns in on-chip 
memories through error pattern transformation, in 2016 ACM/IEEE 43rd Annual International 
Symposium on Computer Architecture (ISCA), 2016
 10. M.S. Gupta, K.K. Rangan, M.D. Smith, G.-Y . Wei, D. Brooks, Towards a software approach 
to mitigate voltage emergencies, in International Symposium on Low Power Electronics and 
Design (ISPLED), 2007
 11. V .J. Reddi, M.S. Gupta, G. Holloway, G.-Y . Wei, M.D. Smith, D. Brooks, V oltage emergency 
prediction: using signatures to reduce operating margins, in IEEE International Symposium on 
High-Performance Computer Architecture (HPCA) , (2009)
 12. V .J. Reddi, S. Kanev, W. Kim, S. Campanoni, M.D. Smith, G.-Y . Wei, D. Brooks, V oltage 
smoothing: Characterizing and mitigating voltage noise in production processors via software- 
guided thread scheduling, in International Symposium on Microarchitecture (MICRO) , (2010)
 13. M.S. Gupta, K.K. Rangan, M.D. Smith, G.-Y . Wei, D. Brooks, Towards a software approach 
to mitigate voltage emergencies, in International Symposium on Low Power Electronics and 
Design (ISPLED) , (2007)
 14. R.  Joseph, D.  Brooks, M.  Martonosi, Control techniques to eliminate voltage emergencies 
in high performance processors, in IEEE International Conference on High-Performance 
Computer Architecture (HPCA), 2003
 15. T.N.  Miller, R.  Thomas, X.  Pan, R.  Teodorescu, VRSync: characterizing and eliminat -
ing synchronization-induced voltage emergencies in many-core processors, in International 
Symposium on Computer Architecture (ISCA), 2012
 16. M.D. Powel, T.N. Vijaykumar, Pipeline muffling and a priori current ramping: architectural 
techniques to reduce high-frequency inductive noise, in International Symposium on Low 
Power Electronics and Design (ISPLED) , (2003)
 17. M.S. Gupta, K.K. Rangan, M.D. Smith, G.-Y . Wei, D. Brooks, DeCoR: a delayed commit and 
rollback mechanism for handling inductive noise in processors, in International Conference on 
High-Performance Computer Architecture (HPCA), 2008
 18. M. Ketkar, E. Chiprout, A microarchitecture-based framework for pre- and post-silicon power 
delivery analysis, in International Symposium on Microarchitecture (MICRO), 2009
 19. Y . Kim, L.K. John, Automated di/dt stressmark generation for microprocessor power delivery 
networks, in International Symposium on Low Power Electronics and Design (ISPLED) , (2011)
 20. Y .  Kim, L.  K. John, S.  Pant, S.  Manne, M.  Schulte, W.L.  Bircher, M.S.S.  Govindan, 
AUDIT: stress testing the automatic way, in International Symposium on Microarchitecture 
(MICRO), 2012
 21. Z.  Hadjilambrou, S.  Das, M.A.  Antoniades, Y .  Sazeides, Leveraging CPU electromagnetic 
emanations for voltage noise characterization, in 2018 51st Annual IEEE/ACM International 
Symposium on Microarchitecture (MICRO) , (2018)
 22. Z. Hadjilambrou, S. Das, P.N. Whatmough, D. Bull, Y . Sazeides, GeST: an automatic frame -
work for generating CPU stress-tests, in 2019 IEEE International Symposium on Performance 
Analysis of Systems and Software (ISPASS) , (2019)
 23. L.L. Shen, I. Ahmed, V . Betz, Fast voltage transients on FPGAs: impact and mitigation strat -
egies, in 27th IEEE International Symposium On Field-Programmable Custom Computing 
Machines (FCCM), 2019
 24. J.L. Núñez-Yáñez, Energy proportional neural network inference with adaptive voltage and 
frequency scaling. IEEE Trans. Comput 68(5), 676–687 (2019)
 25. C.R. Lefurgy, A.J. Drake, M.S. Floyd, M.S. Allen-Ware, B. Brock, J.A. Tierno, J.B. Carter, 
Active management of timing guardband to save energy in POWER7, in International 
Symposium on Microarchitecture (MICRO), 2009G. Papadimitriou and D. Gizopoulos47
 26. J. Leng, A. Buyuktosunoglu, R. Bertran, P. Bose, V .J. Reddi, Safe limits on voltage reduction 
efficiency in gpus: a direct measurement approach, in IEEE/ACM International Symposium on 
Microarchitecture (MICRO), 2015
 27. I. Ahmed, Sh. Zhao, J. Meijers, O. Trescases, V . Betz, Automatic BRAM testing for robust 
dynamic voltage scaling for FPGAs, in 28th International Conference on Field Programmable 
Logic and Applications (FPL), 2018
 28. L. L. Shen, I. Ahmed, V . Betz, Fast voltage transients on FPGAs: impact and mitigation strate -
gies, in 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom 
Computing Machines (FCCM). IEEE, 2019
 29. I. Ahmed, L.L. Shen, V . Betz. Becoming more tolerant: designing FPGAs for variable supply 
voltage, in 2019 29th International Conference on Field Programmable Logic and Applications 
(FPL). IEEE, 2019
 30. S. Salamat, et al., Workload-aware opportunistic energy efficiency in multi-FPGA platforms, in 
2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 2019
 31. D. Ernst, N. Sung Kim, S. Das, S. Pant, R. Rao, T. Pham, C. Ziesler, D. Blaauw, T. Austin, 
K. Flautner, T. Mudge, Razor: a low-power pipeline based on circuit-level timing speculation, 
in Proceedings of the 36th Annual IEEE/ACM International Symposium on Microarchitecture 
(MICRO-36) , (2003)
 32. Y .  Zu, C.R.  Lefurgy, J.  Leng, M.  Halpern, M.S.  Floyd, V .J.  Reddi, Adaptive guardband 
scheduling to improve system-level efficiency of the POWER7+, in Proceedings of the 48th 
International Symposium on Microarchitecture - MICRO-48, 2015
 33. F.  Salehuddin, I.  Ahmad, F.A.  Hamid, A.  Zaharim, A.  Maheran, A.  Hamid, P.S.  Menon, 
H.A. Elgomati, B.Y . Majlis, Optimization of process parameter variation in 45nm p-channel 
MOSFET using L18 orthogonal array, in Proceedings of IEEE International Conference on 
Semiconductor Electronic (ICSE ‘12) , (2012)
 34. W. Schemmert, G. Zimmer, Threshold-voltage sensitivity of ion-implanted m.o.s. transistors 
due to process variations. Electron. Lett. 10(9), 151 (1974)
 35. N. James, P. Restle, J. Friedrich, B. Huott, B. McCredie, Comparison of split-versus connected- 
core supplies in the POWER6 microprocessor, in 2007 IEEE international solid-state circuits 
conference. Digest of Technical Papers, 2007
 36. V .J. Reddi, S. Kanev, W. Kim, S. Campanoni, M.D. Smith, G.-Y . Wei, D. Brooks, V oltage 
smoothing: characterizing and mitigating voltage noise in production processors via software- 
guided thread scheduling, in 2010 43rd Annual IEEE/ACM International Symposium on 
Microarchitecture , (2010)
 37. E. Le Sueur, G. Heiser, Dynamic voltage and frequency scaling: the laws of diminishing returns, 
in International Conference on Power Aware Computing and Systems (HotPOWER) , (2010)
 38. C.R. Lefurgy, A.J. Drake, M.S. Floyd, M.S. Allen-Ware, B. Brock, J.A. Tierno, J. B. Carter, 
Active management of timing guardband to save energy in POWER7, in International sympo -
sium on Microarchitecture (MICRO), 2009, pp. 1–11
 39. A. Bacha, R. Teodorescu, Dynamic reduction of voltage margins by leveraging on-chip ECC 
in Itanium II processors, in International Symposium on Computer Architecture (ISCA), 2013, 
pp. 297–307
 40. A. Bacha, R. Teodorescu, Using ECC feedback to guide voltage speculation in low-voltage 
processors, in International symposium on Microarchitecture (MICRO), 2014, pp. 306–318
 41. A. Bacha, R. Teodorescu, Authenticache: Harnessing cache ECC for system authentication, in 
International symposium on Microarchitecture (MICRO), 2015, pp. 128–140
 42. J. Leng, A. Buyuktosunoglu, R. Bertran, P. Bose, V . J. Reddi, Safe limits on voltage reduc -
tion efficiency in GPUs: a direct measurement approach, in International Symposium on 
Microarchitecture (MICRO), 2015, pp. 294–307
 43. G.  Papadimitriou, M.  Kaliorakis, A.  Chatzidimitriou, D.  Gizopoulos, P.  Lawthers, S.  Das, 
Harnessing voltage margins for energy efficiency in multicore CPUs, in IEEE/ACM 
International Symposium on Microarchitecture (MICRO 2017), Cambridge, MA, USA, 
October 2017Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level48
 44. G. Papadimitriou, M. Kaliorakis, A. Chatzidimitriou, D. Gizopoulos, G. Favor, K. Sankaran, 
S. Das, A system-level voltage/frequency scaling characterization framework for multicore 
CPUs, in IEEE Silicon Errors in Logic – System Effects (SELSE 2017), Boston, MA, USA, 
March 2017
 45. The Linux Kernel Documentation (Parent Directory), Retrieved 2017 from https://www.ker -
nel.org/doc/Documentation
 46. R.  Bertran et  al., V oltage noise in multi-core processors: empirical characterization and 
optimization opportunities, in 2014 47th Annual IEEE/ACM International Symposium on 
Microarchitecture, 2014
 47. A. Chatzidimitriou, G. Papadimitriou, D. Gizopoulos, S. Ganapathy, J. Kalamatianos, Assessing 
the effects of low voltage in branch prediction units, in IEEE International Symposium on 
Performance Analysis of Systems and Software (ISPASS 2019), Madison, Wisconsin, USA, 
March 2019
 48. A. Chatzidimitriou, G. Papadimitriou, D. Gizopoulos, S. Ganapathy, J. Kalamatianos, Analysis 
and characterization of ultra low power branch predictors, in IEEE International Conference 
on Computer Design (ICCD 2018), Orlando, Florida, USA, October 2018
 49. G. Papadimitriou, A. Chatzidimitriou, D. Gizopoulos, Adaptive voltage/frequency scaling and 
core allocation for balanced energy and performance on multicore CPUs, in IEEE International 
Symposium on High-Performance Computer Architecture (HPCA 2019), Washington, DC, 
USA, February 2019
 50. G. Karakonstantis, K. Tovletoglou, L. Mukhanov, H. Vandierendonck, D. S. Nikolopoulos, 
P. Lawthers, P. Koutsovasilis, M. Maroudas, C. D. Antonopoulos, C. Kalogirou, N. Bellas, 
S.  Lalis, S.  Venugopal, A.  Prat-Perez, A.  Lampropulos, M.  Kleanthous, A.  Diavastos, 
Z.  Hadjilambrou, P.  Nikolaou, Y .  Sazeides, P.  Trancoso, G.  Papadimitriou, M.  Kaliorakis, 
A.  Chatzidimitriou, D.  Gizopoulos, S.  Das, An energy-efficient and error-resilient server 
 ecosystem exceeding conservative scaling limits, in ACM/IEEE Design, Automation, and Test 
in Europe (DATE 2018), Dresden, Germany, March 2018
 51. K.  Tovletoglou, L.  Mukhanov, G.  Karakonstantis, A.  Chatzidimitriou, G.  Papadimitriou, 
M. Kaliorakis, D. Gizopoulos, Z. Hadjilambrou, Y . Sazeides, A. Lampropulos, S. Das, P. V o, 
Measuring and exploiting guardbands of server-grade ARMv8 CPU cores and DRAMs, in 
IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2018), 
Luxembourg, June 2018
 52. A. Chatzidimitriou, G. Papadimitriou, D. Gizopoulos, HealthLog monitor: a flexible system- 
monitoring Linux service, in IEEE International Symposium on On-Line Testing and Robust 
System Design (IOLTS 2018), Costa Brava, Spain, July 2018
 53. A. Chatzidimitriou, G. Papadimitriou, D. Gizopoulos, HealthLog monitor: errors, symptoms 
and reactions consolidated. IEEE Trans. Device Mater. Reliab. 19(1), 46–54 (2019)
 54. M. Kaliorakis, A. Chatzidimitriou, G. Papadimitriou, D. Gizopoulos, Statistical analysis of 
multicore CPUs operation in scaled voltage conditions. IEEE Comput. Archit. Lett. 17(2), 
109–112 (2018)
 55. G.  Papadimitriou, A.  Chatzidimitriou, M.  Kaliorakis, Y .  Vastakis, D.  Gizopoulos, Micro- 
viruses for fast system-level voltage margins characterization in multicore CPUs, in 2018 IEEE 
International Symposium on Performance Analysis of Systems and Software (ISPASS), 2018
 56. K. Ganesan, J. Jo, W. L. Bircher, D. Kaseridis, Z. Yu, L. K. John, System-level max power 
(SYMPO), in Proceedings of the 19th international conference on Parallel architectures and 
compilation techniques - PACT ‘10, 2010
 57. K.  Ganesan, L.K.  John, MAximum Multicore POwer (MAMPO), in Proceedings of 2011 
International Conference for High Performance Computing, Networking, Storage and Analysis 
on – SC ‘11, 2011
 58. Mersenne Prime Search Software., Prime95, Version 28.10, https://www.mersenne.org/
download/
 59. Stress-ng benchmark. Retrieved July 30, 2017 http://kernel.ubuntu.com/~cking/stress- ng/G. Papadimitriou and D. Gizopoulos49
 60. G. Theodorou, N. Kranitis, A. Paschalis, D. Gizopoulos, Software-Based Self-Test for Small 
Caches in Microprocessors. IEEE Trans. Comput. Aid. Design Integr. Circuits Syst. 33(12), 
1991–2004 (Dec. 2014)
 61. M. Psarakis, D. Gizopoulos, E. Sanchez, M.S. Reorda, Microprocessor software-based self- 
testing. IEEE Design Test. Comput 27(3), 4–19 (2010)
 62. R.E. Kessler, M.D. Hill, Page placement algorithms for large real-indexed caches. ACM Trans. 
Comput. Syst. 10(4), 338–359 (1992)
 63. ARM Cortex-A Series: Programmer’s Guide for ARMv8-A, v1.0, March ‘15. http://infocenter.
arm.com/help/topic/com.arm.doc.den0024a/DEN0024A_v8_architecture_PG.pdf
 64. R.J.  Riedlinger et  al., A 32nm 3.1 billion transistor 12-wide-issue Itanium® processor for 
mission-critical servers, in 2011 IEEE International Solid-State Circuits Conference , (2011)Challenges on Unveiling Voltage Margins from the Node to the Datacentre Level51Harnessing Voltage margins for Balanced 
Energy and Performance
George Papadimitriou and Dimitris Gizopoulos
1  Introduction
During chip fabrication, process variations can affect transistor dimensions (length, 
width, oxide thickness, etc. [ 1]), which have a direct impact on the threshold voltage 
of a MOS device [ 2].1 As technology scales, the percentage of these variations com -
pared to the overall transistor size increases and raises major concerns for designers, 
who aim to improve energy efficiency. Devices variation during fabrication is 
known as static  variation and remains constant during the chip lifetime. On top of 
that, transistor aging and dynamic  variation in supply voltage and temperature, 
caused by different workload interactions, is also of primary importance. Both static 
and dynamic variations lead microprocessor architects to apply conservative guard 
bands (operating voltage and frequency settings) to avoid timing failures and guar -
antee correct operation, even in the worst-case conditions excited by unknown 
workloads or the operating environment [ 3, 4]. However, these guard bands impede 
the power consumption. To bridge the gap between energy efficiency and perfor -
mance improvements, several hardware and software techniques have been pro -
posed, such as Dynamic V oltage and Frequency Scaling (DVFS) [ 5]. The premise of 
DVFS is that the microprocessor’s workloads as well as the cores’ activity vary. 
1 The threshold voltage, commonly abbreviated as V th, of a field-effect transistor (FET) is the mini -
mum gate-to-source voltage V GS (th) that is needed to create a conducting path between the source 
and drain terminals. It is an important scaling factor to maintain power efficiency.
G. Papadimitriou ( *) · D. Gizopoulos 
Department of Informatics and Telecommunications, National and Kapodistrian University of 
Athens, Athens, Greece
e-mail:  georgepap@di.uoa.gr
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_352
V oltage and frequency-scaling during epochs where peak performance is not 
required enables a DVFS-capable system to achieve average energy-efficiency 
gains without affecting peak-performance adversely. However, energy-efficiency 
gains are limited by the pessimistic guard bands.
Revealing and harnessing the pessimistic design-time voltage margins offer a 
significant opportunity for energy-efficient computing in multicore CPUs. The full 
energy savings potential can be exposed only when accurate core-to-core, chip-to- 
chip, and workload-to-workload voltage scaling variation is measured. When all 
these levels of variation are identified, system software can effectively allocate 
hardware resources to software tasks matching the capabilities of the former (under -
volting potential of the CPU cores) and the requirements of the latter (for energy or 
performance). Although characterization studies for CPUs and GPUs have been 
presented recently [ 3, 6–9], they primarily focus on coarse-grained identification of 
the Vmin values, i.e., the voltage level at which no type of anomaly is observed in 
program execution of a particular core. Furthermore, these studies focus primarily 
on x86 and POWER-series enterprise-class server systems, whose summary is 
shown in Table  1; studies on GPU chips have been reported as well.
The ability to cap peak power consumption has also gained strong interest in 
several important computing domains (e.g., mobile devices to data centers [ 15]) 
since the state-of-the-art high-end microprocessors currently support such features 
within their power management subsystem. Power capping is realized through 
power-performance knobs such as DVFS, pipeline throttling, or memory throttling 
[14, 16].
In this chapter, we present a detailed system-level voltage scaling single-core 
characterization study for ARMv8-based CPUs manufactured in 28  nm. The 
study’s backbone is a fully automated system-level framework built around Applied 
Micro’s (APM) X-Gene 2 micro-server (as presented in the previous chapter). The 
automated infrastructure aims to increase the throughput of massive undervolting 
campaigns that require multiple benchmarks execution at several voltage supply 
levels of all individual cores. The automated characterization process requires min -
imal human intervention and records all possible abnormalities due to undervolt -
ing: silent data corruptions (SDC, e.g., program output mismatches without any 
hardware error notification), corrected errors, uncorrected (but detected) errors 
(provided by Linux EDAC driver [ 10]), as well as application and system 
crashes [ 11].
Table 1  Summary of studies on commercial chips
ISA Processor Technology Reference
POWER 7/7+ IBM Power 750, 780 45/32 nm [6, 12]
IA-64 Intel Itanium 9560 32 nm [7, 8]
x86–64 Intel i7-3970X, Intel 
i5-4200U32/22 nm [13]
Nvidia Fermi/
KeplerGTX 480, 580, 680, 780 40/28 nm [9]G. Papadimitriou and D. Gizopoulos53
Towards the formalization of the behavior in undervolting conditions we also 
present the definition of a simple consolidated function; the Severity function . 
Severity function aggregates the effects of reduced voltage operation in the cores of 
a multicore CPU by assigning values to the different abnormal observations. The 
lower the voltage level, the higher the value of the severity function. The severity 
function assists an undervolting classification of the cores of a CPU chip for a given 
benchmark: different core, benchmark, and voltage values lead to different severity 
patterns, some with an abrupt increase to the severity (e.g., the benchmark keeps 
executing correctly until a voltage level at which the system crashes), while others 
have a “smooth” severity increase while voltage is reduced (the system remains 
responsive throughout a range of voltage values but it generates ECC errors or pro -
duces SDCs). The fine-grained analysis of the behavior of the machine using the 
severity function can assist energy efficiency decisions for task-to-core allocation 
by the system software. Understanding the behavior in non-nominal conditions is 
very important for making software and hardware design decisions for improved 
energy efficiency that preserves correctness of operation. The characterization mod -
eling of our study can be effectively used to support design and system software 
decisions to harness voltage margins and thus improve energy efficiency while pre -
serving operation correctness.
Finally, we are based on two recent state-of-the-art, ARMv8-compliant, multi -
core CPUs, Applied Micro’s X-Gene 2 and X-Gene 3, to present a new software- 
based scheme for these server-grade machines, which provides large energy savings 
while maintaining high performance levels. X-Gene 3 specifically (which is the 
most recent microprocessor of the X-Gene family) has comparable performance to 
high-end Intel Xeon microprocessors 15. However, neither X-Gene 2 nor X-Gene 3 
microprocessors have predefined power management states as in x86 and IBM 
POWER architectures. Although X-Gene microprocessors support dynamic fre -
quency scaling, the voltage is always the same for every different frequency step 
(which is equal to Vnominal ) and can be only explicitly modified.
2  System Architecture
For the needs of the studies presented in this chapter, we employ two different state- 
of- the-art ARMv8 micro-processors: Applied Micro’s (now Ampere Computing) 
X-Gene 2 and X-Gene 3, which consist of 8 and 32 64-bit ARMv8-compliant cores, 
respectively. Both microprocessors offer high-end processing performance and 
come along with a subsystem that features a Scalable Lightweight Intelligent 
Management processor (SLIMpro) to enable flexibility in power management, 
resiliency, and end-to-end security for a wide range of applications. The dedicated 
SLIMpro processor monitors system sensors, configures system attributes (e.g., 
regulates supply voltage, etc.) and can be accessed by the system’s running 
Linux kernel.Harnessing Voltage margins for Balanced Energy and Performance54
In a separate chapter, we present the main characteristics of X-Gene 2 micro -
processor. The X-Gene 3 microprocessor is a more powerful, larger scale machine 
compared to X-Gene 2. Specifically, X-Gene 3 microprocessor has a main power 
domain that includes the CPU cores, the L1, L2, and L3 cache memories, and the 
memory controllers, which is called PCP (Processor ComPlex) power domain, as 
shown in Fig.  1, and is the one that consumes the largest part of the overall power 
consumption. Figure  1 presents the architecture of X-Gene 3, however, the 
X-Gene 2 has a similar structure; the difference is that it has 8 cores instead of 
32, and the L3 cache, which is 8 MB instead of 32 MB, is located in a differ -
ent domain.
The operating voltage of the main power domain can change from 980  mV 
downwards in X-Gene 2 and from 870 mV downwards in X-Gene 3 by at least 
5 mV steps. While all the CPU cores operate at the same voltage, each pair of cores 
(PMD – Processor MoDule) can operate at different frequency in both chips. The 
frequency can range from 300 MHz up to 2.4GHz in X-Gene 2, and from 375 MHz 
L1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1D
L1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DL1I
ARMv8 
Core
L2 Cache 256KBL1DL1I
ARMv8 
Core
L1DCentral Switch (CSW)
Shared 32MB L3 CacheDDR4 DDR4 DDR4 DDR4
DDR4 DDR4 DDR4 DDR432 x ARMv8 Cores @ 3GHz
8 x DDR4  @ 2667M HzPCP
PMpro SLIMproStandby  
Power Domain
Fig. 1  X-Gene 3 block diagramG. Papadimitriou and D. Gizopoulos55
Table 2  Basic parameters of X-Gene 2 and X-Gene 3
Parameter X-Gene 2 X-Gene 3
ISA 64-bit OoO (4-issue)
Pipeline ARMv8 (AArch64, AArch32, thumb)
CPU 8 cores 32 cores
Core clock 2.4 GHz 3.0 GHz
L1 instruction cache 32 KB per core (parity protected)
L1 data cache 32 KB per core (parity protected)
L2 cache 256 KB per PMD (SECDED protected)
L3 cache 8 MB (SECDED protected) 32 MB (SECDED protected)
Technology 28 nm (bulk CMOS) 16 nm (FinFET)
Max TDP 35 W 125 W
Nominal voltage 980 mV 870 mV
to 3GHz in X-Gene 3 (at 1/8 steps of the maximum clock frequency in both micro -
processors). Table  2 presents the main characteristics of the two microprocessors. 
Both platforms run CentOS 7.3 with Linux kernel version 4.11.
3  Measuring Voltage Guard Bands of Server-Grade 
ARMv8 CPUs
3.1  Effect Categorization
We study the behavior of 3 different X-Gene 2 chips by using representative bench -
marks from the SPEC CPU2006 suite to explore the voltage guard bands for each 
core of the chip, and thus to detect the safe Vmin in which the benchmarks can be 
executed correctly. We also study any abnormal behavior that can be exposed (SDC, 
ECC errors, crashes, etc.) below the safe Vmin levels, for a comprehensive character -
ization. We present our findings for three different chips: one typical chip (TTT), 
and two corner chips (TFF, and TSS). More specifically, the letters F, T, and S, 
describe for fast, typical, and slow chips. Circuits are most likely to fail at the cor -
ners of the design space, so nonstandard circuits should be simulated at all corners 
to ensure they operate correctly in all cases [ 17, 18].
3.2  Regions of Operation
Using the automated framework presented in a separate chapter, we extensively 
characterize the three X-Gene 2 chips. The characterization process can reveal for 
each core of the CPU three different regions of operation when the microprocessor 
operates beyond nominal voltage conditions. These are the safe and unsafe  Harnessing Voltage margins for Balanced Energy and Performance56
operating regions and the region in which the system cannot operate ( crash region ). 
To isolate the impact of temperature that can affect our results, we stabilize the 
temperature at 43 °C, and thus, all benchmarks complete their execution at the same 
temperature. In Figs.  2, 3, and 4, we present the results for 10 SPEC CPU2006 
benchmarks [ 19]. All programs ran on a single core in each PMD at 2.4 GHz, while 
the remaining six cores (the other 3 PMDs) reliably operated at 300. In order to 
consider the non-deterministic behavior of such experiments, we ran every under -
volting campaign ten different times (taking into account the long execution times). 
Figures  2, 3, and 4 present in detail for all benchmarks the highest Vmin values and 
the highest crash  voltage values of the ten campaigns for the three different chips 
and all the cores of each chip. In all benchmarks, we can notice the three regions of 
operation according to the collected results. These regions are:
• Safe region (blue):  The characterization runs that correspond to this region had 
a normal operation (NO) without any SDCs, errors, or crashes.
• Unsafe  region (grey):  The characterization runs that correspond to this region 
generate an abnormal behavior (SDC, CE, UE, AC), but not a system crash.
• Crash  region (black):  This region includes voltage values in which at least one 
characterization run led to a system crash.
3.3  Vmin Experimental Results
We experimentally obtain the Vmin values of the 10 SPEC CPU2006 benchmarks on 
the three X-Gene 2 chips (TTT, TFF, TSS), running the entire time-consuming 
undervolting experiments 10 times for each benchmark. These experiments were 
performed during 6 months on a single X-Gene 2 machine (for all the three micro -
processor chips). This part of our study focuses on a quantitative analysis of the safe 
Vmin for different chips of the same architecture in order to expose the potential 
guard bands of each chip, as well as to quantify how the program behavior affects 
the guard band and to measure the core-to-core and chip-to-chip variations.
The voltage guard band for each program is the smallest (safe) margin between 
the nominal voltage of the microprocessor and its Vmin. Our single-core experiments 
were performed in the highest available frequency of the X-Gene 2, which is 
2.4 GHz. In that frequency, we observed divergences of the Vmin values as shown in 
Fig. 5. For a significant number of benchmarks, we can see variations between dif -
ferent programs and different chips. Figure  5 represents all ten benchmarks for the 
most robust core for each chip (Core 4 in all three microprocessors chips), and for 
these programs the Vmin varies from 885 mV to 865 mV for TTT (blue line), from 
885 mV to 860 mV for TFF (orange line), and from 900 mV to 870 mV for TSS 
(green line). Considering that the nominal voltage for the X-Gene 2 microproces -
sors is 980 mV , there is a significant reduction of voltage without affecting the cor -
rect execution of programs, which is equal to at least 9.7% for the TTT and TFF 
chips, and 8.2% for the TSS chip. We also notice in Fig.  5 that the workload-to- 
workload variation remains the same across the three chips of the same architecture; G. Papadimitriou and D. Gizopoulos57
Fig. 2  X-Gene 2 characterization results for 4 SPEC CPU2006 benchmarks ( bwaves , dealII , les-
lie3d , milc) on three different chips (TTT, TFF, TSS). Blue represents the Safe region; grey repre -
sents the Unsafe region; and black represents the Crash regionHarnessing Voltage margins for Balanced Energy and Performance58
Fig. 3  X-Gene 2 characterization results for 4 SPEC CPU2006 benchmarks ( cactusADM , gro-
macs , mcf, namd ) on three different chips (TTT, TFF, TSS). Blue represents the Safe region; grey 
represents the Unsafe region; and black represents the Crash regionG. Papadimitriou and D. Gizopoulos59
Fig. 4  X-Gene 2 characterization results for 2 SPEC CPU2006 benchmarks ( soplex , and zeusmp ) 
on three different chips (TTT, TFF, TSS). Blue represents the Safe region; grey represents the 
Unsafe region; and black represents the Crash region
850860870880890900910920930)Vm( ddVGuardband
TTT TFF TSS
Fig. 5  V min results at 2.4 GHz for 10 SPEC CPU2006 programs on three different X-Gene 2 chips 
(TTT, TFF, TSS)Harnessing Voltage margins for Balanced Energy and Performance60
however, there is a relatively large variation among the chips. This means that there 
is a program dependency of Vmin behavior in all chips.
3.3.1  Process Variation
Figures 2, 3, and 4 present the detailed information about the safe Vmin for all bench -
marks and cores of the three chips, as well as the range of the unsafe region. More 
specifically, in this section, we discuss the chip-to-chip and core-to-core variations.
Chip-to-Chip Variation  As Figs.  2, 3, and 4 show, PMD 2 (cores 4 and 5) is the 
most robust PMD for all three chips (up to 3.6% more voltage reduction compared 
to the most sensitive cores). Green line in Figs.  2, 3, and 4 presents the average Vmin, 
and the Red line represents the average Crash voltage point for each chip. Thus, we 
can notice that the TFF chip has lower Vmin points than the TTT chip, in contrast to 
TSS (the chip with lower leakage), which has significantly higher Vmin points than 
the other two chips, and thus, lower power savings. For the unsafe region, on the 
other hand, we notice only small divergences among the chips.
Core-to-Core Variation  As shown in Figs.  2, 3, and 4, there are significant diver -
gences among cores for the same benchmark due to process variation. Process vari -
ations can affect transistor dimensions (length, width, oxide thickness, etc.) which 
have direct impact on the threshold voltage of a MOS device. More specifically, 
variation has a minor impact on dynamic energy, but a major impact on static leak -
age energy [ 20]. Variation shifts the minimum energy and energy-delay product 
(EDP) operating points toward a higher supply and threshold voltage, and reduces 
the potential benefits in operating at these points, and thus, the guard band of each 
core. This variation among cores of the same chip can result in high energy savings 
by using the appropriate task scheduling. We present and discuss this method in the 
following subsection.
3.3.2  Abnormal Behaviors Below Vmin
Variation can also cause circuits to malfunction, especially at low voltage. Previous 
studies on Intel Itanium CPUs [ 7, 8] have shown a large region of voltage values that 
contains only ECC corrected  errors during undervolting. By reducing  the voltage 
on those chips, the number of corrected errors increases gradually for quite many 
voltage steps until it exposes other types of abnormal behavior (SDCs, uncorrected 
errors, crashes). In such systems, ECC corrected errors can serve as proxies for the 
effects of undervolting. In contrast to these studies, a major finding of our charac -
terization for ARMv8-compliant multicore CPUs is that silent data corruptions 
appear at higher voltage levels than corrected errors alone for any benchmark . In 
[7] and [ 8], the reported range of voltage levels with corrected errors alone offers a 
significant opportunity for energy savings without jeopardizing correctness of G. Papadimitriou and D. Gizopoulos61
operation. High correctable error rate is helpful to an ECC-guided voltage specula -
tion, but this is not the case in the APM X-Gene 2 in our case.
In contrast, ECC corrections appear at a higher voltage on the Itanium compared 
to SDCs and system crashes. We attribute the increased robustness to timing- failures 
on the Itanium to circuit-level dynamic-margin mitigation techniques such as the 
capability to perform continuous clock-path de-skewing during dynamic operation 
[21]. The X-Gene 2 does not deploy such circuit-level techniques, and thereby, gen -
erates SDCs due to timing-path failures. Having the occurrence of SDCs first , it is 
not possible to easily guide the voltage speculation for prediction based on the man -
ifested errors. For that reason, we present the severity function  both for quantifying 
the severity and for illustrating the scaling of abnormal behaviors due to voltage 
reduction. The new metric’s contribution is twofold: (1) to aggregate the results 
produced by multiple runs, and (2) to quantify a microprocessor’s ability to operate 
beyond nominal conditions and especially beyond the safe Vmin.
3.3.3  Severity Function
Each characterization run can manifest multiple effects. For instance, in a run, both 
SDC and CE can be observed; thus, both of them are reported for this run. Due to 
the non-determinism of the characterization in real hardware, all the information 
collected during multiple campaigns of the same benchmark (iterative execution) is 
also reported by our severity  function. To quantify  the criticality of the effects of 
different experimental runs of different campaigns with the same setup, we define 
the “severity function” Sv, where v is the voltage, as follows:
 SWSDC
NWCE
NWUE
NWAC
NWTO
NWSC
NvS DC CE UE AC TO SC =• +• +• +• +• +•
 
In this function, the parameters SDC , CE, UE, AC, TO , and SC can take values 
from 0 to N (N is the number of runs at voltage level v), and represent the times that 
the effect appears to these runs (for example, if k of the N runs lead to UE, then 
parameter UE is set to k; the actual number of uncorrected errors during each run is 
not taken into consideration). Parameters WSDC, WCE, WUE, WAC, WTO and WSC repre -
sent “weights” that can be arbitrarily set to characterize the severity of each effect 
(effects have been shown in a separate chapter). The higher the weight, the more 
critical the effect is considered by our function, and the main role of these weights 
is to “translate” the behaviors (SDCs, etc.) to numbers in order to fit into the equa -
tion. We use the values presented in Table  4 as the values for our severity function 
(but different weight values can be also used according to the importance of each 
observed abnormal behavior in a particular system study).
Figure 6 show the severity of one benchmark on TTT chip. As an example, Fig.  6 
has a significantly large unsafe region , and it also provides a smooth gradual increase 
of severity while the voltage is reduced. These results are derived from ten execu -
tions of the same campaign. According to the severity value for each voltage level, Harnessing Voltage margins for Balanced Energy and Performance62
Table 4  Weights used in our experiments
Weight Value
WSC 16
WAC 8
WTO 8
WSDC 4
WUE 2
WCE 1
WNO 0
16.013.3 16.0 16.0 16.07.0 14.7 11.7 13.35.04 .3 9.71 2.016.0 4.04 .0 6.0 4.716.0 16.0 16.0 13.3 4.04 .0 4.0 4.012.3 9.7 13.3 12.3 4.0 4.05.34 .0 8.34 .3 1.34.04 .0 8.04 .04.02 .7 4.02 .7
Core0C ore1 Core2C ore3 Core4C ore5 Core6C ore7850860870880890900910920930940950960970980mVbwaves Core severit y
Fig. 6  bwaves benchmark severity on TTT chip cores
one can decide if and when it is possible to reduce the voltage further (lower than 
the safe Vmin where severity is 0) for more aggressive energy efficiency.
The severity function is essential primarily for speculation, because (1) it collects 
all needed information in one metric, (2) it incorporates the small divergences 
across multiple executions (ten executions in our case), (3) it measures the mean 
severity for each voltage step in each core, and (4) it assigns numbers to behaviors 
(SDC, CE, etc.), and thus, it is easy to use by any software daemon.G. Papadimitriou and D. Gizopoulos63
3.4  Suggestions for Undervolting Effects Mitigation
A static Vmin point does not contain any information about the severity of operating 
at voltages below the safe Vmin, but severity does so. Therefore, having knowledge 
about the severity below the safe Vmin for each workload, one can decide if it is pos -
sible to be more aggressive to set the voltage below the safe Vmin, and thus, to save 
more power.
Depending on the actual characterization findings ( Vmin and severity) for a CPU 
core during undervolting, certain hardware-based or software-based mitigation 
approaches can be employed to maximize the energy savings while preserving the 
correctness of program execution.2 The primary aspect that determines the most 
suitable approach is the first observed effect  as undervolting goes down the voltage 
levels. We select the following behaviors using the severity function described 
before. For each case we describe the behavior, discuss the severity function values 
and corresponding mitigation approaches.
Nothing abnormal  (severity = 0).  The voltage range is absolutely safe (above the 
Vmin of a core); no mitigation action is required. System operation in this range is 
the most conservative option and no mitigation provision is needed. Energy- 
savings are the minimum.
Corrected errors first  (severity = 1).  This is a voltage range with the behavior as the 
one observed in [ 7, 8] for Intel’s Itanium (not in our X-Gene 2 machine). In such 
a case, ECC hardware serves as a proxy for abnormal behavior due to undervolt -
ing, but program operation is still correct. Significant energy savings can be 
obtained without any mitigation other than the ECC correction, but going further 
down the voltage is risky.
SDCs alone  (severity  =  4) or  with corrected and uncorrected errors  (sever -
ity = 5–7).  V oltage ranges with these behaviors generate incorrect program out -
puts and require extra mitigation approaches. The characterization of our X-Gene 
2 system shows that the first abnormal behavior generated by undervolting 
belongs here for the majority of benchmarks, and corrected errors as observed in 
[7, 8] do not appear first alone in our system. In particular, the cases where SDCs 
appear alone (severity = 4) are the worst ones since there is no indication about 
the malfunction of the system; these areas should be avoided. When an eventual 
SDC (output mismatch) is accompanied by corrected or uncorrected error 
 notifications, recovery actions can be employed such as rollback to a previously 
stored check-point or program re-execution in safe voltage and frequency com -
binations. There are also many applications that can tolerate SDCs and benefit 
2 Our severity metric can be used above existing circuit-based techniques such as adaptive clock -
ing. For instance, in the mechanism proposed in 0, adaptive-clocking can reduce the voltage at 
which SDCs occur. The frequency with which adaptation is deployed can be an input to our frame -
work, thereby limiting the potential for performance degradation due to excessive deployment of 
adaptive-clocking induced frequency slowdown.Harnessing Voltage margins for Balanced Energy and Performance64
from the severity function. These applications are (1) approximate computing 
algorithms, (2) video streaming and other image and video processing, (3) 
security- oriented applications such as jammer attacks detectors, etc. These appli -
cations are tolerant to faults, as they have minor impact on the returned output. 
For such applications, severity ≤4 can be used for improving energy efficiency.
Application and system crashes (or application timeouts) with or without cor -
rected and uncorrected errors  (severity 8–19).  V oltage levels with this behavior 
(the result of massive hardware malfunction) are well beyond the limits of cores 
operation in undervolted conditions. Application or system unresponsiveness is 
systematic in these ranges and unless serious hardware redesign is employed 
these ranges are unusable.
4  Balancing Energy and Performance
In the previous chapter, we discussed that, in order to improve the microprocessor’s 
efficiency (in terms of either power or performance), several hardware and software 
techniques have been proposed, such as Dynamic V oltage and Frequency Scaling 
(DVFS) [ 5] as well as several power capping approaches [ 14]. The ability to cap 
peak power consumption has recently gained strong interest in several important 
computing domains (e.g., mobile devices to data centers [ 15]) since the state-of-the- 
art high-end microprocessors currently support such features within their power 
management subsystem. Power capping is realized through power-performance 
knobs such as DVFS, pipeline throttling, or memory throttling [ 14]. In the previous 
section, we also presented a comprehensive characterization study, which exposes 
the pessimistic voltage margins for single-core executions at the maximum fre -
quency of the X-Gene 2 microprocessor.
In this section, we are based on two recent state-of-the-art ARMv8-compliant 
multicore CPUs, Applied Micro’s X-Gene 2 and X-Gene 3, to present a new software-  
based scheme for these server-grade machines, which provides large energy savings 
while maintaining high performance levels. X-Gene 3 specifically has comparable 
performance to high-end Intel Xeon microprocessors [ 15]. However, neither X-Gene 
2 nor X-Gene 3 microprocessors have predefined power management states as in x86 
and IBM POWER architectures. Although X-Gene microprocessors support dynamic 
frequency scaling, the voltage is always the same for every different frequency step 
(which is equal to Vnominal ) and can be only explicitly modified.
Particularly, the main contributions of this section are
• We expose the pessimistic voltage guard bands of the two state-of-the-art ARMv8 
microprocessors of the same family of products (manufactured in 28 nm and 
16 nm – the X-Gene 2 and X-Gene 3, respectively) to identify the safe Vmin points 
of the CPU chips in multicore executions. Note that in the previous chapter we 
presented an extensive characterization study for single-core executions (where 
we were aiming to identify the core-to-core variability among others). Having G. Papadimitriou and D. Gizopoulos65
characterized the multicore executions in this chapter, we show that as the num -
ber of active threads increases, core-to-core and workload-to-workload variabil -
ity have a minimal impact on Vmin.
• We present measurements on the correlation of the safe Vmin to the voltage droop 
magnitude, and show that in multicore executions the emergency voltage droop 
events occur regardless of the workload. However, for executions in a single or 
very few cores, core-to-core and workload-to-workload variability exist (as we 
also presented in the previous chapter).
• We perform an extensive study to identify and analyze the tradeoffs between 
energy and performance at different voltage and frequency combinations, as well 
as at different thread scaling and core allocation configurations. Our analysis 
reveals that depending on the course-grain characteristics of a program and the 
number of active threads, there is an optimal combination of voltage, frequency, 
and core allocation for better energy efficiency.
• We also developed a simple online monitoring daemon which monitors the run -
ning processes on the system and guides the Linux scheduler to take the appro -
priate decisions regarding: (a) the core(s) to which a new process should be 
assigned, and (b) when one or more running processes should be migrated to 
other cores. At the same time, the daemon dynamically adjusts the V/F settings 
according to the optimal policies.
• Finally, we evaluate the optimal energy efficient scheme by running the monitor -
ing daemon in a realistic scenario of a server’s operation, which (a) randomly 
selects the issued programs, (b) dynamically migrates the running processes on 
the system, and (c) dynamically adjusts the voltage and frequency settings. We 
report several comparisons among different configurations to present a detailed 
evaluation of the optimal scheme, and show that it can achieve on average 25.2% 
energy savings on X-Gene 2, and 22.3% energy savings on X-Gene 3, with a 
minimal performance penalty of 3.2% on X-Gene 2 and 2.5% on X-Gene 3 com -
pared to the default voltage and frequency microprocessor’s conditions.
4.1  Experimental Setup
In this analysis, we use 25 benchmarks from three different benchmark suites as 
shown in Table  5: the NAS Parallel Benchmark Suite v3.3.1 (NPB) [ 22], the SPEC 
CPU2006 suite [ 19], and the PARSEC v3.0 suite [ 23]. NPB are programs designed 
to evaluate the performance of parallel supercomputers and have been used in sev -
eral studies regarding performance and energy efficiency [ 24, 25]. Given that this 
study is primarily based on the multicore execution, we use the NPB parallel bench -
marks and the PARSEC parallel benchmarks for multi-thread executions, and the 
SPEC CPU2006 single-thread benchmarks (both FP and INT class), for evaluating 
multiple copies of single-threaded executions.
Note that, in a parallel execution with N threads (assume an NPB or PARSEC 
program with N parallel threads), all active threads compute parts of the same work, Harnessing Voltage margins for Balanced Energy and Performance66
Table 5  Benchmarks description
Name Description Suite
CG Conjugate gradient NAS parallel benchmarks 
v3.3.1 [ 22] EP Embarrassingly parallel
FT Discrete 3D FFT
IS Integer Sort
LU Lower-upper gauss-Seidel solver
MG Multi-grid on a sequence of meshes
Namd Scientific, structural biology SPEC CPU2006 [ 19]
cactusADM Physics/general relativity
leslie3d Computational fluid dynamics
dealII Solution of partial differential equations
Bwaves Computational fluid dynamics
Gromacs Chemistry/molecular dynamics
Zeusmp Physics/magneto-hydrodynamics
Milc Physics/quantum chromodynamics
Mcf Combinational optimization
Swaptions Uses the heath-Jarrow-Morton (HJM) framework to 
price a portfolio of swaptionsPARSEC v3.0 [ 23]
Blackscholes Black-Scholes partial differential equation
Fluidanimate Simulates an incompressible fluid for interactive 
animation purposes
Canneal Simulated annealing (SA) to minimize the routing 
cost of a chip design
Bodytrack Tracks a human body with multiple cameras
Dedup Compresses a data stream with a combination of 
global and local compression
which is executed once. On the other hand, when we execute N copies of the same 
single-threaded benchmark, the microprocessor executes N times the same work. 
Therefore, we cannot directly compare the two cases as they refer to different 
amounts of work and thus, the energy values of the single-threaded benchmarks are 
normalized to the number of running instances in order to deliver a fair comparison 
between the two groups of programs. For example, if the microprocessor executes 
N instances of a single-threaded benchmark for the SPEC CPU2006 suite (e.g., 
namd ), the energy will be equal to energy_of_N_instances/N .
For a comprehensive coverage of as many different execution behaviors as pos -
sible, we executed all 25 benchmarks (Table  5) in three different threading configu -
rations in both X-Gene 2 and X-Gene 3 systems:
Max threads:  In all available cores of each microprocessor (8 cores for X-Gene 2 
and 32 cores for X-Gene 3).
Half threads:  In half of the cores (4 cores for X-Gene 2 and 16 cores for X-Gene 
3), and.G. Papadimitriou and D. Gizopoulos67
Quarter threads:  In one quarter of the cores (2 cores for X-Gene 2 and 8 cores for 
X-Gene 3).
For these three different thread-scaling options, we executed the programs at the 
maximum frequency of each microprocessor (2.4GHz for X-Gene 2 and 3GHz for 
X-Gene 3), and at the half frequency (1.2GHz and 1.5GHz, respectively). It is 
essential to highlight, however, that clock frequencies larger than the half clock of 
both microprocessors have similar safe Vmin as in the highest clock frequency, and 
frequencies smaller than the half clock have similar safe Vmin as in the half clock. 
The reason is that both microprocessors support clock skipping and clock division, 
which, in combination, set the effective frequency of the PMD relative to its clock 
source, as shown in Table  6. Clock ratios greater or less than 1/2 on the input clock 
are implemented via clock skipping on the input clock. Clock ratio equal to 1/2 is 
naturally implemented via clock division on the input clock. For this reason, we do 
not present any results for the intermediate frequencies because they provide exactly 
the same Vmin points.
Exceptionally, for X-Gene 2 only, we also present results at 0.9GHz, in which we 
noticed a significant reduction of the Vmin, and thus, much larger energy savings 
compared to 1.2GHz, with minimal impact on performance. The reason is that these 
micro-servers implement the most recent CPPC (Collaborative Processor 
Performance Control) power and performance management specification of ACPI 
5.1 [26], as shown in Fig.  7.
CPPC is a new way to control the performance of cores using an abstract con -
tinuous scale in frequency, instead of a discretized P-state scale (as in legacy ACPI). 
Therefore, during runtime, when there is a request for 1.2GHz (desired frequency), 
in practice the actual frequency of the microprocessor is scaled below and above the 
1.2GHz (frequency reduction tolerance), so that it effectively provides an average 
frequency of 1.2GHz. As a result, the actual frequency properties are limited by the 
highest frequency setting being used, which, in this case, is above half (without 
clock division). This is a frequency interleaving strategy which is provided by the 
CPPC and cannot be changed by software. Although X-Gene 3 operates also with 
CPPC specification, we did not observe the same behavior below the 1.5GHz as in 
X-Gene 2. This is an interesting finding of the characterization part of this work, and 
Table 6  Frequency scaling in X-Gene microprocessors
Clock Ratio Clock Skipping Clock Division X-Gene 2 Frequency X-Gene 3 Frequency
8/8 Yes No 2.4 GHz 3.0 GHz
7/8 2.1 GHz 2.625 GHz
6/8 1.8 GHz 2.250 GHz
5/8 1.5 GHz 1.875 GHz
4/8 No Yes 1.2 GHz 1.5 GHz
3/8 Yes Yes 0.9 GHz 1.125 GHz
2/8 0.6 GHz 0.750 GHz
1/8 0.3 GHz 0.375 GHzHarnessing Voltage margins for Balanced Energy and Performance68
1PMD0
Core 0CorePMD1
Core 2Core 3PMD2
Core 4Core 5PMD3
Core 6Core 7
PMD0
Core 0C ore1PMD1
Core 2C ore3PMD2
Core 4C ore5PMD3
Core 6C ore74Tspreaded
4Tclustere d
Fig. 8  Four running threads in two different core allocation configurations: spreaded & clustered
Maximu m
FrequencyDesire d
Frequenc y
Frequenc y
ReductionToleranceMinimu m
Frequenc yFrequenc yAllowed Range
Fig. 7  Power and performance management in X-Gene microprocessors  – the Collaborative 
Processor Performance Control
thus, we report experiments in X-Gene 2 for three different frequencies that repre -
sent all different behaviors: 2.4GHz, 1.2GHz, and 0.9GHz and, in X-Gene 3, we 
report our experiments at 3GHz and 1.5GHz.
Furthermore, for the needs of our core-allocation analysis, when we execute 
multicore experiments with the number of threads being less than the available 
cores of each chip, we characterize different combinations of core allocation, and 
we have two main categories: spreaded  threads and clustered  threads. As shown in 
Fig.  8, spreaded thread configuration refers to the threads running in the separate G. Papadimitriou and D. Gizopoulos69
PMDs each, while the clustered thread configuration, to the threads running in con -
secutive cores (both cores of the PMD are occupied).
4.2  Voltage Margins Identification
This part focuses on a quantitative analysis of the safe Vmin for two micro-servers of 
the same architecture in order to expose the potential guard bands of each chip, as 
well as to quantify the factors that determine the Vmin of multicore executions.
4.2.1  Exposing Safe Vmin Values
We experimentally obtain the safe Vmin values on the two different technology 
micro-servers: X-Gene 2 and X-Gene 3. For all of our experiments, we consider a 
voltage level as a safe Vmin if the program passes it 1000 times. Safe Vmin is the mini -
mal working voltage. Note that we also study the error behavior for each program 
operating below its safe Vmin point, but we run it 60 times for each configuration 
(frequency, core allocation, and thread scaling) through the entire voltage range 
from the safe Vmin until the system crash point.
Figures 9 and 10 show the Vmin characterization results for the 25 benchmarks on 
X-Gene 2 and X-Gene 3, respectively. The reported Vmin for each program is the 
lowest (safe) voltage setting where all 1000 executions of each program completed 
successfully, without hardware errors notification, program output mismatches 
(silent data corruptions  – SDCs), or other abnormal behavior, such as a process 
timeout, system crash, or thread hang. Figure  9 presents the 8-thread, 4-thread, and 
2-thread executions of the benchmarks in X-Gene 2 for the three different frequen -
cies: 2.4GHz, 1.2GHz and 0.9GHz. Figure  10 presents the Vmin results for the same 
benchmarks on X-Gene 3 with 32, 16, and 8-thread executions for 3GHz and 
1.5GHz. Figure  9 and 10 clearly show, that for the same number of threads and at 
the same frequency, the safe Vmin for all 25 benchmarks is virtually the same. There 
are some cases, where a benchmark has a little lower Vmin than the rest; however, the 
maximum difference is only 10 mV or ~ 1% of the nominal voltage.
A major finding is that, in multicore executions, the safe Vmin marginally depends 
on the workload (different program), but heavily depends on the number of active 
cores and the frequency, as shown in Figs.  9 and 10. We discuss the reasons for these 
differences in Sect. 4.3.
4.2.2  Unsafe Region Investigation
As we described in the previous subsection, in multicore executions, the safe Vmin 
for every program is virtually the same for the same frequency and number of 
threads. Interestingly, we can notice such a behavior across different benchmarks, Harnessing Voltage margins for Balanced Energy and Performance70
Fig. 9  The complete V min characterization results. This graph presents the X-Gene 2 safe V min 
points for all benchmarks with 8 threads, 4 spreaded and clustered threads, and 2 clustered threads 
in 2.4 GHz, 1.2 GHz, and 0.9 GHz clock frequenciesG. Papadimitriou and D. Gizopoulos71
Fig. 10  The complete V min characterization results. This graph presents the X-Gene 3 safe V min 
points for all benchmarks with 32 threads, 16 spreaded and clustered threads, and 8 clustered 
threads in 3.0 GHz, and 1.5 GHz clock frequenciesHarnessing Voltage margins for Balanced Energy and Performance72
Fig. 11  Probability of Failure (pfail) in all voltage levels from nominal level down to the levels of 
complete failure for different frequency, core allocation, and thread scaling options
frequencies, and number of active threads also in the region below the safe Vmin (the 
unsafe region, where at least one run faces an abnormal behavior). Figure  11 shows 
the cumulative probability of failure ( pfail) at each voltage level below the Vmin (the 
Vmin is the last voltage step with pfail = 0). Each line of the graph corresponds to the 
average pfail of the 25 benchmarks in two different core allocation and thread scal -
ing options. Similar to Figs.  9 and 10, for the same configuration (number of threads 
and frequency), the workloads have very small differences on the safe Vmin; how -
ever, for different core allocation options, we observe different behavior in Vmin. The 
same pattern appears also in the unsafe region.
Consider for example the configurations with “max threads” and with spreaded 
“half threads” at maximum frequency (8 T @ 2.4GHz and 4 T (spreaded) @ 2.4GHz 
in X-Gene 2, and 32 T @ 3GHz and 16 T (spreaded) @ 3GHz in X-Gene 3). As 
shown in Fig.  11, these two lines of the graph are virtually identical and have the 
most severe behavior ( pfail = 100% means that all identical executions failed to 
complete. On the other hand, a pfail = 10% means that there are 90% chances for an G. Papadimitriou and D. Gizopoulos73
application to execute correctly in that voltage). On the contrary, if we change the 
core allocation of “half threads,” we notice that the behavior changes significantly. 
Consider now, the “clustered” core allocation option (8 T @ 2.4GHz and 4 T (clus -
tered) @ 2.4GHz in X-Gene 2, and 32 T @ 3GHz and 16 T (clustered) @ 3GHz in 
X-Gene 3). Although the frequencies between “max threads” and “half threads” are 
the same, the pfail (and also the safe Vmin) are very different (“half threads” configu -
ration has lower safe Vmin and pfail than “max threads”) because the core allocation 
was changed. We demonstrate in Sect. 4.3. how this observation is correlated to the 
voltage droop magnitude.
Based on this experimentation, we conclude that the dominant factors which can 
affect the safe Vmin in multicore executions and also the failure probability, are (a) 
the frequency, and (b) the core allocation. The workload itself has only a marginal 
impact on the Vmin in multicore execution for the same number of threads in different 
frequencies (see Figs.  9 and 10); however, we can see that lower frequencies have 
significantly lower safe Vmin (and pfail, respectively) for all the benchmarks than at 
the maximum frequency. For the same number of threads, we also notice that, by 
reducing the frequency to the “half speed,” we can further decrease the operating 
voltage by approximately 3%, while the 0.9GHz runs show a significant reduction 
(approximately 15%) at the Vmin due to the clock division that is activated at that 
frequency. Moreover, using a different core allocation strategy ( clustered  or 
spreaded ) for the same number of threads, we can achieve further voltage reduc -
tion by 4%.
4.3  Analysis of  Vmin Impact Factors
In this section, we study the correlation of the contributors in Vmin presented before, 
and identify the best combination of workload characteristics, frequency, and core 
allocation towards the highest energy efficiency. We also discuss how these findings 
can be exploited in runtime.
4.3.1  Impact of Frequency and Core Allocation on Safe Vmin
Apart from the reduced frequency, in which the operating voltage can be decreased, 
the second major factor that can reduce the safe Vmin is the core allocation. We show 
that the Vmin variation among different workloads (multi-threaded or multiple copies 
of a single-threaded application) fades away as the number of running threads 
increases, and the remaining key factors that determine the Vmin are the frequency 
and core allocation. It is known that the larger the number of threads running in the 
microprocessor, the more noise is generated from voltage droops and the interfer -
ence of running processes in the system. The findings of our analysis quantify the 
magnitude of this dependence.Harnessing Voltage margins for Balanced Energy and Performance74
To understand this phenomenon, we study the voltage droop magnitude of the 
microprocessors for all the different frequency and core allocation configurations 
by leveraging the embedded oscilloscope in the X-Gene 3 microprocessor. The 
X-Gene 3 microprocessor consists of a V oltage Droop Mitigation logic (VDM) 
which is responsible (if enabled) to detect and respond to occasional bad voltage 
droop events. The response upon detecting unusually large supply voltage droops is 
to temporarily reduce the clock frequency by using the same signals that are used 
for normal clock division and skipping. The goal in adding this feature is to be able 
to reduce the frequency/voltage guardband need to avoid failure. The VDM has two 
states: first, it detects the supply droop magnitude, and second, when the voltage 
droop exceeds a specified threshold, the VDM starts the mitigation sequence to 
avoid the failure. However, the mitigation sequence can be disabled. For this study, 
we have disabled the mitigation sequence, while the voltage droop detection remains 
enabled. With such a way, we are able to set different voltage thresholds in the VDM 
in order to count the voltage droop magnitude for each benchmark. PMU 
(Performance Monitoring Unit) counters, which can be accessed by the Perf tool 
[27], are located in the microprocessor and can be used to monitor the frequency 
and the magnitude of voltage droop events.
Figure 12 presents two different ranges of voltage droop magnitude when the 
microprocessor operates at 3GHz: (a) 55 mV and 65 mV , in which we present the 
configurations of all programs that produce voltage droops more than or equal to 
55  mV and less than 65  mV (this voltage range actually corresponds to thresh -
old = 5 of the VDM), and (b) 45 mV and 55 mV , in which we present the configura -
tions of all programs that produce voltage droops more than or equal to 45 mV and 
less than 55 mV (this voltage range actually corresponds to threshold = 4 of the 
VDM). Both graphs show the total number of droop detections per 1 M cycles for 
each program. As we can see in the left-hand graph of Fig.  12, the configurations 
with 32 threads and 16 spreaded threads, which means that all 16 PMDs of the 
microprocessor running at 3GHz frequency (note that the frequency can be changed 
per pair of cores – PMD) produce voltage droop magnitude between 55 mV and 
65 mV . However, the configuration of 16 clustered threads (meaning 8 PMDs of the 
microprocessor running at 3GHz) has almost zero droops in the range of [55 mV , 
65 mV) for all programs. On the other hand, in the right-hand graph of Fig.  12, the 
configurations with 16 clustered threads and 8 spreaded threads (8 PMDs operate at 
3GHz frequency in both configurations) produce voltage droop magnitude in the 
range of [45 mV , 55 mV). However, the configuration with 8 clustered threads (4 
PMDs) has almost zero droops in that range for any program.
4.3.2  Impact of the Workload on Frequency and Core Allocation
Apart from the magnitude of the correlation of the safe Vmin to frequency and core 
allocation, we also quantify the impact of the workload class (CPU-intensive vs. 
memory-intensive) to: (a) the safe Vmin, (b) the workloads’ performance, and (c) the 
energy consumption. Figure  13 shows an illustrative example of the difference in G. Papadimitriou and D. Gizopoulos75
Fig. 12  V oltage droop detections for each program in two different voltage droop magnitudes. The 
top graph presents the droop detections in the range between 55 mV and 65 mV , and the bottom 
graph presents the droop detections in the range between 45 mV and 55 mVHarnessing Voltage margins for Balanced Energy and Performance76
-9.6%
-7.5%-7.5%
-6.7%-5.9%
-5.7%-4.5%
-4.4%-4.0%
-3.7%-3.6%
-3.3%-3.2%
-2.3%-1.7%
-1.4%-1.3%0.0%
0.7%1.4%4.4%6.6%10.1%13.0%14.2%
-15%-12%-9%-6%-3%0%3%6%9%12%15%18%
02004006008001000120014001600
bodytrack
IS
EP
hmmer
LU
cactusADM
namd
zeusmp
h264ref
swaptions
MG
gromacs
bwaves
gcc
blackscholes
dealII
bzip2
fluidanimate
leslie3d
mcf
canneal
milc
CG
FT
dedup
Difference(%)Energy(J)
Benchmarks4T(Clustered)
 4T (Spreaded)
Difference
Fig. 13  Energy of all benchmarks for two different core allocations. The benchmarks on the left 
of the dashed line are CPU-intensive, while the ones on the right are memory-intensive
energy of all 25 programs when running at the maximum frequency, with the same 
number of threads (4 threads in this case) but in different core allocations (4 T clus -
tered vs. 4 T spreaded) on the X-Gene 2 (the observation is similar in X-Gene 3). 
The red line with numbers at the top of each pair of bars shows the energy consump -
tion difference between the two core allocations. We can see that the energy differ -
ence between these two configurations varies from −9.6% to 14.2%, depending on 
the characteristics of each workload. Negative percentages indicate that the 
spreaded- thread configuration needs higher energy than the clustered-thread con -
figuration, while positive percentages indicate the opposite. In particular, the bench -
marks shown at the right-hand side of the dashed line have better energy when their 
threads are spreaded across the cores, unlike the benchmarks shown at the left-hand 
side of the vertical dashed line, which are more energy efficient when executed in 
consecutive cores (clustered configuration, see Fig.  8). The reason is that the right -
most benchmarks are the most memory-intensive benchmarks, while the leftmost 
benchmarks are the most CPU-intensive.
As the previous sections present, frequency reduction and the optimal core allo -
cation can enable significant opportunities of lowering supply voltage; however, 
reduced frequency also translates to degraded performance. Reduced frequency in 
CPU cores impacts their performance without affecting the lower memory levels 
(L3 cache and DRAM). This means that a program that is highly computational 
(CPU-intensive) will be proportionally affected by the reduced frequency. On the 
other hand, a program that experiences long stalls waiting for the memory to respond 
will be less affected from the core frequency reduction, as this will be hidden by the 
long memory delays (memory-intensive programs). The key difference between 
them is that in CPU-intensive programs the system part that acts as a performance 
limiter is the CPU core part (pipeline, L1, and L2 caches) while for the memory 
intensive programs, it is the memory part (L3 and DRAM).G. Papadimitriou and D. Gizopoulos77
In practice, we can use this property to combine lower frequency and lower volt -
age with small performance difference for memory intensive workloads, to increase 
their energy efficiency while still complying with high performance constraints. 
Previous studies (e.g., [ 28] and [ 29]), have exposed the phases of a program which 
are memory-intensive, and have proposed this feature as a proxy in a single-core 
microprocessor to guide the DVFS to reduce the frequency in those specific pro -
gram phases. In this work, we further extend this practice to show how this property 
can be also used to guide core allocation decisions and how this feature impacts the 
microprocessor’s energy and program’s performance. In order to identify the class 
of each program, we follow the method proposed in [ 30] to track the access rates of 
the lower memory hierarchy and, more precisely, L3 cache accesses. High L3 access 
rate means high memory activity in the lower memory hierarchy (memory-intensive 
program). To find the exact threshold level that separates the two classes, we ini -
tially identify what programs are memory intensive and then the L3 access rates of 
these benchmarks.
Figure 14 presents the performance impact of each program when we introduce 
contention on the shared CPU resources. We do that by executing multiple copies of 
the same program on all cores. Programs that are affected the most have high activ -
ity on the shared resources (and thus the contention negatively impacts the perfor -
mance). As an example, we can see the CG and FT, which are the most 
memory-intensive benchmarks because their execution time is significantly reduced 
in a multi-threaded execution compared to the single-threaded one (ratio is much 
smaller than 1) due to the high contention at the memory system. On the other hand, 
the namd  and EP are the most CPU-intensive benchmarks because their execution 
time in multi-threaded execution is virtually the same as in the single-threaded exe -
cution (ratio is very close to 1).
We then use the performance monitoring counters of the microprocessor as an 
indication of the workload class (CPU vs. memory intensive). In particular, we use 
the L3 Cache (L3C) memory access rate (by monitoring the L2 miss counters), 
Fig. 14  Relative performance of all benchmarks. The y-axis presents the ratio of the execution 
time of one instance of the single-threaded execution divided by the execution time of multiple 
instancesHarnessing Voltage margins for Balanced Energy and Performance78
Fig. 15  L3 Cache access rate per 1 M cycles for the 25 benchmarks and the three threading con -
figurations (32, 16 and 8 threads)
which will allow us to identify the class of each program during runtime. Figure  15 
shows the L3 Cache access rate for the three different threading configurations of 
32, 16, and 8 threads (for this example, we used the X-Gene 3 platform; the same 
behavior occurs in X-Gene 2 also), measured at 3GHz clock frequency. Based on 
the L3C access rate metric, and also on the experimental analysis of the safe Vmin, 
we found that the threshold which defines the high memory activity is 3 K accesses 
per 106  cycles. Executions above this threshold are the most memory-intensive, 
while those below the threshold are the most CPU-intensive.
4.4  Performance and Energy Trade-Offs
4.4.1  Energy Efficiency
Figure 16 shows the energy consumption for six benchmarks (all other benchmarks 
follow the same pattern as the ones presented) and all configurations of X-Gene 2 
and X-Gene 3 systems. The benchmarks are sorted (from left to right) from the most 
CPU-intensive to the most memory-intensive ones, as it is also shown in Fig.  14 
(namd , dealII , and EP are the most CPU-intensive benchmarks, while milc, CG, and 
FT are the most memory-intensive ones).
X-Gene 2 reports significant energy savings for all cases when running at 
0.9GHz, which is attributed to the significantly lower safe Vmin voltage (as shown in 
Figs.  9 and 10, which is possible due to the clock division) and the lower frequency. 
For CPU-intensive benchmarks ( namd , dealII , and EP), the frequency reduction 
(from 2.4GHz to 1.2GHz) does not have an observable impact on the total energy 
consumption. We can only notice energy improvements for 1.2GHz on the memory-
intensive applications ( milc, CG, and FT). Lower CPU speed reduces the perfor -
mance gap between CPU and memory and leads to a more balanced system. This is 
why we can gain significant power savings without sacrificing too much perfor -
mance, and thus achieve better energy efficiency.G. Papadimitriou and D. Gizopoulos79
Fig. 16  Energy consumption in Joules for 8, 4, and 2 threading options for three different frequen -
cies (2.4GHz, 1.2GHz, 0.9GHz) of X-Gene 2 (top) and 32, 16, and 8 threading options for two 
different frequencies (3GHz, 1.5GHz) of X-Gene 3 (bottom)
Similar observations hold for X-Gene 3. The low-frequency behavior of X-Gene 
3 (1.5GHz) matches the one of the 1.2GHz of X-Gene 2. For the CPU-intensive 
benchmarks, the highest frequency (3GHz) gives the best energy (due to faster exe -
cution), but again we can see that memory-intensive applications are more energy 
efficient in lower frequency. Note that, in X-Gene 3, we do not present results for 
frequencies lower than 1.5GHz, because, as we have described in Sect 4.1, fre-
quency settings bellow 1.5GHz have the same safe Vmin as in 1.5GHz due to clock 
skipping, and thus, there is only performance impact.Harnessing Voltage margins for Balanced Energy and Performance80
4.4.2  Combined Energy and Performance Considerations
Energy consumption itself is a valuable metric that directly translates to cost; how -
ever, it occasionally implies very slow system configurations (very low frequen -
cies), which could violate latency and throughput requirements on a server 
environment. To avoid this bias in the comparisons of different configurations, other 
metrics such as the energy delay product ( EDP=ExD) and the energy-delay 
squared product ( ED2P = E x D2) have been proposed for high-end systems [ 31]. 
Given that our work focuses on server-grade CPUs, we have chosen to present the 
energy delay squared product (ED2P) for all of our experiments to show a fair com -
parison among benchmarks between different microprocessors and, more impor -
tantly, to provide a fair representation of the relation between energy and 
performance. In this subsection, we focus on this metric for the comparison of the 
different X-Gene 2 and X-Gene 3 configurations.
Figure 17 shows the ED2P for 2.4GHz, 1.2GHz and 0.9GHz configurations for 
the X-Gene 2, and 3GHz and 1.5GHz configurations for the X-Gene 3. In the first 
three benchmarks ( namd , dealII , and EP) in both X-Gene 2 and X-Gene 3, which are 
the most CPU-intensive benchmarks, we can see that the higher the frequency, the 
more efficient (in terms of ED2P) the configuration (i.e., the blue lines in X-Gene 2 
and green lines in X-Gene 3 are always lower in all cases). However, we can see that 
the trend lines are totally different among the three memory-intensive benchmarks 
(milc, CG, and FT) and the CPU-intensive ones. We can see that the frequency is 
inversely proportional to ED2P efficiency for all the thread-scaling options (grey 
lines vs. blue lines in X-Gene 2, and blue lines vs. green lines in X-Gene 3). Our 
analysis shows that the identification of the program class (CPU vs. memory-  
intensive) in runtime can guide the selection of the optimal system configuration 
(frequency and threading) to achieve high energy savings without compromising 
performance.
4.5  Mitigating Energy: A Real System Implementation
4.5.1  Online Monitoring Daemon
According to our study and observations, we developed a simple online monitoring 
daemon which encapsulates all these conditions and constraints as we described in 
previous sections and guides process placement, core frequency, and supply voltage 
to achieve higher energy savings on both X-Gene 2 and X-Gene 3 platforms. The 
daemon has two main functionalities: monitoring and placement.
The monitoring part of the daemon acts as a watchdog, which periodically moni -
tors the utilized PMDs (which correspond to the droop magnitude shown in Table  7) 
and the L3C accesses of each running process (except for the system processes). For 
each process, it counts the L3C accesses during 1 M cycles (this actually varies 
from 300 ms to 500 ms in our systems; it depends on the IPC rate of each process) G. Papadimitriou and D. Gizopoulos81
Fig. 17  Energy Delay Squared Product (ED2P) for 8, 4, and 2 threading options for three different 
frequencies (2.4GHz, 1.2GHz, 0.9GHz) of X-Gene 2 (top) and 32, 16, and 8 threading options for 
two different frequencies (3GHz, 1.5GHz) of X-Gene 3 (bottom)
Table 7  Correlation of voltage droops magnitude with frequency and core allocation ( Vmin 
columns concern X-Gene 3)
Droop Magnitude Utilized PMDs Thread Scaling Vmin @ 3GHz Vmin @ 1.5GHz
[25 mV , 35 mV) 1, 2 PMDs 1 T, 2 T, 780 mV 770 mV
4 T (clustered)
[35 mV , 45 mV) 4 PMDs 8 T (clustered), 800 mV 780 mV
4 T (spreaded)
[45 mV , 55 mV) 8 PMDs 16 T (clustered), 810 mV 790 mV
8 T (spreaded)
[55 mV , 65 mV) 16 PMDs 32 T, 16 T (spreaded) 830 mV 820 mVHarnessing Voltage margins for Balanced Energy and Performance82
Fig. 18  Placement flow chart
and if the L3C accesses are more than 3 K (see Fig.  15), then it classifies this pro -
cess as memory-intensive, otherwise it classifies it as CPU-intensive. Moreover, it 
classifies the processes according to the utilized PMDs in order to estimate the cur -
rent Vmin.
The second part (Fig.  18) of the daemon is the Placement. Figure  18 presents 
how the system reacts to a process list change and how the placement function of 
the daemon operates. Placement is the part that places the processes to the CPU 
cores (or migrate them) and adjusts the voltage and frequencies accordingly. As we G. Papadimitriou and D. Gizopoulos83
presented earlier, each group of core allocation options corresponds to a specific 
droop magnitude class (Table  7) with a distinct safe Vmin for each frequency. 
Moreover, as shown in Fig.  12, for each core allocation option, all programs pro -
duce the same maximum droop magnitude. Given that we do not use any sophisti -
cated mechanism for predicting the safe Vmin because the prediction schemes for Vmin 
that have been proposed in the literature are error-prone (e.g., [ 31–35]) and can lead 
to system failures in real microprocessors.
To this end, the daemon has been equipped with a fail-safe mechanism as shown 
in Fig.  19: either before the process(es) are invoked or before the frequency should 
be increased in one or more PMDs (i.e., a CPU-bound process), the daemon first 
increases the voltage to the next safe Vmin level (see Table  7) and then, if the voltage 
can be decreased according to utilized PMDs (this information is provided by the 
monitoring part), the daemon will set the voltage accordingly (as shown in Fig.  18). 
By following this policy, there is a minimal increase of the total power consump -
tion; however, this guarantees the reliable execution on a real system. The Placement 
Process Handling
(Fail-Safe Mechanism)
Start Process
List Change
Set Safe Vmin
FINISH OF
CHANGE
STATEProcess
State ?
START
CPU-Intensive
Process List
Call Placement ()
End Process
List Change
Fig. 19  Process handling (Fail-Safe Mechanism)Harnessing Voltage margins for Balanced Energy and Performance84
part uses the classification performed by the Monitoring part to guide its decisions 
and is invoked upon every process list or classification change.
The online monitoring daemon is minimally intrusive and has no impact on the 
safe Vmin. Its performance overhead is also negligible, as it is only running periodi -
cally to read the performance counters and upon every process list change, to invoke 
the placement process, which has equal impact as a process migration of the Linux 
kernel. The daemon is invoked only after
 (a) Either a new process is issued to the system or when a process finishes its exe -
cution (to check if a process migration is required), or.
 (b) Wwhen a process changes its state (from CPU-intensive to the memory- 
intensive and vice versa).
In case (a), it reads the process mapping table and estimates the result, and in 
case (b), it periodically counts the L3C accesses (as we described in Sect. 4.3.1 ). 
Note that, in case (b), the utilized PMDs cannot be changed. Utilized PMDs can 
only be changed when a new process is invoked or when a process finishes its 
execution.
To count the L3C accesses, we leverage the built-in performance monitoring 
counters of the microprocessors. To do so, we developed a kernel module able to 
provide access to the performance counters from user-space in order to be part of 
our online monitoring daemon. It is lightweight (thus fast) because it acts in the 
kernel space and provides near zero overhead to the total microprocessor’s opera -
tion. We did not use tools like Perf [ 27] or PAPI [ 36] because these tools impose an 
extra overhead in measurements (± 3%), while we need very accurate values to take 
correct decisions. To count the L3C accesses, only one read of one PMU counter 
and one read of the same register after 1 M cycles are required. Afterwards, the 
kernel module subtracts these two values to produce the final result.
4.5.2  Evaluation Results
For the purposes of our experimental evaluation, we also developed a “workload 
generator” which creates a typical server workload from a “pool” of programs 
(which includes all the 29 SPEC CPU2006 and the 6 NPB benchmarks; in total 35 
different programs). The generator can generate workloads of configurable duration 
by randomly selecting benchmarks from this pool and randomly defining the times -
lot in which each benchmark will be invoked. The workload includes heavy load 
periods, average load periods, and light periods, including also a few idle periods, 
resembling a typical server workload on a given time window. The generator is con -
figured to guarantee that the number of active processes is never more than the avail -
able cores of the microprocessor. The generated workload can be then invoked 
multiple times, allowing multiple experiments under the same load conditions, using 
different policies or configurations. Our exploration has revealed potential energy 
efficiency improvements by adjusting the CPU voltage, frequency, and the core allo -
cation. In this subsection, we present the results for a simple realistic G. Papadimitriou and D. Gizopoulos85
experimentation we performed in our two systems. To provide detailed results for a 
comprehensive analysis, we ran four different configurations for the same workload 
sequence:
• Baseline : In this configuration, we run the workload sequence with the default 
microprocessor’s frequency scaling and scheduler settings (on demand governor 
is enabled).
• Safe Vmin: In this configuration, we change the nominal voltage of the micropro -
cessor to the safe Vmin, according to Table  7, (again with on demand governor 
enabled). With this configuration, we evaluate the impact of pessimistic voltage 
guard bands in energy.
• Placement : In this configuration, we run the simple online monitoring daemon 
to perform the proposed frequency and core allocation (the on demand governor 
is now disabled), but keeping the voltage at its nominal value. With this configu -
ration we evaluate the impact of the proposed frequency and core allocation 
options in energy and performance.
• Optimal : In this configuration, we run the simple online monitoring daemon and 
adapting the voltage and frequency settings (with on demand governor disabled). 
This is the best scenario which integrates all the conditions targeting the best 
energy efficiency.
Figures 20 and 21 show the average power for a 1-hour execution of randomly 
generated workload for the default microprocessor’s settings (Baseline) and the 
Optimal scheme, for X-Gene 2 and X-Gene 3, respectively. We generated two work -
loads, one for X-Gene 2 using a maximum constraint of 8 cores, and one for X-Gene 
3 with a constraint of 32 cores. Each workload was executed under both Baseline 
and Optimal settings.
In Figs.  20 and 21 that compare the Baseline and the Optimal configuration, we 
can see that the average power consumption for the optimal configuration compared 
to the default one (for the same 1-hour workload) is significantly reduced for both 
microprocessors. As presented in these figures, the system has a load with phases of 
high utilization and others with low utilization, resembling a typical server Fig. 20  Average power for the Baseline and the Optimal configurations in X-Gene 2 during 1-hour 
executionHarnessing Voltage margins for Balanced Energy and Performance86
Table 8  X-Gene 2 results for the four configurations
Baseline Safe Vmin Placement Optimal
Time (s) 3707 3707 3829 3829
Avg. power (W) 6.90 6.10 5.46 5.00
Energy (J) 25578.30 22612.07 20906.34 19145.00
Energy savings – 11.6% 18.3% 25.2%
ED2P (workload) 351 × 109311 × 109307 × 109281 × 109
ED2P savings – 11.6% 12.8% 20.1%
Table 9  X-Gene 3 results for the four configurations
Baseline Safe Vmin Placement Optimal
Time (s) 3748 3748 3846 3846
Avg. power (W) 36.49 32.51 30.78 27.63
Energy (J) 136773.26 121847.48 118379.88 106283.56
Energy savings – 10.9% 13.4% 22.3%
ED2P (workload) 19 × 101117 × 101117 × 101115 × 1011
ED2P savings – 10.9% 8.9% 18.2%Fig. 21  Average power for the Baseline and the Optimal configurations in X-Gene 3 during 1-hour 
execution
workload. We can see that some peaks reach the maximum capability of the system, 
indicating that the system was occasionally pushed towards its limits.
In Tables 8 and 9, we compare the four different configurations described above. 
We can see a significant reduction in the total energy: 25.2% in X-Gene 2 (Table  8) 
and 22.3% in X-Gene 3 (Table  9). This was achieved without compromising the 
performance of CPU-intensive programs, while slightly reducing the performance 
of memory-intensive programs, as described in Sect. 4.2, targeting the highest 
ED2P efficiency. The use of ED2P metric guarantees that the selected policies do 
not violate high performance constraints and at the same time, significantly reduce 
the energy consumption.G. Papadimitriou and D. Gizopoulos87
The Optimal scheme with the online monitoring daemon did also slightly shift 
the completion time of the workload as a result of the small performance impact on 
some memory-intensive programs (as described in Sect. 4.2). The total time was 
shifted by 3.2% in X-Gene 2 and 2.5% in X-Gene 3. We can also notice in Tables 8 
and 9 that the frequency and core allocation options (Placement) are the major con -
tributors on the total energy reduction (18.3% in X-Gene 2 and 13.4% in X-Gene 3), 
compared to the voltage reduction with the on demand  governor (Safe Vmin). In any 
case, the Optimal scheme can achieve more than 22% energy savings with a mini -
mal performance penalty.
5  Conclusions
In this chapter, we first presented a detailed system-level voltage scaling character -
ization study for single-core executions in ARMv8-based multicore CPUs manufac -
tured in 28 nm. Towards the formalization of the behavior in undervolting conditions, 
we also presented a simple consolidated function; the Severity function , which 
aggregates the effects of reduced voltage operation in the cores of a multicore CPU 
by assigning values to the different abnormal observations. Moreover, we presented 
a detailed system-level voltage scaling characterization study for multicore execu -
tions in two recent ARMv8-based multicore CPUs manufactured in 28  nm and 
16 nm. Based on this study, we discussed several important observations and pre -
sented a new software-based scheme for these server-grade machines, which con -
siders all the diverse aspects of the increased energy consumption and provides 
large energy savings while maintaining high performance levels. Particularly, we 
showed that the proposed software scheme can achieve on average 25.2% energy 
savings on X-Gene 2, and 22.3% energy savings on X-Gene 3, with a minimal per -
formance penalty of 3.2% on X-Gene 2 and 2.5% on X-Gene 3 compared to the 
default voltage and frequency microprocessor’s conditions.
Summarizing the outcomes of our analysis, we can conclude that future research 
should focus on building microarchitectural solutions which will be able to mitigate 
the emergency voltage conditions that lead the microprocessor designers to overes -
timate the voltage margins. Moreover, another interesting future direction will be a 
comprehensive comparison among different architectures and microarchitectures of 
heterogeneous platforms regarding the voltage reduction tolerance for different 
types of applications. Such a study could be very useful, especially on large data 
centers that use heterogeneous distributed computing systems that consist of con -
temporary general-purpose processors (CPUs), general-purpose graphic processing 
units (GP-GPUs), and field-programmable gate arrays (FPGAs). The utilization of 
heterogeneous architectures poses several challenges primarily due to increased 
power consumption, and thus, voltage reduction could deliver significant energy 
savings.Harnessing Voltage margins for Balanced Energy and Performance88
References
 1. F.  Salehuddin, I.  Ahmad, F.A.  Hamid, A.  Zaharim, A.  Maheran, A.  Hamid, P.S.  Menon, 
H.A. Elgomati, B.Y . Majlis, Optimization of process parameter variation in 45nm p-channel 
MOSFET using L18 Orthogonal Array, In Proceedings of IEEE International Conference on 
Semiconductor Electronic (ICSE’12), 2012
 2. W. Schemmert, G. Zimmer, Threshold-voltage sensitivity of ion-implanted m.o.s. transistors 
due to process variations. Electron. Lett. 10(9), 151 (1974)
 3. V .J. Reddi, S. Kanev, W. Kim, S. Campanoni, M.D. Smith, G.-Y . Wei, D. Brooks, V oltage 
smoothing: Characterizing and mitigating voltage noise in production processors via software- 
guided thread scheduling, in 2010 43rd Annual IEEE/ACM International Symposium on 
Microarchitecture, 2010
 4. N. James, P. Restle, J. Friedrich, B. Huott, B. McCredie, Comparison of split-versus connected- 
core supplies in the POWER6 microprocessor, in 2007 IEEE International Solid-State Circuits 
Conference. Digest of Technical Papers, 2007
 5. E. Le Sueur and G. Heiser, Dynamic voltage and frequency scaling: the laws of diminishing 
returns, In Proceedings of the 2010 International Conference on Power Aware Computing and 
Systems (HotPower’10). USENIX Association, Berkeley, CA, USA, 2010
 6. C.R. Lefurgy, A.J. Drake, M.S. Floyd, M.S. Allen-Ware, B. Brock, J.A. Tierno, J.B. Carter, 
Active management of timing guardband to save energy in POWER7,” in Proceedings 
of the 44th Annual IEEE/ACM International Symposium on Microarchitecture  – 
MICRO-44’11, 2011
 7. A. Bacha and R. Teodorescu, Dynamic reduction of voltage margins by leveraging on-chip 
ECC in Itanium II processors, in Proceedings of the 40th Annual International Symposium on 
Computer Architecture – ISCA’13, 2013
 8. A.  Bacha and R.  Teodorescu, Using ECC Feedback to Guide V oltage Speculation in 
Low-V oltage Processors, in 2014 47th Annual IEEE/ACM International Symposium on 
Microarchitecture, 2014
 9. J. Leng, A. Buyuktosunoglu, R. Bertran, P. Bose, V .J. Reddi, Safe limits on voltage reduction 
efficiency in GPUs, in Proceedings of the 48th International Symposium on Microarchitecture – 
MICRO-48, 2015
 10. The Linux Kernel Documentation (Parent Directory), Retrieved 2017 from https://www.ker -
nel.org/doc/Documentation
 11. G. Papadimitriou, M. Kaliorakis, A. Chatzidimitriou, D. Gizopoulos, G. Favor, K. Sankaran, 
S. Das, A system-level voltage/frequency scaling characterization framework for multicore 
CPUs, in 13th IEEE Workshop on Silicon Errors in Logic  – System Effects (SELSE’17), 
Boston, MA, USA, 2017
 12. Y .  Zu, C.R.  Lefurgy, J.  Leng, M.  Halpern, M.S.  Floyd, V .J.  Reddi, Adaptive guardband 
scheduling to improve system-level efficiency of the POWER7+, in Proceedings of the 48th 
International Symposium on Microarchitecture – MICRO-48, 2015
 13. G. Papadimitriou, M. Kaliorakis, A. Chatzidimitriou, C. Magdalinos, D. Gizopoulos, V oltage 
margins identification on commercial x86-64 multicore microprocessors, in 2017 IEEE 23rd 
International Symposium on On-Line Testing and Robust System Design (IOLTS), 2017
 14. X.  Fan, W.-D.  Weber, L.A.  Barroso, Power provisioning for a warehouse-sized computer, 
in Proceedings of the 34th Annual International Symposium on Computer Architecture  – 
ISCA’07, 2007. [Online]. Available: https://doi.org/10.1145/1250662.1250665
 15. L.  Gwennap, Performance arms X-gene 3 for cloud, 2017. http://www.linleygroup.com/
uploads/x- gene- 3- for- cloud.pdf . Accessed: 25 July 2018
 16. P. Koutsovasilis, C. Antonopoulos, N. Bellas, S. Lalis, G. Papadimitriou, A. Chatzidimitriou, 
D. Gizopoulos, The impact of CPU voltage margins on power-constrained execution. IEEE 
Trans. Sustain. Comput. (2020). https://doi.org/10.1109/tsusc.2020.3045195
 17. N.H.E. Weste, D.M. Harris, CMOS VLSI Design: A Circuits and Systems Perspective , 4th edn. 
(Addison-Wesley, 2010)G. Papadimitriou and D. Gizopoulos89
 18. K.  Bernstein, K.  Carrig, C.  Durham, P.  Hansen, D.  Hogenmiller, E.  Nowak, N.  Roher, 
High Speed CMOS Design Styles . Springer US, 1999 [Online]. Available: https://doi.
org/10.1007/978- 1- 4615- 5573- 5
 19. J.L. Henning, SPEC CPU2006 benchmark descriptions, ACM SIGARCH Comp. Arch. News. 
34(4), 1–17 (Sep. 2006) [Online]. Available: https://doi.org/10.1145/1186736.1186737
 20. R.  Rao, A.  Srivastava, D.  Blaauw, D.  Sylvester, Statistical estimation of leakage current 
considering inter- and intra-die process variation, in Proceedings of the 2003 International 
Symposium on Low Power Electronics and Design - ISLPED’03, 2003 [Online]. Available: 
https://doi.org/10.1145/871506.871530
 21. R.J. Riedlinger, et al., A 32nm 3.1 billion transistor 12-wide-issue Itanium® processor for 
mission-critical servers, in 2011 IEEE International Solid-State Circuits Conference, 2011 
[Online]. Available: https://doi.org/10.1109/ISSCC.2011.5746230
 22. NAS Parallel Benchmarks Suite, v3.3.1. https://www.nas.nasa.gov/publications/npb.html
 23. C.  Bienia, S.  Kumar, J.P.  Singh, K.  Li, The PARSEC benchmark suite, in Proceedings of 
the 17th International Conference on Parallel Architectures and Compilation Techniques  - 
PACT’08, 2008 [Online]. Available: https://doi.org/10.1145/1454115.1454128
 24. B.  Lepers, V .  Quema, A.  Fedorova, Thread and memory placement on numa systems: 
Asymmetry matters, in Proceedings of the 2015 USENIX Conference on Usenix Annual 
Technical Conference, USENIX ATC’15, (Berkeley, CA, USA), pp.  277–289, USENIX 
Association, 2015
 25. M. Curtis-Maury, Improving the efficiency of parallel applications on multithreaded and mul -
ticore systems. PhD thesis, Virginia Tech, 2008
 26. Advanced Configuration and Power Interface (ACPI) Specification, 2014. https://www.uefi.
org/sites/default/files/resources/ACPI_5_1release.pdf . Accessed 1 Aug 2018
 27. Perf: Linux Profiling with Performance Counters. Retrieved 2017 from https://perf.wiki.ker -
nel.org/index.php/Main_Page
 28. C. Isci, G. Contreras, M. Martonosi, Live, runtime phase monitoring and prediction on real 
systems with application to dynamic power management, in 2006 39th Annual IEEE/ACM 
International Symposium on Microarchitecture (MICRO’06), 2006 [Online]. Available: 
https://doi.org/10.1109/MICRO.2006.30
 29. Q. Wu et al., A dynamic compilation framework for controlling microprocessor energy and 
performance, in 38th Annual IEEE/ACM International Symposium on Microarchitecture 
(MICRO’05) [Online]. Available: https://doi.org/10.1109/MICRO.2005.7
 30. H. Sasaki, A. Buyuktosunoglu, A. Vega, P. Bose, Characterization and mitigation of power 
contention across multiprogrammed workloads, in 2016 IEEE International Symposium 
on Workload Characterization (IISWC), 2016 [Online]. Available: https://doi.org/10.1109/
IISWC.2016.7581266
 31. D.M. Brooks et al., Power-aware microarchitecture: design and modeling challenges for next- 
generation microprocessors. IEEE Micro. 20(6), 26–44 (2000) [Online]. Available: https://doi.
org/10.1109/40.888701
 32. W. Jia, K.A. Shaw and M. Martonosi, Stargazer: Automated regression-based gpu design space 
exploration, in 2012 IEEE International Symposium on Performance Analysis of Systems & 
Software, 2012 [Online]. Available: https://doi.org/10.1109/ISPASS.2012.6189201
 33. P.J. Joseph, K. Vaswani, M.J. Thazhuthaveetil, Construction and use of linear regression mod -
els for processor performance analysis, in The Twelfth International Symposium on High- 
Performance Computer Architecture, 2006. [Online]. Available: https://doi.org/10.1109/
HPCA.2006.1598116
 34. B.C.  Lee, D.M.  Brooks, Accurate and efficient regression modeling for microarchitectural 
performance and power prediction, in Proceedings of the 12th International Conference on 
Architectural Support for Programming Languages and Operating Systems – ASPLOS-XII, 
2006 [Online]. Available: https://doi.org/10.1145/1168857.1168881Harnessing Voltage margins for Balanced Energy and Performance90
 35. A. Biswas, N. Soundararajan, S.S. Mukherjee, S. Gurumurthi, Quantized A VF: A means of 
capturing vulnerability variations over small windows of time, in Silicon Errors in Logic – 
System Effects (SELSE), 2009.
 36. S. Browne, C. Deane, G. Ho, P. Mucci, PAPI: A portable interface to hardware performance 
counters, in Proceedings of Department of Defense HPCMP Users Group Conference, 
June, 1999.G. Papadimitriou and D. Gizopoulos91Exploiting Reduced Voltage Margins: 
From Node- to the Datacenter-level
Panos Koutsovasilis, Christos Kalogirou, Konstantinos Parasyris, 
Christos D. Antonopoulos, Nikolaos Bellas, and Spyros Lalis
1  Introduction
Due to increasing cost, power delivery and cooling constraints, modern datacentres 
and high-performance computing (HPC) systems will be called to operate under a 
tight power budget. This is a challenging undertaking considering that in the past 
decade power and cooling costs have actually doubled [ 1]. In particular, CPUs 
account for up to 60% of the total power consumption of compute nodes [ 2], 
whereas datacentre cooling has a power footprint equal to that of the compute infra -
structure. An important exercise for both the datacentre and HPC domains is to 
design and deploy techniques that optimize power and energy efficiency.
At the same time, aggressive CMOS technology scaling into lower nanometer 
geometries has led to manufacturing variability, and, as a consequence, to larger 
variability of transistor characteristics. Dealing with this variability traditionally 
involves extra provisioning. Besides reducing operating frequencies and adding 
extra error correction circuitry, CPUs are configured to operate at increased voltage 
margins that are specified at design time, considering the implementation technol -
ogy, power budget, worst-case timing paths, operating conditions, and fabrication 
process variations.
The average power overhead of CPU voltage margins can be very significant, in 
the order of 35% [ 3]. Still, most of the time these margins are overly excessive, 
given that those worst-case combinations of adverse conditions may appear only 
rarely or never during the operation of a given CPU. Thus, a holistic approach that 
P. Koutsovasilis · C. Kalogirou · C. D. Antonopoulos ( *) · N. Bellas · S. Lalis 
Department of Electrical and Computer Engineering, University of Thessaly, V olos, Greece
e-mail:  cda@uth.gr  
K. Parasyris 
Lawrence Livermore National Lab, Livermore, CA, USA
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_492
identifies and appropriately reduces CPU voltage margins would translate to signifi -
cantly improved power and energy efficiency of computer systems.
However, as we discussed in Chap. 2, this is not an easy task. Operating the CPU 
at reduced voltage margins in a naive way can compromise the reliability of com -
puter systems, leading to various errors or even complete machine crashes. To make 
the situation even more complex, the width of voltage margins differs between 
CPUs of different families, as well as between different parts of the same family. 
Moreover, we have observed that, on certain CPU architectures, part of the margin 
is workload-dependent and may vary among different workloads, or even during the 
execution life of each workload. Last but not least, modern multicore CPUs often 
execute simultaneously multiple jobs, with diverse characteristics. All this makes 
the exploitation of CPU voltage margins on real-world systems, running realistic 
workloads, a particularly challenging undertaking.
In this chapter, we study the reduction of CPU voltage margins based on the 
premise that they are typically overly pessimistic. At the same time, the aggressive 
reduction of CPU voltage margins may compromise the reliability of the system. 
This mandates an experimental and theoretical study that ultimately answers the 
following questions:
• “How to exploit the potential of reduced voltage margins?”, and
• “Is system reliability affected?”.
We then study the trade-off between the reduced energy cost thanks to operation 
at reduced voltage margins, and the Service Level Agreement (SLA) violation pen -
alties due to node failures that may be caused by such voltage margins reduction at 
the scale of cloud datacentres, to answer the question:
• “Is operation at reduced voltage margins beneficial, in terms of profit, for cloud 
infrastructure providers?”
2  Related Work
A set of techniques have been traditionally used to improve the energy efficiency of 
cloud datacentres.
For example, VM consolidation focuses on mapping VMs to as few nodes as 
possible, in order to switch unused nodes to a sleep state. However, consolidation 
may affect the quality of service (QoS) VMs receive to the extent of breaking con -
tractual agreements (SLAs) and incurring penalties for the infrastructure provider. 
Another challenge in VM consolidation is the dynamically changing workloads and 
resource requirements. The authors in [ 4–6] introduce heuristics to deal with such 
system dynamics. Recent works [ 7–9] optimize VM consolidation by also reducing 
the necessary VM migrations. Going a step further, the policy in [ 10] takes into 
account the characteristics of jobs running in the datacentre, differentiating between 
CPU- and IO-bound computations.P. Koutsovasilis et al.93
Modern processors support DVFS, which enables CPU operation at different 
nominal voltage and frequency (V ,f) operating points and thus different points of 
performance/power consumption. [ 11] introduces a VM allocation algorithm for a 
DVFS-enabled cluster. In [ 12] the authors apply DVFS to reduce the energy foot -
print of tasks with deadline constraints.
The combination of VM consolidation with DVFS has the potential to improve 
energy efficiency, while at the same time limiting the effect on the perceived QoS 
[13, 14]. These works extend the policies in [ 4] in order to integrate DVFS. The 
authors in [ 13] report average energy savings up to 37.86% compared with previous 
work in [ 4].
A significant body of work recently focuses on identifying and reducing CPU 
voltage margins. On certain architectures, there is graceful behaviour when 
approaching the zone of erratic operation. For example, in [ 15] and [ 16], the authors 
introduce heuristics that dynamically reduce voltage margins while preserving safe 
operation, based on the error correction ECC hardware of modern processors such 
as the server-class Intel Itanium 9560. A key observation on this architecture is that, 
as the CPU supply voltage is lowered, ECC correctable errors appear before uncor -
rectable errors (SDCs and CPU crashes). Similarly, in [ 17] the authors propose a 
severity function that can predict safe, SDC-free undervolt levels for each core of an 
X-Gene 2 processor (ARM v8 architecture), mainly thanks to the manifestation of 
SDCs before the system crashes. In a more recent paper [ 18], the same authors pres -
ent a comprehensive statistical analysis for the same platform that improves predic -
tion on dynamic, workload-dependent variations.
The authors in [ 19] exploit the voltage margins of a server-class 8-core Power7+ 
processor by utilizing specialized hardware mechanisms, such as critical path moni -
tor sensors and digital phase-locked loops, to detect and exploit the margins. The 
more cores are utilized, the larger IR droops are thus reducing the power gains. The 
authors in [ 20] perform a study of voltage margins on Kepler and Fermi NVIDIA 
GPUs. The authors predict safe voltage values using a linear regression and a neural 
network model, and quantify the non-negligible energy gains achieved.
Several works introduce design approaches at the circuit or microarchitectural 
level that trade reliability for lower voltage, by reducing the voltage down to the 
point that produces a maximum number of allowed errors without causing cata -
strophic failures [ 21]. The Razor processor is designed with built-in support for 
dynamic detection and correction of timing failures of the critical paths [ 22]. EV AL 
is a framework for the dynamic adaptation of supply voltage, processor frequency 
and body bias voltage between the substrate and the source (or drain) of transistors 
using a machine learning algorithm [ 23]. Similar ideas include dynamic pipeline 
adaptation by transferring the time slack of faster pipeline stages to the slower ones 
(ReCycle) [ 24], and using variable latency techniques to mitigate the impact of 
variations on the register file and execution units in a microprocessor [ 25]. The 
author in [ 26] proposes a multi-core processor that can scale-down its resources and 
the number of operating cores to meet certain power constraints. Several approaches Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level94
[27–29] employ regression analysis to map certain performance counters to micro -
architectural events and power-delivery estimation. The authors in [ 30] introduce a 
workload-dependent technique to identify the paths excited by individual applica -
tions on ultra-low-power microprocessors and reduce voltage to a level that meets 
the timing of those paths (instead of all paths).
3  Exploiting Workload-Dependent Voltage Margins 
at the Node Level
In this section, we discuss the exploitation of CPU voltage margins at the node level, 
in order to improve the power efficiency of a single node. We focus on the workload- 
dependent component of the CPU voltage margin. This is the most challenging to 
exploit, as its width may change dynamically with different workloads, as well as 
between different phases of the same workload. The goal is to reduce the margins as 
much as possible, yet without compromising the reliability of the system. This is 
achieved using a run-time Extended Dynamic V oltage Scaling (xDVS) governor, 
which is designed to operate on off-the-self systems by continuously monitoring 
CPU resource pressure at run-time and dynamically configuring CPU voltage at a 
“just-right” level. xDVS uses a prediction model to translate resource pressure to 
predictions of lower than nominal, but nevertheless safe, CPU voltage.
3.1  Hardware Platform
In our work, we use Intel Skylake Xeon E3–1220 v5 systems as an indicative server-
class platform. Each Skylake CPU has 4 cores, with a baseline clock frequency and 
nominal supply voltage (V dd) of 3.0GHz and 1.15 V , respectively. The chip is manu -
factured with a 14 nm technology and has a TDP of 80 W. For the characterization 
and training, we use a set of benchmarks that activate different components of the 
architecture.
The latest generations of Intel CPUs, including the Skylake architecture, feature 
the Fully Integrated V oltage Regulator (FIVR) [ 31] mechanism, which autono -
mously selects the supply voltage V dd for CPU cores according to the operating 
frequency and the executing workload. This allows system software to alter the sup -
ply voltage by writing an offset value into Model-Specific Registers (MSRs). The 
value written to these registers (MSR Offset) is interpreted as a reduction to be applied 
on the V dd decided by the FIVR. We should point out that a single MSR Offset is 
applied to all cores (control at the core granularity is not supported).P. Koutsovasilis et al.95
3.2  Offline Quantification of Voltage Margins
A first step in the exercise of exploiting operation at reduced CPU voltage margins 
is to quantify the minimum safe supply voltage (V min) that is lower than the nominal 
voltage yet still achieves error-free operation. We use both single- and multi- 
instance/threaded benchmarks from the SPECCPU 2006 [ 32] and Parsec [ 33] 
benchmark suites, the Linpack [ 34] benchmark, as well as a number of stress tests 
(Prime95 [ 35], Firestarter [ 36], Stress-NG [ 37]). Each benchmark is executed in 
two different configurations, where it occupies a single core and all cores of the 
CPU, respectively.
We deem a CPU voltage level as safe when the workload at hand executes cor -
rectly (terminates with no error messages at the application- and system-level and 
produces correct results) for 10 consecutive runs. As an additional robustness test 
for the value of V min identified in each case, we execute the workload at hand for 
1000 more times, at the corresponding voltage, and verify correct execution as dis -
cussed earlier. Notably, for the CPUs used in our work, the transition to unreliable 
operation, when going below the identified V min, is abrupt and always leads to 
crashes. This is in contrast to previous work, where correctable and uncorrectable 
errors are experienced as early signs of unreliable operation.
Figure 1 illustrates the experimentally identified distribution of MSR offset values, 
for 4 different Skylake systems (Skylake 1-4), running 34 benchmarks, when either 
all CPU cores or a single core (the one experimentally identified to be less amenable 
to voltage reduction) is occupied. V oltage margins span from 17% to 24% of the 
nominal Vdd. The difference between these minimum and maximum voltage margin 
values (7% of nominal Vdd) is the workload-dependent margin. The reader can also 
observe in Fig.  1 non- negligible chip-to-chip variation due to manufacturing vari -
ability. Moreover, by comparing the top and bottom charts of the figure, one may 
observe that executions fully utilizing the processor have, on average, narrower 
margins than when running the benchmarks in a single-threaded/instance 
configuration.
3.3  Modelling and Estimation of Workload-Dependent 
Voltage Margins
Next, we focus on building a model that is able to predict CPU voltage margins in a 
safe way, for the specific CPU part, execution configuration and workload charac -
teristics. Figure  2 depicts the overall approach for exploiting dynamic, workload- 
dependent voltage margins. It consists of the following four steps:
 1. We determine the V min for different cores (and the chip altogether) of the target 
CPU part for different workloads via offline characterization, as discussed in the 
previous section.Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level96
%HQFKPDUNV
0652IIVHWP9Full CPU Utiliz ation
6N\ODNH 6N\ODNH 6N\ODNH 6N\ODNH

















%HQFKPDUNV
0652IIVHWP9Single (W eakest) Core Utilizatio n
6N\ODNH 6N\ODNH 6N\ODNH 6N\ODNH
Fig. 1  Evaluation of the width of voltage margins (MSRoffset) for 34 benchmarks on 4 Intel 
Skylake systems, for full and single-core (weakest) CPU utilization
Offline Voltage 
Margins 
Charact erization
Voltage 
Margins 
ModellingOnline Performance 
Counters Monitoring
(Features Set)
xDVS GovernorConfigurationTraining Se t 
Applications
Offline 
Profiling 
(Perform ance 
Counters)Workload 
ApplicationsTrained 
ModelVoltage
Margins
Predictions
Offline Mode lling Online Volt age Adaptati on
Fig. 2  Exploiting workload-dependent voltage margins at the node levelP. Koutsovasilis et al.97
 2. We profile the same workloads to quantify their interaction with the CPU using 
the performance monitoring infrastructure (performance counters) available on 
all modern CPUs.
 3. We combine V min characterization and the hardware/software interaction data to 
train a model that dynamically predicts a safe supply voltage V ΄dd that is lower 
than the nominal voltage V dd.
 4. Finally, the predictions of the model are used by a dynamic voltage scaling gov -
ernor to dynamically adjust the CPU supply voltage at runtime, while executing 
the current workload.
3.3.1  Profiling
Intel x86–64 Performance Monitoring Units (PMUs) on Skylake can simultane -
ously monitor up to 8 counters per core. We consider 85 performance metrics, 
including all the metrics used by Intel’s Top-down Microarchitecture Analysis 
Method (TMAM) [ 38], the CPU operating temperature, CPU and DRAM power 
consumption, and CPU core utilization. To collect data for all these metrics, we 
perform multiple executions for each benchmark configuration, and in each execu -
tion, we record a subset of performance counters until all metrics are covered. We 
sample the counters every 100 ms.
3.3.2  Model
Our goal is to estimate a safe MSR offset according to the resource utilization and 
pressure quantified by the performance metrics. We employ a machine learning 
ensemble technique, called Random Forest Regression (RFR) [ 39], which can cap -
ture non-linear phenomena. A random forest is a collection of regression decision 
trees, each used to independently predict a value based on an input vector. The 
model predicts by averaging over the predictions of all regression trees.
In order for the model to be applicable in an online manner, the number of per -
formance metrics/features needs to be reduced so as to use up to 8 performance 
counters per core (the maximum that is supported by Intel PMU). This also avoids 
overfitting and decreases the inference overhead of the model. We rank the impor -
tance of the 85 metrics collected during profiling by estimating their mutual infor -
mation (MI) [ 40] with the MSR offset target variable. MI between the input and the 
output of an estimator ranks different features by assigning weights, so that the 
higher the weight the more important the feature for modelling. The algorithm 
assigns the highest importance to the metrics that essentially characterize the 
instruction mix of the workload. After the MI ranking step, we repeat the experi -
ments so that the selected metrics are collected during the same execution. These 
data are normalized to take values between 0 and 1.Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level98
Note that MSR offset, besides being dependent on the workload, also depends on 
the resilience of the specific cores (inter-core variation) and degree of CPU utiliza -
tion. Moreover, in a realistic scenario, the operating system may, independently of 
our mechanisms, modify the topology of active cores via thread migration. To cap -
ture these variations, we construct two forests (for each specific CPU part). One 
forest covers the case of single-core execution (single-core model) and is pessimis -
tically trained using the MSR offset of the weakest core of the target CPU part identi -
fied in each experiment. The other forest covers the case where more than one cores 
are occupied (multi-core model) and is trained, again pessimistically, using the data 
of the benchmark configurations that use all cores of the CPU.
3.3.3  Model Training
Many applications include execution phases, resulting in different CPU resource 
pressure and performance characteristics. However, the offline characterization 
determines the V min across all execution phases of an application. This can nega -
tively affect the effectiveness of model training as it may overgeneralize by trying to 
correlate wildly varying performance counter patterns (of each phase) with the same 
Vmin (for the entire application execution).
Therefore, we bias the training input set to include mainly applications with few 
execution phases such as hmmer  and swaptions . We first rank the 34 benchmarks 
according to the number of their phases. Then, we select the top 90% (most stable) 
applications for training and validation (using 90% of the selected applications for 
training and 10% for validation). The remaining 10% of applications (the ones with 
the largest number of phase changes) serve as the testing (evaluation) set. More 
specifically, the validation set includes body track , freqmine , gcc, and the testing set 
includes facesim , zeusmp , fluidanimate , stress_ng .
The samples (collected at 10 Hz) of the 8 performance counters in the features 
set, is used to train the Random Regression Forest to minimize the Root Mean 
Square Error (RMSE) between the predicted V ΄dd and the V min determined via the 
offline characterization. The models that minimize the RMSE for both the training 
and validation set consist of only three estimators, with the maximum depth of each 
estimator being equal to three. The average RMSE is 7.01 mV .
Apart from evaluating the accuracy of the model, we also use RMSE to reduce 
the risk of compromising the reliability of the system in case of voltage underpre -
dictions. More specifically, as a safety precaution, we subtract the RMSE from the 
MSR offset suggested by the model. The robustness of our approach is verified as part 
of the experimental evaluation, where no failures are observed during application 
execution during the entire range of execution campaigns that were performed to 
quantify the energy gains of xDVS.P. Koutsovasilis et al.99
3.4  Extended Dynamic Voltage Scaling
The model introduced in the previous subsection enables fine grained, adaptive 
undervolting at runtime. In this subsection, we discuss an extended dynamic voltage 
scaling governor (xDVS), which periodically feeds the model with the performance 
counter measurements collected during the previous execution interval and uses the 
MSR offset suggested by the model to apply a subnominal yet safe supply voltage V ΄dd 
for the next interval.
The xDVS governor is implemented as a Finite State Machine (FSM), depicted 
in Fig.  3. The V ΄dd selected for the next interval depends on the prediction of the 
model, the number of active cores and the current state of the governor. Below we 
describe the states and the logic of xDVS:
Back-Off:  In this state, xDVS does not have enough or sufficiently reliable infor -
mation to perform undervolting. Therefore, it does not invoke the model and, 
instead, applies the nominal V dd. This is the case, for example, when a core starts 
executing a new process/thread, or after a long idle period that allowed the oper -
ating system to place a core in a higher C-state (lower power mode).
Step-Up:  xDVS invokes the model. If the model suggests a large reduction to the 
supply voltage, this is applied gradually, in smaller steps of 5 mV . This is done to 
filter abrupt and potentially risky (in terms of reliability) voltage reductions, just 
in case the observed workload behaviour that led the model to suggest a deep 
voltage reduction is merely temporary.
Stable:  The governor samples performance counters on each core and invokes the 
model. Then, it configures the CPU using the suggested MSR offset.
Notably, whenever the model suggests a large increase to the supply voltage, this 
is implemented immediately (without any stepping), irrespective of the state of 
xDVS, as this is considered an emergency and any delay could compromise the reli -
ability of the system. Also, increasing the supply voltage does not introduce any risk 
if the workload suddenly changes; in the worst case, an opportunity for energy sav -
ings will be missed.
Entry
Back-Off Step-Up Stable CPU ActiveStable
Target Vdd
Prediction
dV dd> 5m V CPU IdledV dd
> 5m V
CPU Idle
Fig. 3  FSM diagram of the xDVS governorExploiting Reduced Voltage Margins: From Node- to the Datacenter-level100
Fig. 4  MSRoffset applied by xDVS on Skylake 2, for sudden transitions between mixes of appli -
cations with different margins
Figure 4 illustrates the level of undervolting applied by xDVS on Skylake 2 for 
an indicative scenario with abruptly alternating workloads mixing applications with 
significantly different margins, namely gromacs  and h264ref . The CPU supply volt -
age can be reduced by 257 mV and 198 mV for gromacs  and h264ref  respectively. 
In every transition from workloads that can afford stronger undervolting ( gromacs ) 
to workloads that have a lower undervolting tolerance ( h264ref ), the level of under -
volting drops immediately to increase the CPU supply voltage. In the reverse transi -
tions, the undervolting level gradually increases (CPU voltage decreases) through 
the step-up process discussed above, until it reaches the level suggested by the model.
3.5  Experimental Evaluation
As a next step, in this subsection, we experimentally evaluate whether an online 
governor (like xDVS) can identify voltage margins based on a prediction model and 
can effectively exploit it to improve the energy efficiency of the CPU.
We quantify the resulting energy gains using the four benchmarks in the test set, 
which have not been used during model training and validation ( stress_ng , zeusmp , 
fluidanimate , facesim ). In addition, we use four larger-scale applications, namely 
the Gem5 simulator [ 41], a CPU miner [ 42], the compilation of the Linux Kernel 
[43] and Polybench [ 44]. Each of those complex applications implements multiple 
algorithms and alternates through phases during its execution life.
As a reference for the energy gains that are achieved by the dynamic exploitation 
of reduced voltage margins, enabled by xDVS, we use the energy that is consumed 
to run the same workloads with the off-the-shelf Intel P-state DVFS governor. 
Energy consumption is monitored via the Linux Perf [ 45] tool. Figure  5 shows that 
the xDVS governor achieves an average energy gain of 36% (and can even reach up 
to 42.68%) on fully loaded Skylake CPUs, compared with the Intel P-state gover -
nor. As expected, gains are lower (23.18% on average – not shown in the figure) 
when just one CPU core (the one less amenable to undervolting) is utilized.
As discussed above, the MSR offset applied by xDVS includes an extra safety mar -
gin that decreases the amount of undervolting predicted by the model, thus we 
expect it to be, on average, more pessimistic than the MSR offset that was identified for 
the same workloads in the offline characterization. This is indeed the case, as shown P. Koutsovasilis et al.101

VWUBQJ
]HXV
IGDQLP
IFVLP
NHUQHO
FPLQHU
SOEQFK
JHP
VWUBQJ
]HXV
IGDQLP
IFVLP
NHUQHO
FPLQHU
SOEQFK
JHP
VWUBQJ
]HXV
IGDQLP
IFVLP
NHUQHO
FPLQHU
SOEQFK
JHP
VWUBQJ
]HXV
IGDQLP
IFVLP
NHUQHO
FPLQHU
SOEQFK
JHP
6N\ODNH 6 N\ODNH 6N\ODNH 6 N\ODNH
Energy gains (% )
Fig. 5  Energy gains of xDVS, compared with Intel P-state governor, on the 4 Skylake systems

VVWWUUBBQQJJ
]]HHXXVV
IIGGDDQQLLPP
IIFFVVLLPP
NNHHUUQQHHOO
FFPPLLQQHHUU
SSOOEEQQFFKK
JJHHPP
VVWWUUBBQQJJ
]]HHXXVV
IIGGDDQQLLPP
IIFFVVLLPP
NNHHUUQQHHOO
FFPPLLQQHHUU
SSOOEEQQFFKK
JJHHPP
VVWWUUBBQQJJ
]]HHXXVV
IIGGDDQQLLPP
IIFFVVLLPP
NNHHUUQQHHOO
FFPPLLQQHHUU
SSOOEEQQFFKK
JJHHPP
VVWWUUBBQQJJ
]]HHXXVV
IIGGDDQQLLPP
IIFFVVLLPP
NNHHUUQQHHOO
FFPPLLQQHHUU
SSOOEEQQFFKK
JJHHPP
66NN\\OODDNNHH6 6NN\\OODDNNHH6 6NN\\OODDNNHH6 6NN\\OODDNNHH
Average dynamic voltage offset Offline characterizationMSR Offset (mV)
Fig. 6  The bars show the average dynamic MSR offset applied by xDVS, for the 4 Skylake machines. 
The respective minmax plots show the minimum and the maximum MSR offset applied by xDVS. The 
black diamond represents the MSR offset as identified by the offline characterization at the granular -
ity of the whole application (no offline characterization was performed for the large-scale 
applications)
in Fig.  6. More specifically, the voltage applied by xDVS is on average 9.3 mV 
higher than the one identified by offline characterization for each of the Skylake 
machines. Still, thanks to the ability of xDVS to identify the phases within a given 
application, there are cases where it adjusts the supply voltage to lower values than 
those identified in the offline characterization, without compromising system 
reliability.
Figure 7 shows the dynamically applied MSR offset when the four smaller bench -
marks are scheduled for consecutive execution on the same core of the Skylake 4 
machine (similar behaviour is observed in all other machines). As can be seen, the 
proposed adaptive CPU voltage margins reduction methodology can successfully 
capture the dynamic nature of the applications. This is more clearly visible in fac-
esim, which is an iterative application that executes 3 separate kernels per iteration, 
where the MSR offset varies between 196 mV and 213 mV as the model captures the 
different phases of the application. This variation is a proof that the prediction 
model can correctly generalize and behave as expected with multi-phase applica -
tions, even though such applications are not included in the training process. Also, Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level102
VWUHVVBQJ] HXVPSI OXLGDQLPDWH IDFHVLP$SSOLFDWLRQV



























7LPHVHFSkylake4
MSR Offset (mV)
['96 2IIOLQH&KDUDFWHUL]DWLR Q
Fig. 7  Timeline showing the MSR offset for consecutive single core executions of the four smaller 
benchmarks on Skylake 4. The MSR offset of the offline characterization is in gray
note that in the case of facesim  xDVS drives undervolting even more aggressively 
than what would be dictated by a static MSR offset based on the offline characteriza -
tion (grey line). The steep MSR offset drops (blue line) when the OS schedules a new 
application are due to the xDVS governor transitioning to the Back-Off state.
The observations are similar when the four larger-scale applications are executed 
on each machine. There are both relatively large margin variations between the dif -
ferent machines/CPU parts, as well as variations on the same machine due to differ -
ent workload characteristics. Moreover, the dynamic methodology we discuss can 
capture intra-application variations (e.g. due to the different algorithms that are 
invoked consecutively as part of the CPU miner).
The performance overhead of optimized implementations of dynamic, feedback- 
driven voltage adaptation techniques, even when these are implemented in software 
like the xDVS governor, can be quite affordable. For example, the model invocation 
by xDVS for MSR offset inference requires only 160 ns, while changing to the new 
MSR offset requires 155us. On average, the performance overhead is merely 0.04% of 
execution time when xDVS operates at a 100 ms interval. Even at more frequent 
adaptation intervals (e.g. 10 ms, for which the average overhead raises to 3.5% at 
full CPU utilization), xDVS still provides significant energy gains compared to the 
Intel P-State CPU governor (not shown here).
4  System Reliability When Operating at Reduced 
Voltage Margins
We have not observed any crashes or SDCs in the experiments that were performed 
for the purpose of the evaluation presented in the previous section. Nevertheless, 
voltage underscaling inevitably comes at the cost of increased probability of system 
failure compared with nominal CPU operation (where malfunctions occur too, yet 
with lower probability).P. Koutsovasilis et al.103
In this section, we evaluate the potential impact of such increased unreliability 
on system robustness and recovery overhead. We start by discussing a long-running 
campaign that was conducted to validate the robustness of the system when employ -
ing the mechanisms presented in Sect. 3. Based on the results of the validation, we 
estimate the effect of operation at reduced voltage margins on the Mean Time 
Between Failures (MTBF). Finally, we investigate whether the energy gains due to 
operation at reduced voltage margins outweigh the extra cost – due to the potentially 
shorter MTBF – of checkpointing in large-scale deployment scenarios.
4.1  Validation Experiment
To assess the risk of CPU operation at reduced voltage margins, we performed a 
long experimental evaluation on 16 identical Xeon E3–1220 systems. During this 
campaign, the xDVS governor was continuously enabled on all systems (the model 
was trained separately for each system following the procedure that was described 
in Sect. 3) while executing randomly selected combinations of applications from the 
full set of benchmarks discussed in Sect. 3.3.
The experiments were concluded after 552 hours (23 days) per system, amount -
ing to a total of 8832 device hours, without any observed failure such as crash, silent 
data corruption (SDC), or any detected error. This highly positive result is a solid 
indication that the dynamic exploitation of CPU voltage margins reduction is pos -
sible without reducing the reliability of operation in a notable way. In addition, we 
can use the outcome of this validation experiment to estimate a pessimistic lower- 
bound of the node-level MTBF.
4.2  Effect of Operation at Reduced Voltage Margins 
on Node MTBF
Prior studies show that the MTBF of individual nodes in large-scale facilities can be 
captured by a Weibull distribution with decreasing hazard rate (DHR) [ 46–49]. In 
particular, we use the following equation:
 MTBF/g32/g14/g167
/g169/g168/g183
/g185/g184111
/g79/g78/g42
 
where Γ refers to the gamma distribution and λ is the scale parameter. For the case 
where the shape parameter κ = 0.7 the distribution becomes a Weibull with DHR, 
considering infant mortality phenomena. This is consistent with our observations 
during the characterization process, where the selection of CPU voltages below V min 
leads to failures almost instantly.Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level104
In order to calculate the scale parameter ( λ), we solve the following Weibull 
Cumulative Distribution Function (CDF) equation for λ
 Ft etk
/g11/g12/g32/g16/g16/g167
/g169/g168/g183
/g185/g1841/g79
 
Based on the results of the validation campaign discussed earlier, we pessimisti -
cally assume that 1 out of the 16 machines actually experiences a transient error 
after 552 hours of operation. Thus, in the above equation t is set equal to 552 hours 
and F (t = 552 hours) is set equal to 6.25% since we assume that 1/16 systems expe -
rienced a failure.
Furthermore, given the limited number of systems tested (16 machines), we 
apply the chi-squared distribution to extrapolate and calculate the most pessimistic 
(minimum) MTBF for any confidence level (CL) [ 50, 51] based on the following 
equation:
 MTBFMTBF
XnCL
CL/g32/g14/g11/g12/g162
2212
 
where X2 refers to the chi-square distribution, MTBF is the experimentally observed 
MTBF and n is the number of failures. We assume a distribution with 2 degrees of 
freedom (n = 0) [ 50, 51], since we did not observe any errors during testing.
It is common for industry assessments to have a confidence level of 60% [ 52]. In 
our case, the value we get for this confidence level is MTBF 0.6 ≥ 1595.65 days.
4.3  Effects of Operation at Reduced Voltage Margins in Large, 
Scale-Out Deployments
To deal with the increased probability of failure, large-scale systems typically per -
form checkpointing. Given that such mechanisms introduce significant overhead, 
we evaluate whether the energy savings achieved thanks to operation at reduced 
voltage margins outweigh the overhead of increased checkpointing to deal with the 
increased failure probability. Although there are more sophisticated checkpointing 
mechanisms, in the following discussion we pessimistically assume a simple, block -
ing, coordinated checkpointing scheme that is performed at the optimal period given 
the expected MTBF.
Assuming a platform that employs N nodes to execute a workload, the respective 
MTBF is equal to 
MTBFMTBF
Nplatnode= P. Koutsovasilis et al.105
where MTBF plat and MTBF node are the MTBF values for the whole platform and 
the individual node, respectively, for any continuous failure distribution [ 46], as is 
the case with DHR Weibull. We then use the following equations of the analysis in 
[46], which estimate the optimal checkpointing period ( Topt) and the corresponding 
system overhead (WASTE):
 T MTBF DR Copt plat /g32/g16 /g16 /g11/g122 
 WAST EC
TM TBFDRT
opt platopt/g32/g16 /g16/g167
/g169/g168/g168/g183
/g185/g184/g184/g16/g14 /g14/g167
/g169/g168/g183
/g185/g184/g167
/g169/g168/g168/g183
/g185/g184/g18411 11
2 
where C is the checkpointing cost, R is the cost of restoring the checkpoint and D is 
the downtime.
Figure 8 illustrates the expected energy gains for large-scale deployments when 
dynamic exploitation of reduced CPU voltage margins (in this case implemented by 
the xDVS governor) is combined with periodic checkpointing, compared with an 
idealistic, completely fault-free execution at nominal CPU settings without any 
checkpointing. Without loss of generality, we choose a constant D = 2 minutes and 
let C = R vary from 5 up to 60 minutes
This analysis shows that xDVS (and operation at reduced margins in general) can 
provide significant energy gains, even for large-scale deployments and costly check -
pointing implementations. For instance, in a system with 1000 nodes, xDVS still 
         11XXPPEEHHUURRIIQQRRGGHHVV((PPHHUUJJXXUUHHGGXXFFWWLLRRQQ
0077%%))GGDD\\VV(QHUJ\FRQV ['96QRFKHFNSRLQWLQJ 
Fig. 8  Estimated energy reduction of xDVS with checkpointing vs. execution at nominal settings 
without checkpointing, for MTBF 0.6 along a range of different C-R parameters. The dashed black 
line represents the estimated energy gains when xDVS is used without checkpointing and assum -
ing no node failuresExploiting Reduced Voltage Margins: From Node- to the Datacenter-level106
achieves energy gains of 17.5% for MTBF 0.6, even for a checkpointing and recovery 
time of 40 minutes each.
Note that the comparison against a fault-free execution without checkpointing 
for nominal CPU operation is unfair for xDVS. In practice, when executing at large 
scale, nodes do fail and checkpointing/restart provision is needed even if the CPU 
operates at nominal settings. Thus, the above energy gains are conservative. For the 
same assumptions (no failures and no checkpointing), xDVS would achieve energy 
gains of almost 37% vs. nominal CPU operation.
5  Exploiting CPU Voltage Margins in Cloud Infrastructures
In this section, we focus on answering the question whether operating cloud data -
centre nodes at reduced CPU voltage margins can prove beneficial in terms of net 
profit for cloud infrastructure providers. As discussed earlier, on the one hand, oper -
ation at reduced margins can improve the energy efficiency of nodes and reduce 
energy costs for the operator. However, on the other hand, even if applied judi -
ciously, margins reduction may adversely affect the reliability of nodes, resulting in 
lower Quality of Service (QoS) and thus potentially activating SLA violation penal -
ties payable to the customer. There is an obvious trade-off for the cloud infrastruc -
ture operator, between lower energy costs and higher SLA violation costs.
In order to evaluate this trade-off at scale, we perform simulations using traces of 
real cloud workloads. The simulator implements XM-VFS, a VM scheduling and 
node configuration policy able to exploit conventional techniques for improving 
energy efficiency (VM consolidation, CPU DVFS) together with operation at 
reduced CPU voltage margins. Simulation parameters in terms of voltage margins, 
energy efficiency – at both the CPU-and the node-level (to the plug) – performance 
Voltage Margins 
Charac terization
Power / 
Performance 
Charac terizationReal, Intel-based
nodes 
Reliability
Charact erizationGoogle Trac es
Realistic
Hardware
Para metersExternal Constraints / 
Para meters: Power Cost, 
Service Level 
Agreements (SLAs)
Cloud Infras tructure 
Prov ider Prof it 
Estimation
Fig. 9  The approach for evaluating the potential of exploiting operation at reduced CPU voltage 
margins at the datacentre levelP. Koutsovasilis et al.107
and failure probability come from the characterization of real Intel-based nodes. 
Figure  9 outlines the approach we follow in this section.
5.1  Problem Modelling
We introduce an analytic system model which captures the most important server 
configuration and VM scheduling aspects, when considering operation at reduced 
CPU voltage margins. We assume that nodes are homogeneous in terms of CPU 
hardware. It is, however, straightforward to extend the model in order to capture 
CPU heterogeneity.
5.1.1  Nodes and CPUs
We assume a datacentre with N nodes ni, 1 ≤ i ≤ N with finite memory Mem i. The 
node’s CPU can operate in various nominal ( V, f) points, where V is the supply volt -
age and f is the frequency. ( Vmax, fmax) corresponds to the highest power and perfor -
mance operating point. For each nominal operating point ( V, f), the CPU can operate 
at a sub-nominal and more energy-efficient point ( Vx, f) where Vx < V, exploiting the 
available CPU voltage margins.
5.1.2  Workloads – VMs
Each application is packaged as a separate virtual machine (VM). The notion of a 
VM serves here purely as an abstraction for isolated/contained execution of 
application- specific services. Let there be M different VMs VM m, 1 ≤ m ≤ M, which 
are submitted for execution on the server farm. Each VM has different resource 
requirements, defined in a service level agreement (SLA) to which the provider 
commits when accepting a VM. We also assume that VMs are stateless, and thus can 
be restarted at any node without requiring elaborate state management or live 
migration.
Each VM m requests an amount of memory ( MemmSLA), specified in the SLA, which 
remains allocated during the lifetime of VM. CPUmfSLA
,max is the maximum CPU com -
putational capacity that may be requested by VM m, also defined in the SLA. This is 
expressed as the fraction of the full CPU computational capacity at the maximum 
frequency fmax. However, the VM typically does not need this capacity at all times. 
Let CPUC PUmfreq
mfSLA
,,maxm ax≤  be the CPU capacity that is actually requested by VM m at 
runtime, again expressed as a fraction of the full CPU capacity at the maximum 
frequency. For a CPU operating at a lower frequency f, the effectively requested 
CPU capacity by VM m is obviously higher ( CPUC PUmfreq
mfreq
,,max≥ ). The relation 
between CPUmfSLA
,max and CPUmfSLA
, depends on the sensitivity of the VM’s performance Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level108
to frequency scaling, with compute-intensive applications being more sensitive to 
frequency scaling than memory- or I/O-intensive applications.
5.1.3  Scheduling
Scheduling is done periodically and each period has a fixed length equal to K times -
lots of slotT  time units each. We assume that new VMs arrive and existing VMs 
finish at the boundaries of scheduling periods. After each period, the scheduler 
determines (a) the mapping of new VMs, as well as potential movement of existing 
VMs to other nodes, and (b) the CPU voltage and frequency configuration ( Vi, fi) of 
each node ni, for the next period.
As part of the scheduling decision, a VM may be restarted on a different node 
than its current host. Let Res m be the number of timeslots needed to restart VM m. 
Without loss of generality, we assume that VM restarts are fast, i.e., 1  ≤ Res m ≪ K. We 
also assume that a restart does not create a significant load on the new host before 
the VM actually resumes execution there. Note that a VM remains unavailable dur -
ing a restart. The binary variable Stmk indicates if VM m is running at the kth timeslot 
(Stmk=1) or VM m is unavailable ( Stmk=0).
5.1.4  CPU Allocation to VMs
Let nodeVM sip be the list of VMs that are hosted on node ni for the period p. The 
CPU load of ni in timeslot k is equal to the total effectively requested CPU capacity 
of all VMs that are running on the node:
 Load CPUik
mnodeVM sStmfreq
ip
mki/g32
/g143/g32/g166
,,
1  
Although VMs have steady CPU requirements during a period, the load of the 
node may vary within a period due to VM relocations and the delayed resumption 
of arriving VMs on the node.
The CPU capacity allocated to each VM during a given timeslot depends on the 
load of the node and the availability of the VM. If the node suffers an overload (if 
Loadik>1) the CPU capacity is shared among VMs proportionally to their requests. 
Otherwise, (if Loadik≤1) VMs get their requested CPU capacity. Note that if the 
VM is unavailable ( Stmk=0), it will not get any CPU capacity.
 CPUCPUi fLoad andSt
CPU
Loadimallockmfreq
ik
mk
mfreqi
i
,,,
,,
/g32/g100/g3211
iik ik
mkifLoad andSt
otherwise,
,/g33/g32/g173
/g174/g176
/g176/g176
/g175/g176
/g176
/g17611
0
 P. Koutsovasilis et al.109
5.1.5  Failures
We assume that a node configured to operate at a sub-nominal point ( Vi, fi) may fail 
during a scheduling period with probability Pfai lVfii,. To focus on the essence of the 
problem, we adopt a simple failure model. We assume that every failure leads to a 
node crash, affecting all VMs that are hosted on the node. This is consistent with the 
behaviour we have observed on real systems. Also, failures occur at the beginning 
of the period (again, this is consistent with the infant mortality behaviour observed 
during the characterization and discussed in Sect. 4.2), and the failed node remains 
unavailable during the entire period. We assume that node failures are transient and 
non-systematic: a failed node recovers and is able to host VMs by the next period. 
Those are realistic assumptions, provided that failures are due to undervolting and 
that the scheduling period is relatively large.
If VM m is hosted at a node that fails, it is not re-scheduled to another node during 
the period when the failure occurs. As a consequence, VM m does not run and remains 
unavailable during the entire period, St kKmk/g32/g100 /g100 01, . However, it can be restarted 
at the next period on any node (also on a node that has recovered after a failure) 
without delay.
5.1.6  SLA Violations Cost
A VM may not get the requested CPU capacity at every timeslot, because it is 
restarting, the host node is overloaded, or the host node has suffered a failure. In all 
these cases, the provider pays an SLA violation penalty to the VM owner.
The SLA violation cost is modelled similarly to the approach presented in [ 53]. 
More specifically, we assume that it is equal to the product of the normalized extent of 
the violation (expressed as the difference between the provisioned CPU resources 
adjusted to the computational capacity of the CPU at frequency fi CPUmfSLA
i, and the 
CPU resources that were actually allocated to the VM CPUmfallock
i,,, divided by CPUmfSLA
i,), 
and the duration of the violation. For any given timeslot k, this can be expressed as
 SLAV qCPUC PU
CPUslotTp ricimk
mmfSLA
mfallock
mfSLAii
i,,,,
,/g32/g16
/g117/g117 /g117 eem
 
where qm is a multiplier that increases the severity of the penalty according to the 
importance of VM m, and price m is the price charged to the client for executing VM m 
per time unit as agreed in the SLA.
The SLA violation cost over an entire period for all VMs running on node ni that 
does not fail ( SLAinofai l) or a node that fails ( SLAifail), can be expressed as
 SLAV SLAV SLAVinofai l
ifail
kK
mnodeVM simk
ip/g32/g32
/g32/g143/g166/g166
1,
 Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level110
Note that in the case of a node failure or a VM restart, CPUmfallock
i,,=0, while in the 
case of a node overload, 0<<CPUC PUmfallock
mfreq
ii,,
,.
5.1.7  Energy Cost
Let Power (V, f, u) be the power consumed by a node at an operating point ( V, f) 
when the CPU utilization is u. We determine this function via experimental charac -
terization of real hardware (Sect. 5.3.3 ).
The CPU utilization at timeslot k is given by
 u Loadik
ik/g32 /g94/g96min ,1 
Assuming a fixed unit price UnitPrice  for energy, the total energy cost for node 
ni over an entire period is
 Energy UnitPrice Powe rV fu slotTi
kK
ii ik/g32 /g11/g12
/g32/g166/g117/g117
1,,
 
Note that if ni is not assigned any VMs, it will remain idle and its utilization u 
will be 0. Depending on whether idle nodes are shut-down or merely enter a low- 
power state, Power (V, f, 0) may return zero or a non-zero value, respectively.
5.1.8  Overall Operating Cost
Based on all the above, we can estimate the cost of ni for the next scheduling period, 
for the decided placement of VMs, and assuming the node will operate at ( Vi, fi), as
 cos,, tP fail SLAV PfailS LAVE neriV fifail
Vf inofai l
ii ii/g32/g14 /g16/g11/g12 /g14 /g117/g117 1 ggyi /g11/g12 
Either the node will fail and the provider will pay the SLA violation cost for the 
VMs that are hosted on the node for the full scheduling period, or the node will not 
fail and the provider will pay some SLA violation costs if the node is overloaded, 
plus the cost of the energy consumed by the node to run the VMs. This equation is 
an important tool when looking for the optimal operating point of each node for the 
next period. It considers the energy consumption as well as the cost of any potential 
failures or overloads of the node.
The total estimated cost for the next period over all nodes is 
iN
it
/g32/g166
1cos. In the next 
section, we outline a policy that takes this cost into account in order to reconfigure 
nodes and schedule the VMs on nodes in an educated manner. It exploits the 
extended CPU margins to save energy as long as this outweighs the potential cost P. Koutsovasilis et al.111
due to the increased probability of SLA violations as a result of failures, node over -
loads and VM restarts.
5.2  VM Scheduling and System Configuration
In order to improve the energy efficiency of systems beyond what is possible by 
VFS and VM consolidation, we discuss a policy ( XM-VFS ) which also exploits 
operation at reduced CPU voltage margins. The policy is based on the model dis -
cussed in Sect. 5.1 and tries to identify points, in the trade-off between energy cost 
and potential SLA violation penalties, beneficial for the cloud infrastructure pro -
vider. It schedules the VMs to nodes by applying a set of heuristics, and it selects the 
configuration of each node (nominal or extended) that introduces the minimum cost.
The policy is executed between scheduling periods (right after a period finishes 
and before the next one starts). Scheduling consists of four steps: (a) detection of 
overloaded nodes and selection of candidate VMs to be moved away from these 
nodes, (b) mapping of these VMs and any newly arriving VMs to their new hosting 
nodes, (c) powering-off underutilized nodes if their VMs can be moved away to 
other nodes, and (d) re-configuration of the ( Vi, fi) operating point of each node ni as 
needed. The first three steps are similar to previous work on VM consolidation [ 4]. 
Actually, we use the same heuristics as [ 4] for step (c). In the following discussion, 
we focus more on steps (a), (b) and (d).
To determine the configuration of the nodes and the allocation of the VMs on 
them for the next period p, XM-VFS  first identifies what would have been the most 
power efficient operating point Vfii ipefficient,/g11 /g12/g16.1 of each node ni for the previous period 
p − 1. This step considers both node and VM characteristics.
Then, XM-VFS  uses historic information for the last 10 periods with exponential 
aging, to predict the requested resources by VMs for the next period p. Given this 
prediction, it estimates the required computational capacity that corresponds to fre -
quency fipefficient
,−1 for each node ni. XM-VFS  uses a static utilization threshold, MaxUtil , 
to prevent node overloads. If the expected load of ni exceeds MaxUtil , XM-VFS  
selects the least costly VMs for movement and restart on another node, until the 
expected load of ni drops to MaxUtil . This step is performed at the beginning of each 
scheduling period, independently and concurrently on each node.
Next, VMs selected for the restart and any newly arriving VMs are allocated to 
nodes. As this is a bin packing problem (VMs correspond to items and nodes to 
bins), we use the Best Fit Decreasing ( BFD ) algorithm to allocate VMs to nodes. 
During this step, XM-VFS  respects: (a) The configuration Vfii ipefficient,/g11 /g12/g16.1 of the node ni, 
and (b) the MaxUtil  threshold, by allocating VMs to nodes so that the expected load 
of each node ni is not exceeding MaxUtil , given the Vfii ipefficient,/g11 /g12/g16.1 CPU configuration.
At this point, the algorithm uses the heuristic in [ 4] to identify opportunities to 
power-off nodes. More specifically, for all nodes, starting from the least utilized 
one, it evaluates whether all hosted VMs can be restarted on other nodes without Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level112
overloading them. Potential destination nodes are identified using again the BFD  
heuristic.
Finally, XM-VFS  finds the optimal configuration (using a nominal or sub- nominal 
CPU voltage setting) of each node, given the finalized VM placement for the next 
scheduling period. The optimal node configuration minimizes overall cost at the 
node level, therefore for a fixed VM-to-nodes mapping the total cost for the entire 
datacentre as well, considering both energy consumption and potential SLA viola -
tion costs. We use the equation derived in Sect. 5.1.8  to estimate the cost for each 
operating point (nominal and extended) and we choose the one that introduces the 
lowest cost. This step is executed independently on each node.
5.3  Hardware Characterization
To evaluate the viability and cost-effectiveness of datacentre operation at reduced 
voltage margins, we extract realistic parameters to be used in simulations from the 
Intel-based (Xeon E3 Skylake) nodes discussed in Sect. 3. The parameters of inter -
est extracted by experimental characterization of the nodes are: (i) the reduced mar -
gins operating points, (ii) the failure probability when operating at such points, (iii) 
the power consumed by the whole node (at the plug) at different operating points 
and utilization levels, and (iv) the impact of frequency scaling on the performance 
of applications.
5.3.1  Voltage Margins
Concerning the characterization of the nominal operating points, we let the off-the- 
shelf P-State manager/DVFS governor dynamically control the operating points ( V, 
f) on the Skylake processor. We pick four frequency points at 3.3, 3.0, 2.5 and 
2.0 GHz, which cover the range of the most common frequencies applied in the bal -
anced mode of the governor. The latter is optimized for balancing performance and 
energy efficiency. We identify the average nominal supply voltages for these fre -
quencies being 1147, 1075, 922 and 850 mV , respectively.
For each nominal operating point ( V, f), we experimentally determine a corre -
sponding reduced margins operating point ( Vx, f). We apply the methodology dis -
cussed in Sect. 3.2. The only difference is that, in this case, we are interested in the 
workload-neutral part of the CPU voltage margin (static margin). We focus on the 
static margin due to the fact that in datacentres the visibility of operators to work -
load characteristics may be contractually limited. Therefore, for each frequency f 
we experiment with all workloads in the voltage margins characterization set and 
we gradually reduce the voltage as usual; however, we stop when any of the experi -
ments leads to any type of abnormal behaviour (failure). We identify the immedi -
ately preceding voltage level (that did not result into a failure) as Vx for the frequency P. Koutsovasilis et al.113

   9ROWDJH9
)UHTXHQF\*+]
9ROWDJHPDUJLQ
Fig. 10  V oltage margins characterization for the Xeon E3 (Intel Skylake) CPUs
f, resulting to a reduced margins operating point ( Vx, f). We then validate the safety 
of the ( Vx, f) operating point by performing additional runs, exactly as explained in 
Sect. 3.2.
Figure 10 illustrates the degree of undervolting that can be applied to each nomi -
nal operating point of the Skylake CPUs. The shaded area corresponds to the width 
of CPU voltage margins at different frequencies. The top boundary of the shaded 
area corresponds to nominal operating points, whereas the bottom boundary to 
reduced margins operating points revealed by the characterization. For the Skylake 
CPU, the sub-nominal voltages for the four frequency points of 3.3, 3.0, 2.5 and 
2.0 GHz are 929, 865, 741 and 666 mV , respectively.
5.3.2  Failure Probability
We validate the safety of the identified sub-nominal static voltage margins by exe -
cuting multiple experimental campaigns of random workloads at each extended 
operating point for 6 consecutive days each. Being unrealistically pessimistic, we 
assume that the 6 days of execution time without any errors is the mean time to 
failure ( MTTF ), for all tested extended margin points. To put the degree of pessi -
mism into perspective, the analysis in Sect. 4.2 resulted in an MTTF  ≥ 1595.65 days, 
at a 60% confidence level, while exploiting not just static but also dynamic, 
workload- dependent CPU voltage margins.
The 6  days MTTF corresponds to a rate of 1 failure every 518,400  seconds. 
Being once again pessimistic, we assume this failure to be fatal, ignoring the pos -
sibility of correctable errors [ 15]. This failure rate is then used to estimate the Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level114
probability of failure Pfai lVfx, within a scheduling period for a node that operates at 
any of the reduced margins points ( Vx, f). For example, assuming a scheduling 
period of 300 seconds, if a node is configured to operate at reduced voltage margins, 
the failure probability is 300/518400 = 0.000579. We use this Pfai lVfx, in our simula -
tions to show that, even assuming a very large failure probability, educated under -
volting can provide a significant increase of profit for the infrastructure provider.
5.3.3  Power
We estimate the power consumption function Power (V, f, u) for the target hardware 
platform as follows: First, we run a series of experiments that exhibit different utili -
zation levels, by varying the number of CPU cores used for numerous nominal 
voltage-frequency points. We record the power consumption, measured at the socket 
using a powerwall meter. Then, based on these data, we apply linear regression to 
compute the parameters of the model Power  = A + B × u × V2 × f used to estimate 
the power consumption as a function of CPU utilization u, supply voltage V and 
frequency f. This model serves as the power estimation function in our simulation 
experiments.
More specifically, for the Skylake CPU, we get A = 34.01 and B = 18.98, which 
predicts power consumption with R2 = 0.98 and a root mean square error (RMSE) 
of 4.40 for the unseen configurations at nominal operating points. The low RMSE 
indicates that the predictions of the power model are close to the actual power con -
sumption. Experiments for configurations at reduced voltage margins verify the 
accuracy of the power estimation model at non-nominal operating points as well.
5.3.4  Performance Sensitivity to Frequency Scaling
The performance of an application depends on CPU operating frequency, but some 
applications are more sensitive to frequency scaling. To quantify this in a realistic 
way, we measure the execution time of different applications for each of the fre -
quency points considered. The applications we use in these measurements are 24 
benchmarks from the SPECCPU 2006 [ 32], and the whole CloudSuite benchmark 
suite [ 54]. After extracting the performance profile for each application on each 
operating frequency point, we apply a K-Means clustering algorithm to form clus -
ters of applications with common behaviour.
The experiments reveal three main clusters of applications with distinctly differ -
ent behaviour. As shown in Fig.  11, the first cluster (sensitive) includes applications 
with performance degradation which scales almost linearly to CPU frequency 
reduction, with a ratio of approximately 1. Those are typically compute-bound 
applications. The second cluster (medium sensitivity) is affected less by the fre -
quency reduction, however, aggressive frequency reduction leads to significant per -
formance loss (close to 27%). The third cluster (insensitive) includes applications P. Koutsovasilis et al.115




    33HHUUIIRRUUPPDDQQFFHHOORRVVVV))UUHHTTXXHHQQFF\\UUHHGGXXFFWWLLRRQQZZUUWWKKLLJJKKHHVVWWQQRRPPLLQQDDOO
,,QQVVHHQQVVLLWWLLYYHH00HHGGLLXXPPVVHHQQVVLLWWLLYYLLWW\\66HHQQVVLLWWLLYYHH
Fig. 11  Classes of applications, in terms of performance sensitivity to frequency reduction on the 
Skylake node. The error bars quantify the standard deviation of performance loss among applica -
tions within each cluster, for each frequency step
that are relatively insensitive to frequency reduction. Memory- or I/O-bound appli -
cations typically fall within this group.
In our simulation experiments, we assume three equally probable performance 
classes for the workload VMs, corresponding to the three clusters identified above. 
Depending on the class of VM m, we apply the corresponding performance scaling 
behaviour to transform the requested CPU capacity CPUmfreq
,max at the maximum CPU 
frequency fmax, to the equivalent CPU capacity CPUmfreq
, at a lower frequency f.
5.4  Experimental Evaluation
In this section, we perform simulations to evaluate whether operation at reduced 
CPU voltage margins can prove profitable for cloud infrastructure providers. We use 
the parameters attained by characterizing the Skylake nodes as discussed above, and 
apply XM-VFS to control VM-to-node mapping and node configuration. We also 
use real-world values for the energy cost [ 55] and the VM pricing [ 56]. The energy 
price in our experiments is $0.15 per KWh and the VM price range for the types of 
VMs in the traces is $0.0057 – $0.4608 per hour.
The simulation environment is based on an extension of CloudSim, which we 
developed to support operation at reduced CPU voltage margins, implement the 
XM-VFS scheduler, and to introduce support for using Google traces [ 57]. Each 
experiment simulates a time period of 1 day, using a subset of Google traces consist -
ing of 10,000 VMs. We use a scheduling period of 300 seconds. We compare opera -
tion at reduced CPU voltage margins and XM-VFS against LrMmt-VFS  (local 
regression minimum migration time, combined with voltage and frequency scaling) Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level116
Fig. 12  Parametric analysis of cost w.r.t. failure rate. The horizontal axis is in a logarithmic scale, 
with failure rate progressively doubling. Cost is normalized w.r.t. the cost achieved by LrMmt- 
VFS. The vertical, dotted red line corresponds to the baseline, pessimistic failure rate we assume. 
The black line indicates the percentage of the nodes configured at reduced voltage margins at each 
failure rate
[13], a state-of-the-art algorithm which optimizes the configuration of a datacentre 
using the VM consolidation techniques discussed in [ 4], in synergy with DVFS at 
the node level.
Figure 12 illustrates the cost benefits of operation at reduced CPU voltage mar -
gins ( XM-VFS ) vs. operation at nominal (V ,f) points ( LrMmt-VFS ). Operator costs 
are itemized as energy cost and SLA violation penalty cost due to VM movement 
between nodes, node overloads and node crashes. All costs are normalized w.r.t the 
total cost of the LrMmt-VFS  policy. Figure  12 includes results at different failure 
rates. Crashes occur randomly in our simulated environment following the corre -
sponding failure rate. Each experiment is repeated 10 times and the figure reports 
averages among all runs. Notably, the reported energy consumption gains are at the 
plug (not merely on the CPU).
First, we discuss the results of the baseline failure rate (the vertical, red dotted 
bar). Operation at reduced CPU voltage margins ( XM-VFS ) can achieve 16.32% 
lower cost in average w.r.t. operation at nominal configuration ( LrMmt-VFS ). The 
energy consumption is reduced by 17.35%, and this is not outweighed by the cost of 
potential SLA violation penalties due to crashes, which increases the total cost by 
merely 2.52%. This leads XM-VFS  to aggressively configure almost all nodes at P. Koutsovasilis et al.117
extended margins, despite the pessimistic baseline failure rate we assume. One 
more important observation is that the percentage of node crashes throughout all 
experiments did not exceed 0.37% for any scheduling period, again despite the pes -
simistic baseline failure rate. For most periods there were no crashes at all which 
means that the cost of node restarts is negligible.
Moving from lower (left of the figure) to higher failure rates (to the right of the 
figure), we observe that, as expected, the total cost gradually increases. However, 
for all failure rates XM-VFS  outperforms LrMmt-VFS , highlighting the benefits of 
educated, adaptive, cost-driven operation at reduced CPU voltage margins. More 
specifically, as illustrated by the black line in Fig.  12, moving to higher failure rates 
results to less nodes being configured to operate at non-nominal (V ,f) points. For 
extremely high failure rates, all nodes operate at nominal settings. This is reflected 
to the cost of energy (blue area), which increases with the decrease of the percent -
age of nodes operating at reduced margins. An interesting trend can be observed in 
the SLA violation costs due to crashes (orange area). As the failure rate increases, 
these costs also increase, as expected. However, after a certain failure rate, XM-VFS  
decides that it is no longer cost-effective to configure so many nodes at reduced 
margins. This, in turn, reduces the number of crashes and, consequently, the respec -
tive SLA violation costs.
6  Conclusions
In previous chapters, we discussed the existence of voltage margins on modern 
hardware and techniques for revealing and quantifying them on CPUs. In this chap -
ter we took a leap further: we focused on whether it is possible, practical and profit -
able to exploit them, using Intel Xeon E3 Skylake CPUs as a case study.
We first quantified the CPU voltage margins of Intel Xeon E3 processors. We 
replicated findings of chip-to-chip variability observed on X-Gene (ARM v8) CPUs. 
Contrary to ARM CPUs, however, we found that the voltage margins on Intel CPUs 
are both significantly wide (up to 24% of nominal supply voltage) and partially 
workload-dependent. We then addressed the challenge of predicting the voltage 
margins at execution time, using as input metrics of hardware/software interaction 
attainable from performance counters available on most modern CPUs. We exploited 
the predictor for controlling CPU voltage at sub-nominal levels dynamically at exe -
cution time, resulting to 36% more energy-efficient execution on average, without 
penalizing performance and without observing erratic behaviour.
As a next step, we went on to evaluate whether those gains are sustainable in 
larger-scale systems. We first validated the resilience of educated operation at 
reduced CPU voltage margins with a long running (23 days) experiment on 16 sys -
tems. The workloads terminated successfully, without any indication of erratic 
behaviour. Based on this result, we estimated the MTBF of a single node at more 
than 1595.65  days, with 60% confidence. Then, we evaluated whether there are Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level118
benefits when combining multiple nodes in a single system and considering the 
overhead of error recovery mechanisms (checkpointing and restart). We found that 
the benefits, in terms of energy consumption, are significant on large scale systems 
(i.e. 1000 nodes), even under very pessimistic assumptions on checkpointing 
sophistication, checkpointing cost and restore cost.
As a last step, we considered the practicality of operation at reduced CPU volt -
age margins for cloud infrastructure providers, from a profit maximization perspec -
tive. There is an interesting tradeoff: operation at reduced margins reduces energy 
costs; however, it may trigger SLA violation penalties in case of downtime (due to 
VM remapping and node crashes) and lower performance (due to node overloads). 
Using simulations driven by realistic parameters (VM traces, energy price cost, 
SLA violation cost, hardware parameters), we found that, even by very pessimistic 
assumptions, operation at reduced CPU voltage margins results in 16.32% lower 
cost of operation, on average, for the infrastructure provider.
In summary, the discussion in this chapter indicates that it is possible, realistic 
and profitable to exploit CPU voltage margins in real-world settings and large-scale 
systems.
References
 1. J. Koomey, Growth in data center electricity use 2005 to 2010  (The New York Times, 2011)
 2. M. Dayarathna, Y . Wen, R. Fan, Data center energy consumption modeling: a survey. IEEE 
Commun. Surv. Tutorials 18(1), 732–794 (2016)
 3. S. Das, D. Roberts, S. Lee, S. Pant, D. Blaauw, T. Austin, K. Flautner, T. Mudge, A self-tuning 
DVS processor using delay-error detection and correction. IEEE J. Solid-State Circ. 41(4), 
792–804 (April 2006)
 4. A. Beloglazov, R. Buyya, Optimal online deterministic algorithms and adaptive heuristics for 
energy and performance efficient dynamic consolidation of virtual machines in Cloud data 
centers. Concurr. Comp. Pract. Exp. 24, 1397–1420 (2012)
 5. Y .C. Lee, A. Zomaya, Energy efficient utilization of resources in cloud computing system. 
J. Supercomput. 60, 268–280 (2012)
 6. C. Ghribi, M. Hadji, D. Zeghlache, Energy efficient VM scheduling for cloud data centers: 
Exact allocation and migration algorithms, in 2013 13th IEEE/ACM International Symposium 
on Cluster, Cloud, and Grid Computing, 2013, pp. 671–678
 7. Z. Cao, S. Dong, An energy-aware heuristic framework for virtual machine consolidation in 
cloud computing. J. Supercomput. 69, 429–451 (2014)
 8. F. Farahnakian, P. Liljeberg J. Plosila, Energy-efficient virtual machines consolidation in cloud 
data centers using reinforcement learning, in 22nd Euromicro International Conference on 
Parallel, Distributed, and Network-Based Processing, 2014
 9. Y . Chang, C. Gu, F. Luo, G. Fan, W. Fu, Energy efficient resource selection and allocation 
strategy for virtual machine consolidation in cloud datacenters. IEICE Trans. Inform. Syst. 
101, 816–1827 (2018)
 10. Z. Zhou, J. Abawajy, M. Chowdhury, Z. Hu, K. Li, H. Cheng, A.A. Alelaiwi, F. Li, Minimizing 
SLA violation and power consumption in cloud data centers using adaptive energy-aware algo -
rithms. Futur. Gener. Comput. Syst. 86, 836–850 (2018)P. Koutsovasilis et al.119
 11. G. V on Laszewski, L. Wang, A.J. Younge, X. He, Power-aware scheduling of virtual machines 
in dvfs-enabled clusters, in 2009 IEEE International Conference on Cluster Computing and 
Workshop, IEEE, 2009, pp. 1–10
 12. Q. Huang, S. Su, J. Li, P. Xu, K. Shuang, X. Huang, Enhanced energy-efficient scheduling for 
parallel applications in cloud, in 2012 12th IEEE/ACM International Symposium on Cluster, 
Cloud and Grid Computing (ccgrid 2012), IEEE, 2012, pp. 781–786
 13. P. Arroba, J.M. Moya, J.L. Ayala, R. Buyya, Dynamic voltage and frequency scaling-aware 
dynamic consolidation of virtual machines for energy efficient cloud data centers. Concurr. 
Comput. Pract. Exp. 29, e4067--n/a (2017)
 14. E. Arianyan, H. Taheri, V . Khoshdel, Novel fuzzy multi objective DVFS-aware consolidation 
heuristics for energy and SLA efficient resource management in cloud data centers. J. Netw. 
Comput. Appl. 78, 43–61 (2017)
 15. A.  Bacha and R.  Teodorescu, Using ECC feedback to guide voltage speculation in low- 
voltage processors, in Proceedings of 47th Annual IEEE/ACM International Symposium on 
Microarchitecture (MICRO), 2014
 16. A. Bacha, R. Teodorescu, Dynamic reduction of voltage margins by leveraging on-chip ECC 
in Itanium II processors, in Proceedings of the 40th Annual International Symposium on 
Computer Architecture, 2013
 17. G.  Papadimitriou, M.  Kaliorakis, A.  Chatzidimitriou, D.  Gizopoulos, P.  Lawthers S.  Das, 
Harnessing voltage margins for energy efficiency in multicore CPUs, in Proceedings of the 
50th Annual IEEE/ACM International Symposium on Microarchitecture, New  York, NY , 
USA, 2017
 18. M. Kaliorakis, A. Chatzidimitriou, G. Papadimitriou, D. Gizopoulos, Statistical analysis of 
multicore CPUs operation in scaled voltage conditions. IEEE Comput. Archit. Lett. 18(2), 
109–112 (2018)
 19. Y . Zu, C. Lefurgy, J. Leng, M. Halpern, M.S. Floyd, V .J. Reddi, Adaptive guardband schedul -
ing to improve system-level efficiency of the POWER7, in Proceedings of the 48th Annual 
IEEE/ACM International Symposium on Microarchitecture (MICRO) , (2015)
 20. J. Leng, A. Buyuktosunoglu, R. Bertran, P. Bose, V .J. Reddi, Safe limits on voltage reduction 
efficiency in GPUs: a direct measurement approach, in Proceedings of the 48th Annual IEEE/
ACM International Symposium on Microarchitecture (MICRO), 2015
 21. A.B. Kahng, S. Kang, R. Kumar, J. Sartori, Designing a processor from the ground up to allow 
voltage/reliability tradeoffs, in Proceedings of the 16th International Symposium on High- 
Performance Computer Architecture (HPCA-16), 2010
 22. D.  Ernst, et  al., Razor: a low-power pipeline based on circuit-level timing speculation, in 
Proceedings of the 36th Annual IEEE/ACM International Symposium on Microarchitecture 
(MICRO-36), 2003
 23. S. Sarangi, B. Greskamp, A. Tiwari, J. Torrellas, EV AL: utilizing processors with variation- 
induced timing errors, in Proceedings of the 41st Annual International Symposium on 
Microarchitecture (MICRO), 2008
 24. A. Tiwari, S.R. Sarangi, J. Torrellas, ReCycle: pipeline adaptation to tolerate process variation, 
in Proceedings of the 34th International Symposium on Computer Architecture, (ISCA), 2007
 25. X.  Liang, D.M.  Brooks, Mitigating the impact of process variations on processor register 
files and execution units, in Proceedings of the 39th Annual International Symposium on 
Microarchitecture (MICRO) , (2006)
 26. N.S. Kim, Resource and core scaling for improving performance of power-constrained multi -
core processors. Patent US Patent 9, 606,842 (2017)
 27. P.J. Joseph, K. Vaswani, M.J. Thazhuthaveetil, Construction and use of linear regression mod -
els for processor performance analysis, in Proceedings of the 12th International Symposium 
on High-Performance Computer Architecture (HPCA), 2006
 28. W. Jia, K.A. Shaw, M. Martonosi, Stargazer: automated regression-based GPU design space 
exploration, in Proceedings of the International Symposium on Performance Analysis of 
Systems Software (ISPASS), 2012Exploiting Reduced Voltage Margins: From Node- to the Datacenter-level120
 29. B.C.  Lee, D.M.  Brooks, Accurate and efficient regression modeling for microarchitectural 
performance and power prediction, in Proceedings of the 12th International Conference on 
Architectural Support for Programming Languages and Operating Systems , pp. 185–194, 2006
 30. H. Cherupalli, R. Kumar, J. Sartori, Exploiting dynamic timing slack for energy efficiency 
in ultra-low-power embedded systems, in Proceedings of the 43rd ACM/IEEE Annual 
International Symposium on Computer Architecture (ISCA), 2016
 31. E.A. Burton, G. Schrom, F. Paillet, J. Douglas, W.J. Lambert, K. Radhakrishnan, M.J. Hill, 
FIVR  — fully integrated voltage regulators on 4th generation Intel® Core™ SoCs, in 
Proceedings of Twenty-Ninth Annual international conference on Applied Power Electronics 
Conference and Exposition (APEC) , (2014)
 32. J.L. Henning, SPEC CPU2006 benchmark descriptions. SIGARCH Comput. Archit. News 34, 
1–17 (2006)
 33. C. Bienia, S. Kumar, J.P. Singh, K. Li, The PARSEC benchmark suite: characterization and 
architectural implications, in Proceedings of the 17th international conference on Parallel 
architectures and compilation techniques (PACT), 2008
 34. J.J. Dongarra, P. Luszczek, A. Petitet, The LINPACK benchmark: past, present and future. 
Concurr. Comput. Pract. Exp. 10(9), 803–820 (2003)
 35. G. Woltman, Prime95,  2012
 36. D. Hackenberg, R. Oldenburg, D. Molka, R. Schöne, Introducing FIRESTARTER: a processor 
stress test utility, in Proceedings in the fourth Conference on International Green Computing 
(IGC), 2013
 37. Stress-NG, [Online]. Available: http://kernel.ubuntu.com/~cking/stress- ng/
 38. J. Marusarz, S. Cepeda, A. Yasin, How to tune applications using a top-down characterization 
of microarchitectural issues, in Technical report, Intel, 2013
 39. A. Liaw, M. Wiener, Classification and regression by randomForest. R. News 2(3), 18–22 (2002)
 40. A.  Kraskov, H.  Stögbaue, P.  Grassberger, Estimating mutual information. Phys. Rev. 
83(1) (2011)
 41. N. Binkert, B. Beckmann, G. Black, S.K. Reinhardt, A. Saidi, A. Basu, J. Hestness, D.R. Hower, 
T. Krishna, S. Sardashti, R. Sen, K. Sewell, M. Shoaib, N. Vaish, M.D. Hill, D.A. Wood, The 
Gem5 simulator. SIGARCH Comput. Archit. News 39, 1, 82,011–7
 42. A. Jeff Garzik, CPU-Miner
 43. Linux Kernel, 2017. [Online]. Available: https://www.kernel.org/
 44. L.-N. Pouchet, Y . Tomofumi, PolyBench/C 3.2, [Online]. Available: https://sourceforge.net/
projects/polybench/
 45. Perf: Linux Profiling with Performance Counters, 2017. [Online]. Available: https://perf.wiki.
kernel.org/index.php/Main_Page
 46. J. Dongarra, T. Herault, Y . Robert, Fault tolerance techniques for high-performance computing, 
in Fault-Tolerance Techniques for High-Performance Computing (Springer, 2015), pp. 3–85
 47. T.J. Hacker, F. Romero, C.D. Carothers, An analysis of clustered failures on large supercom -
puting systems. J. Parall. Distrib. Comput. 69, 652–665 (2009)
 48. Y .  Liu, R.  Nassar, C.  Leangsuksun, N.  Naksinehaboon, M.  Paun, S.L.  Scott, An optimal 
checkpoint/restart model for a large scale high performance computing system, in 2008 IEEE 
International Symposium on Parallel and Distributed Processing, 2008
 49. B. Schroeder, G.A. Gibson, A large-scale study of failures in high-performance computing 
systems, in International Conference on Dependable Systems and Networks (DSN’06), 2006
 50. M.  I. L.  HDBK-338, Military Handbook. Electronic Reliability Design Handbook,  US 
Department of Defense Washington, DC, 1998
 51. I. Bazovsky, Reliability theory and practice  (Courier Corporation, 2004)
 52. Intel, Reliability Report, 2017. [Online]. Available: https://www.intel.com/content/dam/www/
programmable/us/en/pdfs/literature/rr/rr.pdf
 53. S.K.  Garg, S.K.  Gopalaiyengar, R.  Buyya, SLA-based resource provisioning for heteroge -
neous workloads in a virtualized cloud datacenter, in International Conference on Algorithms 
and Architectures for Parallel Processing, Springer, 2011, pp. 371–384P. Koutsovasilis et al.121
 54. M.  Ferdman, A.  Adileh, O.  Kocberber, S.  V olos, M.  Alisafaee, D.  Jevdjic, C.  Kaynak, 
A.D. Popescu, A. Ailamaki, B. Falsafi, Clearing the clouds: a study of emerging scale-out 
workloads on modern hardware. SIGPLAN Not. 47, 37–48 (2012)
 55. Eurostat, Electricity Price Statistics, [Online]. Available: https://ec.europa.eu/eurostat/
statistics- explained/index.php?title=Electricity_price_statistics
 56. Amazon, Amazon EC2 Pricing [Online]. Available: https://aws.amazon.com/ec2/pricing/
 57. C. Reiss, J. Wilkes, J L. Hellerstein, Google cluster-usage traces: format+ schema, Google 
Inc., White Paper,  pp. 1–14, 2011Exploiting Reduced Voltage Margins: From the Node to the Datacentre Level123Improving DRAM Energy-efficiency
Lev Mukhanov and Georgios Karakonstantis
1  Introduction
In this chapter, we discuss organization of DRAM devices and DRAM operating 
parameters that have a strong impact on the power efficiency of such devices. We 
consider different factors that affect DRAM reliability, such as temperature and 
program inherent features. Then, we demonstrate a machine learning technique that 
enables us to relax the DRAM operating parameters without compromising DRAM 
reliability. This technique allows to significantly reduce the DRAM power without 
obtaining any DRAM errors on a typical Edge server.
1.1  DRAM Organization and Background
The modern memory subsystems have several channels to access to a number of 
DRAM modules or dual in-line memory modules (DIMMs). Data is transferred 
from/to a DIMM using a memory controller, which sends commands and accessed 
memory addresses to the DIMM, as shown in Fig.  1. Each DIMM implements an 
address decoder  that translates the accessed addresses to the physical memory lay -
out addresses that point to the DRAM cells containing data. DRAM cells are com -
bined into memory array  which is stored in several DRAM chips. Note that DIMMs 
usually have two ranks (sides) with DRAM chips, i.e. one memory array per rank. 
Memory array is organized into banks that consist of two-dimensional arrays based 
on rows and columns. Each cell contains a capacitor and an access transistor, and all 
L. Mukhanov · G. Karakonstantis ( *) 
Queen’s University , Belfast, UK
e-mail:  g.karakonstantis@qub.ac.uk
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_5124
)
Access transis tor
Capacit orBitlin e
Intra-word coupling eﬀ ectsAddress Decode r
Fig. 1  DRAM organization (DDR3 8Gb,1 Gig)
access transistors are in a row connected by a wire called wordline , controlling 
access to the capacitors. When DRAM is accessed, an entire row is read, and sense 
amplifiers measure the charge on each capacitor through bitlines , which are con -
nected to access transistors [ 2–4, 7] (Fig.  1). Then the data from the row is trans -
ferred to Row buffer  for sending it to the memory controller. Importantly, there are 
two different types of DRAM cells: true-cells  and anti-cells  [5–7]. True-cells in the 
charge state store a logic value of ‘1’, while anti-cells store a logic value of ‘0’ in 
the same state. As a result, true-cells manifest errors where a logic value of ‘1’ 
changes to ’0’, while anti-cell trigger errors where ‘0’ a logic a value of changes to ‘1’.
1.2  Mapping Data to Memory Physical Layout
Mapping data to memory physical layout has a strong impact on the performance 
and reliability of DRAMs [ 8]. Figure  1 illustrates how 8 Kbyte chunks of data are 
mapped to physical memory layout for 8GB DDR3 DRAM devices on a typical 
ARM-based Edge server, APM X-Gene2. We see that each 8 Kbyte data chunk is 
mapped to exactly one DRAM row. However, consecutive 8 Kbyte data chunks are 
mapped to rows in different banks. For example, the first 8 Kbyte chunk is mapped 
to the first row of Bank1, while the second row is mapped to the first rowofBank2 . 
Thus, the first 8 Kbyte chunk of data, the 9th data chunk and the 17th data chunk are 
mapped to the first three adjacent rows of the first bank, i.e. Row1.Bank1 , Row2.
Bank1  and Row3.Bank1 . Note that the elements within each 8 Kbyte chunk are 
mapped to the same row but different columns. The function for mapping of 
addresses to physical memory layout for the DIMMs used in our study is shown in 
Fig. 2. In some rows, the order of the cells can be changed due to scrambling or row 
remapping [ 4, 8]. For example, in Fig.  1, the physical memory layout address space 
is scrambled for Rown.Bank1 . In this row, the right neighbour of the first cell should L. Mukhanov and G. Karakonstantis125
Fig. 2  The mapping function of 64-bit aligned addresses to physical memory layout (8  GB 
DDR3 DIMM)
be located in the second column. However, due to the data scrambling, this neigh -
bour is a cell from the third column. Moreover, vendors may remap DRAM col -
umns with faulty cells to the redundant columns which do not have such cells. For 
example, in Fig.  1, the 4th column of Bank1  which has a faulty cell is remapped to 
the last column of this bank. Thus, two consecutive addresses do not always point 
to the data from neighbour cells.
1.3  DRAM Operating Parameters
The main drawback of the DRAM technology is the limited retention time [ 8] of a 
cell’s charge. To avoid any errors induced by the charge leakage over time, DRAMs 
employ an auto-refresh mechanism that recharges the cells in the array periodically. 
Conventionally, all DDR technologies adopted are fresh period, T REFP, of 64ms (or 
32ms) for refreshing each cell of DIMMs, although many cells may have a much 
higher retention time than this T REFP [9]. Another critical parameter for the operation 
of DRAMs is supply voltage. The nominal supply voltage, V DD, of DRAM chips is 
chosen conservatively by vendors to ensure that each chip operates correctly under 
a wide range of conditions [ 10–11].
1.4  DRAM Temperature
Apart from the above circuit parameters, one of the main environmental conditions 
affecting the reliability of DRAM is temperature [ 12–13]. To test DRAM reliability 
under high temperatures, previous studies develop specific thermal testbeds [ 7, 13]. 
There are two alternative approaches for designing such testbeds: i) use a heating 
chamber, in which an entire experimental server is placed, and ii) use heating ele -
ments which are explicitly connected to DRAM devices. For example, the second 
approach is discussed in a recent study [ 7]. In this study, to perform the experiments 
under controlled temperatures, authors implement a unique temperature-controlled 
server testbed using heating elements [ 7]. Figure  3 shows the APM X-Gene2 server 
with four DIMMs fitted with our custom adapters. The temperature of each element 
is regulated by a controller board, as shown in Fig.  4, which contains a Raspberry Pi 
3 [7], four closed-loop PID controllers [ 7] and eight solid-state relays controlling 
the resistive elements of each DIMM and rank independently (Fig.  5).Improving DRAM Energy-efficiency126
Fig. 3  The X-Gene2 server
Fig. 4  Temperature controller board
1.5  Workload-Dependent DRAM Behaviour
Previous studies have shown that the data and memory access pattern of a running 
program may significantly affect DRAM error behaviour [ 3, 13]. The effect of data 
patterns is explained by the fact that the retention time of a cell depends on the value 
stored in this cell. At the same time, specific memory access patterns cell-to-cell 
interference [ 7], which increases the probability of some cells to leak charge. This 
effect has been widely exploited for ‘rowhammer’ attacks [ 7]. Specifically, the data 
in Row2.Bank1  from Fig.  1 may be compromised when Row1.Bank1  and Row3.
Bank1  are accessed too often. Thus, inherent program features that change the data 
and memory access pattern of a running workload may have a significant effect on L. Mukhanov and G. Karakonstantis127
DIMM0 DIMM1
DIMM2 DIMM310102103104
rank 0rank 
1 rank 0
rank 1rank 0rank
 1rank 0
rank 1
kmeans
memcached63X63663X1000X
00X00000X001000Fig. 5  The number of 
1-bit errors detected for 
kmeans and memcached 
when DRAM operates 
under 2.283 s T REFP and 
lowered V DD (1.428 V) at 
50 °C
DRAM reliability. Figure  3 illustrates how DRAM error behaviour may vary when 
running different workloads. This figure shows the spatial and density distribution 
of the detected single-bit errors across four DIMMs (8 ranks) when running kmeans 
and memcached benchmarks for DRAM operating under relaxed T REFP and V DD at 
50 °C (2-h runs). We present the distribution as a polar plot, where θ−axis specifies 
DIMM slots and ranks, while ρ−axis reflects the number of single-bit errors detected 
by hardware (ECC). We see that the number of errors manifested by the benchmarks 
in DIMM3/rank0 differs by 1000x.
1.6  DIMM-to-DIMM Variation
It was shown that DRAM reliability may vary across chips [ 4, 13]. This variation is 
due the manufacturing process [ 14] and the internal design of DRAM modules [ 7]. 
For example, Fig.  3 shows that the number of errors manifested by memcached in 
DIMM2/rank0 is 633x higher than the number of errors triggered by the same 
benchmark in DIMM3/rank0.
1.7  DRAM Errors
DRAM errors may occur in the address decoder, read/write circuity and memory 
cell array [ 15]. To detect the errors , vendors run specific test suites, such as MARCH 
tests and MATS tests [ 15, 16], which can reveal the vast majority of DRAM errors. Improving DRAM Energy-efficiency128
However, these tests were originally designed to stress DRAM reliability at the 
interword level [ 15, 16] and identify coupling faults triggered by bits within a word 
(see Fig.  1a). Thus, such tests may be not effective for detecting intra-word coupling 
faults [ 15, 16]. Moreover, the MARCH and MATS test simply that the physical 
design of the actual memory cell array layout is known, since it is important to map 
data and access to the particular cells in a specific order for testing DRAM reliabil -
ity. However, the physical layout is not known, and it is not provided by vendors. 
Furthermore, in some particular rows, the order of the cells can be changed due to 
scrambling or row remapping [ 15, 16]. Thus, any testing of DRAM reliability by 
storing a specific data pattern in adjacent cells may be incorrect when the address 
mapping scheme for the scrambling process is not known. Moreover, vendors may 
apply some advanced mapping techniques which are not exposed to the user. As a 
result, the MARCH and MATS tests may be not effective for the characterization of 
DRAM reliability when running these tests without knowing the physical memory 
cell layout.
1.8  Error Detection and Correction Hardware
There are several types of errors that may manifest in DRAM chips due to the 
DRAM cell-to-cell interference or charge leakages [ 17]. It is known that the vast 
majority of these errors are single-bit, i.e. errors where only 1 bit is corrupted per a 
64-bit word (or a 72-bit ECC word) [ 17]. To handle such errors, vendors implement 
a special hardware (ECC) for automatic correction of single-bit errors. Note that 
there are different types of ECC schemes, and some of them may correct more than 
1 bit per word. The vast majority of servers implement ECC SECDED (single error 
correction- double error detection) and report all the errors to the Linux kernel, 
providing information about the DIMM, bank, rank, row and column in which an 
error occurred. Table  1 presents three types of memory errors that may occur when 
ECC SECDED is enabled: single-bit errors (or correctable errors, CE); detected 
errors where more than 1 bit is corrupted (or uncorrectable errors, UE); and errors 
where more than 2 bits are corrupted, which are not corrected and not detected by 
ECC. The last type of errors manifest so-called silent data corruption (SDC), since 
such errors are invisible for hardware. Note that if the Linux kernel detects an UE, 
then it panics and halts.
Table 1  Types of DRAM errors (ECC SECDED)
Num. of failure bits Type of errors Abbreviation
1 Corrected CE
>1 Uncorrected/detected UE
>2 Uncorrected/undetected SDCL. Mukhanov and G. Karakonstantis129
2  Revealing DRAM Refresh Rate Margins
In this chapter, we discuss the effect of program inherent features on DRAM error 
behaviour. A characterization study on workload-dependent DRAM error behaviour 
is presented in a recent work [ 19]. We demonstrate that this behaviour can be mod -
elled and predicted using a machine learning model based on inherent program 
features. Finally, the chapter demonstrates how such a model can be used to reveal 
DRAM operating guardbands.
2.1  Characterization of Workload-Dependent DRAM 
Error Behaviour
In a previous work [ 19], authors conducted a systematic study on the effect of dif -
ferent workloads on DRAM error behaviour. In particular, authors run different 
benchmarks from Rodinia and Parsec benchmark suites, such as backprop , nw, 
srad, kmeans  and fmm benchmarks, which represent a variety of compute-intensive 
algorithms. To evaluate how parallelism and processing power affect the character -
ization, authors run these benchmarks with 1 and 8 threads. To investigate the effect 
of popular caching and analytics workloads on DRAM reliability, authors also run 
the memcached  benchmark, the PageRank algorithm ( PageRank ), the betweenness 
centrality algorithm ( bc) and the breadth-first search algorithm ( bfs). Finally, authors 
run each benchmark allocating 8 GB of data to exclude the effect of the data size 
factor on DRAM errors.
2.2  A Typical Edge Server
In this work, authors use a 64-bit ARMv8-based server (see Fig.  3), APM X-Gene2, 
which is a typical Edge server. The X-Gene2 SoC consists of eight 64-bit ARMv8 
cores running at 2.4GHz. The X-Gene2 has four DDR3 memory controller units 
(MCUs). In the experimental campaign, authors are experimenting with 4 Micron 
DDR3 8GB DIMMs at 1866 MHz, with one DIMM per MCU. In total, we are char -
acterizing 72 chips of 4Gb x8 DDR3, since each DIMM includes 16 and 2 DRAM 
chips for data storage and ECC, respectively.
Notably, the X-Gene2 provides access to a scalable lightweight intelligent man -
agement processor (SLIMpro), which is a special management core that is used to 
boot the system and provide access to the on-board sensors to measure the tempera -
ture and the power of the SoC and DRAM. The SLIMpro also reports all memory 
errors corrected or detected by SECDED ECC to the Linux kernel, providing infor -
mation about the DIMM, bank, rank, row and column in which the error occurred. Improving DRAM Energy-efficiency130
Finally, SLIMpro allows the configuration of the parameters of the MCUs, such as 
TREFP and V DD. Specifically, T REFP may be changed from the nominal 64 ms to 2.283 
s, which is the maximum on the X-Gene2 server. The server runs a fully fledged OS 
based on CentOS 7 with the default Linux kernel 4.3.0 for ARMv8 and support for 
64 KB pages.
2.3  Experiments: Workload-Dependent DRAM 
Error Behaviour
Previously, it was discovered that the memory cell leakage  may change over time 
due to a phenomenon called variable retention time (VRT) [ 18]. As a result, DRAM 
error behaviour may vary across runs of the same application, and thus, it is essen -
tial to run each application several times until a target DRAM error metric con -
verges to a specific value. To this end, authors run each application for 2 hours with 
DRAM operating under the maximum T REFP (i.e. 2.283s) and lowered V DD (1.428 V) 
at 50 °C. In their study, authors discovered that 120 minutes is sufficient for identi -
fying the vast majority of error-prone memory locations and characterizing DRAM 
behaviour when running a specific benchmark.
CE (Correctable Errors, Single-Bit Errors)
To measure the rate of single-bit errors, authors introduce a specific metric, WER , 
which is defined as:
    WERN
MEMCE
SIZE=
 
where N CE is the number of unique 64-bit word locations where CEs have mani -
fested and MEM SIZE is the size (in 64-bit words) of memory allocated by the appli -
cation. WER shows the probability of a word being erroneous regardless of the size 
of memory allocated by the application.
In their study, authors investigate how WER varies across benchmarks when 
DRAM operates under different T REFP at 50 °C and 60 °C. Authors run benchmarks 
for DRAM operating under 0.618 s, 1.173 s, 1.727 s, 2.283 s T REFP and lowered 
VDD. Figures  6 and 7 illustrate how WER changes with scaling T REFP at 50 °C. These 
results show that WER varies across benchmarks significantly. Such a variation of 
DRAM error behaviour across workloads, including single-threaded and multi- 
threaded programs, is attributed to different types of access and data patterns in the 
workloads. Authors also demonstrate that WER grows exponentially with T REFP 
[19], which follows the results obtained in other studies [ 3].
To investigate how WER  varies across different DIMMs and ranks, authors 
grouped all the collected errors by a source DIMM/rank. Figure  8 shows WER  mea -
sured on different DIMMs and ranks when DRAM operates under 2.283 s T REFP at L. Mukhanov and G. Karakonstantis131
backprop
backprop(par)kmeans
kmeans(par)nw
nw(par)srad
srad(par)fmm
fmm(par)pagerankbfsbc
memcached0.00.20.40.60.81.0WER (T REFP=0.618 s)x10-9 TREFP(0.618 s)
012345
WER (T REFP=1.173 s)x10-9 TREFP(1.173 s)
Fig. 6  WER for DRAM operating under 0.618s and 1.173s at 50 °C
backprop
backprop(par)kmeans
kmeans(par)nw
nw(par)srad
srad(par)fmm
fmm(par)pagerankbfsbc
memcached012345678WER (T REFP=1.727 s)x10-8 TREFP(1.727 s)
0123456
WER (T REFP=2.283 s)x10-7 TREFP(2.283 s)
Fig. 7  WER for DRAM operating under 1.727s and 2.283s at 50 °C
backpro pbackpro pkmean sk mean s nw nw srad srad fmmf mm pageran k bfsb cm emcached
(par) (par) (par)( par) (par)0.00.51.01.52.02.5WERx10-8DIMM0/rank0
DIMM0/rank1
DIMM1/rank0
DIMM1/rank1
DIMM2/rank0
DIMM2/rank1
DIMM3/rank0
DIMM3/rank1
Fig. 8  WER per DIMM/rank obtained for DRAM operating under 2.283 s TREFP at 50 °C
50 °C. It is seen that WER  varies across DIMMs/ranks by up to 188x; in particular, 
WER  incurred by the bc benchmark on DIMM2/rank0 and DIMM3/rank1 are 
1.75 × 10−7 and 9.31 × 10−10, respectively. Thus, we may conclude that DRAM error 
behaviour varies across different workloads and DRAM chips.Improving DRAM Energy-efficiency132
UE (Uncorrectable Errors, Multi-bit Errors)
In the experiments with DRAM operating at 50 °C and 60 °C, authors have discov -
ered no Silent Data Corruptions (SDCs) or uncorrectable errors (UEs). However, 
authors encounter UEs and system crashes when raising the DRAM temperature to 
70 °C and scaling T REFP up to 1.45 s under lowered V DD. Note that in the used server, 
any UE triggered by the Linux kernel or a user-level program, once detected by 
ECC, will result in a system crash.
To measure the effect of different workloads on multi-bit errors, i.e. UE, authors 
introduce a new metric, the probability of uncorrectable errors, P UE:
    PN
NUEUE
EXP=
 
where N UE is the number of experiments with the application that resulted in an UE 
and N EXP is the total number of experiments with the application.
Figure 9 shows P UE measured across all benchmarks for DRAM operating under 
1.450 s, 1.727 s, 2.283 s T REFP and lowered V DD at 70 °C. To estimate this probabil -
ity, authors repeat each 2-hour experiment with a specific benchmark ten times. It is 
seen that P UE varies significantly across benchmarks for DRAM operating under 
1.450 s T REFP; it achieves 0.8 for fmm(par) , whereas it equals to 0 for memcached  
and PageRank . We can also see that P UE is greater than 0 only for the parallel com -
pute-intensive benchmarks, while it is 0 for all the single-threaded benchmarks 
except for srad. The P UE averaged over all benchmarks for DRAM operating under 
1.450s T REFP is below 0.4. However, when we increase T REFP up to 1.727s, then the 
likelihood of crashing averaged over benchmarks grows by 2.15× (see Fig.  9). 
Moreover, for DRAM operating under this T REFP, there is no benchmark with P UE = 
0. Finally, all the benchmarks trigger UEs in 100% of the experiments when we use 
the maximum T REFP (2.283 s) at 70 °C. These results show that T REFP and the DRAM 
temperature have a dominant effect on the likelihood of an UE.
Figure 10 depicts the probability to obtain an UE on a specific DIMM/rank when 
ECC detects an UE. We see that the vast majority of UEs are triggered by DIMM0/
rank1 and DIMM2/rank0, while DIMM3/rank1 does not trigger UEs at all. Thus, 
similarly to WER, the probability of UE varies across DIMMs.  Importantly, we 
have discovered no SDCs when running experiments under different T at 50 °C, 
60 °C and 70 °C.
backprop kmeans nwsrad fmm
backprop(par)
kmeans(par)
nw(par)
srad(par)
fmm(par)memcachedpagerankbfs bc
Average0.00.20.40.60.81.0PUE1.450 s 1.727 s 2.283 s
Fig. 9  P UE obtained for different workloads when DRAM operates under 1.450 s, 1.727 s and 
2.283 s T REFP at 70 °CL. Mukhanov and G. Karakonstantis133
0.00.10.20.30.40.50.60.7The probability 
to obtain an UE0.020.24
<0.01 <0.010.67
0.05<0.01 0
DIMM0
rank0DIMM0
rank1DIMM1
rank0DIMM1
rank1DIMM2
rank0DIMM2
rank1DIMM3
rank0DIMM3
rank1
Fig. 10  The probability to obtain and UE on a specific DIMM/rank when DRAM operates under 
1.450 s, 1.727 s and 2.283 s TREFP at 70 °C
0.2 0.0 0.2 0.4 0.6
rS(PUE)0.60.40.20.00.20.40.60.8rS(WER)MCU accessesMemory accesses
CPICPIwait cycles
CPU utilizationHDP
Treuse
Fig. 11  The Spearman's 
correlation coefficient for 
249 program features and 
WER and P UE
Correlation of Program Inherent Feature with DRAM Error Behaviour
To investigate software-level factors that may affect DRAM reliability, authors 
extract 249 most significant program inherent features, such as the data entropy 
(HDP), the memory reuse distance (T reuse), memory access rate and others [ 19]. To 
identify those features that may have a strong impact on DRAM reliability, authors 
correlate the extracted program features for each benchmark with both W ER and P UE 
metrics. To identify and quantify any dependency between program features and the 
DRAM error metrics formally, authors use the Spearman’s rank correlation coeffi -
cient (r s). This correlation coefficient allows us to detect both linear and nonlinear 
relationships [ 19]. Coefficient values lie in a range [ −1, +1] in which −1 or +1 
occurs when there is a perfect monotonic relationship between two variables.
Figure 11 shows the correlation coefficients for 249 program features and WER 
on the Y-axis, whereas the correlation coefficients for these features and P UE are 
shown on the X-axis. We see that the number of memory accesses per cycle is 
highly correlated with WER, as r s is above 0.57, indicating a positive direction of Improving DRAM Energy-efficiency134
the correlation; in other words, WER grows with the number of accesses per cycle. 
It is also seen that the group of performance indicators that reflects the number of 
issued memory read and write commands per cycle in different MCUs is also highly 
correlated with WER. However, the number of such commands is determined by the 
number of memory read and write instructions executed by the processor per cycle.
Another inherent program feature that is strongly correlated with WER is wait 
cycles  (rs is 0.4). This feature reflects the ratio of the number of cycles spent on wait -
ing for data to the total number of program cycles. Nonetheless, wait cycles  is 
implicitly determined by the number of memory accesses per clock cycle, as it 
encapsulates idle cycles due to memory access stalls which explains its correlation 
with WER.
Authors’ experiments show that the data entropy (i.e. H DP), which reflects the 
data pattern of a running application, is also correlated with WER as the r s is 0.39 
(see Fig.  11). Although it is higher than the r s obtained for T reuse, it is by 51% lower 
than the r s observed for the memory access rate.
Similarly to WER, authors discovered a correlation between P UE and the memory 
access rate, the number of issued memory read and write commands per cycle in 
different MCUs, H DP and wait cycles . However, the level of this correlation is lower 
than in the case of WER; for example, the r s for the memory access rate and P UE is 
0.43, which is 35% less than the same r s for WER. It is noteworthy that unlike previ -
ous studies, which have indicated a strong impact of T reuse or H DP [19], we obtain the 
highest r s for the memory access rate among all the program features when correlat -
ing it with WER and P UE metrics.
These results indicate that it is possible to predict workload-aware DRAM error 
behaviour for a running workload based on its performance inherent characteristics. 
However, apart from considering the effect of a running workload on DRAM reli -
ability, a prediction model should also take into consideration of DRAM reliability 
across chips. To this end, it is reasonable to use a machine learning model to predict 
DRAM error behaviour based on performance inherent features of a running work -
loads for specific DRAM chips.
2.4  Modelling DRAM Error Behaviour
To collect data for training the a ML model, authors run a set of representative 
benchmarks (workloads) under varying DRAM operating parameters, such as T REFP 
, V DD and temperature, and measure WER and P UE, as shown in Fig.  12 [19]. In addi -
tion, authors run each benchmark to collect all the inherent program features using 
DynamoRio and the perf tool ( Profiling phase ). Then, they combine collected pro -
gram inherent features with the WER or P UE measurements.
In their study, authors demonstrate that KNN (K-nearest neighbours  algorithm)  
provides the highest accuracy in WER and P UE (the average error of prediction is 
12.3% and 4.1% for WER and PUE, respectively) predictions when using the 
DRAM temperature, the DRAM refresh rate and the following program: wait cycles 
memory accesses , H DP (the data entropy)  and Treuse  (the reuse time ) [19]. Note that in this study, L. Mukhanov and G. Karakonstantis135
   Data 
polic yExtract progra m
features:  
 - perf
 - DynamoRi oProgram featur es:
 - the memory 
         access rate
 - TREUSE  Workload 1
  Workload 2
  Workload N
  ...
  400500: ldr    x1, [sp]
  400504: add  x2, sp, #0x8
  400508: mov x6, sp
  40050c: ldr    x0, 40052 0
  400510: ldr    x3, 400528
  400514: ldr    x4, 400530
  ...
Program features:
 - the memory 
         access rate
 - TREUSE
 - HDPDRAM characterization:
Run workloads  under 
varying T REFP,VDD and 
temperatur e DRAM error 
metrics :
 - WER
 - PUE
Profiling phaseDRAM characterization 
phaseData collection
 Train the 
 ML model Search the 
marginal TREFPOFFLINE
 Measure 
TEMP DRAM
 Set the 
marginal TREFPRUNTIME
 Monitor
Correctable 
errors (CEs ) Set the
nominal DRAM
parameters
 Handle an 
abnormal DRAM
behavio rNO The number 
of CEs > 
the threshol dYESThe governo r
NO
Proactivepolic yReacti ve
Build data set.MODEL INPUT : 
TREFP
, V
DD
,TEMP
DRAM
, T
REUS
E, H
DP
,MODEL OUTPUT : WER, P
UE
Fig. 12  Overview of the data collection and validation processes
several solvers have been used, such as SVMs (support vector machines)  and 
Random Decision Forests (RDF) .
2.5  Revealing DRAM Operating Margins Using 
the Workload- Aware DRAM Error Behaviour Model
Once the model is trained, it allows to investigate the effect of various program 
inherent features on DRAM reliability without long-running characterization cam -
paigns. By applying this model, it becomes possible to identify the set of program 
inherent features (the worst-case set of program features) that are likely to incur the 
highest WER or P UE for a specific T REFP, V DD and DRAM temperature. Furthermore, 
through an exhaustive search, we can find the set of DRAM circuit parameters for 
which the maximum possible WER or P UE does not exceed a user-defined margin, 
according to the ML model. Specifically, to achieve maximum power savings, we 
need to find a pair of T REFP and V DD which minimizes the DRAM power. However, 
provided that the scaling of V DD has a negligible effect on DRAM reliability, to 
minimize the power, it is sufficient to find the maximum T REFP under the lowered 
VDD (1.428V) for which WER or P UE does not exceed the margin. The algorithm of 
such a search is presented in Fig.  13, where Dev is the DRAM device; SFTRS defines 
possible values of program inherent features that have been used for training of the 
model; M is a predicted DRAM error metric (WER or P UE); and TREFPMAX is the maxi -
mum T REFP that matches the search criteria. Note that the program features used in 
our study are continuous, and thus it is impossible to check all possible values of the 
program inherent features. As a result, to find the parameters that trigger the worst- 
case DRAM error behaviour, authors vary the values of program features in the 
following ranges: T reuse∈[0.05,10]; H DP∈[0,31]; the memory access rate ∈ 
[0.001,0.5]; and wait cycles ∈ [0.001,0.5]. We change T reuse, H DP, wait cycles and the 
memory access rate in increment of 0.05, 1, 0.01 and 0.01, respectively.Improving DRAM Energy-efficiency136
Fig. 13  The search algorithm
By applying this search algorithm, we can find the marginal T REFP for which the 
maximum possible WER obtained in the worst-case scenario is 0 ( according to the 
ML model) when DRAM operates at a specific temperature. In other words, this 
model enables us to find the T REFP for which it is likely that none of workloads will 
trigger errors, including single-bit errors (or CEs). This algorithm can be also 
applied to find the marginal T REFP for which the maximum possible P UE obtained in 
the worst-case scenario is likely to be 0. Although this T REFP will be higher than the 
marginal T REFP obtained for WER=0 and, theoretically, will provide more power 
savings, DRAM operating under such a T REFP may manifest correctable errors. 
However, in the vast majority of data centres, even CEs are not desirable, especially 
in case of hundreds or thousands of CEs, since detected CEs indicate an abnormal 
hardware behaviour. To follow this, in our study, we search for the marginal T REFP 
that doesn’t manifest any errors (WER= 0). It is important to note that it is extremely 
challenging to find the marginal T REFP(WER= 0) without applying the ML model, 
since the model enables us to vary program inherent features and detect the worst-
case set of program features that is likely to incur the highest WER. It is hardly 
possible to detect such features by characterizing DRAMs, even with hundreds of 
benchmarks where the set of program inherent features is fixed.
Figure 14a shows the discovered marginal T REFP when WER=0 for each DIMM/
rank predicted by the model for DRAM operating at 50 °C. We see that the pre -
dicted T REFP varies across DIMMs by almost nine times: for example, the marginal 
TREFP detected by our model for DIMM1/rank1 is 1447ms, while for DIMM2/rank1, 
this T REFP is 135ms. In other words, according to our model, there exists a workload 
that may trigger single-bit errors on DIMM2/rank1 when it operates under T REFP 
which is higher than 135 ms at 50 °C. Thereby, according to our model, we almost 
cannot relax T REFP for DIMM2/rank1 without a risk to obtain CEs. Thus, a guard -
band T REFP of 64ms used by the vendor is almost optimal for this specific 
DIMM. However, in the case of DIMM1/rank1, such a guardband for T REFP is too 
pessimistic, since the model shows that T REFP can be increased by nine times without 
observing errors. Notably, in the worst-case sets of program features, discovered by 
the search algorithm for different DIMMs and temperatures, T reuse is always slightly 
higher than T REFP. Accordingly, a workload can degrade DRAM reliability signifi -
cantly only when T reuse > T REFP, regardless of values of other program features, such L. Mukhanov and G. Karakonstantis137
Fig. 14  The marginal 
TREFP predicted by the 
model for DRAM 
operating at 50 °C ( a), 
60 °C ( b) and 70 °C ( c)
as H DP. By setting the marginal T REFP, our governor enables power and energy sav -
ings without compromising DRAM reliability. The governor sets the T REFP for a 
DIMM which is the minimum of the marginal T REFPs discovered for both ranks of 
the DIMM. Figure  14b shows the power savings averaged over all DIMMs that can 
be achieved by scaling T REFP and V DD. In this figure, we provide the DRAM power 
measurements taken for the PageRank benchmark, which incurs the highest DRAM 
power in our experiments, and power measurements averaged over all benchmarks. 
We see that in both cases, the DRAM power does not change if the T REFP is higher 
than 872 ms. This is explained by the fact that the DRAM power is almost logarith -
mically proportional (the base<1) to T REFP, and it does not change a lot with increas -
ing T REFP after some point. Interestingly, the marginal T REFP discovered by the ML 
model for DIMM0, DIMM1 and DIMM3 operating at 50 °C are exactly 872ms or Improving DRAM Energy-efficiency138
even higher (1447ms for DIMM1/rank1 andDIMM3/rank0). Thus, our governor 
achieves the maximum possible power savings, which is 24% on average, for these 
DIMMs by setting the marginal T REFP and V DD at 50 °C. However, if authors raise 
the DRAM temperature to 60 °C, then the governor can scale T REFP only for DIMM1 
and DIMM3 without compromising reliability. Moreover, according to the ML 
model, T REFP cannot be relaxed for any of the DIMMs when they operate at 70 °C 
without a risk to obtain an error (see Fig.  14c). As a result, the governor will not 
provide any savings at this temperature.
2.6  Power Savings
Authors validated the predicted marginal T REFP by running different benchmarks 
and virtual machines (VMs) for DRAM operating at various temperatures for 3 
weeks, and we have not discovered any CEs or UEs. Figure  15 shows the DRAM 
power measurements taken during one of the validation experiments. In this particu -
lar experiment, authors run radix (Parsec), FFT (Parsec) and lulesh benchmarks, 
which have not been used for training of the model. Authors also executed black -
scholes, x264 and fluidanimate benchmarks running on QEMU/KVM VMs. We see 
that the DRAM power varies across benchmarks: FFT incurs the highest DRAM 
power (more than 11W), while the benchmarks running on VMs consume less than 
5W.  This is explained by the fact that the memory access intensity drops when 
authors use VMs, and as a result, the dynamic DRAM power also decreases. 
Nonetheless, the governor enables us to reduce the average power and the peak 
DRAM power by 24% and 38% (FFT), respectively, when DRAM operates at 
50 °C. When authors run benchmarks at 60 °C, the DRAM power savings reduce 
slightly, since the governor reduces T REFP for DIMM0 to avoid memory errors fol -
lowing the model.
Any incorrect prediction from the model may theoretically result in an UE, and 
thus a system crashes when T REFP and V DD are scaled. To address this issue, we need 
to implement an additional mechanism that will detect any abnormal behaviour for 
DRAMs operating under scaled circuit parameters and prevent system crashes. To 
implement a mechanism that automatically detects any abnormal DRAM error 
behaviour, authors extend the governor with a specific policy (a reactive policy ). 
The main idea behind this policy is that no errors should be manifested when DRAM 
operates under the marginal T REFP and V DD (WER=0) according to our model. 
Fig. 15  The DRAM power savings provided by the governor when using the marginal DRAM 
parametersL. Mukhanov and G. Karakonstantis139
Therefore, manifested CEs can be an indicator that DRAM operates abnormally. 
Once the governor detects CEs, it sets T REFP and V DD to the nominal values.
To validate the reactive policy of the governor in this experiment, authors injected 
100 CEs into the system log file monitored by the governor when running black -
scholes and x264 (VMs) at 60 °C. In Fig.  15, we see that the governor sets the nomi -
nal values for T REFP and V DD when 100 CEs have been detected, and as a result, there 
is no power savings for these benchmarks at 60 °C. Finally, when authors run our 
experiment at 70 °C, the governor uses the nominal DRAM settings according to the 
predictions provided by model.
3  Conclusion
This chapter discussed the energy efficiency challenge that faces all the state-of-the- 
art Edge servers. Particularly, the Edge servers should operate under strict power 
constraints since they are placed outside standard data centres. It is projected that 
the DRAM subsystem will soon be responsible for more than 40% of the overall 
power consumption within most servers due to the growth of IoT. One of the rea -
sons for a high energy consumed by the DRAM devices is the usage of pessimistic 
DRAM operating parameters, such as voltage, refresh rate and timing parameters, 
set by the vendors. In this chapter, we present a machine learning technique that 
allows to identify the marginal DRAM parameters that do not compromise reliabil -
ity and thus enable the vendor significantly to reduce the DRAM power.
References
 1. B. Giridhar, M. Cieslak, D. Duggal, R. Dreslin-ski, H. M. Chen, R. Patti, B. Hold, C. Chakrabarti, 
T. Mudge, D. Blaauw, Exploring dram organizations for energy-efficient and resilient exascale 
memories, in Proceedings of the International Conference on High Performance Computing, 
Networking, Storage and Analysis  (ACM, 2013), p. 23
 2. K. K. Chang, A. Kashyap, H. Hassan, S. Ghose, K. Hsieh, D. Lee, T. Li, G. Pekhimenko, 
S. Khan, and O. Mutlu, Understanding latency variation in modern dram chips: Experimental 
characterization, analysis, and optimization, SIGMETRICS Perform. Eval. Rev. , vol. 44, no. 1, 
pp. 323–336 2016. [Online]. Available: http://doi.acm.org/10.1145/2964791.2901453
 3. S. Khan, D. Lee, Y . Kim, A. R. Alameldeen, C. Wilkerson, and O. Mutlu, The efficacy of 
error mitigation techniques for dram retention failures: a comparative experimental study,” 
SIGMETRICS Perform. Eval. Rev. , vol. 42, no. 1, pp.  519–532, 2014. [Online]. Available: 
http://doi.acm.org/10.1145/2637364.2592000
 4. S. Khan, C. Wilkerson, Z. Wang, A. R. Alameldeen, D. Lee, O. Mutlu, Detecting and mitigating 
data-dependent dram failures by exploiting current memory content, in Proceedings of the 50th 
Annual IEEE/ACM International Symposium on Microarchitecture , ser. MICRO-50’17 (ACM, 
New York, 2017), pp. 27–40 [Online]. Available: http://doi.acm.org/10.1145/3123939.3123945
 5. B. Keeth, R.J. Baker, B. Johnson, F. Lin, DRAM Circuit Design: Fundamental and High-Speed 
Topics , 2nd edn. (Wiley-IEEE Press, 2007)Improving DRAM Energy-efficiency140
 6. X.-C. Wu, T. Sherwood, F. T. Chong, Y . Li, Protecting page tables from rowhammer attacks 
using monotonic pointers in dram true-cells, in Proceedings of the Twenty-Fourth International 
Conference on Architectural Support for Programming Languages and Operating Systems , 
ser. ASPLOS ’19. New  York, NY , USA: Association for Computing Machinery, 2019, 
pp. 645–657. [Online]. Available: https://doi.org/10.1145/3297858.3304039
 7. L.Mukhanov, D.S.Nikolopoulos, G.Karakonstantis, Dstress: Automatic synthesis of dram reli -
ability stress viruses using genetic algorithms, in 2020 53rd Annual IEEE/ACM International 
Symposium on Microarchitecture (MICRO) , 2020, pp. 298–312
 8. L.-T. Wang, C.-W. Wu, X. Wen, VLSI Test Principles and Architectures: Design for Testability 
(Systems on Silicon)  (Morgan Kaufmann Publishers Inc., San Francisco, 2006)
 9. M. Jung, D. M. Mathew, C. C. Rheinlander, C. Weis, N. Wehn, A platform to analyze DDR3 
dram’s power and retention time, IEEE Des. Test. 34(4), 52–59 (2017) [Online]. Available: 
https://doi.org/10.1109/MDAT.2017.2705144
 10. K. K. Chang, A. G. Yauglikcci, S. Ghose, A. Agrawal, N. Chatterjee, A. Kashyap, D. Lee, 
M.  O’Connor, H.  Hassan, O.  Mutlu, Understanding reduced-voltage operation in modern 
dram devices: experimental characterization, analysis, and mechanisms, in Proc. ACM Meas. 
Anal. Comput. Syst., vol. 1, no. 1, pp. 10:1–10:42, 2017. [Online]. Available: http://doi.acm.
org/10.1145/3084447
 11. H. David, C. Fallin, E. Gorbatov, U. R. Hanebutte, O. Mutlu, Memory power management via 
dynamic voltage/frequency scaling, in Proceedings of the 8th ACM International Conference 
on Autonomic Computing , ser. ICAC’11 (ACM, New  York, 2011), pp.  31–40 [Online]. 
Available: http://doi.acm.org/10.1145/1998582.1998590
 12. T. Hamamoto, S. Sugiura, S. Sawada, On the retention time distribution of dynamic random 
access memory (dram). IEEE Transactions on Electron Devices  45(6), 1300–1309 (1998)
 13. J. Liu, B. Jaiyen, Y . Kim, C. Wilkerson, and O. Mutlu, “An experimental study of data reten -
tion behavior in modern dram devices: Implications for retention time profiling mechanisms,” 
in Proceedings of the 40th Annual International Symposium on Computer Architecture , ser. 
ISCA ’13. New York, NY , USA: ACM, 2013, pp. 60–71. [Online]. Available: http://doi.acm.
org/10.1145/2485922.2485928
 14. K. Kim and J. Lee, “A new investigation of data retention time in truly nanoscaled drams,” 
IEEE Electron Device Letters , vol. 30, no. 8, pp. 846–848, Aug 2009.
 15. L.-T. Wang, C.-W. Wu, X. Wen, VLSI Test Principles and Architectures: Design for Testability 
(Systems on Silicon)  (Morgan Kaufmann Publishers Inc., San Francisco, 2006)
 16. P.  Girard, N.  Nicolici, X.  Wen, Power-Aware Testing and Test Strategies for Low Power 
Devices , 1st edn. (Springer, Cham, 2009)
 17. V . Sridharan, N. DeBardeleben, S. Blanchard, K.B. Ferreira, J. Stearley, J. Shalf, S. Gurumurthi, 
Memory errors in modern systems: the good, the bad, and the ugly. in Proceedings of the 
Twentieth International Conference on Architectural Support for Programming Languages 
and Operating Systems , ser. ASPLOS’15, 2015, pp. 297–310
 18. P.J. Restle, J.W. Park, B.F. Lloyd, Dram variable retention time. in 1992 International Technical 
Digest on Electron Devices Meeting , December 1992, pp. 807–810
 19. L. Mukhanov, K. Tovletoglou, H. Vandierendonck, D. S. Nikolopoulos, G. Karakonstantis, 
Workload-aware dram error prediction using machine learning. in 2019 IEEE International 
Symposium on Work- load Characterization (IISWC) , 2019, pp. 106–118L. Mukhanov and G. Karakonstantis141Total Cost of Ownership Perspective  
of Cloud vs Edge Deployments of IoT 
Applications
Panagiota Nikolaou, Yiannakis Sazeides, Alejandro Lampropulos, 
Denis Guilhot, Andrea Bartoli, George Papadimitriou, 
Athanasios Chatzidimitriou, Dimitris Gizopoulos, Konstantinos Tovletoglou, 
Lev Mukhanov, Georgios Karakonstantis, Marios Kleanthous, 
and Arnau Prat
1  Introduction
We are currently witnessing the development of the IoT era. IoT refers to the net -
worked interconnection of everyday objects denoted as “things” that are used to 
achieve certain design goals [ 1]. These devices are mainly embedded systems that 
communicate with other devices by sending data through the Internet [ 1]. The num -
ber of Internet-connected devices is growing daily and will soon be in the order of 
tens of billions. Figure  1 depicts the increase of these Internet-connected devices 
throughout the last few years. In order to process the data that are sent to the Internet, 
large DCs have increased in number and size in all over the world. These DCs differ 
from the traditional hosting facilities because they consist of large-scale servers in 
the “cloud” with well-connected processing and storage resources, commonly 
referred to as cloud computing [ 2].
However, the rise of cloud computing, where most compute power is located in 
the DC, comes with a number of challenges. One of the key challenges is the 
P. Nikolaou ( *) · Y . Sazeides 
University of Cyprus, Nicosia, Cyprus
e-mail:  panagiota.nikolaou@cs.ucy.ac.cy  
A. Lampropulos · D. Guilhot · A. Bartoli 
Worldsensing, Barcelona, Spain 
G. Papadimitriou · A. Chatzidimitriou · D. Gizopoulos 
National and Kapodistrian University of Athens, Athens, Greece 
K. Tovletoglou · L. Mukhanov · G. Karakonstantis 
Queen’s University Belfast, Belfast, UK 
M. Kleanthous 
Meritorious Audit Limited, Nicosia, Cyprus 
A. Prat 
Sparsity, Catalonia, Spain
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_6142
Billions of Devices
50
40
30
20
10
0
1988 1992 1996 2000 2004 2008 2012 2016 20202016
22.9B2017
28.4B2019
42.1B
2018
34.8B2020
50.1B
2014
14.2B2015
18.2B
2013
11.2B
2009
loT
Inception1992
1M2003
0.5B2012
8.7B
Fig. 1  Internet-connected devices
communication that causes huge network traffic. In addition to this, there are some 
other challenges such as security [ 3]. Thus, it is essential to design a new architec -
ture by considering many parameters such as reliability, scalability and QoS. Edge/
Fog computing is a recently introduced approach that has the potential to ensure the 
sustainability and scaling of the Internet in the IoT era. This paradigm advocates for 
the execution of services closer to the sources of data [ 4, 5], aiming this way to 
reduce application latency between the end user and the DC and at the same time 
relaxes the pressure on network bandwidth. The most critical challenge for the suc -
cessful Edge deployment is the efficient use of the limited power of an Edge instal -
lation. Exceeding the power cap of the facility is unacceptable as there will be a 
disruption of power. To avoid such overloads, both Edge and Cloud deployments 
rely on power capping schemes that enforce the power budgets of individual servers 
[6] or over ensembles [ 7, 8]. In this regard, the use of more power efficient servers 
is inevitable.
This chapter evaluates the trade-offs among the power efficiency and QoS when 
running two IoT applications in Edge vs Cloud environments. To accomplish this, 
we rely on the Total Cost of Ownership (TCO) metric. The two applications that are 
evaluated are the Polaris application and the SocialCRM application. Basically, 
these applications process data sent by sensors or active clients either in Edge or in 
Cloud deployments.P. Nikolaou et al.143
2  Edge Computing
The need for fast response times in various IoT applications necessitates deploy -
ments with tight QoS timing requirements. Many applications cannot tolerate laten -
cies that exceed one or two hundred of milliseconds [ 9, 10]. Even though Cloud 
Computing is centralized and requires minimal management effort or service pro -
vider’s interaction, it hardly meets the QoS and response time requirements for IoT 
applications, due to the network latency between the sensors and a remote DC. On 
the other hand, deployments closer to the data facilitate meeting QoS requirements 
by avoiding network latency [ 11, 12]. This distributed deployment near the sources 
of data at many sites is referred as Edge (or Fog) computing [ 4]. Edge Computing 
is not meant to replace traditional Cloud architectures, but Cloud and Edge can 
work in unison to reduce the total end-to-end response time. Edge is well suited for 
IoT applications, where sensors collect data and send them to Edge sites for pro -
cessing, thus avoiding high network latencies compared to a centralized DC. The 
Edge deployment acts as a filter that reduces the network bandwidth pressure to the 
Cloud [ 13]. According to the application scenario and the processing power of the 
different devices, edge computing can be based either on a two-tier [ 14] or on a 
three-tier computational model [ 15]. In both models, the last tier corresponds to the 
Cloud computing resources. Moreover, the first tier includes the IoT-embedded 
devices such as drones, sensors, devices and appliances in smart homes [ 14, 15]. 
These devices need to be self-configured, self-maintained, self-repaired and make 
independent decisions. The main difference between the two models is that in the 
two-tier model, the IoT embedded devices have the computational power to process 
their monitored data. After processing, the data are sent to the last-tier (Cloud) to 
complete the processing of more complex tasks, if needed. On the other hand, on the 
three-tier model, the embedded devices are used only for monitoring the data. Then 
the data are sent to the middle-tier for processing. This tier includes different tech -
nologies such as mobile devices, normal servers or gateways and cloudlets/
micro-DCs.
Mobile Devices Technology  this technology includes laptops, tablets and smart -
watches that are located in the same facility with the IoT devices. This technology 
leverages idle computational power and storage space of the mobile devices to per -
form necessary computations [ 16–18].
Normal Servers or Gateways Technology  analogous to mobile devices, this tech -
nology includes common servers that can be hosted in an Edge facility such as typi -
cal house buildings and provide computational power to the monitored data [ 18]. In 
contrast to the mobile devices, this technology provides more computational power 
and is dedicated and fully utilized only for the processing of the IoT monitored data.
Cloudlets or Micro-DCs (MD) Technology  in this technology each Edge deploy -
ment site can contain one or even numerous servers called cloudlets/MD. Cloudlets/
MD are intermediate layers that are located between the cloud and the IoT-embedded 
devices. The only difference between the two is that in the cloudlet technology the Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications144
software is provided by a cloudlet provider, in contrast to the MD that the users are 
responsible for the software. Due to the fact that cloudlets/(MD) are small clouds 
they can be referred as “DCs in a box” [ 17–20]. The users can rent virtual machines 
(VMs) on the nearest cloudlet/MD to process their jobs [ 18, 19, 21]. The cloudlet/
MD is a self-contained, secure computing environment that includes all necessary 
computation, storage, and networking equipment to run customer applications or 
applications provided by a cloudlet provider. They usually have power budget in a 
range of 1–100KW to meet the application demands.
There are also some other models, called hierarchical, that consist of more than 
three tiers [ 22]. The theoretical comparison between flat typical models with the 
hierarchical shows that the second one is more beneficial in terms of latency [ 22].
Apart from the need to reduce the latency to satisfy an application’s QoS time 
requirement, the communication of the data to the Cloud can lead to serious security 
and privacy issues, which in some cases is unacceptable to the end users [ 23]. 
Furthermore, energy efficiency and cost reduction are some other benefits of the 
edge technologies [ 18].
Figure 2 shows an IoT system architecture that includes both Edge and Cloud 
deployments. The Edge servers are placed near the data and are responsible for data 
collection from various IoT devices, data processing and transferring a concise 
report to the Cloud.
Despite the substantial advantages, Edge Computing has some limitations. A 
major one is that Edge sites/facilities are power constrained [ 24]. Thus, the number 
of servers per site needs to fit the power budget that is provided by an electricity 
provider and is not already allocated for other uses. Edge facilities can be ordinary 
buildings with several other electrical appliances in use. Certainly, an electricity 
provider can increase the power budget at a facility but this comes with an extra 
cost. Consequently, Edge servers that are more power efficient may hold the key for 
Fig. 2  IoT system architecture including edge and central Cloud deploymentsP. Nikolaou et al.145
successful Edge deployment since they will allow more servers per site and process -
ing of data from more sensors without the extra costs to the electricity providers.
Several prior works consider the Edge-Cloud trade-offs to decide where to place 
highly constrained applications and satisfy their requirements without compromis -
ing power and availability [ 22–30]. It is also worth highlighting that existing studies 
of Edge deployments relied on measurements obtained from either too simple 
devices (e.g. raspberry-pi) or too powerful ones (e.g. classical high-end servers) 
which do not strike a fair balance between energy and performance that is essential 
in Edge installations [ 25, 31].
3  Total Cost of Ownership (TCO)
The TCO is determined by its capital and operational expenses and is influenced by 
the following five main parameters:
• Infrastructure Cost: the cost of acquisition of a DC building (real estate and 
development), power distribution and the cooling equipment acquisition cost.
• Server Cost.
• Networking Equipment Cost.
• Operating Expenses: the cost of electricity for servers, networking equipment 
and cooling.
• Maintenance and Staff Expenses: the cost for repairs and the personnel salaries.
DC infrastructure, server and networking equipment costs represent the capital 
expenses, whereas the DC operating and maintenance costs represent the opera -
tional expenses. The TCO is determined by the sum of capital and operational 
expenses. Capital expenses (CAPEX) include the cost of acquisition of a building, 
the power capex costs with the electricity payment, the cooling equipment acquisi -
tion cost, the cost of acquiring the servers including all their components and net -
working equipment costs. On the other hand, operational expenses (OPEX) include 
operation power and maintenance costs. The TCO of a deployment consisting of N 
servers is determined as the sum of Capex Cost ( CCape xi) and Opex Cost COpexi /g11/g12  of 
all the servers (i) as follows [ 32]:
 TCOC C
iN
Cape x Opexii/g32 /g170/g172/g186/g188 /g166 /g14
 
An overview of the simple TCO framework is shown in Fig.  3. For each different 
server configuration type (compute nodes, database nodes, storage nodes), the esti -
mation starts with spares estimation that determines (i) the number of hot spares 
required to mitigate performance variability and ensure meeting performance 
requirement for the peak workload, and (ii) the number of cold spares needed due to 
server failures. The number of active servers, the initial number of servers estimated Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications146
Fig. 3  TCO framework overview [ 9]
assuming no variability plus the hot spares, will determine the costs for DC infra -
structure, server acquisition, networking equipment, and power. The cold spares are 
used to determine the maintenance cost. These costs are then summed together to 
produce the contribution to the TCO of a given server type. The global TCO is the 
sum of the contribution from all server types (shown in the above equation). Prior 
works proposed to guide the DC design, accounting TCO as the key parameter 
[32–38]. The tool presented in [ 32] is a holistic TCO tool consisting of main param -
eters such as performance and power and, thus, provides more accurate TCO results 
than all the other available tools. In particular, this chapter investigates parameters 
that are correlated with the TCO and used to provide optimizations by extending the 
TCO tool in [ 32].
4  TCO Benefits of IoT Applications Running at the Edge
Figure 4 shows the cumulative distribution of the end-to-end latency when the same 
application runs in Edge and Cloud deployments. The end-to-end latency includes 
network and compute time of the application. As Fig.  4 shows, the difference 
between the End-to-End latency for the Cloud and Edge deployments is consider -
able. For a high QoS response time constraint application this extra latency may P. Nikolaou et al.147
Fig. 4  End-to-End Latency for running the application in edge and cloud deployments [71]
render infeasible to run the application on the Cloud or in the best case requires an 
expensive Cloud deployment to ensure fast processing latency.
Recent studies develop schemes that manage the data processing of IoT applica -
tions across distributed DCs [ 22, 24, 25]. In these studies, data are transferred from 
IoT sensors to local micro-DCs for pre-processing and selection which of the data 
to forward to a centralized DC.  The data are collected and sent to a processing 
device. The main QoS requirement is the response time and, therefore, the applica -
tions are naturally suited for Edge deployments. However, servers used to run these 
applications can only process, within a required detection time window, data from a 
limited number of sensors, or put in another way, servers oversubscribed to process 
data from many sensors will suffer from QoS violations. The motivation of this 
chapter is based on our previous work [ 39].
This earlier paper illustrates that for the jamming detection application an Edge 
deployment is more TCO efficient than a Cloud one. The results also show that by 
making Edge more power efficient, approximately by 9%, it translates to consider -
able gains in the TCO/Area-coverage metric. This chapter uses the same methodol -
ogy as in [ 39] to provide an evaluation for two different applications.
5  Characterization Framework 
and Micro-server Architecture
To improve the energy efficiency of a micro-server, we need to investigate the oper -
ation limits of voltage and memory refresh rates. Exposing the safe voltage margins 
of an application is a time-consuming and difficult process due to several abnormal 
behaviours that can exist [ 40–43]. To this end, we developed an automated charac -
terization framework, which is outlined in Fig.  5, (1) to identify the target system’s 
limits when it operates at scaled voltage, frequency conditions and DRAM refresh 
rates, and (2) to record/log the effects of a program’s execution under these Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications148
conditions. The automated framework (outlined in Fig.  5) is easily configurable by 
the user and can be embedded to any Linux-based system, with similar voltage and 
frequency regulation capabilities. The characterization framework [ 40, 41] consists 
of three phases (initialization, execution, parsing). During the initialization phase, a 
user can declare a benchmark list with corresponding input datasets to run in any 
desirable characterization setup. The characterization setup includes the voltage and 
frequency (V/F) values on which the experiment will take place and the cores where 
the benchmark will be run. To reduce the DRAM power, we adopt the framework to 
characterize DRAM reliability operating under different refresh rates and the sup -
ply voltage. Particularly, we use this framework to identify the optimal DRAM 
refresh rate and voltage which does not trigger uncorrectable errors or system 
crashes. The execution phase consists of multiple runs of the same benchmark, each 
one representing the execution of the benchmark in a pre-defined characterization 
setup. The set of all the characterization runs running the same benchmark with dif -
ferent setups represents a campaign. In the parsing phase of our framework, all log 
files that are stored during the execution phase are parsed in order to provide a fine- 
grained classification of the effects observed for each characterization run.
We have also extended the error reporting capabilities of existing mechanisms 
(i.e. ECC in caches and DRAMs) with system configuration values, sensor readings 
and performance counters for identifying correctable (CE) and uncorrectable errors 
(UE). In addition, to account for any undetected error and essentially detect any 
SDC that could go undetected by ECC, we compare the output of each execution 
with a golden reference.
Initialization
ExecutionBenchmark
Results
Voltage 
ReductionConfiguration
Reset Switch
Power Switch
Watchdog 
monitor
Execution Loop
Result 
ParsingFinal CSV
ResultsRaw data
CloudInitialization
PhaseExecution
PhaseParsing
PhaseSerial
Network
Fig. 5  Characterization framework layoutP. Nikolaou et al.149
The framework provides the following features; it:
• compares the outcome of the program with the correct output of the program 
when the system operates in nominal conditions to record SDCs,
• monitors the exposed corrected and uncorrected errors from the hardware plat -
form’s error reporting mechanisms,
• recognizes when the system is unresponsive to restore it automatically,
• monitors system failures (crash reports, kernel hangs, etc.),
• determines the safe, unsafe and non-operating voltage regions for each applica -
tion for all frequencies, and,
• performs massive repeated executions of the same configuration.
The server that we use to characterize the applications on is a state-of-the-art 
64-bit ARM-based Server-on-Chip, the X-Gene 2. X-Gene 2 platform provides 
knobs for under-volting the various components that are explored. The micro-server 
consists of eight 64-bit ARMv8-compliant cores running at 2.4 GHz, grouped in 4 
Processor Modules (PMD), which have a separate 32 KB LI instruction cache and 
32  KB  L1 data cache for each core and a 256  KB unified L2 cache for each 
PMD. There is also an 8 MB L3 cache which is shared across the whole chip (all 8 
cores). There are 4 available memory channels with DDR3 memories.
The X-Gene 2 provides access to a separate Scalable Lightweight Intelligent 
Management Processor (SLIMpro), a special management core, which is used to 
boot the system and provide access to on-board monitors for measuring the tem -
perature and power of the SOC and DRAM. The SLIMpro also reports to the Linux 
kernel all errors corrected or detected by the provided error-correcting codes (ECC) 
and the parity. Finally, SLIMpro has configuration parameters of the Memory 
Controller Units (MCUs), such as refresh period (TREFP). The server runs a fully 
fledged OS based on CentOS 7 with the default Linux kernel 4.3.0 for ARMv8 and 
supports 4 KB and 64 KB pages.
After characterizing it we choose the voltage levels that do not affect the avail -
ability of the system, called safe margins. These margins are used to evaluate the 
efficient Edge deployment.
6  Polaris Application Description and Evaluation
The Polaris platform was developed as a third-party service for Investment Firms 
(IFs) to comply with the European Markets Infrastructure Regulation ( EMIR ). The 
Regulation requires that all derivatives transactions need to be reported to Trading 
Repositories (TRs). The reporting of a derivative transaction should take place at 
T  +  1, where T is the transaction date, and it involves any daily modifications/
updates of the transaction until the termination of the derivative contract. This 
requires the processing and validation of a large amount of information and han -
dling sensitive client’s data.
The platform consists of three main components shown in Fig.  6:Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications150
Client ’s
Tradin g
Platform
Client ’s
Tradin g
Platform
Client ’s
Tradin g
PlatformSystemLink
(Automatio n
System )
PolarisPlatformValidation andR eporting
Module
Feedback
ModuleTrade
Repository
(CME ETR)
Trade
Repository
(REGIS-TR)...
Dashboar d
(Clien t’s
interface)
Fig. 6  Polaris application architecture
• Validation kernel : The process starts with the validation of the “Polaris” reports 
at the Validator kernel. The Validator kernel can be duplicated to several instances 
to improve performance by submitting multiple clients simultaneously and to 
multiple Regulatory Destinations. The main advantage of the Validator kernel is 
that it provides a very fast first run validation of the reports, so that the end user 
can be notified for any errors before the report is submitted to the relevant 
Regulatory Destination. This enforces an improvement in the total life cycle of 
the reporting process.
• Mapping Engine:  Following the validation of the report, the validated data are 
passed to the Mapping Engine, which transforms the data according to the rele -
vant Regulatory Destination. The Mapping Engine is a modular and scalable 
component that can be easily configured and expanded to several Regulatory 
Destinations and Regulations in a very short period of time. At this point, the 
valid data and the invalid data are populated towards the end user through the 
Polaris feedback file, for further actions. At the same time, the Mapping Engine 
polls the Regulatory Destination (if applicable) regularly at scheduled intervals, 
for any returned feedback pending to be processed. Once a pending feedback is 
detected over the Regulatory Destination side, the Mapping Engine downloads 
the feedback and transforms it to the “Polaris” readable form, which is then dis -
tributed to the Feedback Processors.
• Feedback Processor:  The Feedback Processor is an efficient and scalable com -
ponent similar to the Validator kernel, easily adaptable to the Clients’ needs. The 
purpose of the Feedback Processor is to process the feedback returned from the 
Regulatory Destination and transform to an end user readable form. The feed -
back and any rejected trades by the Regulatory Destination are returned to the 
end user through the TR feedback file.P. Nikolaou et al.151
6.1  Polaris Application Requirements
Polaris platform requirements are mainly defined by the below three constraints:
• The regulations T + 1 deadline, where T is the date of the execution of the trans -
action. This suggests that the Investment Firms should report the T date transac -
tions by 23:59 the next day, T + 1.
• The Investments Firms reaction to rejected records. As the regulation reporting 
may return rejected trades due to the client’s misconfiguration during the sub -
mission, the result of the submission should be returned within the same day, for 
the client to take actions and resubmit the rejected transactions still in the T + 1 
deadline.
• The volume of each client and the total volume of all clients. This metric defines 
the required resources that we need to deploy based on our clients’ reporting 
volume. It also defines the allocation of each client to a specific queue to make 
sure that he will get the report in time based on his volume. The volume can be 
estimated before starting the submissions process based on the number of trans -
actions in the files submitted by the client.
Currently, we have the below setup to satisfy our business requirements:
A set of clients that are estimated to have a total of ~0.5 million records are allo -
cated on a single VM for validations and submission which will require about 2.5 h 
for back to back processing (worst-case scenario when they submit all at the same 
time and fill up the queue).
The agreement with all the clients to submit their trades before 21:00 gives us a 
maximum 3-h timeframe before the deadline, thus 2.5 h for processing and 30 min 
for any unexpected events.
A failure with an MTTR of 15 min (time to reboot or reroute to another VM, 
recover to the checkpoints, etc.) will still allow us to send the report of the last client 
before the end of the day.
6.1.1  Availability Requirements
The feedback response should be sent before the end of the day, 23:59:59, in agree -
ment with the client to initiate their submission before 21:00 on T  +  1. This is 
because clients’ submissions may have invalids and they need to get their feedback 
early so they can react and resubmit to avoid missing the T + 1 deadline. Given this 
constrain and the unpredictable behaviour of clients, regarding the submission time, 
we are forced to have that 99% availability; that is about 10 to 15 min of downtime 
per day that can be handle from the current setup.
A lower availability can be tolerated as the application does not require real-time 
interaction with the client and the time slot for reporting on the worst-case scenario 
is long enough, 3 h, while the time to process a day’s report of a single client takes 
15 min on average except few big clients that may require up to 45 min. Thus, by Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications152
dropping the availability and therefore reducing further the 3 h reporting timeframe, 
more and shorter queues will be required. To achieve this, we need to scale up our 
infrastructure to handle the total volume at the worst-case scenario and still satisfy 
our business requirements.
6.1.2  Latency Requirements
The target time to reply to the client is 3  h considering the worst-case scenario 
described above. This means that from the time the client submits the data using the 
data should be validated and return the first feedback from Polaris.
6.1.3  Accuracy and Security Requirements
A 100% accuracy and security are required by this application as any changes in the 
information will lead to the false report. In addition, there is no way to capture the 
errors during the reporting if the problem appears in a text field and does not affect 
the validation rules. For example, if the price of a transactions is reported differently 
but it is still a number then both Polaris and TR will accept this as a valid value but 
can dramatically change the end result of the reporting. Audit checks can be applied 
to check the validity of the reporting that this will reduce the performance of the 
platform significantly.
6.1.4  Data Rate/Performance Requirements
Currently, the application can achieve a minimum of 1800 record validations per 
minute when running on a high performance x64 machine with virtualization using 
KVM. This number was measured for the scenario where all records are valid and 
without any conflicts for the New Trade (NT) message that has the most validations. 
If all records are valid and without any conflicts it means all records will be pre -
pared for reporting and inserted in the local database which is the case that has the 
longest execution time for a specific number of records.
All the described requirements with their corresponding values that used for this 
analysis are summarized in Table  1.
Table 1  Polaris application’s requirements
Requirement Description
Availability 99%
Latency Maximum of 3 h
Accuracy 100%
Security 100%
Data rate/Performance 1800 records validated/minuteP. Nikolaou et al.153
6.2  End-to-End TCO Model and Parameters
In order to estimate the TCO, we use the architecture described in Fig.  7 for this 
application. The figure shows three different scenarios in the Edge deployments. 
The reason that we do not evaluate Cloud, in this case, is because for security rea -
sons Cloud deployment at this case is not trusted by the application clients. In the 
first scenario, the data are processed in a single facility with a number of servers 
hosted in this facility. Let us assume that the servers in this facility use Xgene-based 
servers for processing the data. Figure  7 also shows the power consumption bar with 
a specific power budget per facility. As the figure shows the power consumption 
violates the power budget, thus this scenario is not feasible. In order to satisfy power 
budged the servers are spread as shown in the second scenario. This allows the 
power consumption to satisfy the power budget per facility but it spends a lot of cost 
for building or rending more facilities. Finally, scenario 3, that corresponds to the 
power efficient servers, is shown to have as less facilities as needed to not overcome 
the power budget. This provides less cost than the normal Edge facilities. This is 
made possible from the use of more power efficient servers that allow more servers 
to be operated within the same power budget. We assume facilities with 15m2 area 
in order to host the servers, cooling and other necessary equipment. Also, in this 
analysis, we assume the cost of rending the required facilities.
The metric that we try to optimize in this work is the TCO which captures the 
numbers of facilities that are needed in normal and power efficient Edge servers.
Location1
LocationnLocation2
Location3
…Clients
PowerConsumption
PowerConstraintFacility1Location 1
LocationnLocation2
Location 3
…Clients
PowerConstraintFacility1
Facility2
Facility3Facility1
Facility2
Facility3
Facility4Facility4Location1
LocationnLocation2
Location3
…Clients
PowerConstraintFacility1
Facility2Facility1
Facility2PowerConsumptionPowerConsumptionEdgeFacility EdgeFacilities UniserverEfficientEdgeFacilities
Scenario 1 Scenario 2S cenario 3
Fig. 7  Architecture of Edge server in multiple facilitiesTotal Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications154
Table 2  Nominal and Efficient Operating Settings for Xgene2 platform
Normal settings Efficient settings
PMD voltage 980 940
SoC voltage 950 890
DRAM voltage 1500 1428
DRAM refresh rate in ms 78 2783
6.3  Characterization Results
The Polaris application and its dependencies have been ported and tested on the 
X-Gene 2 platform by using the characterization framework presented in Section 5. 
The generated results need to be deterministic and repeatable as the previous evalu -
ation. We run the application with various numbers of instances to obtain the trends 
of the effectiveness. The characterization process reveals the lowest operating limits 
that achieve the highest power savings without compromising the availability of the 
system as shown in Table  2.
The power savings are around 12%. These savings correspond to the maximum 
number of instances (6 instances in this case) that can be processed in the platform. 
The findings in this Section are used as inputs for the TCO analysis.
6.4  End-to-End TCO Results
Figure 8 illustrates an investigation as a function of per Edge site number of clients 
and the power budget in Watts (x-axis) in X-Gene 2.The results clearly illustrate that 
power efficient Edge servers dominate normal Edge in TCO savings. As the figure 
shows the TCO savings reach 10.5%. The TCO findings are also depicted in the 
number of facilities. The greater number of facilities in the Normal edge, the more 
power savings in the efficient edge. In the case of no difference in the number of 
facilities, the TCO savings are around 1.4% due to the difference in the power and 
temperature of the power efficient Edge between the Normal Edge.
7  SocialCRM Application Description and Evaluation
The SocialCRM is a software as a service (SaaS) application that is used for the 
analysis of entities in social media (currently Twitter). SocialCRM allows clients to 
track the activity of a social network regarding a particular set of topics. Such data 
is consumed from the social network using the social network api, and is modelled 
as an activity graph of users, posts and comments, which is analysed by the 
SocialCRM analytical component. Such component is able to extract influential 
people and communities of users for the given topics.P. Nikolaou et al.155
00.511.522.5
0%2%4%6%8%10%12%14%
60
120
180
240
300
360
420
480
540
600
60
120
180
240
300
360
420
480
540
600
60
120
180
240
300
360
420
480
540
600
60
120
180
240
300
360
420
480
540
600
10002 0003 0004 000
DifferenceintheNumbersofFacilities
#NormalFacilities-#UniserverFacilitiesTCOsavingsinPercentage
Numberofclients
PowerBudget
Fig. 8  TCO savings and respective number of facilities for a number of clients and different power 
budgets in X-Gene2
Fig. 9  SocialCRM 
application architecture`
SocialCRM provides clients with a WebApp that allows the exploration of such 
activity graph, and query for most influential users and the communities of different 
topics. Such information is used to target influencers for marketing campaigns, size 
the target audience, etc. Figure  9 shows the SocialCRM application’s architecture 
with its main components.Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications156
7.1  SocialCRM Application Requirements
The SocialCRM application has several requirements, which depend on the actual 
component of the application (e.g. WebApp, Analytics, etc.). For the purpose of this 
analysis, we will focus on the analytics part.
7.1.1  Clients Degree of Satisfaction Requirement
Data in social networks evolves quickly, thus can become outdated in a matter of 
hours. Since the analytics executed by SocialCRM are expensive, SocialCRM fol -
lows the batch processing approach, that is, from time to time, the latest data is 
downloaded using the social network API and processed.
SocialCRM provides different degrees of satisfaction, to fit the needs of the dif -
ferent clients. Such different QoS levels correspond to the frequency of the data that 
is downloaded and analysed, which are up to 4 and more hours. Clients that are 
served in 4 h are 100% satisfied. When the number of clients is not satisfied at this 
time margin then the Degree of satisfaction becomes less than 100% accordingly.
7.1.2  Availability Requirement
SocialCRM is sold as a SaaS (software as a service). Clients from around the world 
expect to have access to the application at any time, thus, the SocialCRM needs to 
be available most of the time. This includes the analytical component, otherwise if 
the QoS levels could not be met. For this reason, an availability rate of 99% would 
be the minimum.
7.1.3  Execution Time per Client Requirement
The only performance requirement of SocialCRM is that imposed by the QoS. That 
is, if the best QoS is 4 h, the underlying computing system should be able to process 
85 k messages in at most 4 h. In practice, however, this does not suppose a problem 
because processing such number of messages takes in the order of 5  min. This 
means that for each client the execution time is 53.75 s.
All the described requirements with their corresponding values that used for this 
analysis are summarized in Table  3.
Table 3  SocialCRM application requirements
Requirement Description
Degree of satisfaction 4 h to satisfy 100% of users
Average compute time per user 53.75 s
Availability 99%P. Nikolaou et al.157
7.2  End-to-End TCO Model and Parameters
In order to estimate the TCO and the degree of satisfaction of a specific deployment, 
we use the architecture described in Fig.  10 for this application. The figure shows 
three different cases in Cloud deployments. The reason that we do not evaluate this 
application in Edge deployments is because that this is a Cloud-based application 
and it is hard to host it in Edge facilities due to the huge number of clients/servers 
that are needed. In the first scenario, the data are processed in a Cloud facility with 
a huge number of servers hosted in the Cloud. Let us assume that the servers in this 
facility use Xgene-based servers for processing the data.
Figure 10 also shows the power consumption bar with a specific power budget 
for the Cloud deployment and the Clients Degree of Satisfaction. As the figure 
shows the power consumption violates the power budget, thus this scenario is not 
feasible. In order to satisfy power budged the number of servers decreases as shown 
in Fig.  10b. This allows the power consumption to satisfy the power budget per 
facility with the cost of the Degree of Satisfaction, that now is lower than 100%. 
Finally, Fig.  10c corresponds to the Cloud efficient servers, hosts all the number of 
servers that are needed to not overcome the power budget and provide the higher 
clients degree of satisfaction. This is made possible from the use of more power 
efficient servers that allow more servers to be operated within the same power bud -
get. We assume power budget of 10 K–25 K Watts in this analysis, representative 
power constraint for Cloud deployments.
The metric that we try to optimize in this work is the TCO/Degree of 
Satisfaction^2. The reason that the Degree of Satisfaction is squared is because this 
metric is very important for this application, so it needs to be emphasized.
…Client s
Powe rConstra int
DegreeofSatisfactionCloudPowerConsu mption
100%…Clients
Powe rConstraint
DegreeofSatisfactionPowerConsumption
100%
50%…Clients
Powe rConstra int
DegreeofSatisfactionCloudPowerConsu mption
100%CloudCloudFacilit y
(a)CloudFacilit y
(b)Efficien tCloud Facility
(c)
Fig. 10  Architectures of ( a) Normal Cloud Facilities that violate the power Constraint, ( b) Normal 
Cloud Facilities that do not violate the power Constraint but scarify Degree of Clients Satisfaction, 
(c) Power Efficient Cloud Facilities that satisfy power Constraint and provide 100% degree of 
satisfactionTotal Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications158
7.3  Characterization Results
The SocialCRM application and its dependencies have been ported and tested on 
the X-Gene 2 platform by using the characterization framework presented in Section 
5. The generated results need to be deterministic and repeatable as the previous 
evaluation. We run the application with various numbers of instances to obtain the 
trends of the effectiveness. The characterization process reveals the lowest operat -
ing limits that achieve the highest power savings without compromising the avail -
ability of the system as shown in Table  4.
The power savings with the efficient settings are around 12%. These savings cor -
respond to the maximum number of instances (8 instances in this case) that can be 
processed. The findings in this Section are used as inputs for the TCO analysis.
7.4  End-to-End TCO Results
Figure 11 illustrates an investigation as a function of per Cloud site number of cli -
ents and the power budget in Watts (x-axis). The results present the TCO/Degree of 
Satisfaction (DoS)^2 metric of success.The results clearly illustrate that 
Table 4  Nominal and efficient operating settings for Xgene2
Normal settings Efficient settings
PMD voltage 980 930
SoC voltage 950 910
DRAM voltage 1500 1428
DRAM refresh rate in ms 78 2783
0.970.980.9911.011.021.031.041.05
0%1%2%3%4%5%6%7%
5000
6000
7000
8000
9000
10000
11000
12000
13000
5000
6000
7000
8000
9000
10000
11000
12000
13000
5000
6000
7000
8000
9000
10000
11000
12000
13000
5000
6000
7000
8000
9000
10000
11000
12000
13000
10000 15000 20000 2500 0
UniserverDegreeofSatisfactionandNumberof
ServersnormalizedwithBaselineTCO/DoS^2savings
Numberofclients
PowerBudgetTCO/DoS^2Savings NormalizedDegreeofSatisfaction NormalizedServers
Fig. 11  TCO/DoS^2 and respective Normalized Degree of Satisfaction for different number of 
clients and power budgets in X-Gene2P. Nikolaou et al.159
power- efficient Edge servers dominate normal Edge in TCO/DoS^2. The TCO/
DoS^2 reaches 6.5%. The TCO results also depicted in the number of facilities. The 
results also depicted in the Normalized Degree of Satisfaction (z-axis) and the num -
ber of servers per Cloud facility.
8  Conclusions
This chapter articulates that Edge and Cloud servers need to be energy-efficient to 
allow under tight power constraints to deploy as many of them at a facility since 
more servers enable processing data from more sensors of IoT applications. To this 
end, we develop frameworks to understand the limits of operation revealing possible 
extended operation points at the Edge and Cloud in order to make them more power 
efficient and finally, we evaluate the gains of the Power Efficient Edge and Cloud 
deployments in a TCO compared to the Normal Edge and Cloud deployments. The 
results illustrate that by making Edge and Cloud more power efficient, approxi -
mately, we can achieve in many situations considerable gains in the TCO metric.
References
 1. F. Xia, L.T. Yang, L. Wang, A. Vinel, Internet of things. Int. J. Commun. Syst. 25(9) (2012)
 2. L. A. Barroso, J. Clidaras, U. Holzle, The datacenter as a computer: an introduction to the design 
of warehouse-scale machines, Synthesis Lectures on Computer Architecture, pp. 1–154, 2013
 3. L. Tan, N. Wang, Future internet: the internet of things, in 2010 3rd international conference 
on advanced computer theory and engineering (ICACTE), 5, V5–376. IEEE, 2010
 4. F.  Bonomi, R.  Milito, J.  Zhu, S.  Addepalli, Fog computing and its role in the internet of 
things, in Proceedings of the first edition of the MCC workshop on Mobile Cloud Computing, 
pp. 13–16. ACM, 2012
 5. F. Bonomi, Connected vehicles, the internet of things, and fog computing, in The Eighth ACM 
Int. Workshop on VehiculAr InterNETworking V ANET, Las Vegas, USA, 2011
 6. L.A. Barroso, U. Holzle, The Datacenter as a Computer. Morgan Claypool, 2009
 7. P. Ranganathan, P. Leech, D. Irwin, J. Chase, Ensemble level power management for dense 
blade servers, in Proceedings of the 33rd Annual International Symposium on Computer 
Architecture (ISCA), pp. 66–77, 2006
 8. X. Wang, M. Chen, C. Lefurgy, T.W. Keller, SHIP: scalable hierarchical power control for 
large-scale data centers, in Proceedings of the 18th International Conference on Parallel 
Architectures and Compilation Techniques (PACT), pp. 91–100, 2009
 9. M. Jarschel, D. Schlosser, S. Scheuring, T. Hoßfeld, Gaming in the clouds: Qoe and the users 
perspective. Math. Comput. Model. 57(11), 2883–2894 (2013)
 10. R. Kohavi, R.M. Henne, D. Sommerfield, Practical guide to controlled experiments on the web: 
listen to your customers not to the hippo, in 13th ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining. ACM, 2007, pp. 959–967
 11. Y .A. Wang, C. Huang, J. Li, K.W. Ross, Estimating the performance of hypothetical Cloud 
service deployments: A measurement-based approach, in INFOCOM, 2011 Proceedings 
IEEE. IEEE, 2011, pp. 2372–2380Total Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications160
 12. T. H. Luan, L. Gao, Z. Li, X. Yang, G. We, L. Sun, A view of fog computing from networking 
perspective. ArXivPrepr. ArXiv160201509 (2016)
 13.  A.  Froehlich, How Edge Computing Compares with Cloud Computing  (Networking 
Computing Blog, 2018)
 14. Cisco Data in Motion (DMo) technology allows data management and analysis of large vol -
umes of data coming through IoT at the edge
 15. W. Shi, G. Pallis, Z. Xu, Edge Computing [Scanning the Issue]. Proc IEEE 107(8), 1474–1481 
(Aug. 2019)
 16. Y . Mao, C. You, J. Zhang, K. Huang, K.B. Letaief, A survey on mobile edge computing: The 
communication perspective. IEEE Commun. Surv. Tutorials 19(4), 2322–2358 (2017)
 17. I. Stojmenovic, Fog computing: a cloud to the ground support for smart things and machine- 
to- machine networks, in 2014 Australasian Telecommunication Networks and Applications 
Conference (ATNAC), pp. 117–122. IEEE, 2014
 18. K. Bilal, O. Khalid, A. Erbad, S.U. Khan, Potentials, trends, and prospects in edge technolo -
gies: fog, cloudlet, mobile edge, and micro data centers. Comput. Netw. 130, 94–120 (2018)
 19. A. Bahtovski, M. Gusev, Cloudlet Challenges. Proc. Eng. 69, 704–711 (2014)
 20. M. Satyanarayanan, P. Bahl, R. Caceres, N. Davies, The case for vm-based cloudlets in mobile 
computing. IEEE Pervasive Comput. 8(4), 14–23 (2009)
 21. V . Bahl, Cloud 2020: emergence of micro data centers (Cloudlets) for latency sensitive com -
puting (keynote), in Middleware 2015, 2015.
 22. L.  Tong, Y .  Li, W.  Gao, A hierarchical edge cloud architecture for mobile comput -
ing, in INFOCOM 2016, The 35th Annual IEEE International Conference on Computer 
Communications, IEEE. IEEE, 2016
 23. Y .-S.  Chang, S.-H.  Hung, Developing collaborative applications with mobile cloud-a case 
study of speech recognition. J. Internet Serv. Inf. Secur. 1(1), 18–13 (2011)
 24. S. Shekhar, A.D. Chhokra, A. Bhattacharjee, G. Aupy, A. Gokhale. INDICES: exploiting Edge 
resources for performance-aware cloud-hosted services, in Fog and Edge Computing (ICFEC), 
2017 IEEE 1st International Conference on, pp. 75–80. IEEE, 2017
 25. H. El-Sayed, S. Sankar, M. Prasad, D. Puthal, A. Gupta, M. Mohanty, C.T. Lin, Edge of things: 
the big picture on the integration of Edge, IoT and the Cloud in a distributed computing envi -
ronment. IEEE Access 6, 1706–1717 (2018)
 26. S. Clinch, J. Harkes, A. Friday, N. Davies, M. Satyanarayanan, How close is close enough? 
Understanding the role of Cloudlets in supporting display appropriation by mobile users, in 
Pervasive Computing and Communications (PerCom), 2012 IEEE International Conference 
on, pp. 122–127. IEEE, 2012
 27. D. Fesehaye, Y . Gao, K. Nahrstedt, G. Wang. Impact of Cloudlets on interactive mobile Cloud 
applications, in Enterprise Distributed Object Computing Conference (EDOC), 2012 IEEE 
16th International, pp. 123–132. IEEE, 2012
 28. S.  Shekhar, A.  Gokhale, Dynamic resource management across cloud-edge resources for 
performance-sensitive applications, in Proceedings of the 17th IEEE/ACM International 
Symposium on Cluster, Cloud and Grid Computing, pp. 707–710. IEEE Press, 2017
 29. N. Powers, A. Alling, K. Osolinsky, T. Soyata, M. Zhu, H. Wang, B. He, W. Heinzelman, 
J. Shi, M. Kwon, The Cloudlet accelerator: bringing mobile-cloud face recognition into real- 
time, in Globecom Workshops (GC Wkshps), 2015 IEEE, pp. 1–7. IEEE, 2015
 30. T. Soyata, R. Muraleedharan, C. Funai, M. Kwon, W. Heinzelman, Cloud-vision: real-time 
face recognition using a mobile-Cloudlet-cloud acceleration architecture, in Computers and 
communications (ISCC), 2012 IEEE symposium on, pp. 000059–000066. IEEE, 2012
 31. H. Chang, et al., Bringing the cloud to the edge, in Computer Communications Workshops 
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014
 32. D. Hardy, M. Kleanthous, I. Sideris, A. Saidi, E. Ozer, Y . Sazeides, An analytical framework 
for estimating tco and exploring data center design space. in International Symposium on 
Performance Analysis of Systems and Software, pp. 54–63, 2013P. Nikolaou et al.161
 33. P. Nikolaou, Y . Sazeides, L. Ndreu, M. Kleanthous, Modeling the implications of dram fail -
ures and protection techniques on datacenter tco, in Proceedings of the 48th International 
Symposium on Microarchitecture, MICRO-48, pages 572–584, New  York, NY , USA, 
2015. ACM
 34.  C. Patel, A. Shah, Cost model for planning, development and operation of a data center  
(HP TR, 2005)
 35. J. Karidis, J.E. Moreira, J. Moreno, True value: assessing and optimizing the cost of computing 
at the data center level, in 6th ACM Conference on Computing Frontiers, pp. 185–192, 2009
 36. J. Moore, J. Chase, P. Ranganathan, R. Sharma, Making scheduling “cool”: temperature-aware 
workload placement in data centers, in Annual Conference on USENIX, pp. 5–5, 2005
 37. J. Koomey, K. Brill, P. Turner, J. Stanley, B. Taylor, A simple model for determining true total 
cost of ownership for data centers, in White Paper, Uptime Institute, 2007
 38. K.V . Vishwanath, A. Greenberg, D.A. Reed, Modular data centers: how to design them?, in 1st 
Workshop on Large-Scale System and Application Performance, pp. 3–10, 2009
 39. P.  Nikolaou, Y .  Sazeides, A.  Lampropoulos, D.  Guilhot, A.  Bartoli, G.  Papadimitriou, 
A. Chatzidimitriou, D. Gizopoulos, K. Tovletoglou, L. Mukhanov, G. Karakonstantis, On the 
evaluation of the total-cost-of-ownership trade-offs in edge vs cloud deployments: a wireless- 
denial- of-service case study. IEEE Transactions on Sustainable Computing (TSUSC) Special 
Issue on Sustainability of Fog/Edge Computing Systems 2019  (January 2019)
 40. G. Papadimitriou, M. Kaliorakis, A. Chatzidimitriou, D. Gizopoulos, G. Favor, K. Sankaran, 
S. Das, A system-level voltage/frequency scaling characterization framework for multicore 
CPUs, in IEEE Silicon Errors in Logic – System Effects (SELSE 2017), Boston, MA, USA, 
March 2017
 41. G.  Papadimitriou, M.  Kaliorakis, A.  Chatzidimitriou, D.  Gizopoulos, P.  Lawthers, S.  Das, 
Harnessing voltage margins for energy efficiency in multicore CPUs, in IEEE/ACM 
International Symposium on Microarchitecture (MICRO 2017), Cambridge, MA, USA, 
October 2017
 42. G. Karakonstantis, K. Tovletoglou, L. Mukhanov, H. Vandierendonck, D. S. Nikolopoulos, 
P. Lawthers, P. Koutsovasilis, M. Maroudas, C. D. Antonopoulos, C. Kalogirou, N. Bellas, 
S.  Lalis, S.  Venugopal, A.  Prat-Perez, A.  Lampropulos, M.  Kleanthous, A.  Diavastos, 
Z.  Hadjilambrou, P.  Nikolaou, Y .  Sazeides, P.  Trancoso, G.  Papadimitriou, M.  Kaliorakis, 
A. Chatzidimitriou, D. Gizopoulos, and S. Das, An energy-efficient and error-resilient server 
ecosystem exceeding conservative scaling limits, in ACM/IEEE Design, Automation, and Test 
in Europe (DATE 2018), Dresden, Germany, March 2018
 43. K.  Tovletoglou, L.  Mukhanov, G.  Karakonstantis, A.  Chatzidimitriou, G.  Papadimitriou, 
M. Kaliorakis, D. Gizopoulos, Z. Hadjilambrou, Y . Sazeides, A. Lampropulos, S. Das, P. V o, 
Measuring and exploiting guardbands of server-grade ARMv8 CPU cores and DRAMs, in 
IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2018), 
Luxembourg, June 2018.
 44. WorldSensing, https://www.worldsensing.comTotal Cost of Ownership Perspective of Cloud vs Edge Deployments of IoT Applications163Software Engineering for Edge Computing
Dionysis Athanasopoulos
1  Introduction
1.1  Motivation
We apparently live in the data-driven era of interconnected mobile devices. Cisco 
estimates that there will be around 50 billion interconnected mobile devices by the 
end of 2020 [ 1]. As the hardware of the mobile devices is drastically getting 
improved, we have come to expect more from our mobile devices in terms of the 
types of tasks and data processing they can perform. However, these devices are not 
usually able to meet these demands on their own. Due to storage, computational, 
and battery life constraints, mobile devices increasingly require the support of 
remote powerful (e.g., cloud) machines. While this type of architecture is advanta -
geous for resource-constrained mobile devices, the usage of remote services is not 
always the best option for all the types of software applications. In particular, as 
time-sensitive and location-aware applications emerge (e.g., patient monitoring, 
real-time manufacturing, self-driving cars, flocks of drones, or cognitive assistance), 
the distant cloud is not always able to satisfy the ultra-low latency requirements of 
these applications or to provide location-aware services [ 2].
To address the above challenges, Edge/Fog computing has been recently intro -
duced by both industry and academia to quench the need for computing paradigm 
close to mobile devices [ 3]. Edge/Fog computing bridges the gap between the cloud 
and mobile devices by enabling computing, storage, networking, and data manage -
ment in edge nodes within the close vicinity of end-users’ devices.
There are various surveys about Edge/Fog computing in the literature (see the 
Sect. 3 below). What is missing though in the above surveys is the description of the 
D. Athanasopoulos ( *) 
School of Electronics, Electrical Engineering, and Computer Science,  
Queen’s University Belfast, Belfast, Northern Ireland, UK
e-mail:  D.Athanasopoulos@qub.ac.uk
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_7164
software-engineering aspects of the applications that are built/deployed via the 
edge . These aspects would be then useful for abstracting the software-engineering 
process that is/should be followed by practitioners to build their edge-enabled soft -
ware applications .
1.2  Contribution
The contribution of the current chapter is twofold by aiming at covering the follow -
ing objectives:
 1. To highlight the software-engineering aspects of the current edge-computing 
approaches.
 2. To abstract a software-engineering process for edge computing and to outline 
research challenges in this process.
1.3  Chapter Structure
To cover the above objectives, we first specify the core concepts of the general- 
purpose software-engineering process, the multi-tier architecture of edge infrastruc -
ture, and how software applications are deployed to such an infrastructure (Sect. 2). 
Following, we describe the related surveys for edge computing and how the current 
chapter goes beyond those surveys (Sect. 3). We then describe the software- 
engineering aspects of edge-computing approaches (Sect. 4). Finally, we outline the 
view and the role of a software-engineering process for edge computing, along with 
research challenges in this process (Sect. 5).
2  Background
The current chapter presents the related work that comes from the following two 
research fields: (i) the general-purpose software-engineering process and (ii) the 
multi-tier architecture of edge infrastructure and how software applications are 
deployed to such an infrastructure.
2.1  Software Engineering
What the software does is usually specified by the functional requirements  of soft -
ware. When we talk about the quality of software, we usually adopt the term quality 
of service (QoS)  or non-functional attributes  of software [ 4]. The latter term is not D. Athanasopoulos165
related to what the software does but it is related to the software behavior while it is 
executed. Examples of these attributes are the response time of software from an 
end-user perspective, the understandability of the program code, the cyber-security 
of software, etc.
Software engineering is a discipline that is concerned with all aspects of software 
production from the early stages of system specification to the maintenance of sys -
tems. The systematic approach used in engineering software is known as the soft-
ware process . A software process is a sequence of activities that lead to the 
production and facilitate the maintenance of a software system. There are the fol -
lowing generic activities usually met in software processes [ 4]:
 1. Software specification : the functional requirements and the QoS attributes of 
software are specified
 2. Software architecture/design : the artifacts (a.k.a., subsystems, packages, compo -
nents, classes) of the code are defined, along with their relationships
 3. Software implementation/testing : software is programmed to implement/test its 
architecture/design meeting the functional requirements and QoS attributes
 4. Software deployment : software is made available for use (e.g., software installa -
tion to users’ or cloud machines)
 5. Software maintenance : software is modified to reflect changing requirements/
attributes.
Different types of systems may need different development processes. For exam -
ple, real-time software in an aircraft has to be completely specified before develop -
ment begins. In e-commerce systems, the specification and the development 
activities are usually made together. Consequently, these generic activities may be 
organized in different ways and described at different levels of detail depending on 
the type of software being developed.
2.1.1  Software Process
As described above, a software process is a set of related activities that leads to the 
production of a software system. These activities may be followed to develop soft -
ware from scratch or to integrate/extend/modify existing systems. These activities 
are not necessarily followed in a sequential way (e.g., some of them may be exe -
cuted in parallel). Overall, software processes are categorized as either plan-driven 
processes or agile processes [ 4]. Plan-driven processes are processes where all pro -
cess activities are planned in advance and followed in a specified order. In agile 
processes, planning is incremental and it is easier to change the process to reflect 
changing requirements/attributes. The most well-known software processes are the 
following [ 4]:
 1. Waterfall model : it executes the core activities described above (software speci -
fication, architecture, implementation, testing, deployment, maintenance) in a 
row and it considers them as separate phases.Software Engineering for Edge Computing166
 2. Incremental development : it interleaves most of the above activities and devel -
ops the system as a series of versions (increments), with each version adding 
functionality to the previous version.
 3. Reuse-oriented software engineering : it is based on reusable components and 
focuses on integrating these components into a system rather than developing 
them from scratch.
Toward abstracting the software-engineering activities that are executed by exist -
ing edge-computing approaches, we use the  waterfall software-process  as a suitable 
process for edge computing . The purpose of the above process is to provide a start -
ing point for practitioners/developers who want to produce software applications 
deploy-able and run-able to edge infrastructures. The abstract software-process is 
described in the last section of the current chapter.
2.2  Edge Computing
Edge computing has been proposed to mainly provide low latency and location 
awareness, to support geographic distribution and device mobility, along with the 
realization of real-time applications. However, the adoption of edge computing has 
also introduced research challenges [ 5]. Among others, one of the challenges is how 
software applications should be developed to be deploy-able and run-able on the 
edge. The minimum requirement of edge-deployable applications is that they should 
have a modular architecture , i.e., applications consist of separate components that 
are connected together. The components with which end-users interact are known as 
front-end  components. The components that have computation and/or storing 
responsibilities are known as back-end  components. The current subsection focuses 
on the multi-tier architecture of edge infrastructure and how modular software is 
deployed to such an infrastructure.
2.2.1  Multi-tier Edge Infrastructure and Modular Software
The underlying infrastructure of edge computing generally has a multi-tier architec -
ture [ 5]. In particular, the three-tier architecture is one of the most widely used 
architectures in edge computing. The first tier includes end-user devices (e.g., IoT- 
enabled devices, sensors, smartphones, tablets, smart vehicles). These end devices 
are often termed as Terminal Nodes (TNs). TNs usually run the front-end compo -
nents of  modular software  (e.g., GUI pages) .
The second tier that is usually termed the edge/fog layer is composed of network 
devices such as a router, gateway, switch, and access points. These edge nodes can 
collaboratively share storage and computing facilities. The first- and the second-tier 
nodes are usually connected to each other by a single network hop. Edge nodes  usu-
ally run the back-end components of  modular software . Back-ends are mainly D. Athanasopoulos167
responsible for analyzing and storing data. Back-ends are usually accessible over 
the Web and are offered by following the Web-service technology [ 6]. Edge nodes  
may further run software controllers that send data from the application front-ends 
to the application back-ends  (e.g., by using programming service-clients).
The third tier is optional and includes traditional cloud servers and data centers 
that are remotely located from the end-users’ devices. This tier usually has sufficient 
storage and computing resources. If cloud nodes support/participate in the edge 
infrastructure, cloud nodes run the back-end components  of modular software . In 
this case, the interplay between the edge and the cloud node is challenging. In par -
ticular, the following cases of interactions usually appear: edge-to-cloud interac -
tions, edge-to-TN interactions, and edge-to-edge interactions. The above interaction 
cases emerge the following interesting research question: when do the application 
back-ends run to edge nodes or to cloud nodes and how is it decided? We present in 
a next section the current approaches that give answers to the above question.
The description of the layered architecture of edge infrastructure (e.g., physical 
layer, virtualization layers, security layer, transport layer) is out of the scope of this 
chapter.
Please underline that the concept of fog computing has great similarity to edge 
computing. Both of the paradigms focus on the devices near/at the edges of the 
network. Currently, there is no universally accepted open standard on what defines 
the network edge [ 7]. In general, fog computing can be treated as a special form of 
edge computing [ 7]. In particular, an edge infrastructure that makes use of both edge 
devices and the cloud machines is referred to as fog computing [ 8]. We hereafter use 
the term edge to interchangeably refer to edge computing or fog computing . 
Moreover, we survey related approaches and we contribute a software process for 
both edge and fog computing.
2.2.2  Standardizing Edge Computing
The OpenFog Consortium1 includes famous companies and academic institutions 
worldwide that aim at standardizing the concepts used in the field of edge comput -
ing and at creating a reference architecture for edge infrastructures. The consortium 
was founded by ARM, Cisco, Dell, Intel, Microsoft, and the Princeton University 
Edge Laboratory in 2015. It currently counts 57 members across North America, 
Asia, and Europe [ 5]. The current version of the reference architecture is based on 
eight core pillars that include security, scalability, openness, autonomy, reliability, 
availability, serviceability, agility, hierarchy, and programmability [ 5]. What is cur -
rently missing is the software-engineering pillar for edge computing .
1 https://www.iiconsortium.org/pdf/OpenFog_Reference_Architecture_2_09_17.pdfSoftware Engineering for Edge Computing168
3  Related Work
There are related studies that survey the research papers in the field of edge/fog 
computing. Those surveys present the existing approaches from various perspec -
tives that are detailed in the following subsections.
3.1  Architectures, Infrastructures, and Algorithms 
for the Edge
The authors of [ 9] present a comprehensive review of the current literature for edge 
computing with a focus on architectures and algorithms. According to [ 9], the archi -
tectures of edge infrastructures are either application-agnostic or application- 
specific. The application-agnostic architectures have focused on service provision, 
resource management, communication issues, and cloud-fog federation. The 
application- specific architectures have focused on healthcare systems, intercon -
nected vehicles, and smart living. Moreover, the underlying algorithms have focused 
on the data analysis, the data storage/distribution, and the energy consumption.
In a similar vein, the authors of [ 8] survey and categorize the current architec -
tures for edge infrastructures as follows. Data-flow architectures are based on the 
direction of movement of workloads and data from the users’ devices to edge nodes 
or alternatively from cloud servers to edge nodes. Control architectures are based on 
how the resources are controlled, e.g., a single controller or central algorithm may 
manage the edge nodes or alternatively a distributed approach may be employed. 
Tenancy architectures are based on the support provided for hosting multiple enti -
ties, e.g., either a single application or multiple applications could be hosted by an 
edge node.
The authors of [ 8] further detail the parts of the infrastructures for resource man -
agement in edge/fog computing. The hardware part includes devices like network 
gateways, WiFi Access Points, small home servers, cars, and drones. The software 
part (e.g., operating systems, virtualization software) runs directly on hardware 
resources and manages resources. The middleware part runs on an operating system 
and provides complementary services that are not supported by the software part.
Finally, the authors of [ 8] discuss four algorithms for resource management: (i) 
discovery algorithms for identifying edge resources, (ii) benchmarking algorithms 
for capturing the performance of resources for decision-making to maximize the 
performance of deployments, (iii) load-balancing algorithms for distributing work -
loads across resources, and (iv) placement algorithms for identifying resources 
appropriate for deploying a workload.
The authors of [ 7] further focus on the architecture design and the system man -
agement of peer-to-peer edge computing, mobile grid edge computing, and mobile 
crowd edge computing.D. Athanasopoulos169
3.2  Software-Defined Networking for the Edge
The survey in [ 10] presents Software-Defined Networking (SDN) approaches for edge 
computing. SDN is an edge solution that hides all of the orchestration and the manage -
ment tasks from application developers and service providers. In particular, the com -
plexities resulting from deploying the cloud-like services at the edge can be solved by 
the SDN control mechanism. All of the data flow management, the service orchestra -
tion, and other management tasks are accomplished by the SDN controller. For 
instance, SDN deals with service commissioning and migration when the target edge 
devices are occupied. In this case, SDN decides where to commission the service 
based on various performance factors such as server utilization and network conditions.
3.3  Network Applications for the Edge
The authors of [ 5] survey the network applications that run at the edge. According 
to [5], computing and storage devices in data centers are interconnected by Data 
Center Network (DCN). DCN is related to the network topology, the network pro -
tocols, and routing/switching equipment. Virtualized DCN includes servers, routers, 
switches, and links that are virtualized. Some of the requirements of traditional data 
centers (e.g., performance isolation, server utilization) can be met by server virtual -
ization technologies (e.g., VMware and Xen). However, some other requirements of 
data centers (e.g., application deployment, management flexibility) cannot be easily 
addressed by the above technologies.
3.4  Edge Computing for Smart Cities
The authors of [ 11] focus on the aspects of the device connectivity and configura -
tion aspects of edge computing for smart-city applications. In particular, several 
types of edge devices exist for smart cities (e.g., sensors, mobile devices, smart 
watches). Each of these devices can become a leaf node in a software application 
(e.g., IoT application) to perform edge analytics. The tight limits on the power, 
memory, and processing resources of those devices lead to strict upper-bounds on 
state, code space, and processing cycles, making optimization of energy and net -
work bandwidth usage a dominating factor.
According to [ 11], the least implemented features of edge computing are the 
multi-protocol support at the application level, the context discovery/awareness, 
and the semantic annotation. Regarding security, existing approaches propose a 
cloud-based solution that implements user-behavior profiling and can be used to 
mitigate attacks that are focused on data theft.
Edge computing for smart cities should offer solutions for the following charac -
teristics of smart cities: smart economy, smart people, smart governance, smart Software Engineering for Edge Computing170
mobility, smart environment, and smart living. Smart cities are required to accom -
modate growing populations and their needs with limited resources. Smart cities 
also need to make sure the limited edge resources do not run out. The best strategy 
to achieve this is to use resources in the most efficient and optimum ways. To do 
this, it is essential to understand how cities and its citizens behave and consume 
resources. Edge computing can bring efficiency and sustainability to the above tasks.
3.5  Cyber Security and Privacy for the Edge
The authors of [ 12] discuss techniques to address cyber security and privacy chal -
lenges for edge computing. Those techniques concern cyber security and privacy 
related to identity authentication, access control, intrusion detection, resilience to 
sybil attacks, trust management, transient storage, and decentralized computation.
3.6  Software Engineering for the Edge
Overall, we observe that the related surveys present the existing edge-computing 
approaches from the following perspectives: architectures, infrastructures, algo -
rithms, software-defined networking, network applications, smart cities, cyber secu -
rity, and privacy. Thus, the current surveys do not cover the software-engineering 
aspects of the edge-computing approaches.
4  Software-Engineering Aspects of Existing Approaches
Existing edge-computing approaches engineer software applications to make them 
deploy-able and run-able to edge infrastructures. Those approaches may execute all 
or some of the software-engineering activities that described in a previous section. 
We categorize below existing approaches with respect to the above activities. Please 
note that we indicatively present edge-computing approaches for each activity. The 
current chapter is by no means a complete survey of the existing approaches .
4.1  Software Application and Edge 
Infrastructure Specification
The existing approaches take as input the QoS attributes of software applications, 
like response time, availability, cyber-security, hardware (e.g., memory, hard-disk) 
requirements, software requirements (e.g., operating system, database management D. Athanasopoulos171
system), etc. For instance, the approaches allocate the components of software 
applications to machines only if the QoS attributes of application components (e.g., 
CPU) can be med by the machine characteristics (e.g., CPU cores) [ 13].
Other approaches consider the non-functional attributes  (e.g., performance) of 
the underlying edge infrastructure . For instance, the approaches allocate the com -
ponents of software applications to machines only if the overall performance of the 
machines is kept high [ 13].
4.2  Architecture and Implementation of Software Applications
From architecture and implementation perspectives, existing approaches can be 
organized into the following categories: application partitioning  approaches or 
application offloading  approaches.
4.2.1  Partitioning Software Applications
The approaches of this category assume that parts  of (some or all of) the application 
components are executed in edge or cloud machines only if the applications benefit 
from the remote execution. The remote execution is usually adopted to support the 
resource-constrained edge machines with energy savings on parts that benefit from 
the remote execution (e.g., [ 14]). However, these approaches usually rely on pro -
grammers who should specify how and when to partition  and modify  the source 
code of the application components.
4.2.2  Offloading Software Applications
The approaches of this category offload the execution of (some or all of) the appli -
cation components from edge machines to cloud machines and vice versa (e.g., 
[15]). The advantage of the offloading approaches is that they do not depend on 
programmers because applications do not need to get modified. To automate the 
offloading decision-making, the authors of [ 16] propose the conversion of the appli -
cation components to autonomous ones that can build predictive models of the 
application performance and of the underlying machines.
4.3  Deployment of Software Applications
Existing approaches generate deployment plans that map application components to 
edge/cloud machines. We divide existing approaches into two categories: (i) the 
approaches that produce deployment plans to meet the QoS requirements of Software Engineering for Edge Computing172
software applications; (ii) the approaches that produce deployment plans to con -
sider the performance requirements of the underlying edge infrastructure.
4.3.1  QoS Requirements of Software Applications
QoS requirements of software applications are (ranges/sets of) values on the QoS 
attributes of the applications (e.g., response time, availability, etc.). Such require -
ments are taken into account during the initial deployment of applications and may 
be dynamically validated during the execution of applications.
The authors of [ 13] propose an automated mechanism for generating eligible 
deployment plans, i.e., the plans meet the application requirements for the latency 
and the bandwidth of edge-to-edge, edge-to-cloud, and cloud-to-cloud communica -
tion links. Given that the mapping of many back-ends to many machines is an 
NP-hard problem (subgraph isomorphism problem), the authors of [ 13] propose a 
heuristic to get approximate solutions.
The authors of [ 17] propose an automated mechanism for generating deployment 
plans that minimize the delay of an edge-cloud infrastructure to serve application 
requests. To approximate optimal deployment plans, the authors of [ 17] adopt a 
mixed-integer nonlinear programming technique.
4.3.2  Performance of Edge Infrastructure
The performance of edge infrastructure mainly depends on two factors: (i) the 
amount of the computing resources of an infrastructure that are consumed by the 
deployed applications; and (ii) the delay of infrastructure to service application 
requests.
The service delay of an edge infrastructure mainly depends on the workload that 
serves at unit time [ 18]. The infrastructure performance is dynamically checked in 
this approach at the execution time of applications. Central mechanisms (proposed 
by the approach) are used to coordinate and control the initial and the runtime per -
formance of applications.
The authors of [ 19] propose the generation of deployment plans that reduce the 
power consumption of applications in edge machines. The authors of [ 20] build 
deployment plans that meet the application requirements about the power consump -
tion. In particular, the authors of [ 20] map application back-ends to machines via 
checking (traversing) paths of the topology graph of the underlying edge infrastruc -
ture. The authors of [ 21] produce deployment plans that consider both service delay 
of the edge infrastructure and the power consumption from the applications. The 
authors of [ 21] approximate optimal deployment plans balancing service time and 
power consumption.D. Athanasopoulos173
4.4  Maintenance of Software Applications
Existing approaches for deploying and executing applications via edge infrastruc -
tures adopt central mechanisms for controlling the execution of the applications. 
However, a central control mechanism does not scale well with the ever-increasing 
numbers of the deployed applications and the edge devices. Moreover, most of the 
existing approaches do not offer seamless switching of the execution of application 
components between edge and remote machines. On the contrary, the switching is 
performed at deployment time, suspending the application execution, redeploying 
the application components, and resuming the app running.
As we previously discussed, there are approaches that generate initial deploy -
ment plans, i.e., mappings between application components and edge/cloud 
machines. These plans aim at meeting the QoS requirements of applications and/or 
considering the performance of the underlying edge infrastructure.
On top of the above approaches, there are approaches that (re-)generate the 
deployment plans at the runtime of applications. Those approaches remap/redeploy 
application components to other machines if the QoS requirements are violated or 
the infrastructure performance is degraded. These approaches deal with the runtime 
modeling of the QoS attributes of the applications, along with the adaptation of the 
deployment plans of the applications by using the above models.
4.4.1  QoS Requirements of Software Applications
The runtime validation of QoS requirements for time-critical applications is even 
more crucial than other application types because time-critical applications offer 
their functionality only if time-related QoS requirements are met (e.g., smart traffic- 
light, early disaster-warning, cyber-physical applications).
The authors of [ 22] propose an approach for the QoS-driven migration of appli -
cations. It migrates application back-ends from an edge device to another when the 
latency of a back-end exceeds a time threshold. The target edge node is selected via 
checking (traversing) paths of the tree structure of the edge infrastructure. 
This approach does not build predictive models. On the contrary, it uses the current 
latency of back-ends to migrate them to other machines.
The authors of [ 23] present an approach for the runtime modeling of the empiri -
cal performance of applications in function of (software or hardware) control knobs 
(managed by the cloud providers to support QoS) and environmental primitives 
(e.g., number of service requests). To build performance models, the authors of [ 23] 
use machine-learning algorithms. The determined models are related to the response 
time, throughput, availability, and reliability of applications.
The authors of [ 24] build models and verifies rule-based auto-scaling policies for 
cloud-based applications in function of discrete elapsed time. To build and verify 
models, it uses a discrete-time Markov chain. The determined models are related to 
the CPU utilization of the applications.Software Engineering for Edge Computing174
The authors of [ 16] propose the runtime modeling of the response time of appli -
cations in function of the input data and the characteristics of the underlying 
machines. The constructed mathematical models are then used to decide the proper 
binding to the edge or to the cloud.
4.4.2  Performance of Edge Infrastructure
The infrastructure performance should be checked during the execution of applica -
tions. The check of the infrastructure performance for time-critical applications is 
more crucial than the other application types because low performance may nega -
tively affect the time-related requirements of the applications.
The authors of [ 25] propose an analytical model for measuring the service delay 
of an edge infrastructure in function of the elapsed discrete time. Based on this 
model, the authors of [ 25] offload computation from an edge node to another in an 
online manner toward reducing the service delay perceived by end-users.
The authors of [ 18] propose indexing of edge machines with respect to the trade- 
off between the service delay and the power consumption. The determined indexing 
models are in function of the elapsed discrete time. When offloading is needed, the 
edge device with the smallest index is selected.
The authors of [ 15] propose an offloading mechanism for migrating application 
back-ends. This mechanism is initiated by the end-users who provide their budgets. 
Then, the offloading mechanism maps (one-to-one bipartite mapping) back-ends to 
machines based on the computation costs of the machines.
5  Abstract Software Process for Edge Computing
We present in this section the software-engineering activities that should be fol -
lowed by practitioners for deploying and running modular software in edge infra -
structure. In particular, we present a waterfall software process  for edge computing 
(Fig.  1).
In a nutshell, the first activity of the process includes the specification of the 
capabilities of the machines of the target edge infrastructure and the specification of 
the software/hardware requirements of an edge-enabled software application. The 
second activity concerns the definition of the edge-deployable modular architecture 
of software application as a set of service components. The third activity is related 
to the microservice implementation of service components. The fourth activity has 
to do with the generation of edge-eligible deployment plans of service components 
to machines, along with the deployment of those components to the machines. The 
fifth activity includes the maintenance activities that are needed to meet the runtime 
requirements/attributes of application.D. Athanasopoulos175
Fig. 1  The waterfall software process for edge computing.
5.1  Software Application and Edge 
Infrastructure Specification
5.1.1   Specification of Edge-Deployable Modular Software
An edge-deployable software application consists of a set of software components 
(a.k.a., modular architecture). To be accessible over the Web, each component is 
usually developed as a Web service [ 6] and exposes its programming Web API 
(a.k.a., RESTful API [ 26]). Web APIs are lightweight alternatives to WSDL/SOAP-  
based Web services and use REST as the communication protocol and JSON as the 
content format2.
To avoid hardware/software incompatibilities between the service components 
and the edge machines, service components are encapsulated into containers (e.g., 
Docker3).
Each service component usually has its own resource requirements and possibly 
further software requirements. In particular, each service component may have the 
following requirements:
• hardware-resource requirements
 –e.g., CPU, Hard disk, RAM
2 https://www.json.org/json-en.html
3 https://www.docker.com/Software Engineering for Edge Computing176
• software-capabilities requirements
 –operating system (e.g., Linux, Windows)
 –software platform (e.g., JDK, .NET).
To put it more formally, a (front-end and back-end) service component is defined 
as follows.
Definition 1  Edge-deployable service component  is defined by the tuple C =  
( HR[], SC[], URI ), where HR[] denotes the array of the hardware-resource require -
ments, SC[] denotes the (possibly empty) array of software requirements, and URI 
denotes the endpoint address of the service deployed to an edge/cloud machine.
Overall, modular software consists of a set of service components. It is challeng -
ing though the automated generation of the specification of the hardware-resource 
requirements of edge-deployable service components.
5.1.2  Specification of Edge Infrastructure
A virtual or a physical machine of an edge infrastructure is characterized by the 
hardware resources and the software capabilities of the machine, as defined below.
Definition 2  Edge/cloud machine  is defined by the tuple M = ( HR[], SC[] ), where 
HR[] is the array of the hardware resources of the machine and SC[] is the array of 
the software capabilities of the machine.
Overall, an edge infrastructure consists of a set of machines that can be modeled 
as an undirected graph as follows.
Definition 3  Edge infrastructure  is defined by the undirected graph I = ( M[], L[]), 
where the array M[] of the graph corresponds to the edge/cloud machines and the 
array L[] of the graph contains the edge-to-edge, edge-to-cloud, and cloud-to-cloud 
communication links between the machines.
It is challenging though the automated dynamic management of edge 
infrastructures.
5.2  Edge-Deployable Modular Software
An edge-deployable software application can be defined as a composition of service 
components (a.k.a., modular architecture). The service composition can be modeled 
as a directed graph as follows.
Definition 4  Edge-deployable software application  is defined by the tuple A =  
( C[], D[] ). The array C[] of the graph corresponds to the set of the (front-end and 
back-end) service components of the application. The array D[] of the graph corre -D. Athanasopoulos177
sponds to the dependency links between the components. A component depends on 
another component (having a dependency link) if the former component invokes the 
Web API of the latter component.
As described in a previous section, the architecture of an edge-enabled software 
application can follow the partitioning technique or the offloading technique.
5.2.1  Architecture of Edge-Partitioned Modular Software
The approaches of this category assume that while the components of an application 
have been deployed to edge machines, parts of the components are executed in 
powerful (e.g., cloud) machines only if the applications benefit from the remote 
execution. The architecture of edge-partitioned software application is defined below.
Definition 5  Architecture of edge-partitioned modular software  is defined by the 
tuple A = ( C[], D[] ). The array C[] of the graph corresponds to the service compo -
nents of the application. Each component C consists of two partitions, C = (CE, 
CC). The partition CE represents the edge partition of the component. The partition 
CC represents the cloud partition of the component. The array D[] of the graph cor -
responds to the dependency links between the components. Each dependency link 
is instantiated by two graph edges, D = (DE, DC). The edge DE represents a depen -
dency link of the edge partition of the component. The edge DC represents a depen -
dency link of the cloud partition of the component.
5.2.2  Architecture of Edge-Offloaded Modular Software
The approaches of this category offload the execution of some (or all) of the com -
ponents of applications. The architecture of edge-offloaded software application is 
defined below.
Definition 6  Architecture of edge-offloaded modular software  is defined by the 
tuple A = ( C[], D[] ). The array C[] of the graph corresponds to the service compo -
nents of the application. Each component C consists of at most two instances, C = 
(CE, CC). The instance CE represents the edge instance of the component. The 
instance CC represents the cloud instance of the component. The array D[] of the 
graph corresponds to the dependency links between the components. Each depen -
dency link is instantiated by two graph edges, D = (DE, DC). The edge DE repre -
sents a dependency link of the edge instance of the component. The edge DC 
represents a dependency link of the cloud instance of the component.
It is challenging though the (semi-)automated specification and maintenance (of 
high quality) of the architecture of edge-partitioned and edge-offloaded software 
applications.Software Engineering for Edge Computing178
5.3  Microservice Implementation/Test of Modular Software
The implication behind the implementation of the modular components of software 
applications is that the components are light-weight enough so as the components 
are deployable to resource-constrained edge machines. Services whose footprint is 
very small are known as microservices [ 27]. Microservices have emerged in recent 
years as the main target type of service-oriented computing. However, as surveyed 
in [28], the development of complex software applications in the form of microser -
vices is currently performed by software engineers in a manual way.
It is interesting to mention that the platforms that exist for implementing edge- 
enabled software applications assume that the application components have already 
been implemented as microservices and wrapped by containers. For instance, 
SONM platform4 is a powerful distributed Web platform for hosting software appli -
cations that offers a pure edge infrastructure (as an alternative of a cloud or edge- 
cloud infrastructure). To host an application component in SONM, developers 
should prepare a Docker container with the component and upload the container to 
the Docker storage5.
5.4  Edge-Eligible Deployment of Service Components
5.4.1  Edge-Eligible Deployment Plan
An edge-eligible software application is an application whose components can be 
deployed to the machines of an edge infrastructure. To keep track of the mapping of 
software components to machines, a deployment plan is generated. A deployment 
plan usually assumes that all software components of an application have been 
deployed to machines. A deployment plan further assumes that an eligible mapping 
has been produced. Eligible is the mapping that the requirements of the deployed 
software components are met by the used machines, as defined below.
Definition 7  Edge-eligible deployment plan  is an one-to-many boolean mapping 
table map[machines, components]  of (edge and cloud) machines that host applica -
tion components. The value (true or false) of an element of a boolean table means 
that a machine hosts a software component, as formally specified below.
 MN
CHRi MC MHRi CSCj MSCj
/g32/g166 /g62/g64/g13 /g62/g64 /g11/g12 /g100 /g62/g64 /g62/g64 /g62/g64
1.. .. map, and 
 
4 https://docs.sonm.com
5 https://docs.sonm.com/getting-started/as-a-consumer#Task_executionD. Athanasopoulos179
where M is a machine, N is the number of the used machines, C.HR[i] is the value 
of the i-th hardware-resource requirement of a component C, M.HR[i] is the value 
of the i-th hardware-resource capability of a machine M, C.SC[j] is the value of the 
j-th software capability of a component C, and M.SC[j] is the value of the j-th soft -
ware capability of a machine M.
5.4.2  From Deployment Plan to the Deployment to Machines
After having decided/generated a deployment plan, it is then specified in a machine- 
readable format. The commonly used format is YAML6. In particular, deployment 
engines used by edge and/or cloud platforms usually accept as input YAML 
documents.
To automate the cloud deployment and the management (e.g., scaling) of the 
deployed containerized services, Kubernetes platform has been widely used7. More 
recently, KubeEdge platform has been proposed8. The key goal for KubeEdge is to 
extend Kubernetes ecosystem for the cloud and the edge.
It is challenging the scalable deployment of service components to a large num -
ber of edge machines whose resources are potentially controlled by a federation of 
KubeEdge platforms.
5.5  Edge-Enabled Maintenance of Software Applications
The previously mentioned deployment plans are initial mappings between applica -
tion components and edge/cloud machines. These plans aim at meeting the QoS 
requirements of applications and/or considering the performance of the underlying 
edge infrastructure. However, the deployment plans should be adapted at the appli -
cation runtime if the QoS requirements are violated or the infrastructure perfor -
mance is degraded.
5.5.1  QoS-Based Maintenance of Service Components
To enable the redeployment of application components based on their QoS values, 
we extend the definition of an application component as follows (Def. 8 is an exten -
sion of Def. 1).
6 http://cloud.google.com/appengine/docs/flexible/java/yaml-configuration-files
7 https://kubernetes.io
8 https://kubernetes.io/blog/2019/03/19/kubeedge-k8s-based-edge-intro/ =Software Engineering for Edge Computing180
Definition 8  Edge-redeployable service component  is defined by the tuple C =  
( HR[], SC[], QoS[], URI ), where QoS[] is an array of the runtime values of QoS 
attributes (e.g., response time, etc.).
Given the updated Definition 8 of a service component, we define below the 
redeployment plan of service components based on their runtime QoS attributes 
(Def. 9 is an extension of Def. 7).
Definition 9  Edge-enabled QoS-based redeployment plan  contains a one-to-many 
boolean mapping table map of (edge and cloud) machines that host service compo -
nents and minimize the values of the QoS attributes of the service components as 
follows:
 MN
CHRi MC MHRi CSCs MSCs CQoSj
/g32/g166 /g62/g64/g13 /g62/g64 /g11/g12 /g100 /g62/g64 /g62/g64 /g62/g64
1.. .. . map, anda nd  /g62/g62/g64ismin,
 
where M is a machine, N is the number of the used machines, C.HR[i] is the value 
of the i-th hardware-resource requirement of a component C, M.HR[i] is the value 
of the i-th hardware-resource capability of a machine M, C.SC[s] is the value of the 
s-th software-capability of a component C, M.SC[s] is the value of the s-th software- 
capability of a machine M, and C.QoS[j] is the value of the j-th QoS attribute of a 
component C.
5.5.2  Performance-Driven Maintenance of Edge Infrastructure
To enable the redeployment of application components based on the performance of 
the underlying edge infrastructure, we extend the definition of a machine as follows 
(Def. 10 is an extension of Def. 2).
Definition 10  An edge/cloud performance-aware machine  is defined by the tuple 
M = ( HR[], SC[], P[] ), where P[] is the array of the runtime performance of the 
machine.
Given the updated Definition 10 of a machine, we define below the redeployment 
plan of service components based on the runtime performance of the machine (Def. 
11 is an extension of Def. 2).
Definition 11  A performance-driven redeployment plan  is a one-to-many boolean 
mapping table of (edge and cloud) machines that redeploy service components by 
minimizing the values of the performance of the machines as follows:
 MN
CHRi mapM CM HRiandCSCs MSCsandMP j
/g32/g166 /g62/g64/g13 /g62/g64 /g11/g12 /g100 /g62/g64 /g62/g64 /g62/g64 /g62/g64
1.. .. . ,  iismin
 
where M is a machine, N is the number of the used machines, C.HR[i] is the value 
of the i-th hardware-resource requirement of a component C, M.HR[i] is the value D. Athanasopoulos181
of the i-th hardware-resource capability of a machine M, C.SC[s] is the value of the 
s-th software-capability of a component C, M.SC[s] is the value of the s-th soft -
ware-capability of a machine M, and M.P[j] is the value of the j-th performance 
attribute of a machine M.
It is challenging though the scalable (e.g., for a large number of machines) main -
tenance of the deployment plans of software applications to meet the runtime QoS 
requirements of applications and to achieve high runtime performance for the edge 
infrastructures.
6  Conclusions
The distant cloud is not always the best solution for time-sensitive and location- 
aware software applications. Edge computing has been recently introduced by both 
industry and academia to quench the need for a computing paradigm close to mobile 
devices. In this chapter, we specified the core concepts of the software-engineering 
process and how software applications can be deployed to the multi-tier architecture 
of edge infrastructure. We highlighted the software-engineering aspects of existing 
edge-computing approaches. Finally, we abstracted a software-engineering process 
suitable for edge computing and we outlined the research challenges that exist in 
this process.
References
 1. A. McAfee, E. Brynjolfsson, T.H. Davenport, D. Patil, D. Barton, Big data: the management 
revolution. Harv. Bus. Rev. 90(10), 60–68 (2012)
 2. A. Yousefpour, C. Fung, T. Nguyen, K. Kadiyala, F. Jalali, A. Niakanlahiji, J. Kong, J.P. Jue, 
All one needs to know about fog computing and related edge computing paradigms: a com -
plete survey. J. Syst. Architect. 98, 289–330 (2019)
 3. OpenFogConsortium, Openfog reference architecture for fog computing, Available: https://
www.openfogconsortium.org/ra , February 2017
 4. I.  Sommerville, Software Engineering , International computer science series, 8th edn. 
(Addison-Wesley, 2007), 9780321313799
 5. M. Mukherjee, L. Shu, D. Wang, Survey of fog computing: fundamental, network applica -
tions, and research challenges. IEEE Commun. Surv. Trends 20(3), 1826–1857 (2018)
 6. T. Erl, Service-Oriented Architecture: Concepts, Technology, and Design , (Prentice Hall, 2005)
 7. C. Li, Y . Xue, J. Wang, W. Zhang, T. Li, Edge-oriented computing paradigms: a survey on 
architecture design and system management. ACM Comput. Surv. 51(2), 39:1–39:34 (2018)
 8. C.H. Hong, B. Varghese, Resource management in fog/edge computing: a survey on architec -
tures, infrastructure, and algorithms. ACM Computing Surveys 52(5), 97:1–97:37 (2019)
 9. C. Mouradian, D. Naboulsi, S. Yangui, R.H. Glitho, M.J. Morrow, P.A. Polakos, A comprehen -
sive survey on fog computing: state-of-the-art and research challenges. IEEE Commun. Surv. 
Tutor. 20(1), 416–464 (2018)Software Engineering for Edge Computing182
 10. A.C. Baktir, A. Ozgovde, C. Ersoy, How can edge computing benefit from software-defined 
networking: a survey, use cases, and future directions. IEEE Commun. Surv. Tutor. 19(4), 
2359–2391 (2017)
 11. C. Perera, Y . Qin, J.C. Estrella, S.R. Marganiec, A. Vasilakos, Fog computing for sustainable 
cities: a survey. ACM Comput. Surv. 50(3), 32:1–32:43 (2017)
 12. J. Ni, K. Zhang, X. Lin, X. Shen, Securing fog computing for Internet of Things applications: 
challenges and solutions. IEEE Commun. Surv. Tutorials 20, 601–628 (2018)
 13. A. Brogi, S. Forti, QoS-aware deployment of IoT applications through the fog. IEEE Internet 
Things J. 4(5), 1185–1192 (2017)
 14. B.G. Chun, S. Ihm, P. Maniatis, M. Naik, A. Patti, CloneCloud: elastic execution between 
mobile device and cloud, in European Conference on Computer Systems, European confer -
ence on Computer systems, pp. 301–314, 2011
 15. D.H. Tran, N.H. Tran, C. Pham, S.M.A. Kazmi, E.N. Huh, C.S. Hong, OaaS: offload as a 
service in fog networks. ACM Comput. 99(11), 1081–1104 (2017)
 16. D.  Athanasopoulos, M.  McEwen, A.  Rainer, Mobile apps with dynamic bindings between 
the fog and the cloud, in International Conference on Service-Oriented Computing, 
pp. 539–554, 2019
 17. R.  Deng, R.  Lu, C.  Lai, T.H.  Luan, H.  Liang, Optimal workload allocation in fog-cloud 
computing toward balanced delay and power consumption. IEEE Internet Things J. 3(6), 
1171–1181 (2016)
 18. X.  Guo, R.  Singh, T.  Zhao, Z.  Niu, An index based task assignment policy for achieving 
optimal power-delay tradeoff in edge cloud systems, in IEEE International Conference on 
Communications, pp. 1–7, 2016
 19. A.  Brogi, S.  Forti, A.  Ibrahim, How to best deploy your fog applications, probably, in 
International Conference on Fog and Edge Computing, pp. 105–114, 2017
 20. H. Gupta, A.V . Dastjerdi, S.K. Ghosh, R. Buyya, iFogSim: a toolkit for modeling and simula -
tion of resource management techniques in the Internet of Things, edge and fog computing 
environments. Softw. Pract. Exp. J. 47(9), 1275–1296 (2017)
 21. R.  Deng, R.  Lu, C.  Lai, T.H.  Luan, H.  Liang, Optimal workload allocation in fog-cloud 
computing toward balanced Delay and power consumption. IEEE Internet Things J. 3(6), 
1171–1181 (2016)
 22. E.  Saurez, K.  Hong, D.  Lillethun, U.  Ramachandran, B.  Ottenwalder, Incremental deploy -
ment and migration of geo-distributed situation awareness applications in the fog, in ACM 
International Conference on Distributed and Event-Based Systems, pp. 258–269, 2016
 23. T. Chen, R. Bahsoon, Self-adaptive and online QoS modeling for cloud-based software ser -
vices. IEEE Trans. Softw. Eng. 43(5), 453–475 (2017)
 24. A. Evangelidis, D. Parker, R. Bahsoon, Performance modelling and verification of cloud-based 
auto-scaling policies. Futur. Gener. Comput. Syst. 87, 629–638 (2018)
 25. A. Yousefpour, G. Ishigaki, J.P. Jue, Fog computing: towards minimizing delay in the Internet 
of Things, in EEE International Conference on Edge Computing, pp. 17–24, 2017
 26. L. Richardson, S. Ruby, Restful Web Services , 1st edn. (O’Reilly, 2007), 9780596529260
 27. S. Newman, Building Microservices , 1st edn. (O’Reilly Media, Inc., 2015), 1491950358
 28. D. Taibi, V . Lenarduzzi, C. Pahl, A. Janes, Microservices in agile software development: a 
workshop-based study into issues, advantages, and disadvantages, in ACM International 
Conference on Agile Software Development, pp. 23:1–23:5, 2017.D. Athanasopoulos183Overcoming Wifi Jamming and other 
security challenges at the Edge
Charles J. Gillan and Denis Guilhot
1  Introduction
The Internet has become an essential part of daily life for almost everyone in soci -
ety, having grown far beyond its roots in the 1970s as the ARPANET, a network that 
was principally the domain of scientists and engineers. The popularity of the HTTP 
protocol, developed at CERN in the late 1980s, led to the widespread use of the term 
‘the web’ as a generic name for the Internet for many years, at least in the public 
domain. Of course, the Internet is much more than just web browsing, and, in recent 
years, the term cyberspace has become the most popular term to describe interac -
tions over the Internet. Yet, an unambiguous definition of the term is difficult to 
formulate [ 1].
Financial institutions and the operation of national critical infrastructures, such 
as monitoring and control of the electricity supply, are now dependent on the 
Internet. A consequence of this is that cyberattacks become more costly for the vic -
tims and perversely more attractive to the criminals who carry them out [ 2]. The 
advent of the Internet of Things (IoT) and edge computing as a new paradigm cre -
ates the potential for enhanced productivity but at the same time opens up new 
opportunities for cyberattacks while still being exposed to existing attack vectors 
such as the well-known denial-of-service attack (DoS), which can take place in 
many forms [ 3]. At the edge, wireless communications are generally used.
In this chapter, we describe the challenges in building an edge system that is 
secure against cyberattacks at the physical layers. The UniServer project considered 
in some detail the security of the wireless links at the edge and proposed solutions 
C. J. Gillan ( *) 
The School of Electrical and Electronic Engineering and Computer Science (EEECS), 
Queen’s University Belfast, Belfast, Northern Ireland, UK
e-mail:  c.gillan@qub.ac.uk  
D. Guilhot 
Worldsensing SL, Barcelona, Spain
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3_8184
to defend against, for example, jamming attacks. We begin by briefly reviewing the 
architecture of communications over the Internet and later consider the new chal -
lenges that follow from operating the hardware with values of voltage, frequency 
and current that enable more energy efficiency. Edge devices, as one instance of a 
computer system, should of course take account of the many principles defined for 
cyber security and embrace the higher layers of the OSI seven-layer model.
In their 2020 business report on the State of the Cloud [ 6], Flexera reports that 
across all types of users the key challenges in order of significance are the top four: 
security, managing cloud spend, governance and lack of resources/expertise. Related 
to this is the fact that the landscape of regulation is constantly evolving meaning that 
it adds more information security and data protection requirements. The regulatory 
framework impacting cloud operation includes, but is not limited to, the Gramm- 
Leach- Bliley Act for financial transactions and the Health Insurance Portability and 
Accountability Act (HIPPA) in the United States, the Canadian Personal Information 
Protection and Electronic Documents Act, and in the European Union, the General 
Data Protection Regulation (GDPR). The UK National Cyber Security Centre 
(http://www.ncsc.gov.uk ) publishes a number of guidance documents on how to 
best organise cyber security for a business. In this chapter, we do not deal any fur -
ther with these aspects of security, focusing instead on the physical layer attacks.
2  Technical Perspectives on Wi-Fi Technology
2.1  Review of the Theory of Electromagnetic Waves
Electromagnetic waves, alternatively called electromagnetic radiation, are solutions 
of Maxwell’s Eq. [ 1] consisting of interlinked electric and magnetic fields, where 
the letter E denotes the electric field and B labels the magnetic part.
In the case of a plane wave created by an oscillating dipole, the E and B fields 
each exhibit the characteristic shape of a sinusoidal waveform but they are oriented 
at 90° to each other. The value λ, which is the distance between two peaks of the 
oscillations, is the wavelength. All electromagnetic waves move at the same speed, 
c, in a vacuum, where c = 299792458 m/s. The frequency, f, of the two waveforms 
is the same and is related to the speed and wavelength by the equation:
 cf/g32/g79 (1)
Both f and λ are real-valued numbers, meaning that there is continuum of elec -
tromagnetic waves from very low to very high frequencies, known as a spectrum.
When electromagnetic radiation enters a medium other than a vacuum, the wave -
length changes but the frequency remains the same; a further consequence of this 
attribute is that the speed of the wave changes. Given that the frequency is a constant 
for a given wave, for the purposes of labelling, the spectrum of electromagnetic C. J. Gillan and D. Guilhot185
Table 1  List of frequency bands defined by the ITU-R
Frequency range Band nameITU-R band 
numberMetric-related name (wavelength 
in vacuum)
3–30 kHz Very Low Frequency 
(VLF)4 Myriametric
30–300 kHz Low Frequency (LF) 5 Kilometric
300 kHz–3 MHz Medium Frequency 
(MF)6 Hectometric
3–30 MHz High Frequency (HF) 7 Decametric
30–300 MHz Very High Frequency 
(VHF)8 Metric
300 MHz–3 GHz Ultra High Frequency 
(UHF)9 Decametric
3–30 GHz Super High Frequency 
(SHF)10 Centimetric
30–300 GHz Extra High Frequency 
(EHF)11 Millimetric
radiation is divided into bands, with each band being labelled by the frequencies 
that it encompasses. The naming of the bands is standardised by the International 
Telecommunication Union [ 4], and Table  1 shows a definition of subset of the bands 
including the name attached to each. The left-hand column shows that the division 
into bands is such that the higher boundary is a multiple of ten times the lower 
boundary. The right-hand column is an alternative name given to the band and is 
derived from the range of wavelengths in vacuum for waves in that band.
There are a number of alternative labels in use, deriving from different sources 
such as the IEEE [ 5] that further divide the frequency bands. The frequency range 
47–960  MHz, with gaps, is used for FM-type radio broadcasting and terrestrial 
television broadcasting. In the United Kingdom, the 700 MHz region of the spec -
trum is allocated to mobile broadband and 5G services. In many countries, the range 
of these frequencies in use is divided into bands labelled by the Roman numerals I 
to V . A further example is found in the satellite broadcasting industry where the 
label Ku band is given to the frequency ranges 10.7–13.25 GHz and 14.0–14.5 GHz.
As the IEEE standard 802.11 has evolved over the past two decades, it has 
defined several distinct frequency ranges that lie in several different bands across 
the electromagnetic spectrum for use in the physical layer of Wi-Fi. These frequen -
cies include 900 MHz, 2.4 GHz, 3.6 GHz, 4.9 GHz, 5 GHz, 5.9  GHz and 
60 GHz bands.
In general, the government of each country regulates the region of the electro -
magnetic spectrum [ 7] that is used for communications, deeming it to be an exclu -
sive property of the state. The International Telecommunications Union recognises 
this right to ownership. For example, in the United Kingdom the relevant legislation 
is the Wireless Telegraphy Act 2006, while in the United States the Federal 
Communications Commission manages the spectrum for non-government use and 
the National Telecommunications and Information Administration manages the Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge186
spectrum for government use. The European Union has a comprehensive set of 
directives and regulations aiming to shape a future single market for telecommuni -
cations. In most regions of the spectrum, it is necessary to obtain a license to trans -
mit, but clearly, this would present a considerable barrier to use of wireless 
technology such as laptop computers, tablets and smartphones. Therefore, operating 
defined types of equipment in certain defined frequency ranges is exempted in law 
from the requirement to hold a license. The Industrial Scientific and Medical (ISM) 
bands are ranges of frequencies reserved for use without a license [ 8]. The range 
2.4–2.5 GHz is one such ISM band and, as described later in this chapter, is used by 
some of the Wi-Fi standards.
2.2  Modulation: Transmitting Information Using 
Electromagnetic Wave
The plane wave, as described above, is continuous and has no way to carry informa -
tion, other than being turned on and off. A typical mechanism is to transmit pulses 
of such radio waves in groups, and this became the basis of Morse Code transmis -
sion, the oldest and simplest form of radio communication. The technique is known 
as pulse modulation. Originally, a telegraph key operated by a human was the mech -
anism used to produce a radio transmission consisting of pulses of sine waves with 
a constant amplitude interspersed with gaps of no signal. In Morse code, two differ -
ent time durations of the pulses are defined. The short duration is known as a dot and 
the longer as a dash. The English letters A–Z and the digits 0–9 are mapped to a 
unique set of dot-dash combinations. Morse counted the frequency of occurrence of 
letters in printed documents finding that E and then T are the two most commonly 
occurring letters. He assigned these the single pulse codes dot and dash, respec -
tively. He then repeated the process to assign two, three and four pulse codes to 
other letters.
The development of Morse code illustrates some of the basic principles of con -
veying textual information using radio waves. In general, the wave needs to be 
modified in other ways to be able to transmit complex information, and there are 
three attributes that can be varied to add information. These are:
• Amplitude Modulation (AM)
In this method, the transmitter varies the amplitude of the carrier wave in a man -
ner dependent on the input signal.
• Frequency Modulation (FM)
In this type of modulation, the transmitter varies the frequency of the carrier 
wave in relation to the amplitude of the input signal.
• Phase Modulation (PM)
This is a complex modulation in which the phase of the carrier signal is modu -
lated to follow amplitude of the input signal.C. J. Gillan and D. Guilhot187
All of these methods are applied to a carrier wave and can be combined in differ -
ent ways, and also with pulse modulation, to produce more complex modulation 
schemes, and some of these more complex schemes use the Wi-Fi transmission.
The plane waveform has a single unique frequency (the carrier frequency), but 
when modulation is applied the radio wave generated is no longer composed of a 
single frequency but has components of radio waves in a range of frequencies, 
known as sidebands, surrounding the carrier. Figure  1 illustrates this where the car -
rier wave has frequency fc and the modulated signal includes waves with frequen -
cies in the interval [f-fm,fc+fm]. Different versions of the IEEE 802.11 standard for 
wireless transmission mandate different modulation schemes, and these are sum -
marised in Table  2. This frequency range [f − fm, fc + fm] is known as a channel, 
and in the next section, we discuss the channels defined in standards for Wi-Fi.
DSSS stands for direct sequence spread spectrum and OFDM for orthogonal 
frequency division multiplexing. In DSSS, each bit to be transmitted is combined 
with a fixed size bit code, typically using the XOR operation between the bit and 
each bit in order in the code. The bit code can be 128 bits long, meaning that 128 
bits are used to transmit one bit of information that is spreading out over a sequence. 
The coded version of each information bit is modulated onto a carrier wave using as 
many frequencies as possible within a channel [ 9]. OFDM stands for orthogonal 
frequency division multiplexing. Information to be transmitted is spread over sev -
eral frequencies within a channel but these frequencies are chosen to be separated 
such that there is essentially no crosstalk between the frequencies, giving the method 
name ‘orthogonal’. The orthogonality condition is given by the equation:
 /g71fk/g32T 
Fig. 1  Illustration of the occurrence of sidebands after carrier modulationOvercoming Wi-Fi Jamming and Other Security Challenges at the Edge188
Table 2  Modulation schemes used in different versions of the 802.11 standard
Version of standard Frequency band Bandwidth Modulation scheme
802.11b 2.4 GHz 20 MHz DSSS
802.11g 2.4 GHz 20 MHz DSSS, OFDM
802.11n 2.4 and 5 GHz 20, 40 MHz OFDM
802.11ac 5 GHz 40, 80, 160 MHz OFDM
where k is a constant and T is the time in seconds for the duration of transmission 
of information on the frequency.
2.3  Wi-Fi Channels
The ISM band spanning the frequency range 2.4–2.5 GHz is further subdivided into 
14 overlapping channels, each of width 22 MHz, when used for Wi-Fi as shown in 
Table  3. The use of overlapping channels distinguishes Wi-Fi from the use of chan -
nels in other parts of the radio frequency spectrum such as for television and broad -
cast radio transmission. In the latter situation, channels do not overlap. The 802.11 
standard defines the permitted power distribution (i.e. a spectral mask) across the 
frequency range of each channel that is at frequency offsets to the left or to the right 
of the carrier frequency. It follows that the signal power is attenuated relative to the 
power peak at the carrier frequency.
The separation between channels combined with the spectral mask is designed 
such that during normal operation the overlapping signal on any channel should 
only create minimal interference with neighbouring channels. Of course, violation 
of this design principle clearly presents one attack vector for disruption of operation.
National regulators sometimes limit the use of these channels. The FCC in the 
United States does not permit the use of channels 12, 13 and 14. The Canadian regu -
lator permits use of channel 12 but not channels 13 and 14. The United Kingdom 
regulator prohibits use of channel 14. This can cause issues when a user purchases 
equipment in one jurisdiction and then operates it in a different jurisdiction.
Several free applications exist that can be used with mobile devices to capture the 
signal spectrum received at their Wi-Fi chip from nearby access points. Figure  2 
shows an example screenshot in which many access points are visible and showing 
channel overlap.
3  Modulation Schemes Used by Wi-Fi
The ITU defines a classification scheme for radio signals with a three-letter code 
carrying the core detail of how the signal is composed. This information is reported 
on test sheets for certification of Wi-Fi chips. Table  4 shows the first letter used in C. J. Gillan and D. GuilhotTable 3  Channels in the 2.4–2.5 GHz ISM band for Wi-Fi
Channel Lower frequency Upper frequency
1 2.401 2.423
2 2.406 2.428
3 2.411 2.433
4 2.416 2.438
5 2.421 2.443
6 2.426 2.448
7 2.431 2.453
8 2.436 2.458
9 2.441 2.463
10 2.446 2.468
11 2.451 2.473
12 2.456 2.478
13 2.461 2.483
14 2.473 2.495
Fig. 2  Screen shot from a Wi-Fi channel analyser showing the transmission power in dBm for 
each channel for the different SSIDs observed.190
Table 4  A selection of the first characters used in the ITU classification of radio signals
Explanation of the first character
N A carrier wave with no modulation
A Amplitude modulation with double sidebands
H Amplitude modulation with single sideband and full carrier
R Amplitude modulated single sideband. Reduced or variable-level 
carrier
F Frequency modulation
G Phase modulation
P Sequence of unmodulated pulses
K Sequence of pulses with amplitude modulation
the designation and details of the second and third symbols are available from the 
ITU-R [ 10].
Phase modulation is the primary mechanism used in Wi-Fi. Binary Phase Shift 
Keying (BPSK) is the most basic method used in Wi-Fi transmission. With BPSK, 
the phase of the carrier shifts by 180° and a phase shift represents one bit. Quadrature 
Phase Shift Keying (QPSK) involves shifts in the phase of the wave by 90° with 
each shift representing 2 bits. In both cases, the amplitude of the wave remains con -
stant. Adding amplitude variation on top of phase shift keying is called quadrature 
amplitude modulation (QAM), and this technique is used to further increase the 
bandwidth of Wi-Fi signals.
A further step is to partition the frequency range of the channel and transmit on 
multiple frequencies in this range, known as orthogonal frequency division multi -
plexing (OFDM). Each transmission frequency uses a PSK or QAM modulation. 
This has a multiplicative effect substantially increasing bandwidth. One example is 
the 802.11g standard that uses 64 frequencies.
While these complex modulation schemes increase the bandwidth available for 
transmission, they also render the signal susceptible to jamming. A common time- 
frequency trace recorded from commercial jammer devices is a chirp signal, which 
scans the 2.4 GHz band. A key parameter defining the trace is the sweep period that 
is the time to cover the complete frequency range. Research on chip signal jamming 
of 802.11n signals [ 8] shows that an 802.11n network is robust to chirp jamming for 
a sweep period less than 3.2  μs but that for a sweep period between 10  μs and 20  μs, 
the communication can be jammed.
4  Wi-Fi and Its Attack Vectors
The term Wi-Fi has entered common use since it has become the de facto standard 
for the physical layer (PHY) and the media access control (MAC) component of the 
data link layer of Internet data communications for most of the population at home. 
A typical set up is that an ADSL modem connects the property to a fixed line service C. J. Gillan and D. Guilhot191
provided by a telecommunication provider and that the modem also acts as a Wi-Fi 
access point (AP) to which the devices within the home can connect. The model 
extends to Wi-Fi access at the edge of the cloud where this property could be a res -
taurant, train station or factory to name but a few possibilities. In general, then, 
Wi-Fi is a technology for local area networks.
Wi-Fi is the name used to refer to the set of wireless communication standards 
defined by the IEEE, specifically in the standards 802.11. This set of standards con -
tinues to evolve, and each new member is identified by the addition of letters as in 
802.11g, a standard introduced in 2003 to work in the 2.4 GHz band.
In the simplest terms, the details of Wi-Fi communication, and therefore, the 
potential attack vectors are divided into two areas, each corresponding to the two 
lower layers of the OSI seven-layer model.
 1. The modulated electromagnetic waves that constitute the physical layer ulti -
mately convey the bits and bytes backwards and forwards between the AP and 
each of the nodes. Digital signal processing at the receiver converts received 
electromagnetic signals into a data frame structure.
 2. The structure of the data frames in the MAC sub-layer of the data link layer 
implements a protocol which manages and controls the Wi-Fi network as well as 
carrying data which is passed to higher layers of the Internet protocol stack.
Attacks are possible at each layer by exploiting its characteristics. Interfering 
with the electromagnetic signals, a technique known generally as electronic counter 
measures (ECMs) makes it difficult or impossible for the receiver to extract data 
frames. Alternatively, manipulating the protocol at layer 2 by sending rogue man -
agement and control data frames is another attack vector.
Since the use of wireless communication has dramatically increased, a specific 
type of denial of service (DoS) attacks in which malicious emitters cause interfer -
ences to block the original communication has become prominent. These attacks, 
which are relatively easy to apply because of the very own nature of wireless com -
munications, are called jamming attacks. Bohjani et  al. reported on numerous 
research aspects regarding jamming detection, including algorithms and detection 
techniques [ 11]. Some papers in the literature [ 11–14] focus on the detection of 
Wi-Fi jamming attacks using a combination of two or more of the metrics: packet 
delivery ratio (PDR), packet sent ratio (PSR), signal strength (SS), or carrier sensing 
time (CST). PDR is the ratio of the total number of packets correctly received, 
meaning that they pass the cyclic redundancy check, to the total number of packets 
received. PSR is the ratio of acknowledgements at the transmitter to packets trans -
mitted earlier. SS is the strength of the signal at the receiver, and CST is the time that 
a transmitter must wait for the channel to become silent so that it can start to 
transmit.
In the following sections, we address these two different types of jamming 
attacks.Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge192
4.1  Jamming Signals
Jamming is the process of transmitting additional radio signals towards receivers, 
making it difficult for them to detect actual signals. In general, jammers disrupt the 
wireless communication by generating high-power noise across the entire frequency 
range in use. Signal jamming is not just applicable to Wi-Fi but can take place in 
other bands. With the growing dependence on global navigation satellite systems for 
aeronautical and maritime navigation, the problem of jamming these systems has 
been widely studied. Jamming these signals can lead to failure of the end user’s 
receiver or even the presentation of potentially hazardous information [ 15].
Pelechrinis and co-authors [ 16, 17] state that an ideal jamming attack should 
have the following characteristics:
• Consume low power
• Have very low probability of detection
• Disrupt communications to the maximum possible extent
• Be resistant to use of signal processing techniques at the PHY layer to overcome 
the attack
Huang et al. [ 18] described a distributed jammer network, which is made of a 
large number of low-power, tiny radio jammers, pointing out that the state of the art 
in MEMS and NANO technologies [ 19] makes it possible to build nanoscale jam -
mers that can be deployed in quantities of tens of thousands. Game theory provides 
a mathematical framework for analysis [ 20] of the multi-jammer problem.
Jammer devices can have very different specifications [ 21]. First, there are differ -
ent types of physical embodiments:
• Portable: Small form factor but low reach, they can generally affect communica -
tions at a distance of a few meters in the absence of obstacles.
• Stationary: The more powerful, the bulkier the device. These devices have greater 
ranges and often greater frequency band but are more expensive and require 
cooling.
Second, jammer devices can be split in two main rationales:
• Proactive jammers, that emit continuously, independent of whether communica -
tion is taking place or not. They can emit continuous signals, regular packets, or 
random interferences, but are always on.
• Reactive jammers, that listen to the network they want to interfere with and only 
emit when they can affect the communication. These are more difficult to detect 
since there is no clear pattern to when they emit.
Finally, jammer devices can be differentiated on the type of signals they emit, as 
will be developed in the next section.C. J. Gillan and D. Guilhot193
4.1.1  Countering Jamming Attacks on Wi-Fi Networks
The most common pre-emptive mechanism against jamming attacks relies on 
spread-spectrum communication [ 22, 23]. This consists in jumping across the fre -
quency range whilst transmitting the signal. If the receiver is coordinated, the com -
munication protocol is efficient. Nevertheless, some jammer attacks target a broad 
frequency spectrum to be efficient against devices implementing this protocol; some 
even ‘listen’ to predict the frequency-hopping scheme and as such predict which 
band to jam at every moment. Another traditional way of avoiding jamming attacks 
is for a system to rely on several communication bands since jamming of all avail -
able radio communication channels of professional security systems over a large 
area can only be achieved using overall and expensive interference generators. In 
such systems, if the connection is lost in one band, the device switches the commu -
nication to a different band and monitors it for potential jamming or loss of com -
munication. If the connection is also jammed in this band, it repeats this operation 
until it finds an available band.
In the UniServer research project funded by the European Commission in its 
Horizon 2020 program, the company Worldsensing developed a jammer detector 
tool called DoSSensing for use at the edge of the cloud. The tool is a stand-alone 
solution that monitors the full spectrum of wireless channels to detect anomalies 
derived from a denial-of-service attack, jamming attack.
The DoSSensing solution consists of a sensor with an antenna connected to an 
SDR module that digitalizes the radio spectrum to a binary stream and transmits this 
to a processing board. The board processes in real time the incoming data and 
applies filters and algorithms to match the signals found to four types of well-known 
jamming signals:
• Pulsed Jammer, Fig.  3
• Wideband Jammer, Fig.  4
• Continuous Wave Jammer, Fig.  5
• LFM Chirp Jammer, Fig.  6
Fig. 3  Pulsed jammer signalOvercoming Wi-Fi Jamming and Other Security Challenges at the Edge194
Fig. 4  Wideband jammer signal
Fig. 5  Continuous wave jammer signal
The core of the system is the XGene as it can concentrate the main processing of 
high amounts of radio frequency spectrum data on the edge. With such an approach, 
more than one instance of the processing application can be executed on the pro -
cessing board (edge device) by connecting various sensors to it. The detection 
results can be transmitted to the cloud for storage and visualization.C. J. Gillan and D. Guilhot195
Fig. 6  LFM chirp jammer 
signal
4.2  The 802.11 Frame Structure
The data link layer is the protocol layer that transfers data across the physical layer 
between network entities. The data link layer is concerned with local delivery of 
frames between nodes on the same level of the network, whereas routing, which is 
a layer function, transfers IPv4 frames between any nodes on the public Internet. 
The data link layer is itself divided into two sub-layers known as the
• Logical Link Control sub-layer (LLC)
• Media Access Control layer (MAC)
As with fixed line Ethernet, the MAC sub-layer and the LLC sub-layer for Wi-Fi 
have a FORMAT for the FRAMEs transmitted. As with all data transmission proto -
cols, each field has a well-defined meaning and a set of functions that operate on the 
field. The MAC layer for Wi-Fi 802.11 is more complex than fixed-line Ethernet 
because a wireless medium requires several management features and, therefore, 
corresponding frame types not found in wired networks.
The MAC sub-layer of the data link layer for Wi-Fi defines a frame structure 
composed of fields. Figure  7 shows the most general structure of the MAC frame; 
however, any given field transmitted contains, in general, only a subset of these fields.Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge196
Fig. 7  Structure of the 802.11 MAC frame. Fields lengths are in octets (bytes). (Wikipedia)
Table 5  Structure of selected subfields in the Frame Control field and 802.11 MAC frame
Field nameBits 
occupied Description
Protocol 
version0,1 This is always set to zero and is available for future use
Type 2,3 00->management; 01->control; 10->data; 11->extension
Sub-type 4,5,6,7 Used to distinguish different messages within each type
Retry 11 1 means, in the case of a data or management frame, that this is a 
retransmission
The frame control field at the front of the MAC frame has several subfields that 
are defined in Table  5, and it is this information that distinguishes the form that the 
remaining bytes in the MAC frame will take. The data structure for the LLC sub-  
protocol can be found at the start of the frame body, and the LLC format is the same 
as used with Ethernet (IEEE 802.3)
4.3  Frame Types
The different frames that are carried in the Frame Body section of Fig.  7 can be clas -
sified into the following types:
• Data frames . These transfer data from node to node enabling the functionality 
of the higher layers of the OSI stack, therefore allowing applications on different 
nodes to communicate.
• Control frames  work in conjunction with data frames to facilitate the transfer of 
information. The request to send (RTS) and clear to send (CTS) frames exchanged 
between a node and an access point are perhaps the most well-known type of 
control frames as they enable the carrier sense multiple access collision avoid -
ance (CSMA/CA) characteristic of a Wi-Fi network. One way, for example, to 
degrade the performance of a Wi-Fi network is to have a rogue node spoofing 
RTS and or CTS frames.
• Management frames  perform functions such as broadcasting the presence of 
the Wi-Fi network via the SSID and then allowing nodes to join and leave the 
network.C. J. Gillan and D. Guilhot197
The next section considers only the beacon frame and discusses some of the 
ways in which the properties of the beacon frame can be exploited by hackers. A 
comprehensive discussion of other frame types can be found.
4.4  Beacon Frames
Beacon frame is the mechanism by which an access point announces its availability. 
As such, the access point transmits a beacon frame as one of the management frames 
using the format described in the previous section for management frames. An 
access point transmits beacon frames at a set interval, and while this is configurable 
on advanced access points, the default is generally set at 102.4 milliseconds.
The significant fields in the beacon frame are shown in the following table 
(Table  6):
Beacon frames are transmitted by the access point (AP) in an infrastructure basic 
service set (BSS). In IBSS network, beacon generation is distributed among the sta -
tions. For the 2.4 GHz spectrum, when having more than 15 SSIDs on overlapping 
channels (or more than 45  in total), beacon frames start to consume significant 
amount of airtime and degrade performance even when most of the networks 
are idle.
While beacon frames do cause some non-trivial overhead for a network, they are 
vital for the proper operation of a network. Radio NICs generally scan all RF chan -
nels searching for beacons announcing the presence of a nearby access point. When 
a radio receives a beacon frame, it receives information about the capabilities and 
configuration of that network, and is also then able to provide a list of available 
eligible networks, sorted by signal strength. This allows the device to choose to con -
nect to the optimal network.
Beacon frames can be spoofed by a rogue system acting as an access point and 
publishing an expected SSID in the beacon frame.
Table 6  Fields in the beacon frame as defined in the 802.11 standard
Field Size Description
Timestamp Local time at the access point
Beacon 
intervalThe time interval between beacon transmissions
Capability 
information16 
bitsBit fields defining the capability of the network
Service set 
ID32 
bytesA sequence of bytes often set as a readable string value. The string can be 
shorter than 32 characters. This field has evolved over the lifetime of the 
standard, initially not being required to be printable characters. An AP 
can hide its SSID by not broadcasting it. In this situation, the only way to 
connect to that AP is to know in advance its SSID.Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge198
Power Saving Mode Attack (PSM)  PSM allows nodes to save energy while they 
are waiting for the channel to be available for transmission. For example, one node 
will go to a power save mode for a period specified by the access point. During this 
idle time, the access point will buffer the packets destined to that node, and they will 
be sent to it when it wakes up. If for any reason the node wakes up at any other time 
than that expected by the access point due to desynchronization caused by spoofed 
beacon frames, it may lose the buffered information. As a result, the victim node can 
suffer a reduction in its capacity for transmitting.
Point Coordination Function Attack (PCF)  In PCF mode, the access point serves 
as a network referee. It provides the priority mechanisms for the devices. An attacker 
could spoof beacon frames using false clock values. Those values would produce a 
maladjustment in the contention periods of the stations leading to a denial-of- 
service attack.
5  Detecting Wi-Fi Jamming at the Edge
Wireless networking plays an important role in achieving ubiquitous computing 
where network devices embedded in environments provide continuous connectivity 
and services, thus improving human quality of life. However, due to the exposed 
nature of wireless links, current wireless networks can be easily attacked by jam -
ming technology. Also, many of the edge deployments rely on processing capabili -
ties offered by Internet of Things (IoT) elements. Due to the competitiveness of the 
IoT sector, many of the technology providers have prioritised time to market and 
development costs over cybersecurity, which results in more vulnerability.
The jammer detector (DoSSensing) tool from Worldsensing has been developed 
to respond to one of the most critical and impactful threats in critical infrastructures’ 
wireless networks, the denial of service, which is commonly seen in the form of 
jamming at the lowest level, the physical layer. Such activity is illegal, as reminded 
by the US Federal Communications Commission in 2015 [ 24].
5.1  SME Security Business Considerations
In 2019, security breaches had increased by 11% since 2018 and 67% since 2014, 
according to Accenture [ 25]. IoT devices only experience an average of 5,200 
attacks per month [ 22]. Cyberattacks are growing in both frequency and severity, 
particularly against small businesses . Industry experts estimate that cybersecurity 
incidents will cost businesses over $5 trillion within the next five years alone. On 
the other hand, worldwide spending on cybersecurity is forecasted to reach $133.7 
billion in 2022. In the past year alone, 47% of small businesses experienced a cyber -
attack and, out of those, 44% experienced more than one, but only 14% are prepared C. J. Gillan and D. Guilhot199
to defend themselves, according to Accenture [ 25]. Another research suggests that 
up to 70% of attacks are aimed at small and medium enterprises (SMEs). More than 
half of all small businesses suffered a breach within the last year.
These incidents cost US businesses of all size, an average of $200,000 according 
to the insurance carrier [ 27]. This is a setback that many small businesses cannot 
recover from. Meanwhile, the average UK small business successfully affected by a 
cyberattack suffered a cost of over £6,000 in 2019.
5.2  State of the Art
Wireless solutions are being extensively deployed both by big corporations and 
SMEs for several purposes, leveraging on the facility to install them. Security- 
related solutions like video-surveillance cameras, biometric access control solu -
tions, digital certificate card readers, telephones and pagers heavily rely on the 
wireless channel to be robust enough to ensure their operation. In the case of Edge 
or Fog computing, the wireless communication channel is even more critical than 
for Cloud computing.
Jammer detector devices can identify if the wireless network is affected by 
denial-of-service, whether unintentional or malignantly provoked by a jammer. 
Famous cases of jamming are that of the Newark airport, where lorry driver used a 
jammer to stop his employer tracking his every move, and that of the Korean border, 
where South Korea accuses North Korea for repeated GPS jamming since 2010. 
Such attacks cause severe losses for the affected companies, whether directly tar -
geted or not.
The DDoS protection and mitigation market size is expected to grow from USD 
1.94 billion in 2018 to USD 4.10 billion by 2023, at a Compound Annual Growth 
Rate (CAGR) of 16.1%. North America is expected to have the largest market size 
in the DDoS protection and mitigation market, due to the growing trends such as 
Internet of Things (IoT), Internet of Everything (IoE), Industrial Internet of Things 
(IIoT), and Bring Your Own Device (BYOD) that have brought tremendous growth 
in the region. Asia Pacific (APAC) is expected to be the fastest-growing region dur -
ing the forecast period, mainly because the SMEs established there are growing at 
an exponential rate and they are increasingly adopting DDoS protection and mitiga -
tion solutions and services. Furthermore, the market in regions such as Middle East 
and Africa (MEA) and Latin America is expected to grow, due to the increasing 
technological proliferation across manufacturing, and energy and utilities industry 
verticals. Major vendors in the DDoS protection and mitigation market include 
Arbor Network (US); Akamai Technologies (US); F5 Networks (US); Imperva 
(US); Radware (Israel); Corero Network Security (US); Neustar (US); Cloudflare 
(US); Nexusguard (Hong Kong); A10 Networks, (US); Fortinet (US); Huawei 
Technologies (China); Verisign (US); Zenedge (US), Sucuri (US); SiteLock (US); 
Flowmon Networks (Czech Republic); StackPath, (US); DOSarrest Internet Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge200
Security (Canada); Century Link (US) ; NSFOCUS (US); Corsa Technology 
(Canada); Rackspace (US); Allot (Israel); and Seceon(US).
Jammer detectors are commercially available, although most commercial sys -
tems address the GPS frequencies, since it is the band in which most attacks have 
been detected so far, in situations such as car thefts or when lorry drivers want to 
escape driving time regulations. Other systems address different use cases and fre -
quency bands such as Wi-Fi and mobile phone communications. For instance, PDA 
Electronics and Suresafe manufacture devices for GPS and Wi-Fi Jammer detec -
tion. Both the GAARDIAN [ 27] and SENTINEL projects dealt with jamming 
attacks, in which Chronos [ 28] developed GPS jamming and interference detection 
technologies. More than 100 jamming events were recorded in July 2013 and 150 in 
July 2014 only in one location surrounding the London Stock Exchange [ 29]. 
Commercial solutions for 3G and 4G jammer detection have also been reported by 
Sekotech [ 30], whilst PKI reported on radio spectrum analysis [ 31].
5.3  WDOS Application
DoSSensing is a stand-alone solution that monitors a wide part of the wireless spec -
trum to detect denial-of-service anomalies, whether malign (attack) or accidental 
(interference), thus blocking the communication and rendering the wireless devices 
temporarily affected. The approach does not require the alerting device to be inte -
grated in the wireless network, making it easy to deploy as a solution for the most 
heterogeneous and challenging critical infrastructure wireless environments. The 
radio frequency spectrum is first scanned, and a detailed analysis performed so the 
acquired data is processed for potential anomalies. When an anomaly is detected, 
the system can send warning messages or raise an alert, as illustrated in Fig.  8.
The latest embodiment of the DoSSensing solution comprises a sensor with an 
antenna connected to an SDR module. It digitalises the radio spectrum and produces 
a binary stream, transmitted to a processing board. The incoming data is processed 
by the board. Specific filters and algorithms are applied to match the signals against 
four of the main common types of jamming signals: pulsed, wideband, continuous 
wave (CW) and linear frequency modulation (LFM) chirp. If the result of the 
Fig. 8  Jammer interference detection signalC. J. Gillan and D. Guilhot201
matching is positive, the presence of a jamming signal is communicated to the mon -
itoring server. Alerts can be emitted, and the visualization tool will periodically 
display the events detected from the monitoring server to present in real time the 
results of the detection of jamming signals.
Figure 9 presents the main architecture of the solution when it is deployed on an 
edge or fog environment:
In this architecture, running the solution on a powerful edge device such as the 
XGene provides different advantages including the possibility to reduce the form 
factor and cost of the sensor, as well as the amount of data transmitted to the cloud, 
since the main processing of high amounts of radio frequency spectrum data can be 
performed on the edge. In this case, several instances of the processing application 
can be executed at once on the edge device by connecting various sensors to the 
same processing board. Only the storage and visualisation of the detection results 
requires the transfer of data to the cloud, improving the latency of the device.
The visualization tool associated to this device is illustrated in Fig.  10. This 
image displays the detection of pulsed jammer attacks only. It includes a real-time 
JNR (jammer to noise ratio) viewer, a frequency alert widget, an attack counter and 
a speedometer, which displays in real time the jammer detection speed (time 
between subsequent positive detections) calculated by the solution.
Table 7 summarises the main basic requirements of the DoSSensing solution:
The wireless network monitoring is one of the most demanding application of a 
jammer detector. It requires monitoring to be performed at least 99% of the time. 
Other use cases (such as smart construction monitoring) are not so critical, 
availability- wise, since the data is less critical, and the data usage is much less, 
somewhere between a few bytes per hour. In commercial use cases related to the 
public, such as personal communications in shopping malls or train stations, the 
availability requirement is even lower since the access is mainly required for recre -
ational activities, which are considered non-critical.
The latency is represented by the detection time and detection speed is always 
dependent on the bandwidth processed for monitoring. A regular processing board 
results in an event detection latency of approximately 100 ms with a 5 MHz band. 
The time for processing of the whole Wi-Fi band (2.4–2.5 GHz) can be extrapolated 
and a quick estimation of 2 s is obtained. The more powerful the processing board, 
the lower the latency, so a solution like the XGene is ideal. It is worth noting that in 
most cases a more powerful board results in a more expensive solution.
Accuracy can be calculated through simulation, although the noise experienced 
in non-controlled environment must be considered. In our experiments, the false 
Fig. 9  Jammer detector architecture for the Edge embodimentOvercoming Wi-Fi Jamming and Other Security Challenges at the Edge202
Fig. 10  Jammer detector visualisation tool
Table 7  Main basic requirements of the DoSSensing solution
Requirement Description
Availability High: 99%. Moderate: 75%. Low: 50%
Latency (5 MHz band) Low: 100 ms per decision
Accuracy 97%; 3 undetected SDCs in 100 decisions
Data rate Max: 305 Mbps. Min: 30.5 Mbps
positive and true negative rates can represent as much as 3% of all cases, giving a 
best-case scenario of an accuracy specification of 97%.
The data rate between the SDR module and the processing board can be as high 
as 305 Mbps (5 Msps) and the lower rate is 30.5 Mbps (0.5 Msps). The data rate 
influences the results obtained by the device. Higher data rates mean higher granu -
larity, which results in better results. Nevertheless, lower rates can be used success -
fully, depending on the use case, similar to the availability. Only the type of jammer 
detected, its frequency, its power and timestamps are transferred to the monitoring 
server, so the data usage is much lower, approximately 100 bytes of payload per 
packet and minimum 40 packets per second (one packet per decision per algorithm).
Porting of the solution to the XGene2 host platform required deterministic and 
repeatable outputs to compare and validate SDCs (Silent Data Corruption) among 
different runs. To this aim, real-life jammer signals were recorded and used as inputs 
for the application tests, for increased repeatability.
With the current solution, and for detection across the Wi-Fi band (2.4 GHz), an 
area of approximately 25 m2 can be monitored in the absence of obstacles; there -
fore, in most real-life situations, various devices will have to be used together in 
order to provide an efficient solution that covers the whole area of interest. Several 
physical sensors can be deployed on each edge server, and multiple instances of the 
software need to run on the same processing board. This represents a stress for the 
hardware, especially in terms of processing and/or memory requirements.C. J. Gillan and D. Guilhot203
Fig. 11  TCO results for 
Edge and Cloud 
deployments
5.4  TCO Introduction
As discussed in the previous section, the deployment of the jammer detector on the 
state-of-the-art 64bit ARMv8-based microservers XGene board presents several 
advantages. During the Uniserver project, a Total Cost of Ownership (TCO) model 
was built to estimate the gains of running the application at the edge [ 32]. Only the 
conclusions are reported here; for more details regarding the procedure and method -
ology, please refer to the original paper.
As Fig.  11 shows, running the application on the Edge shows obvious advantages 
regarding the TCO, compared to running it on the Cloud, which increases the TCO 
almost 8 times. Factors for this result include the extra cooling costs, power capex 
costs and the deference in the power usage effectiveness. Additionally, it was dem -
onstrated by making Edge more power efficient, 70% gains in the TCO can be 
reached over the area coverage of the sensors.
6  Physical Attacks on Edge Deployments
If an attacker manages to join the Wi-Fi network and access an edge system, they 
gain an enhanced ability to tamper with the system. There are many attacks, still 
mainly at the physical layer, referenced in the literature. In this part of the chapter, 
we present an overview of attacks, providing examples for the most relevant and 
practical attacks, along with examples of suggested countermeasures to those 
attacks. In general, the type of attacks with which we are concerned here are classi -
fied as side-channel attack. This means that rather than exploit software bugs, such 
as buffer overflow or weaknesses in algorithms, the attack gathers information from 
the implementation of the system.Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge204
6.1  Memory Attacks
High-performance, processor-based, systems will generally include the following 
types of memory: L1/L2/L3 cache, DRAM, Flash Firmware and Hard-Disk Drives. 
Each of these is a potential threat vector for an attacker.
6.2  Timing Attacks
Timing attacks exploit the differences in time required to perform specific opera -
tions. For example, the time required to calculate division and multiplication 
instructions, or the time necessary to fetch data when a cache hit, or cache miss, is 
experienced. Similarly, the difference in timings when conditional branching oper -
ates, or when optimisations are used to skip unnecessary operations, may improve 
application performance but at the same time can reveal sensitive information about 
underlying code and values being processed. Kocher [ 33] presented a classic exam -
ple, where the timings for modular multiply operations in exponentiation opera -
tions, and modulo reductions of the Chinese Remainder Theorem (CRT) optimisation 
in RSA, could lead to the discovery of the entire encryption key on a PC.
An example of a remote network-based attack is that of Bernstein in [ 34], dem -
onstrating a timing attack on OpenSSL AES, on a UNIX x86 server. The server was 
profiled using a known key to determine the timing characteristics for the input 
plaintext values. During the attack, plaintexts were sent to the server, with their tim -
ing profiles compared to the profiled reference. The information leakage was 
reported to be due to the non-constant timing of table lookups.
Cache-timing attacks were first proposed by Page in [ 35]. Tsunoo et al. in [ 36] 
demonstrated that DES could be defeated with greater than 90% success rate. 
Tromer et al. [ 37] presented results showing that the full AES key could be extracted 
using DM-CRYPT disk encryption on Linux with only 800 accesses to an encrypted 
file. A significant countermeasure to timing attacks is to try to perform operations in 
constant time. Compiler optimisation, if enabled, can significantly frustrate such 
efforts however. Recent research by Tan et al. [ 38] addressed the topic of dead and 
redundant operation elimination showing that there is still significant research 
required in this topic.
Cache hits and variances in instruction timings are generally outside the control 
of the software engineer. Kocher [ 40] proposed a clock-skipping countermeasure 
that works by inserting random delays to try to break up characteristic timing pat -
terns. Unfortunately, this was later shown to be equivalent to adding noise to the 
power waveforms and could be overcome by analysis with a larger number of traces. 
Tromer et al. [ 37] considered various other countermeasures against cache attacks, 
suggesting the following four cases:C. J. Gillan and D. Guilhot205
 1. Avoid the use of memory accesses by replacing lookups with equivalent logical 
operations. This is a possibility for algorithms such as AES. However, there will 
be a performance trade-off.
 2. Use of a cache no-fill mode, where memory is accessed from the cache during a 
hit, and serviced from memory when there is a cache miss.
 3. Dynamic table storage, where the contents of the table lookup are cycled around 
in memory during encryption operations to de-correlate it.
 4. Use of a bit-slicing approach.
Guidance for coding standards for cryptographic implementations in software 
include the following recommendations
 1. Do not compare secret values on a byte-by-byte basis.
 2. Avoid branching predicated on secret data.
 3. Avoid the use of lookup tables indexed by secret data.
 4. Avoid loops that are bounded by a secret value.
The software developer can also make use of libraries, written with security in 
mind, such as NaCl [ 40] and some processors also include custom instruction sets 
dedicated to cryptography, such as the Intel AES-NI instructions and the ARM cryp -
tography extensions discussed in ARMv8 [ 41].
6.3  DRAM Attacks
Buffer overflow is a well-known type of attack that can open the opportunity to 
execute malicious code. Strategies to counteract this attack include the use of 
improved input validation and bounds checking at the programmer level, or at the 
system level through approaches such as the randomisation of memory layout or the 
structuring of buffer memory to incorporate memory spaces, sometimes termed 
‘canaries’, that actively monitor to detect when unauthorised overflows occur.
The purposeful use of errors, exceptions and crashes can also be used to initiate 
memory dumping, where the entire contents of system memory are exported to 
enable readout of sensitive values stored in memory. It is recommended that sensi -
tive values should not be stored in memory in the clear, rather they should be stored 
in encrypted form, or represented as hashed values and compared against re- 
computed hashes when required.
With direct physical access to a system, such as with an exposed and isolated 
edge server, an attacker can potentially remove DIMM memory modules from the 
system board. The work of Quisquater et al. [ 42] showed that using cooling sprays 
can enable a DIMM memory module to retain memory, without error, for several 
minutes. The memory can then be plugged into another system and sensitive infor -
mation read out. This attack has been shown to make on-the-fly software-based disk 
encryption systems such as BitLocker, FileVault and TrueCrypt vulnerable. One 
countermeasure is to avoid the use of pre-computed tables of information for Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge206
encryption routines, data which would typically be stored in DRAM, although this 
will have performance penalties associated with it since the values will need to be 
computed on demand each time.
RowHammer [ 43] is a more recent memory attack that exploits a weakness iden -
tified in commodity DRAMs, where repeated row activations can cause bits to flip 
in adjacent rows. One instance of this used generic memory functions such as libc, 
memset and memcpy for attack primitives, making the attack more readily acces -
sible since these functions are readily available on any Unix- or Linux-based operat -
ing system.
6.4  Attacking Encryption keys
A particular sub-class of attack targets leakage of information from a system and is 
primarily concerned with the discovery of the secret information such as encryption 
keys that underpins modern cryptographic processing. This is the original restrictive 
definition of the term side-channel attack. The same approach can be targeted at 
modelled leakages of any other high-value information that is processed in a system.
6.4.1  Power Analysis Attacks
Power analysis is a powerful technique used to obtain side-channel information 
from a system. The power analysis attack can be categorised into two types: simple 
power analysis and differential power analysis.
In simple power analysis, the attacker records the individual power waveforms. 
Analysis of these acquisitions seek to extract information from them. Kocher’s work 
[39] observed that a single power consumption trace is sufficient to reveal the entire 
encryption key by simply interpreting the pattern of the power trace, since modular 
multiply operations in exponentiation operations took varying times depending on 
whether the portion of the encryption key was a ‘1’ or a ‘0’.
Countermeasures against power analysis attacks aim to break the statistical link 
between the power consumption and the sensitive intermediate data values. Defences 
against simple power analysis, countermeasures focus on disturbing the power 
waveform to disrupt the observable pattern, and so remove the discernible informa -
tion. Suitable approaches to achieve this include increasing background noise sig -
nals, introducing random insertions or delays, or removing conditional branching 
and employing constant time algorithms.
Using differential power analysis (DPA), a series of power consumption mea -
surements are recorded whilst the device is processing the target information, typi -
cally a secret encryption key, and is then compared against a set of hypothesised 
power models to determine a portion of the key. The analysis is repeated for the 
remainder of the key portions until the complete encryption key is recovered, 
enabling the attacker to decrypt any data, previously encrypted with the same key. C. J. Gillan and D. Guilhot207
Power consumption is typically modelled by estimating the number of 1s in a regis -
ter via a Hamming weight or Hamming distance power model. Several differing 
methods of statistically comparing the modelled versus measured power consump -
tions are commonly used, such as difference of means, distance of means and 
Pearson's correlation coefficient [ 44].
Power analysis attacks are device-specific and it can take any number from sev -
eral hundred, to several million, traces to break an implementation with a DPA 
attack; this is dependent on the signal/noise (S/N) ratio and whether any counter -
measures are present. Research evaluated a multitude of low-frequency embedded 
systems, where the approach has proved very successful. The attack works best 
when a clean voltage signal is available, preferably from the processor core of the 
device, where S/N is typically optimal; however, attacks can also be mounted by 
measuring the global power supply of a device through the voltage drop across a 
small resistor placed between supply and ground.
Relatively fewer published works address the problem of attacks on full-scale 
server boards. This is due in part to the additional complexities introduced by higher 
frequencies of operation, lack of access to processor core voltage, and the additional 
noise generated by numerous system hardware elements. In recent work, a new 
style of attack known as a Power Leakage Attacks: Targeting Your Protected User 
Secret (PLATYPUS) attack, has been developed by Lipp and co-authors [ 45] to 
extract information from the Intel running average power limit (RAPL) interface. 
These authors reported that by observing changes in power consumption with a 
resolution of up to 20 kHz, they could distinguish different executed instructions 
and features of their operands.
Protecting a device from DPA is a very challenging task, since this attack uses 
advanced statistical techniques to extract information from many traces. It is gener -
ally accepted that countermeasures fall into two broad categories, namely, whether 
they aim to hide or mask the data. Hiding approaches try to change the power wave -
form by applying some randomisation or by making it constant. Randomising coun -
termeasures include such approaches as shuffling or skipping of instruction clocks. 
To make the power consumption constant, approaches have been proposed such as 
the use of dual-rail pre-charge (DRP) logic styles, which uses two wires that are 
complementary for each signal. Other logic styles such as Sense Amplifier Balanced 
Logic (SABL) was proposed by Tiri et  al.  [46] to provide resistance against 
DPA. However, these approaches require custom ASIC design with careful layout 
considerations.
The masking countermeasure aims to change the sensitive intermediate values 
by applying and then removing a temporary mask operation. A simple example is an 
XOR with a random value. This then breaks the link between what the power model 
expects and what is processed inside the device. The disadvantage of masking is that 
it can require the application and removal of multiple masks, for example, switching 
between Boolean and multiplicative masks. This has a processing overhead and can 
be complicated to design and implement.Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge208
6.4.2  Electromagnetic Attacks
Electromagnetic (EM) attacks [ 47] are a variation of power analysis attacks. They 
differ in the method of acquisition, which uses an electric or magnetic field probe to 
convert EM radiation surrounding a computer system or component into voltage 
signals that are proportional to the power consumption. The probing is generally 
classed as being either near-field or far-field. Near-field probing is considered to be 
the short-range distance that is typically less than one wavelength from the source. 
At this distance, the field strength is proportional to 1/r3 in strength; therefore, plac -
ing the probe as close as possible to the source will maximise signal strength. A 
more invasive attack can be to remove the chip package surface and enable a fine 
point-tip probe to be placed very close to the exposed integrated circuit (IC); how -
ever, this requires more time and generally a laboratory environment. A less inva -
sive approach is to rest a simple loop antenna or EM probe tip against the surface of 
the IC, and to use active amplification to improve signal strength for appropriate 
quantisation scaling during acquisition.
Far-field EM attacks work at multiple wavelength distances and typically use a 
high-frequency directional antenna to receive signals. The waveforms being cap -
tured here have escaped the confines of the near field and are propagating over free 
space. This form of attack would likely only be possible for exposed, non-shielded 
enclosures.
An EM acquisition can have advantages over that of traditional power analysis 
attacks. Firstly, it can have a lower invasiveness. In comparison to a power analysis 
attack, where a resistor may need to be soldered into place, the EM probe can often 
be placed in close proximity, without any evidence of tampering. Secondly, there is 
the possibility to improve the localisation of the probe, that is, to position it directly 
around the circuitry processing the sensitive information. This can help reduce the 
contributions of the EM fields generated from other elements of the overall power 
consumption. This can improve the S/N ratio, making it easier to visually identify 
leakages using an oscilloscope and improve the statistical analysis.
The countermeasures of hiding and masking, discussed above, also provide gen -
eral protection against both EM analysis. However, for non-invasive attacks with an 
EM probe, physical shielding countermeasures can offer some further resistance.
7  Conclusions
The move from the cloud deployment model to the edge has implications for secu -
rity. In contrast to a cloud data centre, housed within a large building complex with 
a significant level security, the edge deployment will constitute a large number of 
small clusters or individual installations, where high levels of physical security are 
not economically viable. In many situations, physical security of the microserver 
may consist primarily of a lightweight enclosure, designed to protect the system 
from environmental factors and vandalism or casual tampering efforts. For the C. J. Gillan and D. Guilhot209
determined attacker, this may not prove to be an effective barrier, and it should be 
assumed that a realistic worst-case scenario is that an attacker will be able to gain 
full access to the system. This then creates a larger threat surface, now incorporating 
physical attacks that can be used to compromise the individual microserver, and 
potentially, the wider network.
Deployment at the edge still requires the implementation of traditional server 
and network security practices. In addition, deployment at the edge should assume 
that networks are operating over untrustworthy links and, therefore, the use of 
encrypted tunnelling through VPNs, and the use of malware detection, firewalls, 
intrusion detection/prevention systems and DNSSEC should all be considered as 
forming the basis of an endpoint security policy.
The use of virtualisation is a core element of cloud and resource-sharing tech -
nologies; however, it also opens the possibility for attacks exploiting VM Escape. 
Accommodating guests with differing security levels, such as DMZ and internal, on 
the same host, should be avoided.
References
 1. I. Stojmenovic, S. Wen, The fog computing paradigm: Scenarios and security issues, in 2014 
Federated Conference on Computer Science and Information Systems (FedCSIS)  (IEEE, 
2014), pp. 1−8
 2. A. Alrawais, A. Alhothaily, C. Hu, X. Cheng, Fog computing for the internet of things: Security 
and privacy issues. IEEE Internet Comput. 21(2), 34–42 (2017)
 3. M. Mukherjee, R. Matam, L. Shu, L. Maglaras, M.A. Ferrag, N. Choudhury, et al., Security 
and privacy in fog computing: Challenges. IEEE Access 5, 19293–19304 (2017)
 4. International Telecommunications Union Recommendation ITU-R V .431-8, Nomenclature of 
the frequency and wavelength bands used in telecommunications  (2015) Accessible on the 
web at https://www.itu.int/dms_pubrec/itu-r/rec/v/R-REC-V .431-8-201508-I!!PDF-E.pdf
 5. D.L.  Sengupta, V .V .  Liepa, Applied electromagnetics and electromagnetic compatibility  
(Wiley, Hoboken, 2005), p. 467. ISBN 0-471-16549-2
 6. Flexera State of the Cloud 2020 Report, available on the web at: https://info.flexera.com/
SLO-  CM-REPORT-State-of-the-Cloud-2020
 7. Gillan, C.J. Fusco, V .F. Optimizing FDTD electromagnetic field computation on distributed 
networks”, Int. J.  Numer. Modell. Electron. Networks Devices Fields, 11, 6, pp. 277-287. 
https://doi.org/10.1002/(SICI)1099-1204(199811/12)11:6<277::AID- JNM312>3.0.CO;2-O.
 8. UK OFCOM, Frequency bands designated for Industrial, Scientific and Medical use (ISM), 
available on the web at: https://www.ofcom.org.uk/__data/assets/pdf_file/0022/103297/fat-
ism-frequencies.pdf .
 9. S. Weinstein, P. Ebert, Data transmission by frequency-division multiplexing using the dis -
crete fourier transform. IEEE Trans. Commun. Technol. 19(5), 628–634 (1971). https://doi.
org/10.1109/TCOM.1971.1090705
 10. ITU-R WRS-12 Terrestrial Workshop, APPENDIX 1 (REV .WRC-12) Classification of emis -
sions and necessary bandwidths, available on the web at: https://www.itu.int/en/ITU-  R/ter -
restrial/workshops/wrs12/Miscellaneous/Appendix1.pdf .
 11. R. Bhojani, R. Joshi, An integrated approach for jammer detection using software defined radio, 
in 7th International Conference On Communication, Computing And Virtualization, Procedia 
Computer Science,  vol. 79, (2016), pp. 809 – 816. https://doi.org/10.1016/j.procs.2016.03.113 .Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge210
 12. J.G. Proakis, D.K. Manolakis “Digital Signal Processing”, Pearson New International Edition, 
4/E, Pearson, ISBN-10: 1292025735, ISBN-13: 9781292025735.
 13. G. Romero, V . Deniau, E.P. Simon, Mitigation Technique to Reduce the Wi-Fi Susceptibility to 
Jamming Signals, in 2018 2nd URSI Atlantic Radio Science Meeting (AT-RASC) , (Meloneras, 
2018), pp. 1–3. https://doi.org/10.23919/URSI-AT-RASC.2018.8471439
 14. N. Sufyan, N.A. Saqib, M. Zia, Detection of jamming attacks in 802.11b wireless networks. J 
Wireless Com Network 2013  208, 2013 . https://doi.org/10.1186/1687-1499-2013-208
 15. A. Grant, P. Williams, N. Ward, S. Basker, GPS jamming and the impact on maritime naviga -
tion. J. Navig.  62(2), 173–187 (2009)
 16. K. Pelechrinis, M. Iliofotou, S.V . Krishnamurthy, Denial of service attacks in wireless net -
works: the case of jammers. IEEE Commun. Surv. Tutorials  13(3), 245–257 (2011)
 17. K. Pelechrinis, C. Koufogiannakis, S.V . Krishnamurthy, On the efficacy of frequency hop -
ping in coping with jamming attacks in 802.11 networks. IEEE Trans. Wirel. Commun.  9(10), 
3258–3271 (2010)
 18. H. Huang, N. Ahmed, P. Karthik, On a new type of denial of service attack in wireless net -
works: the distributed jammer network. IEEE Trans. Wirel. Commun.  10(7), 2316–2324 
(2011). https://doi.org/10.1109/TWC.2011.052311.101613
 19. B. Otis, Y .H. Chee, R. Lu, N.M. Pletcher, J. M. Rabaey An ultralow power MEMS-based two-
channel transceiver for wireless sensor networks. Proc. IEEE Symp. VLSI Circuits  (2004)
 20. E. Altman, K. Avrachenkov, A. Garnaev, Jamming in wireless networks: The case of several 
jammers, in 2009 International Conference on Game Theory for Network  (Istanbul, 2009), pp. 
585−592, https://doi.org/10.1109/GAMENETS.2009.5137448
 21. W. Xu, W. Trappe, Y . Zhang, T. Wood, The feasibility of launching and detecting jamming 
attacks in wireless networks, in Proceedings of the 6th ACM International Symposium on 
Mobile ad hoc Networking and Computing (MobiHoc 2005) , (New York, 2005), pp. 46–57
 22. K. Grover, A. Lim, Q. Yang, Jamming and anti-jamming techniques in wireless networks: a sur -
vey. Int. J. Ad Hoc Ubiquitous Comput. (2014). https://doi.org/10.1504/IJAHUC.2014.066419
 23. R. Maheswari, S. Rajeswari, A review on types of jamming attack in mobile ad-hoc network, 
in Proceedings of the UGC Sponsored National Conference on Advanced Networking and 
Applications , (2015)
 24. FCC public notice 202/418-0500, January 27, 2015 Enforcement Advisory No. 2015-01
 25. Available on the web at https://www.accenture.com/us-en/insights/security/
cost-cybercrime-  study.
 26. Available on the web at https://www.forbes.com/sites/marcwebertobias/2015/01/29/this- pop -
ular-wireless-alarm-system-can-be-hacked-with-a-magnet-and-scotch-tape/#6b466ec450de
 27. J.R. Tong, R.J. Watson, C.N.V . Mitchell, Singel-receiver sentinel to interference GPS. World 
22(7), 38–40 (2011)
 28. C. Curry, K. Czaplewski, C. Chue, A. Weintrit, The use of the eLoran system for transmission 
of the national time signal, in Marine Navigation , (Taylor and Francis Group, London, 2017), 
pp. 15–26. https://doi.org/10.1201/9781315099132 . ISBN 978 -131- 5099132
 29. Curry C, Detecting GPS Jammers “Gone in 20 Seconds” Update on UK Research into the 
GPS Jamming Threat, CGSIC Annual Conference 8th September 2014. Available on the web 
at https://www.gps.gov/cgsic/meetings/2014/curry.pdf
 30. Available on the web at http://www.sekotech.com/gsm-3g-4g-jammer-detector/st-171-gsm-
3g- 4g-gps-jammer-detector.html
 31. Available on the web at http://www.pki-electronic.com/products/jamming-systems/jammer-  
detector/  (SEKOTECH, PKI)
 32. P.  Nikolaou, Y .  Sazeides, A.  Lampropoulos, D.  Guilhot, A.  Bartoli, G.  Papadimitriou, 
A. Chatzidimitriou, D. Gizopoulos, K. Tovletoglou, L. Mukhanov, G. Karakonstantis, On the 
evaluation of the total-cost-of-ownership trade-offs in edge vs cloud deployments: a wireless- 
denial-of-service case study. IEEE Trans. Sustain. Comput. (2019) https://doi.org/10.1109/
TSUSC.2019.2894018 .C. J. Gillan and D. Guilhot211
 33. P.  Kocher, Timing attacks on implementations of Diffie-Hellman, RSA, DSS, and other 
Systems, in Advances in Cryptology (CRYPTO ’96). Lecture Notes in Computer Science , vol. 
1109, (Springer, 1996), pp. 104–113
 34. D. Bernstein, Cache-timing attacks on AES, (2005)
 35. D. Page, Theoretical use of cache memory as a cryptanalytic side-channel. IACR Cryptology 
ePrint Archive (2002)
 36. Y . Tsunoo, T. Saito, T. Suzaki, M. Shigeri, H. Miyauci, Cryptanalysis of DES implemented 
on computers with cache, in Cryptographic Hardware and Embedded Systems-CHES 
2003 , (2003)
 37. E.  Tromer, D.  Osvik, A.  Shamir, Efficient cache attacks on AES, and countermeasures. 
J. Cryptol 23(1), 37–71 (2010)
 38. J Tan, S Jiao, M Chabbi, X Liu, What every scientific programmer should know about 
compiler optimizations?, in ICS ‘20: Proceedings of the 34th ACM International 
Conference on Supercomputing, Article No. 42, Pages 1–12 (2020, June), https://doi.
org/10.1145/3392717.3392754
 39. P. Kocher, J. Jaffe, B. Jun, Differential power analysis method and apparatus, in U.S. Patent 
7,587,044, (2009)
 40. NaCl: Networking and Cryptography Library. Available from: https://nacl.cy.yp.to
 41. ARM: ARMv8 Instruction Set Overview 2011. Available from: https: //www.element14.com/
community.
 42. J.J. Quisquater, D. Samyde, ElectroMagnetic Analysis (EMA): Measures and counter- mea -
sures for smart cards,” in Smart Card Programming and Security (E-smart 2001). Lecture 
Notes in Computer Science , V ol. 2140, pp. 200−210, (2001)
 43. R. Qiao, M. Seaborn, A new approach for rowhammer attacks,” in Hardware Oriented Security 
and Trust (HOST), 2016 IEEE International Symposium on Hardware Oriented Security , pp. 
161−166, (2016).
 44. O. Mutlu, J.S. Kim, RowHammer: A retrospective. IEEE Trans. Comput. Aided Des. Integr. 
Circuits Syst. 39, 1555–1571 (2020)
 45. M. Lipp, A. Kogler, D. Oswald, M. Schwarz, C. Easdon, C. Canella, D. Gruss, With great 
power comes great leakage: software-based power side-channel attacks on x86 (2020), avail -
able on the web at: https://scholar.google.com/citations?user=gd0o3ngAAAAJ&hl=en
 46. K. Tiri, M. Akmal, I. Verbauwhede, A dynamic and differential CMOS logic with signal inde -
pendent power consumption to withstand differential power analysis on smart cards, in Solid- 
State Circuits Conference (ESSCIRC 2002). Proceedings of the 28th European , (2002)
 47. D. Agrawal, B. Archambeault, J.R. Rao, P. Roha, The EM Side-Channel(s), in Cryptographic 
Hardware and Embedded Systems (CHES). Lecture Notes in Computer Science , vol. 2523, 
(2002), pp. 29–45Overcoming Wi-Fi Jamming and Other Security Challenges at the Edge213A
Adaptive clocking, 17, 63
Advanced Configuration Power Interface 
(ACPI) power management, 26
Aggressive CMOS technology scaling, 91
Aggressive voltage underscaling, 17
Amplitude modulation (AM), 167, 186
Anti-cell trigger errors, 124
Applied Micro’s (APM) X-Gene 2 micro- 
server, 39, 52
ARM-based Edge server, 124
ARMv8-compliant multicore CPUs, 44, 
53, 60, 64
Automated characterization
characterization process, 20
execution phase, 22–23
framework, 20
initialization phase, 20–22
parsing phase, 23
Automated framework, 20
B
Beacon frames, 175–177, 197, 198
Big Data-driven innovation, 1
Bitlines, 124
Blackscholes, 139
Buffer overflow, 205, 203
C
Cache coloring, 29
Cache-timing attacks, 204
Caching and analytics workloads, 129Canadian Personal Information Protection, 
165, 184
Chinese Remainder Theorem (CRT), 204
Chip fabrication, 6, 17, 51
Chip-to-chip variation, 44, 60
Circuit-based techniques, 63
Clients Degree of Satisfaction, 157
Cloud
cloudlet concept, 2
edge computing, 3
Internet/cloud-based services, 3
MCC, 2
operational changes, 6
operational parameters, 6
operation at the edge, 3
purpose-built cloud, 10
traditional cloud architecture, 1
Cloudlets/MD, 143
Cloud operation, 165, 184
Collaborative Processor Performance Control 
(CPPC), 67, 68
Conservative guard bands, 14, 17, 51
Core-to-core variation, 44, 60
Correctable errors (EC), 130, 131
CPU voltage margins in cloud 
infrastructures, 106–117
experimental evaluation, 115–117
hardware characterization
experimental characterization of 
nodes, 112
failure probability, 113–114
performance sensitivity to frequency 
scaling, 114, 115
power, 114Index
© Springer Nature Switzerland AG 2022
G. Karakonstantis, C. J. Gillan (eds.), Computing at the EDGE , 
https://doi.org/10.1007/978-3-030-74536-3214
CPU voltage margins in cloud 
infrastructures ( cont.)
voltage margins, 112, 113
problem modelling
CPU allocation to VMs, 108
energy cost, 110
failures, 109
nodes and CPUs, 107
overall operating cost, 110
scheduling, 108
SLA violations cost, 109, 110
workloads, VMs, 107
simulation environment, 115
VM scheduling and system 
configuration, 111–112
D
Data link layer, 169, 171, 190, 191
Denial of service attack (DDoS), 165, 172, 
178, 179, 183, 191, 199
Dennard scaling, 6, 7
Devices variation, 51
Diagnostic micro-viruses, 44
Differential power analysis (DPA), 206
DIMM-to-DIMM variation, 127
DoSSensing, 173, 179, 180, 193, 200
DoSSensing solution, 180, 201, 202
DRAM
behaviour, 126, 127
cells, 123
cell-to-cell interference, 128, 126
chips, 123
devices, 123
errors, 127, 128
mapping data to memory physical layout, 
124, 125
operating parameters, 123, 125
organization and background, 123
refresh rate, 134
reliability, 123, 125, 128
temperature, 125
DRAM error behaviour
machine learning model, 129
modelling, 134
power savings, 138, 139
typical edge server, 129, 130
workload-dependent DRAM error 
behaviour, 130–134
workloads dependent, 129
DRAM error metric, 135
Dual in-line memory modules (DIMMs), 123Dual-rail pre-charge (DRP) logic styles, 207
DVFS-capable system, 52
Dynamic power consumption, 14
Dynamic voltage and frequency scaling 
(DVFS), 18, 51, 52, 64, 77
DynamoRio, 134
E
ECC-corrected errors, 60
ECC hardware, 63, 93
Edge computing, 2
advantage, 3
applications, 143
deployments, 144
electricity, 144
end-to-end TCO, 10
implementation, 2
IoT applications, 143
limitations, 144
MCC, 2
MEC, 2
mobile devices, 143
multi-access, 2
operation, edge server, 4
sensors, 143
traditional Cloud architectures, 143
WiFi network, 10
Edge deployments, 143, 145, 153
Edge devices, 165, 184
Edge/fog computing, 142, 143, 168, 199
Edge installations, 145
Edge server, 4, 5
Electromagnetic (EM) attacks, 208
Electromagnetic spectrum, 166, 185
Electromagnetic waves, 166, 184, 185
Electronic counter measures (ECMs), 171, 191
Electronic Documents Act, 165, 184
End-to-end TCO, 10
Energy, 13
Energy-delay product (EDP), 60, 80
Energy-delay squared product (ED2P), 
80, 81, 86
Energy efficiency, 3, 4, 6, 7, 9, 51–53, 62, 64, 
65, 73, 77–80, 84, 85, 91–93
Energy-efficient computing, 14, 52
Error detection, 28
Errors detected by hardware (ECC), 127, 128
EV AL (Utilizing Processors with Variation- 
induced Timing Errors), 93
Extended dynamic voltage scaling governor 
(xDVS), 94, 98–103, 105, 106Index215
F
Fast system-level voltage margins 
characterization
APM X-Gene 2 micro-server, 26
characterization procedure, 24
diagnostic micro-viruses ( see 
Micro-viruses)
experimental evaluation, 37–40
libquantum , 25
micro-viruses, 24, 25
PCP, 26
PMD, 26
PMpro, 26
SLIMpro, 26
SPEC-based, 24
SYMPO, 25
Feedback processor, 150
Field-effect transistor (FET), 51
Financial institutions, 163, 183
Fog computing, 2
Frame types, 175, 196
Frequency bands, 166–167, 185
Frequency modulation (FM), 167, 186
Frequency of oscillation, 166, 184
Fully integrated voltage regulator (FIVR), 94
G
General Data Protection Regulation (GDPR), 
165, 184
Gramm-Leach-Bliley Act, 165, 184
H
Harnessing voltage margins, 9
Health Insurance Portability and 
Accountability Act (HIPPA), 
165, 184
HealthLog, 4, 5
High-performance computing (HPC) 
systems, 91
Horizon 2020, 3
HTTP protocol, 163, 183
I
Industrial Scientific and Medical (ISM) bands, 
166, 168, 186, 188, 189
Intel x86-64 Performance Monitoring Units 
(PMUs), 97
Internet, 163, 183
Internet of Things (IoT)
economic impact, 2
and edge computing, 164, 183
end-to-end latency, 3intelligent device, 1
system architecture, 144
velocity, variety and volume, 1
J
Jammer detectors, 178, 179, 198, 200
K
K-nearest neighbours (KNN), 134
L
Latency, 180, 201
Linux kernel, 128
Logic styles, 207
Low-voltage operation, 15
Lulesh benchmarks, 138
M
Machine learning (ML), 139
Mapping engine, 150
MARCH tests, 127
MATS tests, 127
Mean time between failures (MTBF), 
103–105, 117
Media access control (MAC), 169, 170, 174, 
190, 191, 195, 196
Memory array, 123
Memory controller units (MCUs), 129
Microprocessors
energy efficiency, 15
power and energy, 14
reducing energy, 14
Micro-server, 147
DRAM refresh rate, 148
framework, 149
SLIMpro, 149
Micro-viruses, 24, 25
ALU operations, 35–36
caches, 28, 29
in C language, 30
FPU operations, 36
L1 data cache, 30, 31
L1 instruction cache micro-virus, 31, 32
L2 cache micro-virus, 32–34
L3 cache micro-virus, 34–35
memory access patterns, 29
pipeline, 30
pipeline micro-virus, 37
small self-checking pieces of code, 27
vs. SPEC CPU2006 benchmarks, 40
Mist Computing, 2Index216
Mobile Cloud Computing (MCC), 2
Mobile devices, 13, 52, 64, 143, 163, 169, 188
Mobile edge computing (MEC), 2
Model-Specific Registers (MSRs), 94, 96–102
Modulated electromagnetic waves, 170, 191
Mooreʼs law scaling, 7
Morse Code transmission, 167, 186
Multi-access edge computing, 2
Multicore CPUs, 14, 24, 44, 45, 52, 53, 60, 
64, 87, 92
Multicore executions, 64, 65, 69, 73, 87
N
National critical infrastructures, 164, 183
New Trade (NT) message, 152
O
On demand governor, 85, 87
OpenFog consortium, 2
Online monitoring daemon, 65, 80–86
Orthogonal frequency division multiplexing 
(OFDM), 168, 169, 187, 190
Overlapping channels, 168, 188
P
Parsec benchmark suites, 129
Pessimistic design margins, 5
Pessimistic design-time voltage margins, 14, 
16, 24, 25, 39
Pessimistic DRAM operating parameters,  
139
Pessimistic voltage margins, 9, 14
Phase modulation, 169, 190
Physical attacks on edge deployments
DRAM attacks, 205, 206
EM attacks, 208
power analysis, 206, 207
timing attacks, 204, 205
Physically unclonable functions (PUF), 16
Point coordination function attack (PCF), 
177, 198
Polaris application, 154
Polaris feedback file, 150
Polaris platform, 149
accuracy and security, 152
components, 149
feedback, 152
feedback response, 151
lower availability, 151
Mapping Engine, 150
requirements, 151
validator kernel, 150virtualization, 152
Power, 13
Power analysis, 206, 207, 208
Power analysis attacks, 207
Power capping, 52, 64, 142
Power consumption, 14, 51, 52, 54, 64, 83, 85, 
87, 91, 93, 97, 114
Power efficient servers, 142
Power Leakage Attacks: Targeting Your 
Protected User Secret (PLATYPUS) 
attack, 207
Power Management processor (PMpro), 26
Power problems, 13
Power Saving Mode Attack (PSM), 176, 198
Power savings, 15, 44, 60, 78
Processor ComPlex (PCP) power 
domain, 26, 54
Processor MoDule (PMD), 26, 34, 40–44, 
54–56, 60, 67–69, 74, 80–84, 149
Process variations, 5, 17, 44, 51, 60
Program cycles, 134
Propose mitigation techniques, 9
Pulse modulation, 167, 168, 186, 187
Q
QoS time requirement, 144
Quadrature Phase Shift Keying (QPSK), 
169, 190
Quality of service (QoS) VMs, 92
R
Random Decision Forests (RDF), 135
Randomising countermeasures, 207
Raspberry Pi board, 20
Razor processor, 93
Real microprocessor chips, 19
Remote network-based attack, 204
Root mean square error (RMSE), 98, 114
Row buffer, 124
RowHammer, 126, 206
S
Scalable lightweight intelligent management 
processor (SLIMpro), 26, 53
Scrambling process, 128
Scrambling/row remapping, 128
Separate light-weight intelligent processor 
(SLIMpro), 129, 130
Service level agreement (SLA), 92, 106, 107
Severity function, 53, 61–64, 87, 93
Severity metric, 63
Sidebands, 168, 169, 187, 190Index217
Silent data corruptions (SDCs), 20, 52, 53, 55, 
56, 60–63, 69, 128, 132
Single-bit errors, 127
Single-core executions, 64, 87, 98
SocialCRM, 154, 155, 158
application, 156
availability requirements, 156
corresponding values, 156
performance requirement, 156
social networks, 156
Software engineering, 10
challenges, 10
edge computing ( see Edge computing)
State-of-the-art Edge servers, 139
Static guardbanding, 18
Static variation, 17, 51
Supply voltage scaling
comprehensive characterization, 19
deterministic error distribution, 16
EM emanations, 16
error correction mechanisms, 15
error-pattern transformation scheme, 16
existing techniques, 15
failure recognition, 19
guardband results, 18
iterative execution, 19
microarchitectural techniques, 17
microprocessor cores Isolation, 19
MS-ECC design, 15
power consumption, 18
safe data collection, 19
static variation, 17
supply voltage scaling, 17
voltage scaling characterization, 18
Support vector machines (SVMs), 135
System on chip (SoC), 7
System reliability
operation effect at reduced voltage  
margins
in large, scale-out 
deployments, 104–106
on node MTBF, 103–104
validation experiment, 103
T
Telecommunications, 166, 186
Threshold voltage, 6, 17, 44, 51, 60
Timing-error detection and correction, 17
Total cost of ownership (TCO), 10, 203
costs, 145
cumulative distribution, 146
Edge deployment, 147framework, 145
IoT applications, 147
parameters, 145
QoS requirement, 147
QoS response, 146
savings, 154
tool, 146
Traditional cloud architecture, 1
U
Uncorrectable errors (UCs), 132
Undervolting, 14, 32, 40, 41, 44, 52, 53, 56, 
60, 63–64, 87, 99, 100, 102, 109, 
113, 114
UniServer approach, 3
V
Validator kernel, 150
Variable retention time (VRT), 130
Virtual machines (VMs), 138, 144
VM consolidation, 92, 93
V oltage and frequency (V/F) values, 148
V oltage guard bands, server-grade 
ARMv8 CPUs
abnormal behaviors below Vmin, 60–61
characterization process, 55
chips, 55
circuits, 55
in CMOS, 55
microprocessor chips, 56
process variation, 60
regions of operation, 55–59
severity function, 61–62
Vmin experimental results, 56–62
X-Gene 2 chips, 55, 56, 59
V oltage margins for balanced energy and 
performance
analysis, V min impact factors, 73–78
combined energy and performance, 80
DVFS, 51, 52, 64, 77
energy efficiency, 78–79
experimental evaluation, 84
experimental setup, 65–69
generated workload, 84, 85
online monitoring daemon, 65, 80–84
optimal energy efficient, 65
power capping, 52, 64
voltage margins identification, 69–73
workload-dependent ( see Workload- 
dependent voltage margins)
V oltage safety margins, 7, 8Index218
W
Wait cycles, 134
“The web”, 163, 183
WiFi
beacon frames, 175–177, 197, 198
communication, 170, 191
ECMs, 172, 191
electromagnetic waves, 165, 166, 184, 185
802.11 Frame Structure, 174, 195, 196
jamming attacks, 172, 191
jamming signals, 172–174, 192–194
local area networks, 170, 191
physical layer (PHY), 169–170, 190
wireless communication standards, 
170, 191
WiFi channels, 168, 169, 188, 189
WiFi chips, 169, 188
WiFi jamming
DDoS protection, 178, 179, 199
detector devices, 178, 199
DoSSensing, 178, 179, 198, 200
security breaches, 178, 198
visualisation tool, 180, 201, 202
wireless solutions, 178, 199
WiFi network, 10
Wireless networking, 178, 198
Wireless network monitoring, 180, 201
Wireless Telegraphy Act 2006, 166, 185
Wordline, 124
Workload-aware DRAM error 
behaviour model
CEs, 136
circuit parameters, 135
DRAM reliability, 136
guardband T REFP, 136
logarithmically proportional, 137marginal T REFP, 136
ML model, 136, 138
pagerank benchmark, 137
program inherent features, 135
running characterization campaigns, 135
search algorithm, 136
single-bit errors, 136
Workload-dependent DRAM error behaviour
CE, 130, 131
error-prone memory locations, 130
program correlation, 133, 134
UCs, 132
VRT, 130
Workload-dependent voltage margins
experimental evaluation, 100–102
exploitation, 94
hardware platform, 94
modelling and estimation
exploiting dynamic, 95
model, 97, 98
model training, 98
profiling, 97
xDVS ( see Extended dynamic voltage 
scaling governor (xDVS))
at the node level, 96
offline quantification, voltage margins,  
95
X
XGene, 173, 194
X-Gene 2, 26, 129, 149
XGene-based servers, 153, 157
X-Gene 2 microprocessor chip, 26
XGene2 server, 5
X-Gene 3 microprocessor, 53, 64, 74IndexStudies in Computational Intelligence 1072
Aboul Ella Hassanien
Deepak GuptaAnuj Kumar SinghAnkit Garg   Editors
Explainable Edge 
AI: A Futuristic Computing PerspectiveStudies in Computational Intelligence 
V olume 1072 
Series Editor 
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, PolandThe series “Studies in Computational Intelligence” (SCI) publishes new develop-
ments and advances in the various areas of computational intelligence—quickly and 
with a high quality. The intent is to cover the theory, applications, and design methods of computational intelligence, as embedded in the ﬁelds of engineering, computer 
science, physics and life sciences, as well as the methodologies behind them. The series contains monographs, lecture notes and edited volumes in computational intelligence spanning the areas of neural networks, connectionist systems, genetic 
algorithms, evolutionary computation, artiﬁcial intelligence, cellular automata, self-organizing systems, soft computing, fuzzy systems, and hybrid intelligent systems. Of particular value to both the contributors and the readership are the short publica-
tion timeframe and the world-wide distribution, which enable both wide and rapid dissemination of research output. 
This series also publishes Open Access books. A recent example is the book 
Swan, Nivel, Kant, Hedges, Atkinson, Steunebrink: The Road to General Intelligence 
https://link.springer.com/book/10.1007/978-3-031-08020-3 
Indexed by SCOPUS, DBLP, WTI Frankfurt eG, zbMATH, SCImago. 
All books published in the series are submitted for consideration in Web of Science.Aboul Ella Hassanien · Deepak Gupta · 
Anuj Kumar Singh · Ankit Garg 
Editors 
Explainable Edge AI: 
A Futuristic Computing 
PerspectiveEditors 
Aboul Ella Hassanien 
Scientiﬁc Research Group in Egypt (SRGE) 
Cairo University Giza, Egypt 
Anuj Kumar Singh 
School of Computing University of Engineering & Technology (UETR) Roorkee, Uttarakhand, India Deepak Gupta 
Department of Computer Science and Engineering Maharaja Agrasen Institute of Technology New Delhi, Delhi, India 
Ankit Garg 
School of Computing University of Engineering & Technology (UETR) Roorkee, Uttarakhand, India 
ISSN 1860-949X ISSN 1860-9503 (electronic) 
Studies in Computational Intelligence 
ISBN 978-3-031-18291-4 ISBN 978-3-031-18292-1 (eBook) 
https://doi.org/10.1007/978-3-031-18292-1 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2023 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandPreface 
Explainable artiﬁcial intelligence (XAI) derives notions from different ﬁelds 
including cognitive sciences, philosophy, and psychology to yield a range of proce-
dural methodologies that can produce explainable results for the end-users not having 
enough background on artiﬁcial intelligence. The main emphasis has been placed 
toward the progression of XAI that encompasses the human end-user in the cycle 
and, therefore, becomes human-centric. XAI-based intelligent systems will facili-tate enhanced prediction accuracy with comprehensible decision and traceability of 
actions performed and have signiﬁcant impact in the development of the society at large. In the futuristic computing scenario, the goal of explainable edge AI will be to execute the AI tasks and produce explainable results at the edge. The issues 
of transparency, fairness, accountability, explainability, interpretability, data fusion, and comprehensibility that are signiﬁcant for the edge AI are being addressed here through explainable models and techniques. The book has been organized in ten 
chapters highlighting the different technological aspects of explainable edge AI. 
Chapter 1 of the book provides a brief overview of explainable artiﬁcial intel-
ligence (XAI) by covering almost every aspect of XAI. It covers the principles, 
techniques, current state of the art, beneﬁts, and applications of XAI. It can be 
inferred from the chapter that XAI is a great development in AI due to its transparent nature. The chapter also addressed the challenges in XAI, as well as the ﬁeld’s future 
potential. Chapter 
2 of the book serves as a learning platform for academics and 
practitioners who are interested in essential parts of the new and quickly increasing ﬁeld of XAI. The need of trust and transparency in AI and the principal applica-
tion domain of XAI have been primarily discussed in this chapter. Chapter 
3 epito-
mizes contemporary developments in explainable AI that describes explainability in 
machine learning, constituting a ﬁction deﬁnition of explainable machine learning 
that envelopes such prior conceptual propositions with a considerable focus on the audience for which the explainability is needed. 
A wide and insightful view of XAI and its application in various ﬁelds has been 
presented in Chap. 4 of this book. This chapter also includes the future scope of this 
technology and the need for the growth of this type of technology. Chapter 5 presents 
recent challenges on edge AI and its numerous applications. This chapter provides
vvi Preface
a thorough analysis of edge computing-related AI techniques and tools, sometimes 
known as edge AI. The objective is to offer a hypothetical roadmap that may unite 
important players and enablers in order to the development of edge AI. 
Chapter 6 highlights the usage of XAI in healthcare which is a critical application 
for 
the human community. ‘How XAI improves user trust in high-risk decisions’ has 
been elaborated in this chapter. Signiﬁcant AI recommendations, such as surgical treatments or hospitalizations, requiring explanation from providers and patients have 
been discussed here. Chapter 7 focuses on describing the role of explainable edge AI 
to 
resolve real-time problems. Various issues in the applicability of explainable edge 
AI for real-time implementations have been highlighted in this chapter. Chapter 8 
focuses on different data models of fusion, discusses a framework for AI and data 
fusion 
at the edge, and identiﬁes potential challenges and possible solutions in this 
regard. While doing so, the chapter intends to cover the fundamentals of explainability 
in AI, the need to convert the black box system into a transparent one, and the 
associated opportunities for explainable artiﬁcial intelligence. 
In Chap. 9 of this book, a novel technique in data fusion model with security 
and 
data optimization technique in edge computing has been presented. Here, the 
proposed data fusion is carried out using secure sequential discriminant autoencoder 
in which the improvement of data accuracy, as well as for the maximizing of edge 
cloud-based sensor networks lifespan. The fusion of edge cloud data has been carried out using discriminant autoencoder which is integrated with distributed edge cloud 
users, where the security of the network has been enhanced using secure sequential 
fuzzy-based trust model. The integration of edge computing framework with the sensor network that can aid in the data collection, dissemination, and decision making 
has been performed in Chap. 10 of the book. This chapter proposes a LSTM-based 
strate
gy for an edge computing-enabled WSN to determine the status of the node 
depending on the network parameters such as number of communications, number of packets transmitted, and initial energy of the nodes. The proposed protocol is 
implemented using TensorFlow and Keras libraries in Python language. 
A wide range of topics related to the integration of XAI and edge AI have been 
presented 
in this book. We are sure that the book will provide a comprehensive 
knowledge about explainability in edge AI to the readers. 
Giza, Egypt 
New Delhi, India 
Roorkee, India 
Roorkee, India Aboul Ella Hassanien 
Deepak Gupta 
Anuj Kumar Singh 
Ankit GargContents 
1 Explainable Artiﬁcial Intelligence: Concepts and Current 
Progression ................................................... 1 
Kirti Kangra and Jaswinder Singh 
2 Explainable Artiﬁcial Intelligence (XAI): Understanding 
and Future Perspectives ....................................... 19 
Megha Gupta 
3 Explainable Artiﬁcial Intelligence (XAI): Conception, 
Visualization and Assessment Approaches Towards Amenable 
XAI .......................................................... 35 
Tasleem Nizam and Sherin Zafar 
4 Explainable AI (XAI): A Survey of Current and Future 
Opportunities ................................................. 53 
Meet Kumari, Akshit Chaudhary, and Yogendra Narayan 
5 Recent Challenges on Edge AI with Its Application: A Brief 
Introduction .................................................. 73 
Kapil Joshi, Harishchander Anandaram, Manisha Khanduja, 
Rajesh Kumar, Vikrant Saini, and Yasmin Makki Mohialden 
6 Explainable Artiﬁcial Intelligence in Health Care: How XAI 
Improves User Trust in High-Risk Decisions ..................... 89 
Sheeba Praveen and Kapil Joshi 
7 Role of Explainable Edge AI to Resolve Real Time Problem ....... 101 
Ambeshwar Kumar, T. M. Rajesh, Manikandan Ramachandran, and Deepak Gupta 
8 Explainable Data Fusion on Edge: Challenges 
and Opportunities ............................................. 117 
Shweta Sinha and Priyanka Vashisht
viiviii Contents
9 Trust Model Based Data Fusion in Explainable Artiﬁcial 
Intelligence for Edge Computing Using Secure Sequential 
Discriminant Auto Encoder with Lightweight Optimization 
Algorithm .................................................... 139 
D. Prabakar, M. Sundarrajan, S. Prasath Alias Surendhar, 
Manikandan Ramachandran, and Deepak Gupta 
10 A Deep Learning Based Target Coverage Protocol for Edge 
Computing Enabled Wireless Sensor Networks .................. 161 
Pooja Chaturvedi, A. K. Daniel, and Umesh BodkheAbout the Editors 
Prof. Aboul Ella Hassanien is Founder and Head of the Egyptian Scientiﬁc Research 
Group (SRGE) and Professor of Information Technology at the Faculty of Computer 
and Artiﬁcial Intelligence, Cairo University. Professor Hassanien is an ex-dean of the 
Faculty of Computers and Information, Beni Suef University. Professor Hassanien 
has more than 800 scientiﬁc research papers published in prestigious international 
journals and over 40 books covering such diverse topics as data mining, medical images, intelligent systems, social networks, and smart environment. Prof. Hassanien 
won several awards, including the Best Researcher of the Youth Award of Astronomy and Geophysics of the National Research Institute, Academy of Scientiﬁc Research (Egypt, 1990). He was also granted a Scientiﬁc Excellence Award in Humanities from 
the University of Kuwait for the 2004 Award and received the Scientiﬁc University Award (Cairo University, 2013). Also, he was honored in Egypt as the best researcher at Cairo University in 2013. He also received the Islamic Educational, Scientiﬁc and 
Cultural Organization (ISESCO) Prize on Technology (2014) and received the State Award for excellence in engineering sciences 2015. He was awarded the Medal of Sciences and Arts of the ﬁrst class by the President of the Arab Republic of Egypt, 
2017. 
Deepak Gupta received a B.Tech. degree in 2006 from the Guru Gobind Singh 
Indraprastha University, Delhi, India. He received an M.E. degree in 2010 from 
Delhi Technological University, India, and Ph.D. degree in 2017 from Dr. A. P. J. Abdul Kalam Technical University (AKTU), Lucknow, India. He completed his 
post-doc from the National Institute of Telecommunications (Inatel), Brazil, in 2018. He has co-authored more than 207 journal articles, including 168 SCI papers and 
45 conference articles. He has authored/edited 60 books, published by IEEE-Wiley, Elsevier, Springer, Wiley, CRC Press, DeGruyter, and Katsons. He has ﬁled four Indian patents. He is the convener of the ICICC, ICDAM, ICCCN, ICIIP, and DoSCI 
Springer conferences series. He is Associate Editor of Computer and Electrical Engineering, Expert Systems, Alexandria Engineering Journal, Intelligent Decision 
T echnologies. He is the recipient of the 2021 IEEE System Council Best Paper Award. 
He has been featured in the list of top 2% scientist/researcher databases worldwide.
ixx About the Editors
In India, he secured Rank 1 as a researcher in the ﬁeld of healthcare applications 
(as per Google Scholar citation) and ranked #78 in India among Top Scientists 2022 
by Research.com. He is also working toward promoting startups and also serving as Startup Consultant. He is also a series editor of Elsevier Biomedical Engineering 
at Academic Press, Elsevier, Intelligent Biomedical Data Analysis at De Gruyter, 
Germany, and Explainable AI (XAI) for Engineering Applications at CRC Press. He is 
appointed as Consulting Editor at Elsevier. He accomplished productive collaborative 
research with grants of approximately $144,000 from various international funding 
agencies, and he is Co-PI in an International Indo-Russian Joint project of Rs. 1.31 Cr from the Department of Science and Technology. 
Anuj Kumar Singh is working as Professor in School of Computing at University 
of 
Engineering & Technology (UETR), Roorkee, India. He has more than 18 years 
of teaching experience in technical education. He holds a Ph.D degree in the ﬁeld of 
Computer Science and Engineering from Dr. A.P.J. Abdul Kalam Technical Univer-sity, Lucknow. He passed M.Tech degree with First Distinction from Panjab Univer-
sity, Chandigarh and B.Tech degree with First Honours from U.P.T.U Lucknow in 
Computer Science and Engineering. In addition to these, he has also qualiﬁed UGC 
NET. Having published more than 30 research papers in journals and conferences 
including Scopus and SCIE, he has also authored one book and edited two. He has also ﬁled four patents. His areas of specialization include Intelligent Systems, Cryp-
tography, Network and Information Security, Blockchain Technology, and Algorithm 
Design. 
Ankit Garg is working as Associate Professor in School of Computing at Univer-
sity 
of Engineering and Technology, Roorkee, India. He has more than 13 years 
of teaching experience in technical education. He holds a Ph.D degree in the ﬁeld 
of Computer Science from Uttarakhand Technical University, Dehradun, India. He 
passed M.Tech degree from Uttarakhand Technical University, Dehradun, India and MCA degree from HNB, Garhwal University, Chauras Campus, Srinagar, Garhwal, 
India. In addition to these, he has published more than 30 research papers in jour-
nals and conferences including SCIE and Scopus. His areas of specialization include image processing, computer graphics, and computer vision.Chapter 1 
Explainable Artiﬁcial Intelligence: 
Concepts and Current Progression 
Kirti Kangra and Jaswinder Singh 
Abstract AI is a broad term used in computer science to mimic the human mind. 
Machine Learning is also a part of AI and is used in various ﬁelds of decision-
making by using different algorithms. In the last few decades, AI has gained more 
popularity in the ﬁeld of Industry, Medicine, Education, Defense, etc. Based on the 
algorithm AI is categorized into: Interpretable AI and Explainable AI (XAI). In inter-
pretable AI, experts have problems in understanding the working of the model. They 
have no idea of working behind the algorithm so from here the term explainable AI came into existence. The term Explainable AI is coined by DARPA. This is also 
known as Explainable Machine Learning. The term came into existence to provide transparency in the process of decision-making by different Machine learning algo-rithms. In the current scenario, XAI is gaining popularity because of its transparent 
working. XAI is used in every ﬁeld where AI can be used but with some modiﬁca-tion or by adding some techniques of XAI as “SHAP (Shaply Additive exPlanations), DeepSHAP , DeepLIFT, CXplain, and LIME”. The main goal of this chapter is to 
provide a brief overview of XAI by covering almost every aspect of XAI. The chapter is divided into sections, in the ﬁrst section brief introduction is discussed. Related work concerning medicine is stated in Sect. 
2. Later other sections will cover princi-
ples, techniques, current state of art, beneﬁts, and, applications. This chapter infers 
that XAI is a great development in AI due to its transparent nature. The chapter also addressed the challenges in XAI, as well as the ﬁeld’s future potential. 
Keywords XAI ·Interpretability ·Comprehensibility ·Transparency ·Privacy ·
SHAP ·LIME 
Abbreviations 
CMGE Counterfactual multigranularity graph supporting facts extraction 
LNN Logical neural network
K. Kangra (B) · J. Singh 
Department of Computer Science and Engineering, GJUS & T, Hisar, Haryana, India 
e-mail: kirtikangra98@gmail.com 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_1 12 K. Kangra and J. Singh
GNN Graph neural network 
ADR Adaptive dimension reduction 
1 Introduction 
V arious industrial areas have adopted emerging technologies, and Artiﬁcial Intelli-
gence (AI) is at the root of them [ 1]. While the development of AI can be addressed 
back decades, there is general accord that smart systems with learning, reasoning, and 
adaptive skills are of critical signiﬁcance. AI approaches are attaining remarkable 
standards of accomplishment by attempting to address extremely hard computations 
as a result of these capacities, considering these approaches crucial for future growth [
2]. Furthermore, intelligent agents such as Google Assistant, Siri, and others are 
technologies increasingly integrating into individual personal lives. AI models, on the other hand, have become a black box whose internal workings are not completely transparent due to the increasing complexity of its algorithms. The conceptions of 
explainability and interpretability emerge as a result of this trade-off. Models before 2015 have been viewed as examples of Interpretability AI due to abstraction of their whole internal working. XAI was introduced by DARPA in 2015 to provide a thor-
ough knowledge of how AI models work. The XAI initiative intends to develop a set of machine learning approaches that generate more comprehensible models by retaining excellent learning performance. AI is at the base of several activity areas 
that have incorporated modern information technology [
3]. 
Explainability in AI is a discipline that has applications in a range of areas, 
including medical treatment, business operations, privacy, economic and ethical considerations, automated driving, smartphones, and AI for developers [
4]. All of 
the applications will be explained later in detail in the application section. 
The most frequently used terminologies in XAI are:
•Understandability: Describes a model’s ability to convey its function—how it works—to an individual despite having to elucidate its inner core or the 
computational means by which the model processes data internally [
5].
•Comprehensibility: If deployed in tandem with machine learning techniques, 
comprehensibility alludes to learning algorithm’s capacity to express its learned 
facts in a way that is understood by individuals.
•Interpretability: It is described as the potential to express or convey information 
to an individual in a comprehensible way.
•Explainability: Explainability is related to the concept of description as a connec-
tion among individuals and a selection provider. The selection provided by the 
selection provider can be understandable to individuals [ 6].
•Transparency: A model is said to be transparent if it can be understood on its own.1 Explainable Artiﬁcial Intelligence: Concepts and Current … 3
The word “machine learning (ML)” was coined from the term “artiﬁcial intel-
ligence”. Both of these are interconnected and can be used in the same applica-
tion. Even though their functioning models and algorithms are identical. ML has increased in popularity in both research and industry, owing to the effectiveness of 
its numerous algorithms, especially in the medical ﬁeld. In certain applications, a computer glitch or a misinterpretation has some impact, but in the medical profes-sion, it is linked to human life. Early diagnosis of the disease is often crucial for 
patients’ recovery or to reduce the risk of disease from progressing to more severe stages. Explainable Machine Learning and XAI can be used interchangeably. 
The main objective of this chapter is to provide a brief overview of XAI. The 
chapter is divided into sections. Section 1 provides an introduction to XAI. Section 2 
discussed the current ﬁnding of the researchers. Section 3 stated the principles of 
XAI. Later sections discussed Models, Current progression, Applications, Beneﬁts, 
and Challenges of XAI. 
2 Literature Review 
Number of work has been done in the ﬁeld of health care using the XAI model. Some 
of the latest research work has been stated in the section to provide a precise overview 
of XAI in healthcare. The Research work in the section mainly concentrates on the 
medicinal sector. 
In Quellec et al. [ 7] proposed a unique explanatory AI approach for the classiﬁ-
cation of multi-label images for the prediction of diabetes retinopathy. For the clas-siﬁcation, CNN and Pixel-level algorithms were used. Indian Diabetic Retinopathy Image Dataset was used and Python was used for the experiment. The accuracy of 
the proposed model was 93%. 
In Wu et al. [ 8] propose a counterfactual multi-granularity graph supporting facts 
extraction method to extract supporting facts from the irregular EMR. The approach 
used by them was suitable for Lymphedema. Techniques used by the researcher 
were GNN, Counterfactual reasoning. Dataset was acquired from the Local hospital in china. The result of the anticipated model was 99% for each parameter (precision, 
recall, and F1-score value). 
In Kavya et al. [ 9] developed a model for the diagnosis of allergy. Techniques used 
in the model were KNN, SVM, C5.0, MLP , AdaBag, RF, and Condition prediction. 
Dataset was taken from the allergy testing centers of South India and the experiment 
was performed using R-Studio. The result of the anticipated model was 86%, and 
75% for Accuracy and Sensitivity respectively. 
In Amoroso et al. [ 10] developed an EXI system based on adaptive dimension 
reduction to predict breast cancer. For the classiﬁcation clustering technique was used. Dataset was taken from Italy. 
In Dindorf et al. [ 11] design a pathology-independent classiﬁer that presents expla-
nations for the categorization ﬁndings and projection probabilities. Two datasets were used and acquired, the ﬁrst dataset was part of the dissertation project of Friederike4 K. Kangra and J. Singh
Werthmann and the second was part of the dissertation project of Claudia Wolf. One-
class SVM, binary RF, and LIME approaches were used. The result of the classiﬁer 
were F1-80 ± 12%, MCC-57 ± 23%, BSS-33 ± 28%. 
In El-Sappagh et al. [ 12] designed a model for detecting Alzheimer’s disease 
using XAI which was precise and explanatory. In the proposed model two layer RF, SHAP , and Fuzzy techniques were used. Alzheimer’s Disease Neuroimaging 
Initiative dataset was used. The result of the proposed model was at the First layer: 
accuracy: 93%, F1-score: 93%, and at the Second layer: Accuracy: 87%, F1-score: 
87% respectively. 
In Peng et al. [ 13] developed an XAI model to predict Hepatitis. In the proposed 
model LR, DT, KNN, SVM, RF, SHAP , LIME, and partial dependence plots were 
used. Dataset was taken from UCI and the experiment was performed using Python 
3.6.4. The result showed an accuracy of 91%. 
In Sarp et al. [ 14] proposed a CNN-based XAI model to classify chronic wounds. 
CNN and LIME techniques were used. Dataset was taken from ekare Inc. and python 
3.6 was used for the experiment. The results of the anticipated model were Precision, 
Recall, and F1-score scored 95%, 94%, and 94% respectively. 
In Tan et al. [ 15] develop a deep learning model based on high-resolution computed 
tomography scans of the temporal bone to diagnose fenestral otosclerosis. The tech-
nique in the proposed model was LNN. Dataset was taken from the institutional 
committee of Eye and ENT Hospital, Fudan University. The result of the anticipated 
model was AUC, Sensitivity, and Speciﬁcity scored 99%, 96%, and 98% respectively. 
From the literature, it has been seen that XAI is used in healthcare to predict and 
classify the number of diseases like Diabetes, Spine, Chronic wounds, etc. Different 
XAI models such as LIME, SHAP , Fuzzy, Deep attributes, etc. are used to clas-
sify disease. By using the XAI model results of the traditional models have been increased as seen in the literature. Using these models Accuracy has been increased 
to a remarkable extent. So, it can be concluded that XAI has more potential and its potential should be utilized in the ﬁeld of health care. 
3 Principles 
The principles are the collection of rules that should follow by any XAI model. There 
are three principles of XAI (see Fig. 1). 
Fig. 1 Principles of XAIPRINCIPLES 
Transparency Interpretability Explainability 1 Explainable Artiﬁcial Intelligence: Concepts and Current … 5
Principles are explained below:
•Transparency: If the system developer can demonstrate and explain how to obtain 
model parameters from training data and produce labels from testing data [ 16].
•Interpretability: It is described as the capability to express or convey information 
to an individual in a comprehensible way [ 17].
•Explainability: Explainability is related to the concept of description as a connec-
tion between individuals and a selection provider. The selection provided by the 
selection provider can be understandable to individuals. 
In response to the expansion of signiﬁcance of this area, NIST outlined four 
principles of XAI [ 18] in August 2020, which describe the following key principles 
that an AI must adhere to be termed as XAI:
•Explanation: According to this principle, an AI system must provide proof, 
support, or justiﬁcation for whatever choice it makes.
•Meaningful: This principle argues that the AI system’s explanations must be 
comprehensible and signiﬁcant to the users. Because diverse communities may 
have distinct interests and perspectives, the AI system’s interpretation must be smooth to ﬁt each group’s unique traits and requirements.
•Accuracy: This concept asserts that the AI system’s interpretation must properly represent the system’s processes.
•Knowledge limits: This principle asserts that AI systems must recognize situations 
in which they were not intended to work, and in that situation, their responses 
may be unreliable. 
4M o d e l s 
This section of the chapter covers XAI techniques to provide a precise overview. “XAI is a research ﬁeld on ML interpretability techniques whose aims are to understand 
ML model predictions and explain them in human and understandable terms to build trust with stakeholder”. To understand the ML model XAI comes with different models. 
To understand the XAI model user need to understand two terms as shown in 
Fig. 2.
1. Model-Based: That develop interpretable ML models 
2. Post hoc: Derive explanations for complex ML models
•Black Box Models (BB): models whose internal working cannot be easily explainable.
•White Box Models (WB): models whose internal working can easily be understood. 
To understand the working of the BB, WB is used. XAI came into existence due 
to trade-off between “interpretability and explainability. Two approaches: Integrated6 K. Kangra and J. Singh
Fig. 2 Machine learning 
modelMachine Learning Models 
Model Based Post Hoc 
Black Box 
White Box 
Fig. 3 XAI classiﬁcation
(transparency-based) and Post-hoc” models [ 19] came under “interpretability and 
explainability Fig. 3. Provide an overview of the classiﬁcation of XAI. Some terms 
used in Fig. 3 are used further in this section.
•Integrated Methods 
Integrated interpretability is a term that refers to the ability to interpret data, 
Model’s best description is if it properly depicts itself and is straightforward 
to comprehend [ 20]. This method is only applicable to model families with 
a moderate level of complexity such as “linear models, decision trees, and 
rules”. Other model communities, such as “artiﬁcial neural networks and support 
vector machines, boosted trees, and random forests” are labeled as opaque 
models because of their complexity, which makes it problematic for users to 
realize the motivation underlying predictions. 
Post-hoc interpretability retrieves information from a previously learned model 
and is unaffected by the model’s behavior. The advantage of this method is that 
it does not affect the model’s performance because it is considered BB. This 
is familiar with how individuals justify their own decisions when they don’t fully 
understand how their decision-making techniques work.
•Post-hoc Methods 
The high potential beneﬁts of utilizing complicated, opaque models are increasing 
as hardware improves and data becomes more readily available. Interpretability1 Explainable Artiﬁcial Intelligence: Concepts and Current … 7
XAI Feautre based concept 
Model SHAP 
CAM 
Grad-
CAM 
Global 
Approaches Global Attribute 
Mappin g 
Gradient based 
Salienc y Map 
Deep attribute 
map 
Concept Activation Vector 
Local Interpretable Model-
agnostic Explanations 
Layerwise Relevance 
Propa gation 
Fig. 4 XAI models 
and explainability, on the other hand, are challenges that must be appropriately 
addressed. 
Interpretability: The transparent proxy model approach identiﬁes an inter-
pretable model that estimates the black-box model’s predictions generally. 
Explainability: For each prediction, these algorithms typically provide a 
description in the form of feature signiﬁcances for concluding. In [ 21], sensi-
ti
vity analysis and layer-by-layer relevance propagation were used to explain 
deep learning model predictions in terms of input variables. 
To make the BB model explainable XAI approaches came into existence as 
shown in Fig. 4.
4.1 F eatures-Based Approaches 
“Shapley Additive explanation (SHAP)” is a game-theoretic technique to expli-
cate machine learning predictions [ 20]. By modeling the features as participants 
in 
an alliance game, SHAP aims to determine how much each feature inﬂuenced a 
choice. The rewards of the game are an additive indicator of signiﬁcance which is known as the Shapley value that reﬂects the weighted average impact of a speciﬁc 
feature for each and every feasible set of features. As a result, a model’s local and 
global explanations are compatible, and the average prediction is evenly spread along with all Shapley values, allowing for examinations of different explanations. More-
over, although predictive models may include non-independent pay-off splits, the8 K. Kangra and J. Singh
explanation of the Shapley values is not always straightforward if the model is not 
additive. Furthermore, while SHAP is model agnostic, optimizing the SHAP algo-
rithm for all model types is not always simple or effective. 
CNN’s have their class activation maps (CAMs). The per-class weighted linear 
sum of subjective features available at diverse geographical places in a picture is represented by CAMs [
22]. Global average pooling is applied to the last convolutional 
feature map of a network on the output layer. The input features for a fully linked 
layer, which is then output via a loss function, are then pooled with feature maps. 
The portions in the input image that have a stronger effect on the CNNs’ choice are emphasized per class and apparent through a heatmap visualization by projecting 
the output weights back to the earlier convolutional layer. Pre-trained networks and networks that do not follow the stated fully convolutional network design are not eligible for CAMs. Additionally, the completely attached layer and map scaling 
can cause spatial information to be lost. Grad-CAM and Grad-CAM ++ are two expansions of the original CAM model that aim to improve the explainability of CNNs. 
Grad-CAM [ 23] generalizes class activation mapping (CAM) to any random CNN 
architecture without training. Any target class’s gradients are delivered to the last 
convolutional layer, which computes a signiﬁcance score based on the gradients. A 
heatmap illustration of the Grad-CAM, like other approaches, shows which portions of the input image were most essential in the CNN’s judgments. GradCAM, on 
the other hand, only creates coarse-grained conceptions and is unable to describe numerous cases of identical objects in a single image. 
To circumvent these limitations, Grad-Cam ++ [ 24] uses the weighted average 
of the gradients. Feature-oriented methods reveal where a choice is being achieved from the perspective of the input, but they lack at human-level description of how and why the model arrived at those conclusions. 
4.2 Global Approaches 
Global attribution mappings techniques on a global scale [ 25] can interpret as neural 
network’s (NN’s) predictions on a comprehensive level, beyond subpopulations, by framing perception as weighted combined ranks for features with exact semantic descriptions. The beneﬁts include the ability to capture diverse subpopulations using 
a granularity parameter that may be tuned. GAMs employ K-medoids clustering technique to assemble similar local feature signiﬁcances into clusters and determine 
a pair-wise rank distance matrix among features. As a global attribution, the medoid of the respective cluster describes the outline found in each cluster. As a result, this method can be used to investigate features in distinct subpopulations of samples. 
Gradient-based saliency maps [ 26] are mapping approach that displays the abso-
lute value of the dominant predicted class’s gradient (concerning the input features) as a normalized heatmap. The pixels with the highest activation levels are emphasized, 
and they are parallel to the most signiﬁcant locations. The method’s description is the1 Explainable Artiﬁcial Intelligence: Concepts and Current … 9
capability for an individual to sight which features in the image are being considered 
in the categorization of the result. When propagating nonlinear layers, conversely, the 
absolute value signiﬁes that the gradient of neurons with negative input is inhibited. 
Deep attribute maps are proposed by [ 27] as an approach for presenting the 
generalization ability of gradient-based algorithms. The suggested layout also shows comparisons between various saliencey-based explanatory techniques. The gradient of the output is combined with the relevant input to give a heatmap that explains 
a model’s prediction. The colors red and blue denote positive and negative depic-tions of the ﬁnal choice, respectively. Explanations are affected by noisy gradients and input variability. Deep attribute is unable to justify what is the reason that two 
models are providing similar or dissimilar results. 
4.3 Concept Activation V ector Approach 
In [28] concept activation vectors (CA Vs) are the strategy for explaining the global 
internal states of NN’s neurons to the NN’s high-level latent properties. As a result, 
CA Vs deﬁne the notch at which these conceptual attributes refer to a series of human 
ideas that the user has chosen. Human bias is unavoidable, but by illuminating the corresponding perception, any errors in the model’s decision-making process can be 
addressed, such as if some features are erroneously considered important. Beyond that, automatic concept-based explanations [
29] extract CA Vs without the need for 
human intervention, eliminating human bias. Human intelligible perceptions are 
split at numerous spatial dimensions from in-class visuals rather than being chosen. 
Concept-based strategies, on the other hand, rely on the perceptions being unique to the class, and the efﬁciency of description is impaired if the same perception is used 
in various sessions. 
4.4 LIME Approach 
“LIME (Local interpretable model-agnostic explanations)” [ 30] is a model-agnostic 
approach for producing nearby optimized ML model interpretations. LIME uses an understandable surrogate model to acquire the local behavior of predictions from a 
comprehensive “black box” model. For image classiﬁcation, a picture is segmented into chunks of continuous super-pixels, after which a penalized local classiﬁer is 
taught on a novel set of permuted samples of the original image. The concept is to determine what the input delivered to every given class by modifying human-understandable bits of the input data and learning the discrepancies between those 
disruptions and the original image. On the other hand, if the parameters that determine the perturbations are only dependent on preconceptions, these perceptions aren’t always relevant or trustworthy on a human level.10 K. Kangra and J. Singh
4.5 LRP Approach 
“Layer-wise relevance propagation (LRP)” [ 31] explains the output of a multi-layered 
NN in terms of its input using established propagation rules. The method produces 
a heatmap that shows which pixels affect the model’s prediction and to what extent. 
As a result, LRP places a premium on major impacts on a network’s conclusion. 
While LRP can be used on a network that has already been trained, this is a post-hoc approach that only gives a truncated consideration of the features’ contribution to 
the decision. 
5 Current Progression 
AI started in 1950. After that, a new era of human intelligence started. In 1963, the term Machine Learning was coined to train machines. By using ML automated era of 
machines begins, machines can perform analysis tasks by themselves. ML increases accuracy in analysis tasks by reducing human error. Both ML and AI are used in different sectors. But in 1976, Deep learning (DL) came into existence, especially 
for medical services. But AI, ML, and DL have problem that their inner working can’t be explained. From there two terms as explained at beginning of this chapter came into existence. 
In 2015, the term XAI was coined to explain the inner working of different models 
of AI. It uses different approaches explained above to make the model’s working transparent. Now XAI is used in many sectors. 
According to the current status of XAI, it is employed in a variety of industries like 
ﬁnance, manufacturing, défense, stock, and healthcare. It is gaining popularity in the ﬁeld of health care due to its nature and characteristics. It is utilized by medical experts 
in healthcare to gain greater precision in decision-making processes, as there is a need for more accuracy in the medical area. Its various strategies combine with existing 
artiﬁcial intelligence techniques to deliver more accurate results. It is mostly utilized in medical image processing for further analysis. In the sector of image processing, it helps by using saliency maps. For image classiﬁcation, Deep attribute maps are 
considered the best choice. These maps are important for determining which parts of a picture are considered by the model for prediction and which are not to make an accurate prediction. In the sector of Video threat detection, it is used to detect any 
forgery in the video.1 Explainable Artiﬁcial Intelligence: Concepts and Current … 11
In the current scenario, XAI models are used in various applications of 
ML some glimpses are: SHAP is one of the most powerful tool of XAI. It 
can be used in the ﬁeld of medicine. By measuring the contribution of each feature to the prediction, it may be used to explain the results of any machine learning 
model. It is based on the ideas of game theory. LIME is used in Image analysis. All the models which are explained above can be used with any of the ML models to make their working explainable. 
6 XAI in Medicines 
The combination of XAI and healthcare is a powerful digital technology. The emer-
gence of AI-based diagnosis tools has sparked serious doubts regarding the accuracy 
of AI-based outcomes. In the medical and healthcare sectors of the virtual sector, trust is critical to AI’s success. If-else diagnostic models are organically explainable 
since they are made up of feature value sets and deliver a score based on a feature value of a speciﬁc case of disease detection. External symptomatic disease diag-
nosis is highly suited for if-else-based explainable medical diagnosis systems. If the person’s symptoms list has a certain number of matching criteria using If-else-based feature values, it can be determined if the person had a particular disease or not. 
For instance, if the patient has a prior medical history. For example, if the patient has a history of respiratory infection and cough, the odds of developing asthma are increased. To cover a wide range of XAI, an explanation that is independent of the 
AI model is required. Model agnostic are XAI techniques. A model agnostic method is LIME. LIME is a framework for quantifying the weights of all aspects that go into reaching a decision or making prediction. Other model-independent XAI approaches 
exist, such as SHAPLEY [
31]. Deep learning is a valuable tool for precise medical 
diagnosis, but its black-box approach to prediction and conclusion limits it to a few crucial areas of medicine. Some applications of XAI in medicine are shown in Fig. 
5.
6.1 Seven Pillars of XAI in Medicine 
There are 7 pillars of XAI in medicine which are discussed below [ 32]. These are 
important in the implantation of the XAI models and will help to make any model explainable in medicine.
•Transparency: Understandable by any user.
•Domain Sense: The explanation should make sense in the context of the application and to the system’s user.
•Consistency: Across all models, the explanation should be consistent.
•Parsimony: It’s best if the explanation is as straightforward as possible.12 K. Kangra and J. Singh
Fig. 5 XAI application in 
medicine
XAI in 
Medicine Medical 
Image 
Anaylsis 
Diagnosis 
R&D in 
Drug 
Surgery Virtual 
Monitoring Personlized 
Medication 
& care 
•Generalizability: Explainability and interoperability should apply to prediction 
algorithms.
•Trust/Performance: The expectation that the related prediction algorithms for 
e
xplanation should function well.
•Fidelity: Both the implementation and the explanation should be in harmony. 
7 Applications of XAI 
In theory, the scope of XAI should be as broad as that of AI. Natural language 
processing (NLP), health care, engineering, and military are just a few of the impor-
tant areas. NLP and engineering are used in banking, ﬁnance, digitization, and 
automation [ 33].
•Data Security: The European Union and its data protection regulatory agency have a 
“right to explanation” clause, that helps XAI algorithms to come to a decision 
without breaching data security.
•Medical: Based on a patient’s medical history, XAI can provide a diagnosis. The use 
of AI/ML techniques in the healthcare image processing area makes it quicker 
for doctors to spot cancer tumors and other lung diseases in patients.
•Défense: As a result of computerized artillery and monitoring systems, XAI is becoming 
more important in defensive activity. During battle mode training and 
real conﬂict strategy, XAI also gives excellent second-hand help.1 Explainable Artiﬁcial Intelligence: Concepts and Current … 13
•Banking: One of the most important ﬁnancial industries with the largest impact 
on people’s lives is banking. Every day, a large number of fraudulent transactions 
and conns by cheaters take place. Well-trained XAI models can help with both the investigation and decrease false positives in fraudulent transactions. 
8 Beneﬁts of XAI 
The objective of XAI is to describe how certain outcomes or suggestions are made. It 
explains why AI performs as it does and fosters trust among people and AI models. 
The main advantages of XAI are [ 34]:
•Improved explainability and transparency: Businesses can have a deeper under-
standing of advanced AI models and they act in various ways under certain circum-
stances. Humans can utilize an explanation interface to comprehend how these AI models arrive at particular results, even if it’s a black-box model.
•Faster adoption: As organizations expand their understanding of AI models, they will be able to entrust them with more crucial judgments.
•Improved debugging: When a system behaves strangely, XAI can be used to diagnose the problem and assist developers in troubleshooting.
•Allowing for audits to meet regulatory obligations. 
9 Challenges in XAI 
Explainability is both theoretically and socially compelling. XAI is an extremely effective diagnostic mechanism that can deliver information well beyond what stan-
dard linear models could but to implement XAI researchers need to face challenges. Some challenges of XAI are listed below [
35]:
•Conﬁdentiality: An algorithm may be proprietary and revealing it would be a security risk. How can one be conﬁdent that the AI system hasn’t picked up a biased view of the world due to ﬂaws in the training data, model, or goal function?
•Complexity: Algorithms can be simple to grasp but complicated to deploy. As a result, an inexperienced person’s conception is surreal, and this is an area where XAI techniques may be effective. As XAI can provide more efﬁcient and under-
standable algorithms.
•Irrationality: Algorithms that provide plausible, biased, or out-of-line conclusions 
based on credible evidence. How can one be conﬁdent that an AI system’s choices 
are fair? Rationality is logical, but it must be viewed in the context of the particular data input given to AI systems.
•Injustice: To realize the working of the algorithm is easy but to know how the system complies with a legal or moral norm.
•Lack of expertise: The majority of people will lack the necessary competence to 
comprehend the explanation and rate the decision’s fairness.14 K. Kangra and J. Singh
•Context-dependency: Algorithms can’t be discussed at a high level because the 
results vary from person to person.
•Dynamics of data and decisions: Because data and decisions shift during time, 
narratives must alter as well.
•Interference of algorithms: Often, a series of operations is required to collect and process data from multiple sorts of sources, and many, often disparate algorithms are employed. 
10 Future of XAI 
The global XAI market is expected to grow at a CAGR of 18.4% from 2022 to 2030, 
from USD 4.4 billion in 2021 to USD 21.0 billion in 2030. Explainable AI is an 
artiﬁcial intelligence method or technique in which the solution can be evaluated and understood by humans. It differs from standard ML techniques, in which researchers 
frequently fail to comprehend why the system has reached a particular conclusion. Healthcare, retail, media and entertainment, aerospace, and défense are among the areas that are embracing XAI. For example, in the retail industry, XAI is critical for 
anticipating new fashion trends with reason, allowing retailers to stock and display the latest products in their stores. Furthermore, in the e-commerce industry, the notion of recommendation engines is an example of XAI, in which the system offers 
things to the consumer based on his or her search history. Additionally, recommen-dation engines promote accessories/additional products in addition to the one being purchased. This is accomplished through the use of explainable AI, which allows the 
system to recommend things that are beneﬁcial to the customer [
36]. 
Future investigations of medical XAI applications should include medical special-
ists in the design and development stages. Interdisciplinary collaboration is required 
for a good medical XAI application. Medical experts, in particular, should provide 
their prior medical expertise, as well as suggestions and feedback, to help enhance 
the layout of AI algorithms. Medical XAI apps should be able to support doctors in 
drawing explainable clinical inferences, according to AI experts and data scientists. As a result, we expect that medical XAI models will gain acceptance in the medical 
ﬁeld. 
11 Conclusion 
XAI is, without a dispute, an important collaborative study in the AI system. The ethical and quality-of-life considerations of utilizing AI in its existing situation in real-world circumstances are problematic. This chapter delivers insight into XAI, 
which has recently been identiﬁed as a critical necessity for ML methodologies to be embraced in real-world applications. This chapter has focused on this topic by highlighting the search for better interpretable ML algorithms, as well as discussing1 Explainable Artiﬁcial Intelligence: Concepts and Current … 15
the principles that drive model explainability. In this chapter, various XAI techniques 
such as LIME, SHAP , DEEPSHAP , etc. are covered. Every technique has its own 
merits and demerits. In Shap, Shap values are calculated, based on those values researchers will come up with a decision. If there is an error in the calculation of the 
Shap value that will affect the decision of the researcher. This chapter also discussed different applications and their need in the medical ﬁeld and their future perspective in every ﬁeld especially concentrating on the medical ﬁeld. By looking at the whole 
scenario XAI is a powerful technique of AI. It can be concluded that this can act as a game changer in the healthcare sector due to its transparent nature. Physicians can easily make accurate decisions without the intervention of any technical expert. 
References 
1. D.M. West, The future of work: robots, AI, and automation. Futur. Work Robot. AI Autom. 
1–205 (2018) 
2. A. Barredo Arrieta et al., Explainable artiﬁcial intelligence (XAI): concepts, taxonomies, oppor-tunities and challenges toward responsible AI. Inf. Fusion 58, 82–115 (2020). 
https://doi.org/ 
10.1016/j.inffus.2019.12.012 
3. L.F. Huang, Artiﬁcial Intelligence, vol. 4 (2010) 
4. Explainable AI—India | IBM. https://www.ibm.com/in-en/watson/explainable-ai . Accessed 12 
June 2022 
5. G. Montavon, W. Samek, K.R. Müller, Methods for interpreting and understanding deep neural networks. Digit. Signal Process. A Rev. J. 73, 1–15 (2018). 
https://doi.org/10.1016/j.dsp.2017. 
10.011 
6. S.T. Mueller, R.R. Hoffman, W. Clancey, A. Emrey, G. Klein, Explanation in human-AI systems: a literature meta-review. Def Adv Res Proj Agency 204 (2019) 
7. G. Quellec, H. Al Hajj, M. Lamard, P .H. Conze, P . Massin, B. Cochener, ExplAIn: explanatory 
artiﬁcial intelligence for diabetic retinopathy diagnosis. Med. Image Anal. 72(2018) (2021). 
https://doi.org/10.1016/j.media.2021.102118 
8. H. Wu, W. Chen, S. Xu, B. Xu, Counterfactual supporting facts extraction for explainable 
medical record based diagnosis with graph network 1942–1955 (2021). https://doi.org/10. 
18653/v1/2021.naacl-main.156 
9. R. Kavya, J. Christopher, S. Panda, Y .B. Lazarus, Machine learning and XAI approaches for allergy diagnosis. Biomed. Signal Process. Control 69, 102681 (2021). 
https://doi.org/10.1016/ 
j.bspc.2021.102681 
10. N. Amoroso et al., A roadmap towards breast cancer therapies supported by explainable artiﬁcial intelligence. Appl. Sci. (2021). 
https://doi.org/10.3390/app11114881 
11. C. Dindorf et al., Classiﬁcation and automated interpretation of spinal posture data using a pathology-independent classiﬁer and explainable artiﬁcial intelligence (XAI). Sensors (Switzerland) 21(6323) (2021). 
https://doi.org/10.3390/s21186323 
12. S. El-Sappagh, J.M. Alonso, S.M.R. Islam, A.M. Sultan, K. Sup, A multilayer multimodal detection and prediction model based on explainable artiﬁcial intelligence for Alzheimer’s disease (2021) 
13. J. Peng et al., An explainable artiﬁcial intelligence framework for the deterioration risk predic-tion of hepatitis patients. J. Med. Syst. 45(5) (2021). 
https://doi.org/10.1007/s10916-021-017 
36-5 
14. S. Sarp, M. Kuzlu, E. Wilson, U. Cal, O. Guler, The enlightening role of explainable artiﬁcial intelligence in chronic wound classiﬁcation. Electronics 1406 (2021). 
https://doi.org/10.3390/ 
electronics1012140616 K. Kangra and J. Singh
15. W. Tan et al., The use of explainable artiﬁcial intelligence to explore types of fenestral otoscle-
rosis misdiagnosed when using temporal bone high-resolution computed tomography. Ann. Transl. Med. 9(12), 969–969 (2021). 
https://doi.org/10.21037/atm-21-1171 
16. D. Castelvecchi, The black box 2 0 |. Nature 538(7623), 20–23 (2016). [Online]. Available: 
http://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731 
17. L.H. Gilpin, D. Bau, B.Z. Y uan, A. Bajwa, M. Specter, L. Kagal, Explaining explanations: an overview of interpretability (2019). [Online]. Available: 
https://paperswithcode.com/methods/ 
category/interpretability 
18. P .J. Phillips et al., NISTIR 8312 four principles of explainable artiﬁcial intelligence (2021) 
19. F.K. Dosilovic, M. Brcic, N. Hlupic, Explainable artiﬁcial intelligence: a survey, in 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics MIPRO 2018—Proceedings, pp. 210–215 (2018). 
https://doi.org/10.23919/ 
MIPRO.2018.8400040 
20. S.M. Lundberg, S.I. Lee, A uniﬁed approach to interpreting model predictions. Adv. Neural Inf. Process. Syst. 2017(Section 2), 4766–4775 (2017) 
21. W. Samek, T. Wiegand, K.-R. Muller, Explainable artiﬁcial intelligence: understanding, visualizing and interpreting deep learning models (2017) 
22. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for discrim-inative localization, in Proceedings IEEE Computer Social Conference on Computer Vision and Pattern Recognition, vol. 2016, pp. 2921–2929 (2016). 
https://doi.org/10.1109/CVPR.201 
6.319 
23. R.R. Selvaraju, M. Cogswell, A. Das, R. V edantam, D. Parikh, D. Batra, Grad-CAM: visual explanations from deep networks via gradient-based localization, in Proceedings of the IEEE International Conference on Computer Vision, vol. 2017, pp. 618–626 (2017). 
https://doi.org/ 
10.1109/ICCV .2017.74 
24. A. Chattopadhay, A. Sarkar, P . Howlader, V .N. Balasubramanian, Grad-CAM++: generalized gradient-based visual explanations for deep convolutional networks, in Proceedings—2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, vol. 2018, pp. 839– 847 (2018). 
https://doi.org/10.1109/W ACV .2018.00097 
25. M. Ibrahim, M. Louie, C. Modarres, J. Paisley, Global explanations of neural networks 279–287 (2019). 
https://doi.org/10.1145/3306618.3314230 
26. K. Simonyan, A. V edaldi, A. Zisserman, Deep inside convolutional networks: visualising image classiﬁcation models and saliency maps, in 2nd International Conference on Learning Representations ICLR 2014—Workshop Track Proceedings, pp. 1–8 (2014) 
27. M. Ancona, E. Ceolin, C. Öztireli, M. Gross, Towards better understanding of gradient-based attribution methods for deep neural networks (2018) 
28. B. Kim et al., Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCA V), in 35th International Conference on Machine Learning ICML 2018, vol. 6, pp. 4186–4195 (2018) 
29. A. Ghorbani, J. Zou, J. Wexler, B. Kim, Towards automatic concept-based explanations, in NeurIPS (2019) 
30. J. Dieber, S. Kirrane, Why model why? Assessing the strengths and limitations of LIME (iii) 
(2020). [Online]. Available: http://arxiv.org/abs/2012.00093 
31. P . Gohel, P . Singh, M. Mohanty, Explainable AI: current status and future directions, 1–16 
(2021). [Online]. Available: http://arxiv.org/abs/2107.07045 
32. C.E. Muhammad Aurangzeb Ahhmad, A. Teredesai, V . Kumar, K. Inc., Explainable-AI-in-healthcare (2021) 
33. XAI (Explainable AI) & top 5 use cases—GPU ON CLOUD. https://gpuoncloud.com/xai-exp 
lainable-ai-top-5-use-cases/ . Accessed 12 June 2022 
34. Explainable AI (XAI) in 2022: Guide to enterprise-ready AI. https://research.aimultiple.com/ 
xai/. Accessed 12 June 20221 Explainable Artiﬁcial Intelligence: Concepts and Current … 17
35. Explainability of AI: The challenges and possible workarounds | by Rohitha Elsa Philip | 
Medium. https://medium.com/@rohithaelsa/explainability-of-ai-the-challenges-and-possible-workarounds-14d8389d2515. Accessed 12 June 2022 
36. S.T. Mueller, R.R. Hoffman, W. Clancey, A. Emrey, G. Klein, Explanation in human-AI systems: a literature meta-review (2021). 
https://www.nextmsc.com/report/explainable-ai-
market . Accessed 12 June 2022Chapter 2 
Explainable Artiﬁcial Intelligence (XAI): 
Understanding and Future Perspectives 
Megha Gupta 
Explanations are critical for users to comprehend, trust, and 
handle sophisticated artiﬁcial intelligence technologies efﬁciently. 
Abstract Currently, we are seeing a rapid and broad acceptance of artiﬁcial intel-
ligence (AI) in our everyday lives, which is contributing to the acceleration of the 
transition towards a more algorithmic society. The availability of enormous datasets, 
as well as recent advancements in deep learning methods, are allowing AI systems to perform at or even beyond the level of human performance on an expanding variety 
of challenging tasks. However, because of their layered non-linear structure, these 
strong models have traditionally been regarded as “black boxes,” since they provide no information about how they arrive at their predictions and hence cannot be used 
trustily to generate predictions. They are often opaque, but they are capable of making 
accurate predictions that cannot be explained in any other way. All of these choices are having an increasing amount of effect on the lives of individuals. The distrust of 
totally non-human, autonomous artiﬁcial intelligence systems is established as the 
cornerstone of the movement. The root of distrust lies in a lack of knowledge as to why intelligent systems make certain decisions in certain situations. As a result, this 
problem has sparked a fresh discussion over explainable artiﬁcial intelligence (XAI). 
Several lines of research work have since picked up on the deﬁnition, understanding, 
and implementation of explainability, including expert systems, machine learning, 
recommender systems, and approaches to neural-symbolic learning and reasoning, all of which have occurred primarily during different periods in the history of artiﬁcial 
intelligence. This chapter serves as an introduction for academics and practitioners 
who are interested in learning about essential parts of the new and quickly increasing ﬁeld of research on XAI. The ﬁrst section, titled “Introduction,” provides an overall 
summary of the Explainable Artiﬁcial Intelligence. Section 2 describes the need of 
trust 
and transparency in AI, which is what led to the development of the idea of XAI. 
Section 3 discusses the many approaches that contribute to the functioning of XAI. 
The 
principal applications domain of Explainable Artiﬁcial Intelligence is the topic
M. Gupta (B) 
Department of Computer Science, MSCW, University of Delhi, New Delhi, India 
e-mail: meghabis@gmail.com 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. 
E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective , 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_2 1920 M. Gupta
of discussion in Sect. 4. In Sect. 5, the upcoming difﬁculties and potential directions 
that this developing technology may take are discussed. 
Keywords Explainable artiﬁcial intelligence ·Transparency ·Trust in AI ·XAI 
methodology ·Perturbation ·Backpropagation 
Abbreviations 
AI Artiﬁcial intelligence 
ANN Artiﬁcial neural network 
CAM Class activation mapping 
DeConvNet Deconvolution neural network 
HCI Human computer interaction 
IG Integrated gradients 
LIME Local interpretable model-agnostic explanations 
LRP Layer-wise relevance backpropagation 
ML Machine Learning 
SHAP SHapley Additive exPlanations 
SR Salient relevance 
VQA Visual question answering 
XAI Explainable artiﬁcial intelligence 
1 Introduction 
End users may better understand, trust, and control the next generation of AI systems 
with the help of the XAI program’s new or updated ML approaches, which are designed to develop explainable models. XAI is aimed for the end user who relies on 
AI systems to make decisions or suggestions, or take action, and needs to understand the system’s reasoning. If, for example, a big data analytics system recommends that an intelligence analyst follow up on a given behavior, the analyst has to know why. 
It’s also important for an operator to grasp the system’s decision-making process if he or she wants to deploy an autonomous vehicle for future missions. 
AI that is capable of producing explanations for how it works is dubbed XAI 
(explainable Artiﬁcial Intelligence). The difference between present AI process and the contribution of XAI to it could be easily understood through Figs. 
1 and 2.
The purpose of explanations is to aid users in maintaining and effectively using the 
system model, as well as to assist users in debugging the model to avoid and correct wrong conclusions. Explanations, on the other hand, can be used for instructional 
purposes as well as help individuals discover and grasp new concepts in an application2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 21
Input Data Learning 
Process Learn ed 
Function Output to 
User 
Fig. 1 Present AI process 
Input Data Learning 
Process 
Explainable 
Model Output to 
User 
Decision Making/ 
Explainable Process Knowledge 
for Data User could 
Query 
Fig. 2 Emerging XAI process
area. It’s important to keep in mind that explanations play an important role in 
persuading users that the system’s decisions are best for them. 
The goal of an explainable artiﬁcial intelligence (XAI) system is to make its 
behavior more understandable to humans by offering explanations for its actions. There are several broad ideas that may be used to the development of more effective 
and human comprehensible artiﬁcial intelligence systems. When asked to describe 
its capabilities and understandings, the XAI system should be proﬁcient to describe what it has done so far and what it plans to accomplish in future; it should also be 
able to divulge the relevant information on which it is acting. While explainability in artiﬁcial intelligence has lately garnered a great deal of attention, the beginnings of this line of research may be traced back many decades to a time when AI systems were 
primarily created as (knowledge-based) expert systems and other similar systems. 
The authors of works [ 1–3] provide a summary of the increased activity in XAI 
across a variety of ﬁelds and industries. The authors of [ 1] highlight how important 
it is to support the output of a model, e.g., in precision medicine, where specialists need signiﬁcantly more information from the model to support their diagnosis than 
a simple binary prediction. The author of study [
2] stressed that there is a trade-
off between the performance of a model and its transparency, and that this is a very important consideration. It is a common misconception to believe that if one 
just concentrates on performance, the resulting systems would become opaquer.22 M. Gupta
Comprehensibility, as described by the author, is the capacity of a learning algorithm 
to express its learnt information in a human intelligible way; nevertheless, owing to 
the difﬁculty of comprehensibility’s measurement, it is often related to the assessment of the model complexity [
3]. 
There is a substantial body of research and writing on explainable or inter-
pretable machine learning. An overview of interpretable machine learning methods and approaches is offered in the work referred to as [
4]. In addition, a taxonomy 
of explanation models is provided, which divides them into three categories: global techniques, local methods, and introspective approaches. 
For a very long period, aspects of explanations’ understandability for lay users 
were neglected to a greater or lesser extent. As it has been also mentioned in [ 5], the 
majority of implementations do not concentrate on the end-users, who are the ones who are impacted by the model; rather, they concentrate on the machine learning 
developers, who employ explainability to debug the model itself. In actuality, there is a disconnect between the purpose of transparency and the concept of explainability. This is due to the fact that explanations mainly serve “internal” stakeholders rather 
than “external” ones. In order to overcome this difference, reasons need to be human-understandable while also being ﬂexible to the various stakeholders [
6]. For a system 
to be considered trustworthy, its explanations need to be tailored to the speciﬁc needs of the many kinds of users, with each user receiving information that is relevant to their situation and carefully chosen. Because of this, an approach to explainable 
artiﬁcial intelligence that centers on the needs of the user is required. 
For a system to be considered trustworthy, its explanations need to be tailored to 
the speciﬁc needs of the many kinds of users, with each user receiving information 
that is relevant to their situation and carefully chosen. Because of this, a concept to 
explainable artiﬁcial intelligence that centers on the needs of the user is required. In this context, criteria underlying Responsible AI have been described in [
7], which 
state that justice, responsibility, and privacy (particularly in relation to data fusion) should be evaluated while applying AI models in actual contexts. 
Because the end user is going to be a human being, investigating the role that 
explanations play in the interaction between humans and machines will be an essential future study area. The research [
8] looked at the role that humans play in AI that can 
explain itself. A successful human–machine interaction requires, as a precondition, 
the construction of explanations with the appropriate user emphasis, i.e., the asking 
of the appropriate questions in the appropriate manner. 
2 Need for Transparency and Trust in AI 
Most of today’s applications now use AI black box systems. Transparency and 
explainability are not essential if the overall performance of the system is good 
enough with the machine learning model deployed. When these technologies fail, the repercussions aren’t anything out of the ordinary, such as the mobile phone camera 
not being able to identify a person or the translation service producing grammatical2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 23
errors. Thus, the openness and trust requirements for these AI systems are rather 
low. The scenario is substantially different in critical applications. There may be a 
problem with the lack of transparency of ML approaches in this case. It may not be an option to rely on a data-driven system whose rationale is incomprehensible if a 
single bad decision can result in threat to life and health of humans e.g., in case of autonomous driving or in medical ﬁeld or it may lead to huge ﬁnancial losses e.g., in algorithmic trading etc. Because of this lack of transparency, machine learning is 
being used cautiously in industries like healthcare, e-commerce, entertainment etc. To build trust, an AI system must be able to explain its reasoning behind its actions. This is crucial, not just in medical or safety-sensitive applications. 
When compared to an incomprehensible intelligent system, one of the most 
urgent goals of a XAI system is to help end users comprehend how the intelligent system functions. Users’ mental models of the underlying intelligent algorithms are 
improved by machine learning explanations, which provide understandable trans-parency for the complicated intelligent algorithmic structures. Transparency in a XAI system may also enhance user experience by making model outputs easier to 
comprehend and interact with. 
When utilized correctly, XAI may identify and dissuade hostile cases [ 9] while 
also increasing transparency and fairness. For the sake of assessing the accuracy of production projections and fending off competitors, openness is essential. Using an adversarial example to deceive a classiﬁer into thinking a phoney picture is in fact 
genuine might have a negative impact on the accuracy of the classiﬁer. The reliability of AI algorithms to minimize threats [
10] and give transparency in terms of model 
comprehension, textual or visual reporting should be of primary relevance as we 
increasingly depend on autonomous algorithms to assist our everyday lives. 
In order to build conﬁdence in the intelligent algorithm, XAI system provides 
explanations to the end users. System dependability and accuracy may be assessed and calibrated using a XAI system. Consequently, users’ faith in the algorithm leads 
to their dependence on the system. Recommendation systems, autonomous systems, 
and crucial decision-making systems are all examples of how XAI intends to increase 
user trust via its open architecture. 
The information and accessible answers to circumstances, as well as the trust we 
establish, are the primary determinants of our social lives, choices, and judgments. Explanations and rationales are preferable to blind faith in one’s own wisdom when 
it comes to making decisions that aren’t ideal. In order to build trust with end-
users, including specialists, developers, legislators, and others, it is critical to explain 
why a certain choice was taken. As we move towards a networked AI-driven socio-
economic ecosystem, stakeholders and government agencies need to be able to trust 
the predictions of classiﬁers.24 M. Gupta
3 Methods of Explainable AI 
The explainable model’s core algorithmic principle may be divided into many subcat-
egories dependent on how it was implemented as shown in Fig. 3. XAI approaches 
may be divided into two categories: those that concentrate on changes or adjust-
ments in input data, and those that focus on the model’s architecture and parameters. 
Both perturbation-based and backpropagation-based approaches may be used. This 
section summarizes the most important approaches to explainable AI. 
3.1 Perturbation-Based 
Perturbation-based XAI approaches are used to create explanations by repeatedly probing a trained machine learning model with various input variations. These alter-
ations may be made at the pixel level by blurring, moving, or masking pixels, 
replacing certain characteristics with zero or random counterfactual occurrences,
XAI Methods 
Perturbation-Based Back Propagation-Based 
DeConv Saliency Maps 
LIME CAM 
SHAP SR Maps 
Prediction 
Difference Analysis Axiomatic 
Attribution Maps 
Randomized Input 
Sampling 
Explanation PatternNet 
Randomization & 
Feature Testing PatternAttribution 
Fig. 3 Different methodology of XAI 2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 25
or selecting a pixel or set of pixels (super pixels) for explanation. A perturbation-
based XAI approach may be thought of as a strategy that attempts to comprehend 
neural activity and the inﬂuence of particular characteristics on a corresponding class of outputs. It’s all about occlusion, ﬁlling operations, generative algorithms, ﬁltering, 
conditional sampling, etc. in the context of explainable algorithms. In most cases, the attribution representations may be generated using simply the forward pass, without the need of backpropagating gradients. The following are some perturbation-based 
approaches. 
(1) DeConv Nets by Zeiler et al. [ 11]: In this approach, the AlexNet model was 
trained on the ImageNet dataset, and layer-by-layer ﬁlter visualizations were performed as part of this research. Dataset bias and difﬁculties with few training sets were discovered after investigating feature generalization. DeConv Nets 
were used to visualize neural activity by occluding input instances. 
(2) Local Interpretable Model-Agnostic Explanations (LIME) by Ribeiro et al. [ 12]: 
In this approach, a point of interest was perturbed in order to create locally 
accurate explanations. Using LIME as an explanation in human/user research 
was shown to have a positive inﬂuence on an untrustworthy classiﬁcation system. Super pixels were found using Iterative perturbation to input data. 
(3) SHapley Additive exPlanations (SHAP) by Lundberg et al. [ 13]: SHAP outper-
formed LIME time and time again. In user research, SHAP explanations were shown to be in line with human explanations. Although SHAP values are useful 
in producing explanations, several recent research contend that they do not assist ﬁnal decision making. In a game theoretic framework, the author investigates feature correlations by eliminating characteristics. 
(4) Prediction Difference Analysis by Zintgraf et al. [ 14]: In this work, authors 
found a relevance value for each input feature, it was one of the initial steps in determining whether or not an attribute’s positive or negative association with 
the outcome was signiﬁcant. The ImageNet dataset was used to train a variety of models to better understand how different layers of deep nets support different 
output classes. The positive and negative association between individual char-acteristics and the output was discovered by the author via research of f and the removal of individual features from x. 
(5) Randomized Input Sampling for Explanation by Petsiuk et al. [ 15]: Authors 
researched on saliency maps using a method called randomized masking of inputs. ResNet50 and VGG16 were the networks that were used. 
(6) Randomization and Feature Testing by Burns et al. [ 16]: This work presented 
the examination of feature relevance via the use of hypothetical replacements of features. The author used the neural networks BERT and Inception V3.26 M. Gupta
3.2 Backpropagation- or Gradient-Based 
Explainable algorithms that rely on backpropagation do many forward passes through 
the neural network before using partial derivatives of the activations to create attribu-tions during the backpropagation stage. Some examples include class activation maps 
and saliency relevance maps. The preceding sub-section part discussed perturbation-based approaches, which employ ﬂuctuations in the input feature space to explain how f is assigned to a certain class in the output class c. A neural network’s backward infor-
mation ﬂow is used to analyze the effect and signiﬁcance of the input x on the output of the neural network. The bulk of gradient-based approaches, that are mentioned below, concentrate on either visualizing activation of particular neurons with strong 
effect or overall feature attributions moulded to input dimensions. Gradient-based XAI algorithms have an inherent advantage in the development of visual explanations 
that are intelligible to humans. 
(1) Saliency Maps: Gradient-based XAI is based on a set of approaches that were pioneered by the authors [
17]. Gradients, neuronal activity of particular layers 
using DeConv nets, etc., are shown as visuals by the authors. An important factor in DeConvNet’s performance was the weight placed on gradient values during backprop. Traditional CNNs would return zero results for negative gradients 
if they used ReLU activation. However, the gradient value is not trimmed at zero, in DeConvNets. This enabled for a more realistic depiction of what was 
happening. 
(2) Gradient Class Activation Mapping (CAM): For all pooling operations, most saliency algorithms employ the global average layer instead of max-pooling. 
CAM was used by Zhou et al. [
18] to locate class-speciﬁc image areas on an 
input picture in a single forward pass using a modiﬁed global average pooling algorithm. In order to create deeper CNNs and better visualizations, Grad-CAM 
[
19] and GradCAM ++ [ 20] enhanced the CAM process. Class-discriminative 
GradCAM may be used to locate the activity of neurons in a CNN network. Additionally, it provides counterfactual explanations to show which parts of a 
picture are detrimental to a given model’s output. Visual question answering (VQA), picture categorization, and segmentation all beneﬁt from the use of GradCAM. 
(3) Salient Relevance (SR) Maps: SR is a context-aware salience map based on the LRP of the input picture presented by Li et al. [
21]. In order to get started, 
you’ll ﬁrst need to discover the LRP relevance map for the picture of interest 
that has the same dimensions as the input images. Individual pixels are given a 
saliency value using a context-aware salience relevance map algorithm. Pixels 
are said to be “salient” if they can be distinguished from other patches of pixels 
on the same and various scales. This is done in order to distinguish between the image’s foreground and background elements. 
(4) Axiomatic Attribution Maps by Sundararajan et al. [ 22]: There were several 
new axioms for gradient-based approaches. Saliency and gradient timings input maps were improved and according to Ancona et al. [
23], a gradient approach2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 27
in which output corresponding to input is multiplied by that input is beneﬁcial 
in giving interpretable explanations for model results. However, in [ 22], the 
authors suggested Integrated Gradients (IG) and claimed that most gradient-
based techniques are lacking in certain ‘axioms’ which are desirable properties. 
Deconvolutional networks (DeConvNets) [ 11], DeepLift [ 24], and guided back-
propagation [ 25] all include back-propagation logic that some authors found 
contradicts basic assumptions of information theory. 
(5) PatternNet and PatternAttribution by Kindermans et al. [ 26]: In this work, back-
projection of estimated data to input space using an LRP-based approach was 
performed. Root point selection method is used for more accurate attributions. 
Improved gradient-based approaches were presented by Kindermans et al. in 
[26]. They developed PatternNet and PatternAttribution, which are able to esti-
mate the data component that triggered network activations. Similar to identi-
fying gradients, PatternNet uses a layer-by-layer back projection of the predicted 
signal to the input space in order to detect gradients. Unlike LRP , PatternAt-
tribution provides neuron-level attribution of input signals to their associated 
output classes. 
4 XAI Application Domains 
Nowadays industries are adopting the environment in which the demand for IoT 
devices XAI, on the other hand, may have a big impact on a wide variety of industries 
that depend on AI systems as shown in Fig. 4. In this section, we look at some of the 
possible areas in which explainable models is proving to be more useful. 
(1) Transportation: Automated cars have the potential to reduce trafﬁc fatalities and increase mobility, but they also present issues in terms of AI decision-
making that must be explained. Object classiﬁcation is critical in autonomous
XAI Transportation 
Defence 
Academic 
Research 
Architecture Law & Justice Construction Finance Healthcare 
Fig. 4 Different application domains for XAI28 M. Gupta
cars because they must make split-second choices based on what they see. When 
a self-driving automobile exhibits unexpected behavior due to a coding error. 
The results might be disastrous. This isn’t science ﬁction; a self-driving Uber in Arizona just killed a lady. In this case, it was the ﬁrst reported death of a 
completely automated car. Unidentiﬁed individuals said that the car’s software 
detected an item in front of it but handled it as if it were a plastic bag or a piece of tumbleweed blown by the wind as such, it is only via an understandable structure 
that such an incident may become clearer and prevent it from occurring. It is 
possible that the XAI might be used in transportation. There is still a long way to go before self-driving car behavior can be explained.
(2) Healthcare : Human life depends on the medical diagnostic model. How can 
we 
have faith in a black-box model’s instructions to treat a patient? About 
two decades ago, a computer model called an artiﬁcial neural network (ANN) 
was developed to help doctors decide whether or not to admit patients with 
pneumonia to a hospital or treat them as outpatients. Neural networks were shown to be signiﬁcantly more accurate than traditional statistical approaches 
in the early stages of research. In the end, it was discovered that the neural net 
had incorrectly predicted that patients with asthma who had pneumonia would 
have a reduced mortality rate and hence should not be hospitalized. Although 
this may seem paradoxical, the training data showed that asthma patients with pneumonia were admitted to the ICU, intensively treated, and survived [ 27]. It 
w
as subsequently concluded that the AI system should be abandoned since it 
was too risky to deploy in clinical practice. In order to avoid this critical issue, we must read the model. Preliminary effort has been done recently to make 
AI-based healthcare systems understandable [ 27, 28]. According to the rising 
quantity 
of these papers, there is an increasing interest and difﬁculty for utilizing 
XAI techniques in health care. 
(3) Law and Justice : AI has the ability to improve recidivism risk assessments 
and 
lower the costs of both crime and jail in the criminal justice system. It is, 
nevertheless, important to ensure that the criminal decision model is fair, honest, and non-discriminatory when it is used to estimate the likelihood of reoffending 
in court. Mr. Loomis was sentenced to jail because of the use of proprietary risk assessment software in Loomis v. Wisconsin [ 29]. Gender and ethnicity 
are 
taken into consideration while using “Correctional Offender Management 
Proﬁling for Alternative Sanctions: COMPAS,” the software at issue in this case. It was unclear to the judge how the algorithms were employed and what 
the causal audit procedure was. Despite the importance of being able to explain 
the process by which a judgement is reached in the legal system, relatively few 
attempts have been undertaken so far to do so [ 30–32]. 
(4) Finance : The use of artiﬁcial intelligence (AI) techniques in the ﬁnancial 
services 
industry may lead to advancements in wealth management, investment 
advising, and client service. However, there are concerns about data security and 
equitable lending with these new technologies. Loan providers have to adhere to strict regulations in the ﬁnancial business. Using AI-based algorithms in credit 
scores and models presents a big problem since it is more difﬁcult to offer2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 29
borrowers with the necessary “reason code”—the explanation of why they were 
refused credit. Because of an opaque ML algorithm’s output, denial is much 
more difﬁcult. At Equifax and Experian, exciting research efforts are underway to develop automated explanation codes and make AI credit score conclusions 
more understandable and auditor friendly [
33]. 
(5) Defense: Researchers in the military were the ones who started the present, well-known XAI project [
34], and the topic’s increased awareness is primarily due to 
the call for study and the solicitation of DAPRA Projects. The AI explainability dilemma is not exclusive to AI in the military. A paper from the MIT Technology Review examines the limitations of depending on autonomous systems for mili-
tary operations, [
35] Knight. This may lead to ethical and legal quandaries that 
are reminiscent to those encountered in the healthcare industry. With the DAPRA Ambitious XAI programme and a few other research efforts, the academic AI 
research community is strongly represented in this application space. XAI’s requirement is conﬁrmed by the work done in each of the mentioned ﬁelds. These kinds of studies, however, are still in their infancy, and much more study 
has to be done. Other potential uses for XAI include cybersecurity, educa-tion, entertainment, governance, and image identiﬁcation, to name just a few. 
Automated decision-making can cause harm in many different areas of life, including employment, insurance, and social beneﬁts, as well as housing and differential pricing of goods and services. Future of Privacy Forum [
36] illus-
trates the various areas where automated decision-making can do harm as well as the areas where providing automated explanation can turn them into trustful processes. 
5 XAI Challenges and Future Prospective 
How to develop more understandable models, how to build explanation interfaces, and what are the psychologic criteria for successful explanations are all connected research issues posed by this user-centered idea. To solve these difﬁculties, XAI 
research teams are creating new ML approaches to construct explainable models, as well as new HCI tools (such as visualization, language comprehension and gener-ation) for successful explanations. Researchers are also focusing on describing, 
expanding, and using psychologic theories of explanation to address the difﬁculties of XAI working. 
There have been major advances in the area of explainable artiﬁcial intelligence, 
but obstacles remain in the methodologies and theory as well as in the way expla-nations are applied. Using explanation approaches, we can learn more about how the AI model works. However, there are still some limitations to these approaches. 
For example, Heatmaps provide “ﬁrst-order” information, i.e., which input elements have been recognized as being relevant to predicting performance outcomes. But the 
relationship between these qualities, such as whether they are relevant on their own or only when they occur together, is still a mystery. There are several applications30 M. Gupta
where this knowledge is critical, such as ﬁnding groupings of brain areas that work 
together to solve a certain task (brain networks) rather than merely pinpointing the 
most signiﬁcant single voxels in each region. 
In addition, explanations are too simplistic. In a heatmap, you can see which pixels 
are most essential without having to think about more abstract ideas like what is in the picture or what the scene looks like. To comprehend the model’s behavior, humans must interpret the explanations. One of the trickiest parts of this process is interpreting 
what you’ve seen. Model behavior should be explained on a more abstract, human intelligible level by meta-explanations that collect information from these low-level heatmaps. Low-level explanations have been gathered and the semantics of brain 
representations have been quantiﬁed. Meta-explanations are a promising ﬁeld of study for future researchers. 
Primary research areas in which enabling technologies and approaches for XAI 
might possibly be found are: 
(1) Data science: Algorithms based on AI/ML are data hoarders, always searching 
for new sources of information to feed their insatiable appetite for data. This 
data is crucial to the backward route that aims to create better explanations and justiﬁcations. As a result, data science becomes a critical component in the 
process of explaining things. 
(2) AI/ML: To create an explanation, we believe that utilizing AI/ML as a computational process to explain AI/ML is an intriguing work path to follow. 
(3) Nature of Human Conduct: For the purpose of creating artiﬁcial explanations, it is worthwhile to ﬁrst simulate how people explain their actions and behaviors to one another. As a result, using human science ideas may result in new and 
better models that can be explained. 
(4) Human Computer Interaction (HCI): The way a person interacts with a computer may inﬂuence how well he or she understands and trusts the system. It is possible 
to apply approaches from the discipline of Human–Computer Interaction (HCI) to assist and build systems that are more transparent. 
Research on the use of explanations in the human–machine interface is a promising 
avenue for future development. Research on human elements in explainable AI has already begun (e.g., [
8]). Prerequisites for effective human–machine interac-
tion include constructing explanations with the correct user emphasis, which means asking the right questions in a certain manner. In spite of this, additional research is needed to optimize explanations for best human use. There is a lack of a formal and 
widely agreed-upon deﬁnition of what constitutes an explanation in the notion of explainable AI. The development of mathematically sound explanation techniques 
was a ﬁrst step in this direction. According to [
37], for example, the explanation issue 
is approached using Taylor decomposition as a theoretical framework. It’s another potential avenue toward the objective of building a general theory of explainable AI 
using axiomatic techniques [
38]. 
In the end, the utilization of explanations that go beyond imagery is an 
unsolved problem. To increase the model’s performance or minimize its complexity,2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 31
future work should demonstrate how explanations may be included into a wider 
optimization process. 
6 Conclusion 
Human-centered research on competences and knowledge might extend the func-
tion of XAI beyond the job of describing a speciﬁc XAI system and helping its users decide acceptable trust. XAIs have the potential to play important roles in 
society in the not-too-distant future. Some of these roles could include not only acquiring knowledge and imparting it to others, but also working with other agents to communicate knowledge, develop inter-disciplinary insights and mutual under-
standing, collaborate with others to teach individuals and other agents, and draw on previously known knowledge to speed up the discovery and application of new information. Because of this new social perspective on knowledge understanding 
and era, the future of XAI technology is just getting started. 
In addition to serving as a bridge between artiﬁcial intelligence (AI) and the 
general public, explainability is a valuable tool for identifying model and data biases, verifying predictions, ﬁne-tuning models, and acquiring new insight into the situation at hand (e.g., in the sciences). 
Because of this, it’s important to develop and choose XAI methods carefully as 
the current research landscape shows. Findings have shown that relying solely on typical explanation maps may not be helpful. Mission-critical applications may not 
beneﬁt from XAI because of human bias in visual explanations. The XAI assessment landscape seems to be improving as a result of recent advances in human-centered evaluations. 
References 
1. E. Tjoa, C. Guan, A survey on explainable artiﬁcial intelligence (XAI): towards medical XAI. 
arXiv:1907.07374 (2019) 
2. F.K. Dosilovic, M. Brcic, N. Hlupic, Explainable artiﬁcial intelligence: a survey, in 41st 
International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 210–215 (2018) 
3. R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods for explaining black box models. ACM Comput. Surv. 51(5), 1–42 
4. R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods for explaining black box models. ACM Comput. Surv. 51(5), 1–42 (2018) 
5. U. Bhatt, A. Xiang, S. Sharma, A. Weller, A. Taly, Y . Jia, J. Ghosh, R. Puri, J.M.F. Moura, P . Eckersley, Explainable machine learning in deployment, in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Association for Computing Machinery, New Y ork, NY , 2020), pp. 648–657. Retrieved from 
https://doi.org/10.1145/3351095.3375624 
6. M. Ribera, A. Lapedriza, Can we do better explanations? A proposal of user-centered explain-able AI, in Joint Proceedings of the ACM IUI 2019 Workshops Colocated with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI 2019), vol. 2327 (2019). CEUR-WS.org32 M. Gupta
7. A.B. Arrieta, N.D. Rodriguez, J.D. Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garcia, S. Gil-
Lopez, D. Molina, R. Benjamins, R. Chatila, F. Herrera, Explainable artiﬁcial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58, 82–115 (2020). Retrieved from 
https://doi.org/10.1016/j.inffus.2019.12.012 
8. I. Lage, E. Chen, J. He, M. Narayanan, B. Kim, S. Gershman, F. Doshi-V elez, An evaluation of the human-interpretability of explanation. arXiv preprint 
arXiv:1902.00006 (2019) 
9. I. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adversarial examples, in Proceedings of the 3rd International Conference on Learning Representations, ICLR (2015) 
10. A. Kurakin, I. Goodfellow, S. Bengio, Adversarial machine learning at scale, in Proceedings of the 5th International Conference on Learning Representations, ICLR (2016) 
11. D.M. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, ed. by D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars. Computer Vision—ECCV 2014 (Springer International 
Publishing, Cham, 2014), pp. 818–833 
12. T.M. Ribeiro, S. Singh, C. Guestrin, Why should I trust you? in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (New Y ork, USA, 2016), pp. 1135–1144 
13. M.S. Lundberg, I.S. Lee, A uniﬁed approach to interpreting model predictions, in Advances in Neural Information Processing Systems, pp. 4765–4774 (2017) 
14. M.L. Zintgraf, S.T. Cohen, T. Adel, M. Welling, Visualizing deep neural network decisions prediction difference analysis, in Proceedings of the 5th International Conference on Learning Representations, ICLR (2017) 
15. V . Petsiuk, A. Das, K. Saenko, RISE: randomized input sampling for explanation of black-box models, in British Machine Vision Conference BMVC (2018) 
16. C. Burns, J. Thomason, W. Tansey, Interpreting black box models via hypothesis testing. arXiv preprint 
arXiv:1904.00045 (2019) 
17. K. Simonyan, A. V edaldi, A. Zisserman, Deep inside convolutional networks: visualising image classiﬁcation models and saliency maps, in Proceedings of the 2nd International Conference on Learning Representations, ICLR—Workshop Track Proceedings (2013) 
18. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for discrim-inative localization, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE, 2016), pp. 2921–2929 
19. D. Batra, Grad-CAM: visual explanations from deep networks via gradient-based localization, in Proceedings of the IEEE International Conference on Computer Vision (2017) 
20. A. Chattopadhay, A. Sarkar, P . Howlader, V .N. Balasubramanian, Grad-CAM++: generalized gradient-based visual explanations for deep convolutional networks, in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV) (IEEE, 2018), pp. 839–847 
21. H. Li, Y . Tian, K. Mueller, X. Chen, Beyond saliency: understanding convolutional neural networks from saliency prediction on layer-wise relevance propagation. Image Vis. Comput. 83–84, 70–86 (2019) 
22. M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep networks, in 34th International Conference on Machine Learning, ICML 2017 (2017) 
23. M. Ancona, E. Ceolini, C. Oztireli, M. Gross, Towards better understanding of gradient-based attribution methods for deep neural networks, in 6th International Conference on Learning Representations, ICLR 2018—Conference Track Proceedings (2018) 
24. A. Shrikumar, P . Greenside, A. Kundaje, Learning important features through propagating acti-vation differences, in Proceedings of the 34th International Conference on Machine Learning, vol. 70(ICML17), p. 31453153 (2017). JMLR.org 
25. J.T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, Striving for simplicity: the all convolutional net, in 3rd International Conference on Learning Representations, ICLR 2015— 
Workshop Track Proceedings (2015) 
26. P .-J. Kindermans, K.T. Schutt, M. Alber, K.-R. Muller, D. Erhan, B. Kim, S. Dahne, Learning how to explain neural networks: patternnet and patternattribution, in 6th Interna-tional Conference on Learning Representations, ICLR 2018—Conference Track Proceedings (2018)2 Explainable Artiﬁcial Intelligence (XAI): Understanding … 33
27. R. Caruana, Y . Lou, J. Gehrke, P . Koch, M. Sturm, N. Elhadad, Intelligible models for 
healthcare: Predicting pneumonia risk and hospital 30-day readmission, in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1721–1730 (2015) 
28. A. Holzinger, C. Biemann, C.S. Pattichis, D.B. Kell, What do we need to build explainable AI systems for the medical domain? (2017). [Online]. Available: 
https://arxiv.org/abs/1712.09923 
29. J. Lightbourne, Damned lies & criminal sentencing using evidence based tools. 15 Duke Law Technol. Rev. Tech. Rep. 327–343 (2017). 
https://scholarship.law.duke.edu/dltr/vol15/iss1/16 . 
Accessed 6 June 2018 
30. S. Tan, R. Caruana, G. Hooker, Y . Lou, Detecting bias in black-box models using transparent model distillation (2018). [Online]. Available: 
https://arxiv.org/abs/1710.06169 
31. C. Howell, A framework for addressing fairness in consequential machine learning, in Proceedings of the FAT Conference Tutorials, pp. 1–2 (2018) 
32. R. Berk, J. Bleich, Statistical procedures for forecasting criminal behavior: a comparative assessment. Criminol. Public Policy 12(3), 513–544 (2013) 
33. Equifax, Equifax launches NeuroDecision technology (2018). [Online]. Available: https://inv 
estor.equifax.com/news-and-events/news/2018/03-26-2018-143044126 . Accessed 6 June 2018 
34. D. Gunning, Explainable artiﬁcial intelligence (XAI). Defense Advanced Research Projects Agency (DARPA). Accessed 6 June 2018. [Online] 
35. W. Knight, The U.S. military wants its autonomous machines to explain themselves, MIT Technology Review (2017). [Online]. Available: 
https://www.technologyreview.com/s/603 
795/theus-military-wants-its-autonomous-machines-to-explain-themselves . Accessed 6 June 
2018 
36. Future of Privacy Forum, Unfairness by algorithm: distilling the harms of automated decision-making (2017). [Online]. Available: 
https://fpf.org/wp-content/uploads/2017/12/FPF-Automa 
tedDecision-Making-Harms-and-Mitigation-Charts.pdf . Accessed 6 June 2018 
37. G. Montavon, S. Lapuschkin, A. Binder, W. Samek, K.R. Muller, Explaining nonlinear classiﬁcation decisions with deep Taylor decomposition. Pattern Recogn. 65, 211–222 (2017) 
38. M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep networks, in International Conference on Machine Learning (ICML), pp. 3319–3328 (2017)Chapter 3 
Explainable Artiﬁcial Intelligence (XAI): 
Conception, Visualization and Assessment Approaches Towards Amenable XAI 
Tasleem Nizam and Sherin Zafar 
Abstract For last decade, due to the accessibility of huge databases and recent 
advancements in deep learning methodology, machine learning systems have arrived 
at or transcended tremendous performance in a spacious variety of tasks. One can see this speedy development in speech analysis, image recognition, sentiment anal-ysis, strategic game planning and many more, for e.g. in medical ﬁeld, it’s used 
for diagnosing different diseases, like breast cancer etc., based on their symptoms. But many state-of- the-art models is facing lack of transparency and interpretability which is a major hindrance in many applications, e.g. ﬁnance and healthcare where 
visualization, interpretation and explanation for model’s decision is an obligation for trust. This is an implicit problem of the current techniques carried by sub-symbolism (e.g. Deep Neural Networks) that were not shown in the last hype of AI (speciﬁcally, 
rule based models and expert systems). Models underlying this problem come within the so-called Explainable AI (XAI) ﬁeld, which is extensively acknowledged as a racial feature for the practical deployment of AI models. As a result, explainable 
artiﬁcial intelligence (XAI) has turned into scientiﬁc interest in last recent years. So, this chapter epitomizes contemporary developments in Explainable AI that describes explainability in Machine Learning, constituting a ﬁction deﬁnition of explainable 
Machine Learning that envelopes such prior conceptual propositions with a consider-able focus on the audience for which the explainability is needed. Except of this deﬁ-nition, this chapter starts a confabulation on its various techniques that are essentials 
for analysing interpretability and explainability of Artiﬁcial Intelligence, and also gives a comparison between two medical experiments, that are based on predicting 
heart disease using disparate Explainable Artiﬁcial Intelligence techniques, which can give a lead for researchers as well as practitioners or newcomers in the ﬁeld of Artiﬁcial Intelligence for selecting suitable methods with Explainable AI to grasp
T. Nizam (B) 
Jamia Hamdard, New Delhi, India 
e-mail: tashu.nizam@gmail.com 
S. Zafar 
Jamia Hamdard, New Delhi, India 
e-mail: sherin.zafar@jamiahamdard.ac.in 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_3 3536 T. Nizam and S. Zafar
the advances of AI in their action sectors, without any previous bias for its dearth of 
interpretability. 
Keywords Explainable artiﬁcial intelligence ·Machine learning ·Interpretability ·
Explainability ·Black box model ·Post-Hoc method 
1 Introduction 
In the last decemvir, there were many tasks that were initially thought to be computa-
tionally impassable, but since 2012, artiﬁcial intelligence (AI) and machine learning (ML) systems have achieved tremendous human performance in many ﬁelds. 
Because of considerable hardware improvements combined with new optimiza-
tion algorithms these leadings in the AI ﬁeld were achieved. At the same time, there is too much growth in high quality open-source libraries also which allows developers 
as well as researchers to quickly code and test models which also result in great work in AI ﬁeld [
3]. With the upgrading in image classiﬁcation, speech recognition, clas-
sical (board) games, object detection and many more have edged to their procreation to the real-world applications outside research labs, mostly in the area of supervised learning. 
As the time passed, one can see advances in application demanding areas, such as 
bioinformatics, government, medicine, churn prediction and content recommenda-tion but in these applications also crucial trust-related problems are raised. In exten-
sions to additions in aforementioned areas, future applications will also combine interpretable science, reliable ML and cognitive assistance [
4]. Due to these changes, 
many ethical challenges will occur that society will have to accommodate fast in 
order to drive the developments to the positive directions and outcomes. More unfair 
wealth distribution might be done when automation will substantially change the job market. Generation of fake content and content recommendation teamed with other 
technologies will signiﬁcantly impact the social dynamics [
5]. Those algorithms 
will prescribe many things and that will alter human lives in ways, so humans will desire to trust them in order to obtain those prescriptions. Instead of this, to sustain 
trust [
6], i.e. reliability, explanatory justiﬁability, unbiasedness (fairness), usability, 
privacy etc, systems must amuse many criteria (assurances). Since humans are social 
creatures addicted to life in human communities such afﬁrmations are assumed due 
to bias towards human decision making. Due to novel status of Artiﬁcial Intelligence in our lives as well as being of the human formulation, it generates more disbeliefs. 
At starting, AI interpretation was easy and the previous years have endorsed 
the boost of opaque decision systems such as Deep Neural Networks (DNNs). The experimental success of Deep Learning models such as DNNs branches from a 
combination of adequate learning algorithms and their vast parametric space. DNNs are considered as complex black-box models due to latter space which is consist of hundreds of layers and millions of parameters. Transparency is reverse of black-
box-ness which means understanding or searching the technique by which model3 Explainable Artiﬁcial Intelligence (XAI): Conception … 37
works. As black-box Machine Learning (ML) models are widely being used to give 
essentials predictions in crucial contexts, the appeal for transparency is surging from 
the different stakeholders in AI [ 5, 6]. If we use or create any decision which are not 
legitimate, justiﬁable or that simply do not grant gathering accurate explanations of 
their behaviour, then it might be dangerous for us [ 7]. All explanations that support 
the output of a model are important, e.g., experts need many more information from 
the model than a simple binary prediction for encouraging their diagnosis in precision 
medicine. In general, people are restrained to follow techniques that are not precisely 
tractable, trustworthy and interpretable, which results demanding for ethical AI [ 7, 
8]. The systems will be progressively opaque if one only thinks about solely on its 
performance. This is correct in the sense that there is a trade-off between model’s transparency and its performance. However, if one works on the understanding of a system then it can lead to the correction of its dearth. While developing a ML 
model, its implementation can be improved by considering its interpretability as an additional design driver. This interpretability basically helps in impartial decision-making and can act as a guarantee that only useful variables produce the output 
[
9]. 
So, for removing the limitations of effectiveness of the recent generation of AI 
systems, explainable AI (XAI) gives a suite of ML techniques that:
•Generates more explainable models while keeping a high level of learning achievement.
•Empowers humans to appropriately trust, understand and properly balance the imminent generation of artiﬁcially intelligent partners [
10]. 
Previous surveys and overviews of interpretability in machine learning are given 
in [2, 3, 14, 15, 17]. In this chapter we survey the overtures in the explainability and 
interpretability of machine learning models using the supervised learning paradigm. 
This chapter is composed as follows: in Sect. 2 we deal with the deﬁnitions and 
preliminaries. In Sect. 3 we classify the techniques for interpretability and explain-
ability. Section 4 offers discussion and comparison of two research done using AI 
techniques which also discuss the methodology that can be used in XAI for accurate result with proper explanation. And at last, in Sect. 
5, we conclude the chapter which 
also discuss current state of research ideas. 
2 Preliminaries and Deﬁnitions 
Before processing with our literature survey, it is good to ﬁrst build a common point of understanding on which the term explainability stands for in the context of AI and, more speciﬁcally, ML. This is important aim of this section, i.e. to pause at the 
different deﬁnitions that have been used in regards to this concept.38 T. Nizam and S. Zafar
Terminology description: 
In some literature interpretability and explainability are interchangeable misused 
which impedes the formation of common grounds. There are important differences between these concepts. Expressing any procedure or action taken by a model with 
the purpose of detailing or clarifying its internal functions is considered as explain-ability [
10, 11]. Explainability is an active characteristic of a model. By contrast, 
interpretability denotes to a passive characteristic of a model implying to the level 
at which a given model makes sense for a human watcher. This characteristic is also 
considered as transparency. 
To compile the most commonly used terminologies, in this section we simplify 
the differences and sameness among terms often used in the XAI communities and ethical AI.
•Intelligibility (or Understandability): It represents the characteristic of a model to make a human grasp its functions—how the model works—without any commitment for explaining its algorithmic means using which the model operates 
data internally or one can say without discussing its internal structure [
12].
•Comprehensibility: The capability of a learning algorithm to express any model’s 
learned knowledge in a human knowable fashion is known as comprehensibility. 
It is specially used for ML models [ 13].
•Interpretability: The aligning of abstract concept into a domain that humans 
can make sense of is called interpretability. In other words. It is expressed as the 
capability to describe or to give the meaning in knowable terms to a human.
•Explainability: An interface between a decision maker and humans is known as 
explainability. It is considered as a notion for explanation. In other words, it works 
as deﬁnite proxy of the decision maker as well as conceivable to humans [ 13, 14].
•Transparency: If a model is understandable by itself, then it is considered to be 
transparent. 
In all the above deﬁnitions, understandability appears as the most important 
concept in XAI. Both interpretability and transparency are ﬁrmly tied to this concept, 
while transparency attributes the characteristics of a model to be understandable for people by its own whereas understandability measures the level to which a human 
can understand a decision taken by a model. The potential of the audience to grasp the knowledge contained in the model is considered as Comprehensibility. 
None of the aforesaid deﬁnitions is restrictive or speciﬁc enough to facilitate 
formalization. They implicitly build upon user’s preferences, expertise and other circumstantial variables.3 Explainable Artiﬁcial Intelligence (XAI): Conception … 39
3 Techniques for Explainability and Interpretability 
There are two divisions of methods to explainability and interpretability: integrated 
(transparency-based) and post-hoc. Interpretability can be enabled by using Trans-parency. By analogy Transparency is ported to algorithmic matters such as discrim-
ination and unfairness and it was a traditional ﬁrst step also for the preservation of rights in human-based institutions. But, day by day models in AI are getting more compound than human-based institutions and it becomes tough to get exact explana-
tion that users might be able to interpret. Also, human perspective, including our own, is not transparent to everyone and details in the form of interpretations and explana-tions may contradict from the real decision mechanism. In extension, transparency 
and predictive performance are contrary objectives and they have to be cop-out in a model [
15, 16]. In [ 20], it is deﬁned that it is not clear how much transparency needs 
in the lengthy run. If the systems are self-contained and robust it may not be essential. But, if they are component of other systems, then transparency can be needed for debuggability. 
Post-hoc interpretability documents information from already learned model and 
it does not strictly depend on how the model works. The beneﬁt of this method is that it does not impact working of the model which is considered as a black-box 
(BB). This is same mode to how people give justiﬁcations for their own outcomes, without completely knowing the exact working of their decision-making techniques. However, extra care must be considered in order to ignore systems that give plausible 
but false explanations. Such explanations could gratify laws like GDPR, but there is a problem of examining their accuracy [
17, 18]. 
3.1 Integrated Interpretability 
The best description of a normal model is the model itself; it perfectly depicts itself 
and is simple to understand [ 21]. This method is narrowed to the model families 
with lower ﬂexibility or complexity, such as decision trees rules and linear models. 
On the other hand, there are some model families such as support vector machines 
(SVM), artiﬁcial neural networks (ANN), random forests and boosted tree, which 
are considered opaque and their complexity block users from imaging the logic behind predictions. The latter are most often taken as black-boxes and they are 
handled with in a post-hoc manner. This arrangement between performance and transparency is imaginary depicted in Fig. 
1. The large part of task is done for 
classiﬁcation works. Different conditions can be implied on models in order to grow 
their interpretability. Model sparsity, monotonicity and model’s size are some of 
the properties that can be used for increasing interpretability. Even the selection of model family (representation) can be treated a constraint on models that inﬂuences 
the interpretability. Transparent models are both explainable and interpretable.40 T. Nizam and S. Zafar
Fig. 1 
Performance-transparency trade-off [
4] 
User-based observations on interpretability of tree algorithms decision rules tables 
in classiﬁcation were executed in [ 24, 25]. In the recent chapter, decision tables 
were found to be the simplest to use for inexperienced users and that the model 
size normally has negative impact on interpretability, conﬁdence and answer time. 
Interpretability can be divided into two sub-approaches: pure transparent and hybrid. 
In pure transparent method we are bound to use model families that are taken as 
transparent. Evolutionary programming was used in [ 22] to look for sets of inter-
pretable classiﬁcation rules with little number of rules and conditions. Interpretable decision sets are collection of independent if–then rules. Since each rule can be 
used independently, interpretation is simple. The model is constructed by optimizing objective that considers both interpretability and accuracy. Region-speciﬁc predictive models, Oblique treed sparse additive models were proposed in [
23]. They accom-
plished competitive performance with kernel SVMs while providing interpretable model. 
Hybrid technique combines transparent model families with black-box techniques 
in contemplation to get accurate trade-off between the predictive performance and the model interpretability. Combination of SVMs and logistic regression was used for credit scoring in [
24] in contemplation to enhance accuracy of the initial inter-
pretable model. Multi-objective training of hybrid classiﬁers was practiced in [ 25] 
to observe hybrid trees where certain leaves were replaced with black-box classiﬁers 
for enhancing accuracy at the cost of interpretability.3 Explainable Artiﬁcial Intelligence (XAI): Conception … 41
3.2 Post-Hoc Methods 
Predictive performance increases the use of complex, opaque models. This is done 
because of increased availability of data and hardware improvements. However, explainability and interpretability are topics that have to be properly addressed. 
Post-hoc explainability aims models that are not easily interpretable by design 
by resorting to distinct means to improve their interpretability, such as visual expla-nations, text explanations, explanations by example, feature relevance explanations 
techniques etc. Each of these methods envelops one of the simplest ways humans describe processes and systems by themselves. Furthermore, actual techniques, or actual bunch of methods are deﬁned to ease the future work of any researcher that 
aims to search for a speciﬁc method that suits its knowledge. Not only this, the classi-ﬁcation also involves the type of data in which the methods have been applied. There 
is one point to be noted that many methods might be justiﬁable for many different types of data, although the grouping only deals with the type used by the authors that proposed such method [
18–20]. Overall, post-hoc explainability techniques are 
categorized ﬁrst by the aim of the author (explanation technique e.g. Explanation by simpliﬁcation), then, by the technique used (actual technique e.g. sensitivity analysis) and ﬁnally by the category of data in which it was practiced (e.g. images). 
I.Text explanations accord with the problem of carrying explainability for a model by means of learning to build text explanations that help explaining the results 
from the model [
26]. Text explanations also involve every technique generating 
symbols that denote the working of the model. 
II. Visual explanation techniques for post-hoc explainability intents at visualizing 
the model’s action. Many of the visualization techniques existing in the literature 
come along with dimensionality reduction methods that permit for a human interpretable normal visualization. 
III. Local explanations handle explainability by separating the solution space and providing explanations to less complex solution subspaces that are admissible for the whole model. These explanations can be composed by means of methods 
with the differentiating attributes that these only describe part of the whole system’s working. 
IV . Explanations by example deals with the extraction of data examples that 
express the result produced by a certain model, enabling to get an improved 
understanding of the model itself. Similarly, to how people react when trying to describe a given process, explanations by example are actually centred 
in obtaining representative examples that catch the inner correlations and relationships found by the model being analysed. 
V. Feature relevance explanation methods for post-hoc explainability deﬁnes the inner working of a model by computing an important score for its managed variables. These scores compute the sensitivity (affection) a feature has upon 
the output of the model. A comparison of the scores among distinct variables42 T. Nizam and S. Zafar
discloses the effect provided by the model to every such variables when gener-
ating its output. Feature relevance methods can be considered to be an indirect 
technique to explain a model [ 3]. 
4 Empirical Analysis 
The growth in healthcare infrastructure of a nation plays a vital role in ﬁnding the health-related condition of life of its public. The growing trends in artiﬁcial intel-
ligence has aided the service of computational resources to boost the healthcare infrastructure globally [
1–4]. One such situation that has captured a lot of attention 
from research scientists is heart attack [ 5–8], controlling to the fact that 33% of 
deaths annually is because of it. However, the decision-making process in this ﬁeld still relies upon the guesswork of the cardiovascular expert or upon the expertise. This makes it deeply ambiguous to satisfy the quality of the decisions [
9]. Therefore, 
there is an urgency for a system to assist the process of cardiovascular risk judgement. The growth in the number of artiﬁcial intelligence (AI) fanatics have helped in the process by providing remarkable funding for AI-based research in medical decision 
making. Nevertheless, the most important concern in the technique has endured the explainability of the results produced by AI-based systems. Specially, in the case of medical diagnosis, it is too much important for the decisions to have a conceivable 
justiﬁcation. There are many artiﬁcial intelligence methods that can help healthcare decision support systems such as neural networks, genetic fuzzy systems and genetic programming etc. 
There are three important aspects of all AI systems need to be taken into consid-
eration: Reliability which refers to the accuracy of the results, Explainability which considers for giving relevant explanations of the results and Understandability that 
aims on the usability. With this perspective, in last decades, computer technolo-gies with machine learning techniques have made medical aid software as a support 
system for prior diagnosis of heart disease. Identiﬁcation of any heart related diseases at ﬁrst stage can avoid the death risk. Different Machine Learning methods are used in medical data to ﬁgure out the pattern of data and deciding prediction from them. 
Healthcare data are normally enormous in volumes and complicated in structure. Machine Learning algorithms are able to handle the massive data and mine them to ﬁnd the meaningful information. Machine Learning algorithms read from past data 
and make predictions on real time data. This kind of Machine Learning framework for coronary illness expectation can boost cardiologists in deciding quicker actions so more patients can get medicines within a smaller timeframe, thus saving broad 
number of lives.3 Explainable Artiﬁcial Intelligence (XAI): Conception … 43
4.1 Machine Learning 
Machine Learning is a branch of Artiﬁcial research [ 2] and has become a very trendy 
facet of data science. The Machine Learning algorithms are designed to execute a 
huge number of tasks such as classiﬁcation, prediction, decision making etc. To learn 
the Machine Learning algorithms, training data is needed. After the learning stage, 
a model is generated which is taken as an output of Machine Learning algorithm. After this, that model is tested and validated on a group of unseen real time test 
dataset. The ﬁnal accuracy of the model is then compared with the real value, which advocates the overall correctness of predicted result. 
There are many popular methods that are used with Machine Learning Techniques. 
Some of them which are used in Chaps. 1 and 2 are as follows: 
4.1.1 Support Vector Machine 
Support Vector Machine [ 11] is a classiﬁcation method of Machine learning, which 
is used to evaluate data and detect patterns in classiﬁcation and regression anal-ysis. SVM is typically think over when data is classiﬁed as two class problem. In this 
strategy, data is classiﬁed by getting the best hyper plane that segregates all data points of one class to another class. The higher edge or separation between the two classes is, the model is considered more better. Those data points that are lying on limit of the 
margin are considered as support vectors. The actual base of SVM is mathematical techniques/methods that are used to design complicated real-world problems. The author has selected SVM for his experiment because his dataset—Cleveland Heart 
Disease Dataset CHDD has many classes to predict based on different parameters. In SVM, by using a function called kernel (Kernels of SVM), mapping of training data is done. These are—quadratic kernel, linear kernel, Radial Basis Function kernel, 
polynomial kernel, Multilayer Perceptron kernel, etc. Beside from the kernel’s func-tionalities in SVM, few more techniques are available such as sequential minimal 
optimization, quadratic programming and least squares. While creating the model with SVM, most challenging task is method selection and kernel selection to avoid the issue of underﬁtting and overﬁtting. So SVM requires that its model should be 
tested and validated against actual data. 
4.1.2 Decision Tree Decision Tree Algorithm 
With the help of Machine Learning Algorithm, Decision Tree Decision Tree algo-
rithm is used to create the Classiﬁcation models. Decision Tree classiﬁcation algo-
rithm based on the tree-like structure. This is a type of supervised learning, where it is already known what to achieve. Both the numerical and categorical data can be 
applied on Decision tree algorithm. Decision tree has a root node, its branches and leaf nodes. The evaluation of Data is done on the basis of accessing path from the44 T. Nizam and S. Zafar
root to a leaf node. In [chapter index] they used a dataset—CHDD, which contains 
total of 283 tuples that were assessed down the decision tree. For the heart disease 
prediction, they possibly came to a positive or negative assessment. After that, these were compared to the real parameters to check for the false negatives/false positives 
which displays the accuracy, sensitivity and speciﬁcity of the model. 
4.1.3 Naive Bayes Algorithm 
Naive Bayes Algorithm, supervised machine-learning algorithm, is established on 
the Bayes’ Theorem [ 27], which considers that all the features are statistically inde-
pendent to each other’s. In [ 28], the Naive Bayes Classiﬁer is used with high dimen-
sionality of inputs data. For solving problems of computer vision application, Naive Bayes method is mostly used. In speciﬁcally, it gives good results as a classiﬁer. 
4.1.4 Random Forest Classiﬁcation 
Random Forest [ 29] is an association of uncut classiﬁcation-based trees. It provides 
outstanding performance related to number of real-life situations, as there is no 
effect of noise in the dataset of Random Forest Classiﬁcation and there is less risk of 
overﬁtting also. If we compare it to many other tree-based algorithms, it functions 
faster than the other algorithms and generally enhances accuracy for testing and validation data. In Random Forest Classiﬁcation, there are many options to match 
the performance of random forest while constructing a random tree. 
4.1.5 Neural Network 
Artiﬁcial Neural Networks are nearly clumsy electronic models based on the neural 
structure of the brain. 
The brain actually learns from experience. It is a proof given by nature that some 
problems that are above the limit of current computers are actually handled by small energy efﬁcient packages. By using this brain modelling one uses a less technical 
method to design machine solutions. These biologically encouraged techniques of 
computing are considered to be the next vital advancement in the computer ﬁeld. 
Computers have problems recognizing even easy patterns. But now, enhancement in 
biological research provides an initial understanding of the natural thinking mecha-
nism. This procedure of saving information as patterns, making use of those patterns, 
and then solving problems encloses a new ﬁeld in computing, which is called—Arti-ﬁcial Neural Network (ANN). ANN are those types of computers whose architecture 
is created after the brain. They typically made of hundreds of normal processing units which are wired together in a typical communication network. Each node or unit is a simpliﬁed model of real neuron that sends off a new ﬁres or signals, if it receives an 
adequately strong Input signal from another node/nodes to which it is attached. ANN3 Explainable Artiﬁcial Intelligence (XAI): Conception … 45
Fig. 2 Neural network [ 30] 
is computational model or mathematical model, also can be called as an informa-
tion processing paradigm which is motivated by the way biological nervous system, such as brain information system. ANN is consisting of interconnecting artiﬁcial 
neurones. These neurons are programmed like to imitate the properties of n biolog-ical neurons. These neurons work simultaneously to solve any speciﬁc problem. ANN is applied/used for image analysis, speech recognition, adaptive control etc. 
(Fig. 
2). 
4.1.6 Fuzzy Logic 
The concept fuzzy denotes to things that are not understandable or are clear. In the 
real world there are many situations where we face a situation when we can’t evaluate 
whether the state is true or false, their fuzzy logic gives very important ﬂexibility for reasoning. In this way, we can think about the uncertainties and inaccuracies of any 
situation. 
In the Boolean system truth value, 1 denotes the absolute truth value and 0 shows 
the absolute false value. But in the fuzzy logic, there is no concept for the absolute 
truth and absolute false value. But in fuzzy system, it has an intermediate value that 
represents which thing is partially true and partially false (Fig. 3). 
Fig. 3 Fuzzy logic [ 31]46 T. Nizam and S. Zafar
4.1.7 Genetic Algorithm (GA) 
A genetic algorithm (GA) is a type of heuristic search algorithm which is used to solve 
optimization and search problems. This algorithm is also a subset of evolutionary 
algorithms, which are used in computation. Genetic algorithms apply the idea of 
genetics and natural selection for giving solutions to problems. These algorithms are taken as having better intelligence than random search algorithms because they 
consider historical data to perform search to the best performing region within the solution space. GAs is also built on the characteristics of chromosomes and their genetic structure. There is ﬁtness function which helps in giving the features of all 
individuals within the population. The higher the function, the better the solution. 
4.1.8 Adaptive Neuro Fuzzy Inference System (ANFIS) 
Adaptive Neuro-Fuzzy Inference System (ANFIS) mixes properties/advantages of 
both Fuzzy Logic (FL) and Artiﬁcial Neural Networks (ANNs) in a single framework. 
It provides accelerated adaptive interpretation capabilities and learning capacity to 
model complex features and captures nonlinear relationships. ANFIS has been prac-
ticed in various different domains and given solutions to commonly happening prob-lems with improved space and time complexity. The key point of ANFIS is the 
accuracy using the precise fuzzy interpretability and modelling, which enhances its generalization ability. ANFIS has achieved importance amongst researchers for its robustness in modelling fuzzy sets into crisp inputs and providing crisp outputs from 
the fuzzy rules for reasoning purpose. In other words, ANFIS has to justice the accuracy-interpretability trade-off [
3]. 
In Chap. 2, researchers proposes ANFIS-GA algorithm takes the medical data 
of a patient to predict the risk of having heart attack. The dataset used in Chap. 2 
is the Cleveland dataset which have medical reports of 297 patients [ 16]. It shows 
14 features including sex, age, resting blood pressure, chest pain type, fasting blood 
sugar, cholesterol, maximum heart rate, resting electrocardiographic results, depres-
sion induced by exercise relative to rest, exercise induced angina, number of major vessels, the slope of peak exercise, the heart status, and the diagnosis of heart disease. 
136 out of 297 patients given in the dataset have some level of heart disease and 161 patients have no heart disease. 
The identiﬁcation of heart attack is divided into ﬁve different possibilities 
including at no risk (level 0), slight risk (level 1), average risk (level 2), high risk (level 3) and very high risk (level 4) of having heart attack. 40 instances are used 
for testing and 257 instances of the dataset are used for training. First of all, using Takagi Sugeno type fuzzy inference system, grid partitioning technique is applied to create initial membership functions which is based on the training set. Using Gaus-
sian membership functions, if–then rules are derived. After making an initial FIS, the algorithm is trained using ANFIS to recognize fuzzy parameters by processing the dataset given to it. ANFIS used in the algorithm produces a single-output Sugeno 
FIS and then integrates the least-squares and the backpropagation gradient decent3 Explainable Artiﬁcial Intelligence (XAI): Conception … 47
techniques for training FIS membership function parameters. In Chap. 2, the system 
is trained for 1000 epochs. This training process stops whenever the deﬁned epoch 
number is reached. In a novel technique and in next step, GA is used for optimizing the membership function parameters. As a result of bringing together these three algo-
rithms (the neural network, the fuzzy logic, and the genetic algorithm), for predicting heart attack a new classiﬁcation algorithm is generated. GA is set to minimize the cost function (CF) which is deﬁned as root mean squared error (RMSE) between the 
expected (desired) and the predicted heart attack level. 
CF = RMSE =   ⎡ll√ 1 
n nΣ
i=1 (DO i − PO i )2 an (1) 
where, DO = Desired Output and PO = Predicted Output 
N = Number of features provided in the dataset. 
These predictions generated by the algorithm can be classiﬁed into four classes. 
(1) YN = The case where a patient at a risk of having heart attack is predicted to 
be at no risk 
(2) YY = The case where the algorithm successfully predicts a patient being at a 
risk of having heart attack 
(3) NN = The case where the algorithm truly predicts no risk of having heart attack 
for a patient at level 0 
(4) NY = The case where the algorithm predicts being at a risk of having heart 
attack for a patient who is healthy. 
For evaluating the proposed evolutionary ANFIS-GA system, four evaluation 
functions including speciﬁcity, sensitivity, precision and accuracy are deﬁned to be used along with RMSE-the evaluation functions for the training set acquired as the 
speciﬁcity 79.1667%, the sensitivity 91.1504%, the precision 77.4436% and the accuracy 84.4358%. Additionally, speciﬁcity, sensitivity, precision and accuracy for the testing set achieved 81.25%, 79.1667%, 86.3636% and 80%, respectively. In this 
way, researches proved in Chap. 
2 that using ninefold cross validation, the above 
proposed algorithm’s performance was quite satisfactory for predicting heart attack. 
Speciﬁcity = NN NN + NY (2) 
Sensitivity = YY YY + YN (3) 
Precision = YY YY + NY (4) 
Accuracy = YY + NN YY + YN + NN + NY (5)48 T. Nizam and S. Zafar
In Chap. 1, researches have used an online dataset, i.e. UCI Repository, which 
have The Cleveland Heart Disease Dataset. This dataset contains 1025 instances in 
total, which have been used in this research work. Each data is considered based on 14 attributes. This dataset has missing values too, which is manipulated through data 
pre-processing using some statistical techniques. The Class Value 1, is represented as “tested positive for the disease”, and Class value 0 represents “tested negative for the disease”. There is a division in this dataset into certain percentage, such as 20% 
data has been taken as testing dataset and rest 80% data is taken as training dataset. 
Different attributes of the original dataset have some missing values also which can 
give an imprecise result and may degrade the accuracy of model also. To overcome 
this problem, by using a method, “mean of column”, missing values are replaced. 
This method basically replaces 0 with either mean values of neighbourhood values or taking the average of neighbourhood values. Then accordingly it converts the 
0 value with the newly calculated value. After that the values of the dataset were changed from Numeric to Nominal so that it can be dataset compatible with the ML Techniques used. 
The model is made in Weka which is a Data Mining Tool. The Waikato Environ-
ment for Knowledge Analysis (WEKA) is an open-source machine learning software 
which developed by Waikato University, New Zealand. The software easily executes 
different standard data mining tasks such as clustering, data pre-processing, regres-sion, classiﬁcation feature selection and visualization. It provides an easy environ-
ment to load data in the form of URLs, ﬁles or databases. It examines and conceives the true positive, confusion matrix, recall, precision and false negative etc. in a convenient way. 
Four accuracy measures have been taken for comparison of the four models, they 
are as follows: 
(1) Precision or Positive Predictive Value: It is the average probability of relevant 
retrieval. 
Precision = Number of true positives/Number of true positives 
+ False positives (6) 
(2) Recall: It is the average probability of complete retrieval. 
Recall = True positives/True positives + False negative.(7) 
(3) Accuracy: The accuracy of a classiﬁer is given as the percentage of total correct 
predictions divided by the total number of instances. 
Accuracy =  ⎡
Number of True Positives + True Negatives⏋
/[Total Instances ] (8) 
(4) ROC (Receiver Operating Characteristic) Area: ROC depicts the performance 
trade-off between the true positive rate (TPR) and false positive rate (FPR) of a3 Explainable Artiﬁcial Intelligence (XAI): Conception … 49
classiﬁcation model. 
TPR = [Number of True Positives ]/[Number of True Positives 
+False Negatives⏋
(9) 
FPR = [Number of False Positives ]/[Number of False Positives 
+True Negatives⏋
(10) 
Classiﬁer is generally taken as “GOOD” when ROC value is less than 0.80, “FAIR” 
when ROC value is 0.77. The ROC value which is very close to 1 is considered as 
best model with high accuracy. 
At the end of experiment, researchers found that Random Forest and SVM 
performed very well in comparison to Decision tree and Gaussian Naive Bayes. Model developed with SVM produces 98% accuracy which is approximately 13% 
greater than Decision tree and 8% greater than the Naïve Bayes. Similarly, model 
build with Random Forest generates the best prediction result with 99% accuracy, 
which is itself more accurate than our second best SVM model for heart disease 
prediction. Unfortunately, decision tree is not suitable for this dataset. 
5 Conclusion 
By presenting this research, we have tried to analyse the different machine learning 
techniques and to predict if someone in particular, given different individual 
indications and attributes, will get coronary illness or not. 
In Chap. 1, researchers used Cleveland dataset for heart diseases. This dataset 
contains 1025 instances and used percent split to divide the data into two categories which are testing and training datasets. They have also considered 14 attributes and implemented four different algorithms, i.e. SVM, Random-forest, Decision Trees and 
Naïve Bayes, to check the accuracy. By the end of this research, they have discovered that maximum accuracy level is achieved by using Random Forest in dataset whereas Decision Tree is playing out the least with an accuracy level. 
In Chap. 2, a novel Adaptive Neural Fuzzy Inference System (ANFIS)-Genetic 
Algorithm (GA) algorithm for predicting heart attack is proposed. By using the explainablity of fuzzy rules, the optimization power of GA and training capability of 
neural network, the algorithm was executed on the Cleveland dataset. After that, the trained Fuzzy Inference System (FIS) obtained from ANFIS was given into GA to optimize the membership function parameters. As a result, the Root mean Squared 
Error (RMSE) between the predicted ones and the expected results obtained from the algorithm reduced from 0.82754 to 0.75372. The efﬁciency of the proposed algo-rithm was analysed by evaluation functions such as speciﬁcity, sensitivity, accuracy, 
precision and RMSE.50 T. Nizam and S. Zafar
The above results showed that the performance of the ANFIS-GA algorithm 
in predicting heart attack was much satisfactory because this algorithm provides 
explainable result. Providing explainable results are very useful for clinicians and patients. This algorithm was designed in such a way that it gives matrices corre-
sponding to the predicted patients in different classes of having heart attack. Detecting the attributes with highest signiﬁcance and representing the satisfactory performance of the algorithm on the new model may be taken as a good criterion for giving 
explainable predictions based on the relevance of the features. 
References 
1. V . Sharma, S. Yadav, M. Gupta, Heart disease prediction using machine learning techniques, in 
2020 2nd International Conference on Advances in Computing, Communication Control and Networking (ICACCCN), pp. 177–181 (2020). 
https://doi.org/10.1109/ICACCCN51052.2020. 
9362842 
2. M. Aghamohammadi, M. Madan, J. Hong, I. Watson, Predicting heart attack through explain-able artiﬁcial intelligence, pp. 633–645 (2019). 
https://doi.org/10.1007/978-3-030-22741-
8_45 
3. A. Barredo Arrieta, N. Diaz Rodriguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado González, S. Garcia, S. Gil-Lopez, D. Molina, V . Benjamins, R. Chatila, F. Herrera, Explainable artiﬁcial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58 (2019). 
https://doi.org/10.1016/j.inffus.2019.12.012 
4. F. Došilovi´ c, M. Brcic, N. Hlupic, Explainable artiﬁcial intelligence: a survey (2018). https:// 
doi.org/10.23919/MIPRO.2018.8400040 
5. S. Pouriyeh, S. Vahid, G. Sannino, G. De Pietro, H. Arabnia, J. Gutierrez, A comprehensive investigation and comparison of machine learning techniques in the domain of heart disease, in 2017 IEEE Symposium on Computers and Communications (ISCC) (Heraklion, 2017), pp. 204– 207. 
https://doi.org/10.1109/ISCC.2017.8024530 
6. C. Raju, E. Philipsy, S. Chacko, L. Padma Suresh, S. Deepa Rajan, A survey on predicting heart disease using data mining techniques, in 2018 Conference on Emerging Devices and Smart Systems (ICEDSS) (Tiruchengode, 2018), pp. 253–255. 
https://doi.org/10.1109/ICEDSS.2018. 
8544333 
7. Y .-X. Wang, Q.H. Sun, T.-Y . Chien, P.-C. Huang, Using data mining and machine learning techniques for system design space exploration and automatized optimization, in Proceedings of the 2017 IEEE International Conference on Applied System Innovation, vol. 15, pp. 1079–1082 (2017) 
8. S. Chattopadhyay, A neuro-fuzzy approach for the diagnosis of depression. Appl. Comput. Inform. 13, 10–18 (2017) 
9. A.M. Sagir, S. Sathasivam, A novel adaptive neuro fuzzy inference system based classiﬁcation model for heart disease prediction. Pertanika J. Sci. Technol. 25(1) (2017) 
10. D. Richard, D. Mala, GI-ANFIS approach for envisage heart attack disease using data mining techniques. Int. J. Innov. Res. Adv. Eng. (IJIRAE) 5 (2018) 
11. D. Bonanno, K. Nock, L. Smith, P. Elmore, F. Petry, An approach to explainable deep learning using fuzzy inference. Next-Generation Analyst V . (2017) 
12. B.W. Israelsen, ‘I can assure you ... that it’s going to be all right’—a deﬁnition, case for, and survey of algorithmic assurances in human-autonomy trust relationships. ArXiv:170800495 Cs Stat, August 2017 
13. T.T. Nguyen, P.-M. Hui, F.M. Harper, L. Terveen, J.A. Konstan, Exploring the ﬁlter bubble: the effect of using recommender systems on content diversity, in Proceedings of the 23rd International Conference on World Wide Web (New York, NY , USA, 2014), pp. 677–6863 Explainable Artiﬁcial Intelligence (XAI): Conception … 51
14. K. Crawford, Opinion I Artiﬁcial Intelligence’s White Guy Problem,” The New York Times, 
25-Jun-2016. 
15. B. Goodman, S. Flaxman, European Union regulations on algorithmic decision-making and a ‘right to explanation’. ArXiv 1606.08813 Cs Stat, Jun 2016 
16. S. Khedkar, V . Subramanian, G. Shinde, P. Gandhi, Explainable AI in healthcare. SSRN Electron. J. (2019) 
17. E. Tjoa, C. Guan, A survey on explainable artiﬁcial intelligence (XAI): toward medical XAI. IEEE Trans Neural Networks Learn Syst 32(11), 4793–4813 (2021). 
https://doi.org/10.1109/ 
TNNLS.2020.3027314 
18. M.T. Ribeiro, C. Guestrin, Why should I trust you? Explaining the predictions of any classiﬁer, pp. 1135–1144 (2016) 
19. D. Wang, Q. Yang, A. Abdul, B.Y . Lim, Designing theory-driven user-centric explainable AI, in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems—CHI ’19, pp. 1–15 (2019) 
20. G. Marcus, Deep learning: a critical appraisal. ArXiv: 1801.00631 Cs Stat, Jan 2018 
21. S. Lundberg, S.-I. Lee, A uniﬁed approach to interpreting model predictions. ArXiv:1705.07874 Cs Stat, May 2017 
22. A. Cano, A. Zafra, S. Ventura, An interpretable classiﬁcation rule mining algorithm. Inf. Sci. 240, 1–20 (2013) 
23. J. Wang, R. Fujimaki, Y . Motohashi, Trading interpretability for accuracy: oblique treed sparse additive models, in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (New York, NY , USA, 2015), pp. 1245–1254 
24. T. Van Gestel, B. Baesens, P. Van Dijcke, J. Suykens, J. Garcia, T. Alderweireld, Linear and non-linear credit scoring by combining logistic regression and support vector machines. J. Credit Risk I (2005) 
25. R. Piltaver, M. Lustrek, M. Gams, Multi-objective learning of accurate and comprehensible classiﬁers—a case study (2014) 
26. A. Bennetot, J.L. Laurent, R. Chatila, N. Diaz-Rodriguez, Towards explainable neural symbolic visual reasoning, in NeSY Workshop IJCAI 2019 (Macau, China, 2019) 
27. https://en.wikipedia.org/wiki/Bayes27_theorem 
28. https://en.wikipedia.org/wiki/Naive_Bayes_classiﬁer 
29. https://towardsdatascience.com/understanding-random-forest-58381e0602d2 
30. Images of neural network—Bing images 
31. Fuzzy Logic | Introduction—GeeksforGeeksChapter 4 
Explainable AI (XAI): A Survey 
of Current and Future Opportunities 
Meet Kumari, Akshit Chaudhary, and Yogendra Narayan 
Abstract Artiﬁcial Intelligence is the technology that is being used to develop 
machines that could work like humans or simply can have the intelligence relatable 
to that of humans. But the development of this kind of technology model that mimics 
humans involves a lot of complex calculations and complex algorithms that are difﬁ-
cult to explain and understand. For this problem, the concept of explainable artiﬁcial 
intelligence (XAI) is developed and introduced. It is the technology that is developed 
to ease the understanding process of machine learning solutions for humans. It is the concept that is being developed for making it convenient for humans to understand 
and interpret machine language. Black model machine learning (ML) algorithms are very hard to understand for humans who have not developed them. AI models that involve the methods like genetic algorithms or deep learning concepts are very 
difﬁcult to understand. It sometimes becomes a very hard task for the domain experts too to understand the ML algorithms of the black block models, so the need for the development of this type of technology was felt. Many times, results are developed 
with very high accuracy are quite easy to understand for the domain experts. But Explainable artiﬁcial intelligence has a great potential to make a change in domains like ﬁnance, medicines, etc. It plays a vital role where it is important to understand 
the results to build trustworthy algorithms. XAI can play a great role in “third-wave AI systems” which include machines that can interact directly with the environment and that can build explanatory models that allow them to develop the characteris-
tics of real-world phenomena. XAI has the potential to play a great role where the organizations need to build trustworthy AI models and to make them trustworthy the 
explainability of the AI models should be there for others as well. This technology is developed primarily to make AI understandable to those who are practitioners. This book chapter presents a wide and insightful view of XAI and its application in 
various ﬁelds. This chapter also includes the future scope of this technology and the need for the growth of this type of technology.
M. Kumari (B) · Y. N a r a y a n 
Department of Electronics and Communication Engineering, Chandigarh University, Mohali, 
Punjab, India 
e-mail: meetkumari08@yahoo.in 
A. Chaudhary 
Department of Computer Science and Engineering, Chandigarh University, Mohali, Punjab, India 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_4 5354 M. Kumari et al.
Keywords Explainable artiﬁcial intelligence ·Artiﬁcial intelligence ·Machine 
learning ·Explainable methods ·Deep learning 
1 Introduction 
Artiﬁcial intelligence (AI) can be aggrandized as Artiﬁcial Intelligence which means 
the study of science that involves the invention of intelligent machines, computer programs being the most important. It is about using computers to collect and know 
about the intelligence that humans have, but it doesn’t restrict itself to biologically observable methods only. It is a technology invented opposite to the natural things using different tools or machines or machine learning codes. It was previously thought 
that AI was developed just made to mimic human nature but with time its use and importance are growing in various ﬁelds. The use of AI is growing in various ﬁelds such as the web, self-driven cars, robotics, etc. It is being developed for various 
purposes to reduce the human effort to do the hard task and also to reduce the time taken to complete the heavy task. The year 1956 is usually considered to be as the birth 
year of Artiﬁcial intelligence because of the famous conference held at Dartmouth College. In the year 1955, ﬁrst AI-based system was developed by Allen Newell, and Herbert A Simon, which was known as Logic Theorist. Another approach to symbolic 
AI appeared in the 1960s and is further named the Knowledge-Based approach. It was derived during the project development process. This project was developed to execute Dendral Expert system. Gradually, its usage and applicability are growing 
in various tasks. AI plays a very important role in the ﬁeld of robots. V arious robots are developed that mimic human activity. Many advancements are done every day in search of solutions to various problems. V arious kinds of logical algorithms are 
developed and used to deal with various kinds of problems and develop solutions for them [
1]. 
AI is an imitation of human intelligence or knowledge in artiﬁcial machines 
designed to make them behave and think the same as humans. The term can also be used for any computer that displays features of human intelligence, such as learning, thinking, and problem-solving. The ability to think and act with the best possible 
chance of attaining a certain goal is the right quality of practical wisdom. Machine learning is a subset of artiﬁcial intelligence that explains the idea that computer 
programs can automatically learn and adapt to new data without human help. In-depth learning algorithms allow this automatic learning by importing a lot of random data, including text, images, and video [
1, 2]. 
1.1 Understanding Artiﬁcial Intelligence 
The ﬁrst thing which comes to the mind of most people when they hear the word 
intelligence is usually robots. The reason behind this is that high-quality movies4 Explainable AI (XAI): A Survey of Current and Future … 55
often feature machines that behave like humans and that cause destruction in the 
world. But nothing could be further from the truth. Artiﬁcial intelligence wishes that 
human intelligence could be described in a way that makes it become easier for a computer to solve complex problems and perform heavy calculations or tasks. Artiﬁ-
cial intelligence aims to imitate processes in humans. When it comes to deﬁning clear processes, such as learning, thinking, and perception, researchers and developers in the ﬁeld make rapid progress unexpectedly. Some people think that designers will 
soon be able to create better programs than people can read or understand right now. Others, however, continue to hold onto this view because all psychological processes involve quantitative judgment inﬂuenced by human knowledge. The terms used to 
describe artiﬁcial intelligence in the past become obsolete as technology advances. For example, machines that perform simple math or visualization of text are no longer considered a combination of artiﬁcial intelligence because we now consider these 
skills as part of any computer [
3]. 
1.2 Advancement of the AI 
With the advancements of AI in various ﬁelds, the major problem that comes out is the lack of interpretability and trust. These factors are the major drawbacks that 
make its applicability difﬁcult in healthcare and ﬁnance where the need for a rational model to make decisions is important for trust. AI has achieved increasing momentum in its application in many areas to deal with increased complexity, scalability and 
automation, which are also penetrating digital networks today. The rapid increase in complexity and sophistication of AI-powered systems has evolved to such an extent that people do not understand the complex mechanisms by which AI systems work 
or how they make certain decisions—a particular problem when AI-based systems calculate outputs that are unexpected or seemingly unpredictable. This is especially true for opaque decision-making systems, such as those using deep neural networks 
(DNNs), which are considered complex black-box models as shown in Fig. 
1. 
Fig. 1 Black box model illustrative diagram56 M. Kumari et al.
1.3 Special Considerations 
Artiﬁcial intelligence has received criticism from both the scientiﬁc community and 
the general public since its inception. Another theory that emerges is that machines will evolve to the point where people will not be able to keep up with them and will 
move on its own, re-establishing itself in terms of power. Another is that technology has the power to be armed and can attack people’s privacy. Some controversies have focused on artiﬁcial intelligence and whether robots and other intelligent machines 
should be granted the same human rights. Self-driving cars have caused controversy because their cars are usually built considering the small number of accidents and injuries. These vehicles will determine which route would cause the least damage if 
given the opportunity to choose between colliding with one person at a time. 
1.4 How Artiﬁcial Intelligence Can Affect Human Activities? 
There are concerns that people may be forcibly ﬁred from their jobs as more and more businesses try to create certain jobs by using smart devices. Self-driving cars 
may eliminate the need for taxis and car-sharing services, and manufacturers may quickly switch workers into machines, further avoiding the need for human skills. This concept will affect the life of the people who drive cars. The advancements in 
the ﬁeld of AI-based machines are done to reduce human efforts but at the same time, it tends to reduce the job roles for humans. In the upcoming years, it is the tendency that many decisions will be made by intelligent machines, and the human jobs role 
in taking decisions would decrease. 
2 Applications of AI 
AI is used for performing a variety of tasks in various ﬁelds. It can be applied to various processes in a variety of industries and areas. In the ﬁeld of health care, AI 
is evaluated and monitored for surgeries in the operating room, drug management, and a variety of patient care. Other incidents of smart machines include self-driving cars and computers playing chess. These machines have to consider all the factors 
for making decisions because every action has an effect on the outcome. 
Figure 2 shows the applications of AI in different ﬁelds. For self-driving cars 
to work in a way that avoids collisions, a computer program should calculate all 
external data and consider it. Fraudulent operations are used in the banking and 
ﬁnancial sectors to identify and signal suspicious behaviors, such as the use of an unusual debit card and important accounting, that beneﬁt the bank fraud department. 
AI tools also help to simplify and facilitate the trading that is done by developing the provision, demand, and price of securities easy to estimate. It can be used in several4 Explainable AI (XAI): A Survey of Current and Future … 57
Fig. 2 Some of the major applications of artiﬁcial intelligence
ﬁelds without looking at its computer-related features. Some of these are listed below 
[4], Fig. 2. 
•Speech recognition: Computer speech recognition, automatic speech recognition (ASR), or text-to-speech recognition skills are the skills that use a natural language 
processing system (NLP) to convert a person’s speech into a text format. To make voice search, many devices integrate such algorithms into their systems. For example, Siri. Artiﬁcial Intelligence has played a very important role when it 
comes to disability not only physically or partially or for those who are sometimes unconscious or even learning. To get to the world where you are and where you are 
known, the blind can just talk and Artiﬁcial Intelligence will do the necessities. The illiterate get the edge that they do not need to learn to read or write as they can speak their questions or instruct the software to perform certain tasks such as 
calling someone and Artiﬁcial Intelligence can answer questions and make calls.
•Customer service: In today’s world, human agents are being replaced by tangible agents when it comes to customer care. They respond well to frequently asked 
questions (FAQs) for common questions, offer personal advice, suggest user sizes, best-selling products, and change the way of thinking about customer engagement across all websites and forums. Example Messages for bots, Slack, and Facebook 
Messenger. While visiting e-commerce sites, we are confronted with a host of visually impaired agents who ask, “How can I help you?”. These message bots narrow the gap between Queries and Solutions and strengthen the sales process. 
They save our time as we do not need to scroll down to read the description or get out of the app to ask our questions via email.58 M. Kumari et al.
•Computer view: This Artiﬁcial Intelligence feature helps computers and systems 
to extract important information from digital images, videos, and other visual 
inputs, and based on this audio and visual input, it can take action. It differs from image recognition functions in its ability to provide recommendations. Computer 
vision is developed using convolutional neural networks and it is used for tagging images on social media, radiology image in health care, and self-driven vehicles within the vehicles industry. It has made it easy to contact our friends on social 
media and forward a photo or post to them.
•Automotive industry: The automotive industry has beneﬁted greatly from this new use of Artiﬁcial Intelligence. It has made it easier for beginners to drive cars 
with this AI feature.
•Recommendation engines: Artiﬁcial Intelligence algorithms also assist compa-nies in ﬁnding data trends using previous data management which can help in 
the development of highly productive cross-marketing action plans. This can also help in making additional relevant suggestions to customers while undergoing the payment process for online shopping. It empowered businesses and consumer 
goods as AI-assisted them in selecting what to recommend to the consumer so that opportunities to purchase goods and services could be increased. For example, if 
we search for a particular type of shirt on any e-commerce website, then there is a high chance that we will encounter the same type in our standard online drives that force us to think about it, often, which affects our way of thinking and organizing.
•Automatic stock trading: Automated stock trading is designed to improve the portfolio of stocks. Artiﬁcial Intelligence shapes its future as these commercially-driven AI platforms are trading billions of dollars a day without human interven-
tion. Robo Advisors analyze a lot of data, facts, problems, threats, etc. with high accuracy and reduce risk. Only Artiﬁcial intelligence has made the industry faster with rocket fuel. Many companies have introduced programs based on the end of 
Artiﬁcial Intelligence after considering their growing demand and high success.
•Language Learning: Artiﬁcial intelligence has become an integral part of manu-facturing, language learning, e-commerce, ﬁntech, education, skills development, 
medicine, electricity, and the automotive industry in our daily lives. Intelligence startup in learning a foreign language has opened the way for personal education. Integration of Artiﬁcial Intelligence into learning.
•Medicine: Artiﬁcial intelligence can be used in almost every part of medicine. It helps in collecting patients’ data with great accuracy and minimal possibilities of error. When physicians take the readings themselves, being a human tendency to 
ignore some lesser important facts and ﬁgures, they make mistakes. Even getting the medicine prescriptions written using Artiﬁcial Intelligence is highly appreci-
ated as many patients face adverse consequences including cases of death due to doctors’ poor handwriting making it difﬁcult for the pharmacists to ﬁgure out the correct medicines and doses. Researchers in the ﬁeld of AI has devised various 
kind of tools to solve various problems using probability and other mathematical operations. Most of the operations are done by various logic developed. The oper-ations or the algorithms to develop artiﬁcial intelligence help in the innovation of 
many new things. The researchers or the developers develop after examining a lot4 Explainable AI (XAI): A Survey of Current and Future … 59
of possibilities. When the researchers or the developers develop the algorithms 
of the solutions using different operations these developed algorithms are too 
complex that it becomes a very difﬁcult task for the researchers to explain them to the other researchers or developers in the ﬁeld. It becomes a very important task 
to explain it or reproduce it to others for further development when a trustworthy algorithm has to develop for a particular task. Healthcare is also being devel-oped in a variety of areas with computer vision as it is easy to detect unwanted 
diseases or changes in our body structures. To make the advancements more reli-able and trustworthy these algorithms and logic should be developed in such a way that they are easy to interpret by others. To deal with the problems due to the 
complexity of the algorithms, the concept of Explainable Artiﬁcial Intelligence was introduced. It is developed to make it easy for the researchers to explain their developments or research work more easily as it is sometimes very difﬁcult for 
the ﬁeld professionals also to understand the logic or the algorithms behind it. 
3 Importance of AI in Health Industry 
In recent decades large amounts of datasets, high computational powers, and machine learning systems have developed for high-performance tasks. Artiﬁcial intelligence is 
an approach toward the development of technology-based on computational learning and intelligence. AI has the potential of analysing and solving the complex prob-lems of the medical industry. These days AI is playing an important in the medical 
ﬁeld as the medical industry faces challenges in analyzing and application of a lot of knowledge to solve various complex problems. The advancement of AI in the 
ﬁeld of medicine is to help clinicians to formulate the diagnosis, make complex decisions, and in the prediction of the results. It is developed to assist healthcare workers to perform their tasks easily and accurately. In the case of AI development 
for the medical industry, the role of Explainable Artiﬁcial Intelligence becomes more important to deal with complex predictions and therapeutic decisions. In the medical ﬁeld, the algorithms of the complex calculations need to be checked to make 
trustworthy and for this, the complex calculations of the AI algorithms need to be developed in such a way that they are easy to understand. It is very important to deal with complex calculations. Artiﬁcial neural network (ANN) is one of the most 
popular AI techniques used in the ﬁeld of medicine. These computational tools are the tools that are by biological systems. These systems consist of various computer machines connected known as neurons that are capable of data representation and 
processing simultaneously. Their various abilities to retain historical data or exam-ples or analyzing capability and many others make them very attractive and important analytical tools. But the development of AI tools needs precision and to make them 
reliable the algorithms or the codes need to be easily explainable to others to make them reliable [
5, 6]. 
The use of AI is increasing day by day in the ﬁeld of neuroscience. Different 
Artiﬁcial intelligence models help in the interpretation of multimodal datasets that60 M. Kumari et al.
have the potential to provide unbiased insights into brain function and detection of 
disorders of the brain. The ability of AI to make precise classiﬁcations and predictions 
is there but, in some cases, it is unable to explain how inputs and the results are relatable to each other. One big challenge for humans is the inability to keep a check 
and modulate neural activities in real-time. To deal with the complex problems in the ﬁeld of neuroscience, advanced AI solutions need to be developed so that the resulting models are able to are capable of interpreting, modulation, and sensing from 
a huge database. New approaches to deal with these complexities are developed using machine learning. Explainable AI has developed a new way with techniques in which AI and ML complex algorithms are combined to develop easily explainable solutions. 
Data-driven and theory-driven models both have a contrast. The data-driven models are more accurate in terms of prediction while the theory-driven models are more explainable. In general, the models with high performance are less explainable, while 
the explainable models are generally less accurate. These are the general issues that the ML community face. These problems can be solved by using models that are simple and easier to explain or by building appropriate models. 
4 Introduction to Explainable Artiﬁcial Intelligence 
Explainable AI is the form of AI in which the results are developed in such a way that they are easy to understand by people other than the developers of that algorithm. If humans have to accept any algorithm for a particular task, they need to have trust in 
it, and to build trust the algorithms developed should be developed in such a way that they are easily understandable and transparent. To make an algorithm trustworthy transparency becomes a very important aspect for which the algorithm needs to be 
easy to interpret or explain. For the purpose of extending transparency and trustwor-thiness, the system was developed which was known as Truth Maintenance System. This system tracks the reasoning and helps in the justiﬁcation of the conclusions. 
Latest AI approaches like deep learning and machine learning are very complex and non-transparent in nature. To cope with this opaque nature and make the algorithms easy to interpret various types of explainable models are developed like LRP or 
Layer wise relevance propagation. In addition to this, work has also been done to make glass-box models that are much more transparent for the purpose of inspection. XAI models are developed having a primary focus on making it more understandable 
to the people who practice AI rather than making it easier for the end-users. XAI acts as a core for human-centric machine learning developments. There are various 
types of machine learning (ML) models like rule lists, decision trees, simple naïve Bayes Classiﬁers, etc. With the increase in complexity the understandability and the explainability of the algorithms decrease as shown in Fig. 
3.
Explainable AI (XAI) or Interpretive AI or Explainable device gaining knowledge 
of (XML) is an AI wherein the outcomes of an algorithm are easy to understand with the aid of people as shown in Fig. 
3. It contradicts the belief of a “black box” in the 
machine getting to know, which even its designers can’t provide an explanation for4 Explainable AI (XAI): A Survey of Current and Future … 61
Fig. 3 Illustrative diagram 
for explainable AI
why the AI got here to a positive end [ 2]. XAI guarantees to assist users’ paintings 
more efﬁcaciously by way of improving the intellectual models of users of effective 
AI structures and doing away with their defective thoughts. 
XAI may be an enforcement of a right of deﬁnition XAI is crucial even supposing 
there aren’t any prison or regulatory requirements—as an instance, XAI can improve the person enjoy of a service or product by assisting cease customers believe that AI is appearing nicely. In this way, the cause of XAI is to give an explanation for what 
has been completed, what has been carried out, what is going to occur subsequently and to reveal the records on which the moves are primarily based. Those elements make it feasible to (i) afﬁrm existing knowledge (ii) question existing understanding 
and (iii) generate new thoughts. 
Algorithms that are used in AI can be divided into white machine getting-to-
know algorithms and black container (ML). White box fashions ML fashions offer 
clean consequences for professionals within the subject. Black container models, 
alternatively, are extraordinarily tough to outline and aren’t understood even by way of specialists inside the subject. The procedures that extract model parameters from 
the education information and generate the labels within the take a look at data can be explained and stimulated. Translation method a possibility to recognize the 
machine gaining knowledge of the model and lay the basis for choice making in a way that humans can recognize. A deﬁnition of a idea that is considered crucial, but coherent which means does no longer but exist. it is advised that the deﬁnition 
in ML may be thought of as “a summation of the factors of an interpretative space committed to a speciﬁc model for choice making”. while algorithms meet those requirements, they offer the basis that are for validating and to track selections and 
for that reason validating themselves, developing algorithms, and discovering new information. Every now and then it is also feasible to attain the end result with high accuracy with the white ﬁeld ML interpretation set of rules itself. this is especially 
vital in areas along with remedy, law, protection, and ﬁnance and in these cases it’s important to understand choices and construct agree with algorithms [
7, 8]. 
AI structures increase behaviours to fulﬁl mathematically deﬁned goal structures 
chosen by way of gadget designers because the command to “boom the accuracy of trying out how nicely a ﬁlm assessment is within the test database”. The AI can research well-known policies which can be useful in a check set, including “updates 
containing the word ‘horrible’ may be terrible”. but, “opinions with ‘Daniel Day-Lewis’ are generally exact”; these guidelines may not be famous in the event that they62 M. Kumari et al.
appear to be they cannot do matters normally outside of the train set, or if humans 
view the regulation as “dishonest” or “wrong”. The rules in XAI may be researched 
to get a concept of how the system has the potential to generate actual-international information for the future without a check set. AI is undergoing a lot of developments 
these days. Most of the time, every company has plans to install AI, use it actively, or rename its old tech engines with AI-enabled technology. As more companies use AI and has advanced statistics in their business process and automated decisions, it 
demands a speciﬁc reason that how the decision-making models grow bigger and bigger. XAI is based on advanced methodology and strong thinking that depends on how the results are derived for the proposed algorithms so that AI technology 
can be clinically validated. In addition, XAI systems are more focused to build AI mechanism or algorithms that are easy to understand for AI users. Some researchers have encouraged the use of natural-language mechanical learning models, rather than 
the use of post-hoc deﬁnitions, in which a second model is developed to describe the ﬁrst. This is because post-hoc models increase complexity in the decision-making process and in part because it is often unclear how a post-hoc deﬁnition can faithfully 
replicate a completely different model calculation. The main aim of Explainable Artiﬁcial intelligence is to make the AI models explainable.
•Interpretability 
To enable interpretability transparency becomes a very important property that was 
traditionally the ﬁrst step to protecting the rights in institutions. But with the new 
advancements in the ﬁeld of AI, the models are becoming much more complex that are hard to get meaning, and hence the interpretation may vary for the required mech-
anism. With the increase in the interpretability of ML models, it becomes easier to predict the reason behind the decisions. Interpretability is the concept of the advance-ments in AI models that makes it easier for humans to understand the reasoning behind 
the models. The more the interpretability of the model, the easier is the ability to understand the model, and the more it becomes trustworthy for its application.
•Explainability 
It is the extent that involves the development of the models in that form that is easy for the users or the researchers to understand. In the case of explainability, 
ﬁdelity is one of the most important terms. The models with low ﬁdelity lack the explaining ability of the ML models. Explainable artiﬁcial intelligence (sometimes known as shorthand “XAI”) refers to the power of the algorithm or model owners 
to understand how AI has achieved its ﬁndings by making AI technology as bright as possible [
9]. AI is capable of making decisions automatically. Like in case of 
hiring, it is important for the decision-makers of the organization to understand how the decisions are made by the AI models. Many organizations wish to use AI-based mechanism but are uncomfortable allowing the model or AI to take decisions that have 
a signiﬁcant impact because they do not yet trust the model. The description helps in this because it gives the details on how the models take decisions and hence helps to ﬁgure out whether the decisions are favourable or not. Machine learning is growing4 Explainable AI (XAI): A Survey of Current and Future … 63
at a large scale in research as well as in industrial applications. Interpretability and 
explainability play an important role when the question of accountability. 
Major challenges that are faced are that sometimes it becomes difﬁcult to create 
the right model, to get stakeholder purchases the model that could make decisions better than the person. To make better decisions, the right input is must to get the 
best answer. In many cases the authorities require how the decisions are made. They 
need to see how the model feels free to make a healthy and transparent decision. 
5 Background of AI and XAI 
The initial developments in the ﬁeld of AI were easily interpretable. It was found as a 
research topic at the workshop of Dartmouth College, USA. At that workshop, many 
people predicted that machines could be developed that would have intelligence like that of humans, and to make this vision a reality, they were funded with millions of 
dollars. Later on, the AI researchers underestimated the complications and difﬁculty in the process of the development of this project. In 1974, due to the criticisms 
from James Lighthill, pressure from the British Government as well as Congress, the U.S., the funding for this was stopped. Seven years later, after getting inspired by the Japanese Government, Billions of dollars were provided to them. By the late 
1980s, the funding was withdrawn again. The interest in the ﬁeld of AI increased and funding improved in the early decades of the twenty-ﬁrst century. During this time period, machine learning concepts and algorithms were developed and were 
successfully applied in various industries. In the early stage of the development of AI, its applications were restricted to only speciﬁc tasks due to many reasons such as the low computational powers of the processors. AI requires a lot of information 
in the form of databases and in 70’s it was very difﬁcult to build a large database. In the 1980s, an attempt was made to deal with the common-sense knowledge problem. A massive database was created that contained mundane facts remembered by an 
average person. Douglas Lenat, who led this project, stated that no other shortcut is possible, the only way is to teach the concepts of the machine one by one. Now AI has achieved many of its old goals and now is used in many industries. Some of the 
success of AI has been able to due to an increase in computer power. Knowledge engineering helps AI to deal with real-world problems more efﬁciently. With the increase in the use of Artiﬁcial intelligence in various ﬁelds, the complexities of the 
algorithms have also increased with the increase in the use of the concepts like deep learning and machine learning. With advancements, the need for the explainability 
of the complex algorithms was felt and many researchers highlighted the importance of explainability for maintaining transparency and increasing the trust factor [
10]. 
Many mathematical structures are used to discover the ML mechanism. Math-
ematical knowledge generally helps in understanding the resulting pattern. When these patterns are easy to understand it becomes easier to improve those patterns to deal with the deep complexities. Integrating is one of the most commonly used 
methodologies.64 M. Kumari et al.
6 Challenges 
To deal with big and complex problems, we need high-dimensional and large datasets. 
To monitor brain functioning there is a need for development in the acquisition system. There is a need to achieve more accuracy in the solutions developed using 
the latest Explainable models. Next, there is a need for development in different models as most of the models operate with single-level and multi-level modelling is a hard task if classical methods are used. To manage these problems accurately, 
a need for XAI models is felt. For this, we need to use the tools that can help us to understand the transformation of the inputs and the resulting output that happen in the brain network. Explainability is gaining great interest in radiology. In the medical 
ﬁeld, interpretability includes many factors that are sometimes not considered in other ﬁelds, that include the responsibility and risk factor. It is a difﬁcult task for scientists 
to accept the validity and to rely on the developed solutions, without knowing how they are derived. Users need to know the probability when a result can be wrong and without knowing the working model this can’t happen. These days many Deep 
learning libraries have started including their XAI libraries. The assessment criteria like usability and reliability help the Machine learning community to keep a check on how the algorithms are used and how they can be improved. AI and Machine 
learning concepts have a great potential to transform the way of dealing with complex problems in the future. It has the ability to create a great impact and solve the complexities present in various ﬁelds. In many of its applications outside medicine. 
Explainable AI allows the users to understand and interpret how the results are derived. The demand for XAI in medical ﬁeld is increased due to the requirement of fair and ethical results. Giving an explanation to the practitioners allow them to 
make the life- saving decisions more easily. Clinical Decision Support Systems are in great need of Explainable models as these systems help medical practitioners to make complex decisions. The developers are more interested in local explanations. 
The increase in the complexities and the human need to solve the insight problems has given an opportunity for AI to emerge and solve complex problems. To tackle the 
real-world problems, the algorithms developed by AI should be easily understandable [
11, 12]. 
A sound research approach can handle the small issue of detecting study bias. 
However, the intrinsic black-box structure, while improving predictive performance, might cause problems in high-stakes decision-making if adequate justiﬁcations for the choices made aren’t provided. It’s crucial to understand when a model will work 
and when it won’t, as well as the reasons behind any speciﬁc predictions it makes and the degree of its limitations. A common concern among potential users of artiﬁcial intelligence is that it is often unclear how technology can reach their conclusions. If 
AI algorithms are locked in a “black box” that prevents people from knowing how the outcomes were achieved, it might be difﬁcult to trust the developed algorithms because human experts sometimes are unable to explain the ﬁndings of AI. Being 
able to deﬁne AI can help organizations establish greater trust with customers, clients, and investment partners. One of the most important beneﬁts of being able to deﬁne4 Explainable AI (XAI): A Survey of Current and Future … 65
artiﬁcial intelligence is that it can help technology owners determine whether human 
bias has affected the model. This can be especially important when AI is called to 
make life or death decisions, such as a hospital setting where medical professionals may be required to explain to patients and their families why certain decisions have 
been made. In order to move beyond interpretability and investigate the domain of XAI that especially deals with deep learning models, various advancements are undergoing day by day. According to the essay the earliest and most popular XAI 
technique was developed by Ribeiro et al. in their paper Explaining the Predictions of Any Classiﬁer, and it is known as Local Interpretable Model-agnostic Explanations (LIME). The use of LIME on photos to identify a husky as a wolf sheds light on 
the importance of explainability in preventing poor learning. The study shows how a model came to understand that the presence of snow in the image was a key criterion for identifying the animal as a wolf. These explanations make it simple for us to spot 
incorrect learning, which helps us avoid bias in both data and models [
13–15]. 
7 Applications of XAI 
The various XAI applications are shown in Fig. 4. 
•Healthcare 
In the healthcare sector AI can play a crucial role but the risk of using an unreliable AI system is much greater. The choices made by AI models that help doctors categorize serious diseases based on structured criteria or unstructured data such as medical 
imaging naturally have a broad impact. If a system makes a prediction and if the
Fig. 4 The various application ﬁelds66 M. Kumari et al.
doctor needs to spend time to check whether the AI system’s decision is accurate and 
reliable then the model would be of no use. In that case, an AI system that predicts 
with explains the reasons for its conclusion will be much more valuable. To save a life in the healthcare sector the reason for taking a decision while undergoing treatment 
is very important. 
In the medical ﬁeld, when a patient suffering from a disease undergoes diagnosis, 
then, in that case, explainable AI can play a vital role to explain the process of 
diagnosis. It can help the doctors to make a treatment plan, take the correct decisions 
in complex situations, and also to explain their diagnosis process to the patients. This can help in the development of trust between the doctors and patients. For example, 
XAI can help in making decisions is the diagnosis of pneumonia. AI can also help in the diagnosis of cancer in the healthcare sector where medical imaging is used. Currently, many attempts are made to discover how explainable AI models can be 
developed so that they could be applied to neuroscience. With explainability, these models also need to have the capability to discover a data-driven manner [
16]. 
Banking, ﬁnancial services and insurance (BFSI)—BFSI has beneﬁted greatly 
from XAI’s potential to transform the sector. Credit risk assessment based on arti-ﬁcial intelligence is widely used in the banking and insurance industries. In indus-
trialized countries, multimodal premium estimation is becoming more popular with pay-as-you-drive and pay-how-you-drive mechanisms in which machine learning algorithms are used in their decision-making. Industry can implement AI solutions 
more effectively if XAI systems are able to deliver superior results and clear expla-nations while gaining adequate trust and meeting regulatory standards. With larger ﬁnancial stakes, actions such as loan denials, increases or decreases in health insur-
ance or auto insurance costs, or incorrect investment recommendations, XAI can be seen as an invaluable tool in explaining these options to the parties who will be affected.
•Automobiles 
The future of the automotive industry is autonomous driving, which is evolving. 
Self-driving or autonomous vehicles are exciting as long as they don’t make any 
mistakes. In this risky use of AI, one mistake can lead to the loss of one or more lives. Explainability is essential to know about the capabilities and limitations of AI 
models prior to implementation. It is important to understand the limitations of driver assistance systems (or autopilot) when consumers are using them so that the causes of the limitations can be identiﬁed, clariﬁed and addressed as soon as possible. Parking 
assist and voice assistants are one of the attractive features that largely represent a comparatively low-risk model choice. However, there are alternative situations such 
as brake assist or self-driving vehicles.
•Judiciary 
AI technologies are increasingly being used to make decisions in the legal systems of Western countries. As with parole based on the likelihood of repeat offending, biases in AI applications have far-reaching effects, and justice in them is fundamental as 
they affect human rights and freedoms. XAI is undoubtedly a rapidly expanding ﬁeld.4 Explainable AI (XAI): A Survey of Current and Future … 67
It is also heavily researched at ZenLabs because it helps data scientists to understand 
their models in a better way and remove any complexities or problems they may 
inadvertently introduce. 
Explainable AI systems can be useful in situations involving liability, such as 
autonomous vehicles; if there is something wrong with the Explainable AI, one will still have to reckon with one’s actions. Explainable AI models are trained using the concepts of explainable techniques, using human-readable textual descriptions 
to explain the thinking behind the model’s prediction. Today, Explainable tech-niques are used in many different areas of artiﬁcial intelligence, such as natural language processing (NLP), computer vision, medical imaging, health information, 
and more. Other universities where Explainable AI research is being conducted are MIT, Carnegie Mellon University, and Harvard [
17–19]. 
8 Difference Between AI and XAI 
The major difference between AI and XAI models are given in Table 1. 
The main difference between the XAI and AI is that the XAI deals with the 
problems with the practical aspects and it develops solutions that are easily under-standable. The explainable methods used in Explainable AI are highly inﬂuenced by 
the factors that a human needs for practical application. In recent studies or develop-ments, it been has suggested that the pursuit of deﬁnition in AI strategies should be taken as the secondary goal in pursuit of excellence in AI and that the promotion of 
special XAI development may limit the overall performance of AI. XAI models are based on advanced methodology and strong thinking from evidence-based medicine. XAI systems are more focused on making AI systems understandable and more trust-
worthy or transparent to AI. Some researchers have encouraged the use of natural-language mechanical learning models, rather than the use of post-hoc deﬁnitions, in which a second model is developed to describe the ﬁrst. This is because post-hoc 
models increase complexity in the decision-making process and in part because it is often unclear how a post-hoc deﬁnition can faithfully replicate a completely different model calculation [
19–22]. 
A typical AI application is comparable to explainable artiﬁcial intelligence, or 
XAI, except that the processes and outcomes of a XAI algorithm may be described
Table 1 Difference between AI and XAI models [ 19–22] 
S. No. AI models XAI models 
1 Complex Easily understandable 
2 Chances of reliability are less due to less 
understandability More reliability 
3 Understandable mostly by the developers Easily understandable by other developers and users as well 68 M. Kumari et al.
so that people can understand them. Because of its complexity, artiﬁcial intelligence 
makes judgments in real-time based on the knowledge it has learned from the data 
it has been fed. We cannot completely optimise the AI application to be all that it is capable of doing when we do not fully grasp how AI is making these judgments. 
This is due to the fact that the bulk of AI using ML functions in a “black box,” or a space that is unable to offer any obvious insights into how it generates judgments. Transparency and explainability are not strictly required because many AI/ML appli-
cations are somewhat benign decision engines that are integrated with online retail recommender systems. The stakes are substantially greater for other, riskier decision-making processes including medical diagnosis in the healthcare sector, ﬁnancial 
investment decisions, and safety–critical systems in autonomous vehicles. As a result, in order to be seen as trustworthy, dependable, and consistent, the AI employed in those systems must be explicable, transparent, and intelligible. Applications for AI 
and ML are widely used in business today to automate decision-making and to get analytical insights [
23–25]. 
An explainable AI model would allow a brand to enhance revenue by identifying 
the real drivers of sales, whereas data models may be taught to anticipate sales based on variable data. AI is a simulation of human intelligence processes by a computer 
system. AI includes some special features such as natural speech recognition and machine vision. AI is accelerating around the world in a very fast speed. AI requires a set of specialized software and hardware for writing and training the machine to 
get the desired work from the machine. Some of popular programing languages used for AI are Python, R and Java. In short AI work by ingesting a large amount of data (labelled training data) and analysing that data for correlation and patterns and using 
that analysis for predicting the future states. AI focuses mainly on three skills i.e. self-correction, reasoning and learning. artiﬁcial Intelligence is getting used across many industries to offer the whole lot from personalization, automation, monetary 
decisioning, suggestions, and healthcare. For AI to be depended on and ordinary, people need to be able to recognize how AI works and why it comes to make the decisions it makes. XAI represents the evolution of AI, and offers possibilities for 
industries to create AI packages that are relied on, transparent, impartial, and justiﬁed [
26–29]. 
Sometimes while learning, AI models learn unwanted tricks that do a great job 
while training by satisfying the pre-programmed goals. For example, in the year 2017, a program given an image recognition function learned to “cheat” by looking at the copyright tag present with horse pictures, in case of learning to say that the 
horse was actually photographed. In another 2017 edition, a surveyed AI given the task of capturing objects in the physical world learned to cheat as the trick was to be 
placed in between object and viewer in such a way that it seemed to be catching a false object. Explainable AI is deﬁned as AI programs that describe post-prediction thinking. Explainable AI is a term that comes under artiﬁcial intelligence that is called 
as “interpreting.” Interpreting allows us to understand what the model is learning, what other information it provides, and the reasons for its judgments because of the real-world story we are trying to ﬁx. If model metrics are inadequate, interpretation 
is required. Model interpretation enables us to predict how the model will perform in4 Explainable AI (XAI): A Survey of Current and Future … 69
different test environments by comparing it with the training environment. With the 
help of explainable AI, you can provide transparency about how AI systems make 
decisions and make the developed models trustworthy. XAI has its use case in various ﬁelds such as the military, manufacturing industry, and healthcare sector. Explainable 
AI is a new domain in the ﬁeld of research and there are many practical challenges to explainable models that are developed till now. Another challenge to clarity may be due to the accuracy of the model performance, as descriptive performance systems 
tend to have lower performance compared to non-descriptive models or black-box models. Explainable AI is an important part of the future of AI because descriptive artiﬁcial intelligence models explain thinking through their decisions. This provides a 
growing level of understanding between humans and machines, which can help build trust in AI systems. Explainable AI improves the transparency, integrity, integrity, and accountability of AI systems. Explainable AI Systems can be useful when trying to 
understand the thinking behind a particular prediction or decision made by machine learning models [
30–32]. 
Without explaining the inner workings of an AI model and without knowing how 
decisions are made, there is a risk factor to relying on the developed algorithms if the user is unaware of what factors are taken into consideration and how the inputs 
are processed. XAI gives the necessary clarity and helps to build transparency to enable greater conﬁdence in AI-based solutions. Thus, XAI is recognized as a key feature for building artiﬁcial models and making them capable of making trustworthy 
decision-making models and developing them in such a way that takes care of the fundamentals of AI models as well as the fundamentals of the AI users. Trust is one of the major factors for users to adopt AI-based models and solutions that incorporate 
the decisions they make. However, there are some problems in the development of explainable methods or models. It is an important factor that is achieving the easiness of algorithm interpretability. Transparency inﬂuences the high-performance nature 
of complex but opaque models [
32, 33]. 
9 Conclusion and Future Scope 
In this chapter, the current and future opportunities of XAI are surveyed. It is concluded that XAI offers an opportunity where outcomes of AI algorithms can be easily understood with the aid of people. With the development of complex algo-
rithms, the need for the development of explainable AI algorithms is also increasing day by day. The need for explainability, to make a model trustworthy and transparent, 
is increasing. But the questions that arise are whether the development of this tech-nology will affect the job roles of the people? will the employment decrease? will the problem of complexity of the algorithms be reduced, if yes then to what extent? 
XAI can be used in various AI ﬁelds such as neuroscience, healthcare, BFSI, 
automobiles, judiciary, etc. However, it has various challenges like the requirement of high-dimensional and large datasets but the deep learning libraries can allow 
XAI libraries to develop algorithms for real-world applications which can be easily70 M. Kumari et al.
understood. Thus, in the future XAI will play an attractive role to build trustworthy 
AI models. 
References 
1. D. Gunning, M. Steﬁk, J. Choi, T. Miller, S. Stumpf, G.Z. Yang, XAI—Explainable artiﬁcial 
intelligence. Sci. Robotics 4(37), eaay7120 (2019) 
2. A. Páez, The pragmatic turn in explainable artiﬁcial intelligence (XAI). Mind. Mach. 29(3), 
441–459 (2019) 
3. M. Langer, D. Oster, T. Speith, H. Hermanns, L. Kästner, E. Schmidt, K. Baum, What do we want from explainable artiﬁcial intelligence (XAI)?–a stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research. Artif. Intell. 296, 103473 (2021) 
4. A.B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, F. Herrera, Explainable artiﬁcial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58, 82–115 (2020) 
5. J.M. Fellous, G. Sapiro, A. Rossi, H. Mayberg, M. Ferrante, Explainable artiﬁcial intelligence for neuroscience: behavioral neurostimulation. Front. Neurosci. 13, 1346 (2019) 
6. G. Yang, Q. Ye, J. Xia, Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: a mini-review, two showcases and beyond. Inf. Fusion 77, 29–52 
(2022) 
7. J. van der Waa, E. Nieuwburg, A. Cremers, M. Neerincx, Evaluating XAI: a comparison of rule-based and example-based explanations. Artif. Intell. 291, 103404 (2021) 
8. A.M. Antoniadi, Y . Du, Y . Guendouz, L. Wei, C. Mazo, B.A. Becker, C. Mooney, Current challenges and future opportunities for XAI in machine learning-based clinical decision support 
systems: a systematic review. Appl. Sci. 11(11), 5088 (2021) 
9. Wolf, C.T. Explainability scenarios: towards scenario-based XAI design. in Proceedings of the 24th International Conference on Intelligent User Interfaces, (March, 2019), pp. 252–257 
10. H. Hagras, Toward human-understandable, explainable AI. Computer 51(9), 28–36 (2018) 
11. C. Panigutti, A. Perotti, D. Pedreschi, Doctor XAI: an ontology-based approach to black-box sequential data classiﬁcation explanations. in: Proceedings of the 2020 conference on fairness, accountability, and transparency, pp. 629–639 (January, 2020) 
12. T. Rojat, R. Puget, D. Filliat, J. Del Ser, R. Gelin, N. Díaz-Rodríguez, Explainable artiﬁcial intelligence (xai) on timeseries data: a survey. arXiv preprint 
arXiv:2104.00950 (2021) 
13. T.A. Schoonderwoerd, W. Jorritsma, M.A. Neerincx, K. V an Den Bosch, Human-centered XAI: developing design patterns for explanations of clinical decision support systems. Int. J. Hum Comput Stud. 154, 102684 (2021) 
14. Y .S. Lin, W.C. Lee, Z.B. Celik, What do you see? Evaluation of explainable artiﬁcial intel-ligence (XAI) interpretability through neural backdoors. arXiv preprint 
arXiv:2009.10639 . 
(2020) 
15. F. Emmert-Streib, O. Yli-Harja, M. Dehmer, Explainable artiﬁcial intelligence and machine learning: a reality rooted perspective. Wiley Interdisc Rev Data Min Knowl Discovery 10(6), 
e1368 (2020) 
16. E. Tjoa, C. Guan, A survey on explainable artiﬁcial intelligence (xai): toward medical xai. IEEE Trans. Neural Netw. Learn. Syst. 32(11), 4793–4813 (2020), A. Adadi, M. Berrada, Peeking 
inside the black-box: a survey on explainable artiﬁcial intelligence (XAI). IEEE Access 6, 
52138–52160 (2018) 
17. Z. Papanastasopoulos, R.K. Samala, H.P . Chan, L. Hadjiiski, C. Paramagul, M.A. Helvie, C.H. Neal, Explainable AI for medical imaging: deep-learning CNN ensemble for classiﬁcation of estrogen receptor status from breast MRI. in Medical imaging 2020: Computer-aided diagnosis, 
vol. 11314, pp. 228–235 (SPIE, March, 2020)4 Explainable AI (XAI): A Survey of Current and Future … 71
18. M. Saarela, S. Jauhiainen, Comparison of feature importance measures as explanations for 
classiﬁcation models. SN Appl. Sci. 3(2), 1–12 (2021) 
19. J.J. Ferreira, M.S. Monteiro, What are people doing about XAI user experience? A survey on AI explainability research and practice. in International Conference on Human-Computer Interaction (Springer, Cham, July, 2020), pp. 56–73 
20. C. Mencar, J.M. Alonso, Paving the way to explainable artiﬁcial intelligence with fuzzy modeling. in International W orkshop on Fuzzy Logic and Applications (Springer, Cham, 
September, 2018), pp. 215–227 
21. E. Da˘ glarli, Explainable artiﬁcial intelligence (xAI) approaches and deep meta-learning models. Adv. Appl. Deep Learn. 79 (2020) 
22. E. Puiutta, E. V eith, Explainable reinforcement learning: a survey. in International cross-domain conference for machine learning and knowledge extraction (Springer, Cham, August, 2020), pp. 77–95 
23. U. Ehsan, Q.V . Liao, M. Muller, M.O. Riedl, J.D. Weisz, Expanding explainability: towards social transparency in AI systems. in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (May, 2021), pp. 1–19 
24. F. Hussain, R. Hussain, E. Hossain, Explainable artiﬁcial intelligence (XAI): an engineering 
perspective. arXiv preprint arXiv:2101.03613 (2021) 
25. W. Samek, G. Montavon, S. Lapuschkin, C.J. Anders, K.R. Müller, Explaining deep neural 
networks and beyond: a review of methods and applications. Proc. IEEE 109(3), 247–278 
(2021) 
26. https://www.datarobot.com/wiki/explainable-ai/ 
27. https://www.analyticsvidhya.com/blog/2021/01/explain-how-your-model-works-using-explai 
nable-ai/ 
28. https://www.ibm.com/watson/explainable-ai 
29. https://www.ericsson.com/en/reports-and-papers/white-papers/explainable-ai--how-humans-
can-trust-ai 
30. M. El-Assady et al., Towards XAI: structuring the processes of explanations, in Proceedings of the ACM W orkshop on Human-Centered Machine Learning, vol 4 (Glasgow, UK, 2019) 
31. C. Conati, O. Barral, V . Putnam, L. Rieger, Toward personalized XAI: a case study in intelligent tutoring systems. Artif. Intell. 298, 103503 (2021) 
32. U. Pawar, D. O’Shea, S. Rea, R. O’Reilly, Incorporating explainable artiﬁcial intelligence (xai) to aid the understanding of machine learning in the healthcare domain. in AICS (2020, 
December), pp. 169–180 
33. J. Druce, M. Harradon, J. Tittle, Explainable artiﬁcial intelligence (XAI) for increasing user trust in deep reinforcement learning driven autonomous systems. arXiv preprint 
arXiv:2106. 
03775 (2021)Chapter 5 
Recent Challenges on Edge AI with Its 
Application: A Brief Introduction 
Kapil Joshi, Harishchander Anandaram, Manisha Khanduja, 
Rajesh Kumar, Vikrant Saini, and Yasmin Makki Mohialden 
Abstract Edge AI refers to the deployment of AI software on hardware throughout 
the real world. Recently industry 4.0 adopted the cases on plant automation, factory 
robot, edge computing and usage of application in AI domain. With higher bandwidth 
and lower latency, edge computing promises to decentralize cloud applications. This 
chapter provides a thorough analysis of edge computing-related AI techniques and 
tools, sometimes known as edge AI. First section gives the introduction on edge 
AI and 5G technology has been discussed in second section. Rest of section elab-orated the importance and usage of edge AI with complete roadmap. In our vision, 
Edge AI will improve network and radio access, provide adaption for data-driven applications, and enable the development; distributed AI/ML pipeline deployment, optimization, and implementation predetermined experience quality, faith, safety, 
and privacy criteria. The Edge AI community, which spans several ICT, engineering, and perform computer science subﬁelds, researches unique machine learning tech-niques for the edge computing environment. The objective is to offer a hypothetical 
roadmap that may unite important players and enablers in order to the development of edge AI. The outcomes of this chapter is to explain the usage of edge AI in industry 5.0 and will gives the better idea to development of new applications. 
Keywords Edge AI ·XAI ·Application of EDGE AI ·Roadmap
K. Joshi (B) · V. S a i n i 
Uttaranchal Institute of Technology, Uttaranchal University, Dehradun, India 
e-mail: kapilengg0509@gmail.com 
H. Anandaram 
Amrita Vishwa Vidyapeetham, Coimbatore, Tamil Nadu, India 
M. Khanduja 
USCS, Uttaranchal University, Dehradun, India 
R. Kumar 
Meerut Institute of Technology, Meerut, India 
Y . M. Mohialden 
Mustansiriyah University, Baghdad, Iraq 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_5 7374 K. Joshi et al.
1 Introduction 
Edge computing has the potential to decentralize using cloud applications simulta-
neously increasing bandwidth and lowering latency. These beneﬁts are realized by transferring application-speciﬁc calculations across the cloud, data-generating equip-
ment, and elements of the wireless and landline network infrastructure network edges. However, existing Machine learning and artiﬁcial intelligence (AI) (ML) method-ologies presuppose that calculations [
1] are carried out in a robust computing envi-
ronment, also including datacenters with plenty of processing and storage for data capabilities]. The recent Dagstuhl Seminar—“213,421” gathered community input from a varied variety of professionals to provide light on the rapidly expanding 
subject of Edge AI, which combines edge computing and AI/ML. This article is the product of their efforts, and it examines demands for implementing AI techniques as 
in context of edge computing are both technological and societal. 
The phrase ‘Edge,’ as modern development and research progresses, remains a 
nebulous term. Across communities and researchers, a shared understanding of what 
the edge is, where it lives, and who supplies it is lacking. Similarity (latency and 
topology), improved capacity of a network (effectively feasible data transfer rate), lower processing strength, smaller size, and greater device heterogeneity are some of 
its characteristics when compared to the cloud. When compared to end devices (the last step), it offers more computing and storage resources. It is a ﬁctitious entity that allows you to outsource computation and storage without having to go to the cloud. 
Present-day AI and ML techniques, which necessitate sophisticated computa-
tional structures, are becoming a growing source of friction [ 2] an improved request 
supplied the data centre with adequate processing and storage for data capabilities. 
Sending the required raw data to cloud, on the other hand other hand, places a strain 
on the network in terms of bandwidth and throughput. Meanwhile, most businesses are hesitant to share (possibly sensitive) data [
3] with commercial cloud providers. 
Edge AI, a rapidly emerging domain, addresses this tension. On the other end, appli-cation of edge AI can be used in industry 5.0 like smart home, smart agriculture, 
smart vehicles, smart education, smart critical industries, smart e-commerce, smart transport and smart healthcare etc. 
Edge AI has gradually made its entry into regular service areas that is linked 
automobiles, instantaneous gaming, healthcare, and intelligent manufacturing, as shown in Fig. 
1.
Edge environments, from an infrastructure standpoint, create a distinct layer for AI 
and provide possibilities for current technology like federated learning or embedded AI, which aim to reduce RAM use on speciﬁc devices, increase maintaining data privacy on a local device, and reduce communication needs between distributed 
entities. These properties form the basis the usage cases depicted in Fig. 
1.O u r 
Contribution is mentioned below:
•Discussed the use case if Edge AI in current era.
•5G technology in industry 4.0.
•Elaborated the usage of cloud computing in next generation.5 Recent Challenges on Edge AI with Its Application … 75
Fig. 1 Use case of edge AI in today’s era [ 4]
•Modiﬁcation of current in edge AI.
•Designed the road map for edge AI. 
This article intends to provide a plan that can bring people together essential 
performers and facilitators who promote progress the territory of Edge AI, based also 
on group’s contribution of the Dagstuhl Edge Intelligence Seminar. The viewpoints 
of future cloud, AI/ML, and 5G beyond are covered in Sects. 2, 3, and 4, respectively. 
The anticipated route plan and perspective are presented in Sect. 5. 
2 Beyond Perspective with 5G 
The transition from networks of the ﬁfth generation (5G) to the sixth-generation (6G) period shaping networking’s future is established. The advancements on this road 
include network connectivity, including its speed, coverage, and durability, as well 
as computer quality and latency. To realize the full future networks’ possibilities 
with such a complex structure, several technologies must coexist and collaborate 
both in terms of hardware and software. The establishment factors are generated for edge computing and communications, for example or the use of Technologies 
with dynamic network orchestration that are self-learning examples of this. Security assurance techniques, secret communication, computation and learning are all key viewpoints in holistic system trustworthiness. We go over the obstacles and opportu-
nities [
4, 5] in this part. The following literature covers the efﬁciency of edge AI. V , 
Mazzia proposed the structure micro perspective with training dataset. There is also some literature which focuses on the privacy [
4, 6–8] and security [ 9–11] issues.76 K. Joshi et al.
2.1 Communication and Computation Process Using Human 
Using the widespread use of smart phones or other portable electronic gadgets, impor-
tant compute, interaction, and sensor abilities have becoming useful in addressing networking difﬁculties. Acquisition of local data, as used in federated learning, 
decreasing reducing communication costs through device-to-device caching and working together in the execution [
5] of computationally complex tasks are just 
a few examples. In the future, mobile devices will be able to select whether, the how, 
when, and how much of such a task to assign a cloud or edge server. Furthermore, 
individuals can assume the role of a computer worker, organize resource pools, and split jobs according to their preferences in order to maximize their effectiveness and 
usefulness. 
While people or machines operated by people may contribute considerably Edge 
AI, such participation comes with a number of drawbacks. Multi-agent systems can be used to represent such problems. As a result, numerous mathematical methods such as game theory and control can be used to handle problems. Furthermore, when 
there is uncertainty and a lack of knowledge, ML and AI play a signiﬁcant role. The special qualities of people should be considered in all of the preceding procedures. Humans, in particular, rely concerning heuristics and unjustiﬁed inﬂuences to make 
decisions, while taking considerations include societal peer pressure and standards. It’s also critical to think about reciprocal trust and respect each entity’s welfare while interacting with self-interested entities. Finally, employing using people as either a 
data source such as through GPS or body sensors raises privacy worries about are inextricably linked to ethical and legal issues. 
2.2 Applications in Scale 
The future demands of the driving force behind societies produces using examples we 
shift from present networks and systems for 5G and beyond. As a result, developing 
novel technology to address societal use-cases develop critical. In a limited sense, it contradicts present-day usage cases like as well as dependability, which are driven 
by technology advancements instead of derived from society itself. Current vertical trends include resource-efﬁcient manufacturing, the production and distribution of green energy, organic farming, and retail logistics optimization, to name a few. 
To meet their diverse demands, heterogeneous entities are anticipated to create and 
transfer a vast infrastructure for edge computing. Despite sharing certain common objectives, such actors frequently have competing interests; for example, more advan-
tage for one attribute may diminish the beneﬁt for the other (here the utility can correspond to major sustainability, improved environmental factors). The ﬁnding of a Pareto-optimal and [
12] the most reliable answer to this issue is extremely 
difﬁcult due to the fact that different utility measurements are frequently at odds. In fact, the problem is exacerbated since there are multiple decision-makers rather5 Recent Challenges on Edge AI with Its Application … 77
than a single centralized ﬁgure. This is due to asymmetry of knowledge and varied 
types/preferences, as well as self-interest. Artiﬁcial intelligence (AI) can help solve 
this challenge by allowing using distributed systems communicate, acquire knowl-edge, make choices, effectively turning Smart systems are interconnected chunks of 
edge intelligence. 
2.3 Edge Intelligence System and Latest T echnologies 
Beyond 5G Networks 
Beyond Next-generation networks or 5G cover a variety of technologies, all of which 
rely heavily on dependable cloud infrastructure and edge intelligence for successful 
implementation. Campus networks, cooperative sensing and communication, the Network for Open Radio Access (RAN), and intelligent reﬂecting surfaces (IRS) 
are only a few examples. Joint communication and radar sensing technologies, for example, can be implemented in 2 networking architectures: cloud RAN and small 
cell networks. While the latter is compatible with cloud infrastructure, edge intelli-gence is extremely beneﬁcial to both implementations. Because combined Sensing and communication are necessary fast processing of signals, and exact pattern 
identiﬁcation, which are both computationally demanding tasks, this is the case. 
Campus networks are yet another case, which span a geographically conﬁned 
territory in order to meet the communication needs unique to that location. As a 
result of the demand for safe, dependable, and ongoing, ultra-low latency industrial 
communications, a manufacturing organization, for example, can integrate a campus network. Building sites, medical facilities, and agricultural ﬁelds other similar appli-
cations can all beneﬁt from campus networks? Campus networks are being driven by 5G technology, as well as AI and edge computing capabilities. Even in the absence of precise knowledge, they make communication stable and secure, quick and failure-
free calculation, as well as reliable and efﬁcient performance. Edge intelligence is also required for the consistent effectiveness of various other technologies, like IRS. IRS technique is based on ﬁnding the best [
6] Beam arrangement that could 
occur multiple times. Consequently, the edge can handle the requisite low-latency computing. 
2.4 T echnology Meets Business 
The debate over the future of 5G has recently been dominated by hypothetical use 
situations, as well as overarching objective of developing real-time integration of 
communication, AI, and edge computing services that adapt to applications’ changing78 K. Joshi et al.
needs. These predicted solutions, which range from ordinary living smart transporta-
tion and medical beneﬁts, are important, but they must be evaluated in the situation 
with technology and new economic models. 
There are only a few business cases, but we were able to ﬁnd three instances 
where AI technology is being used for whole new tasks and models are yet deﬁned for them:
•Interpreting the results of joint sensing and communication capabilities in 
future networks. Future higher frequency Communications provides some degree 
of “radar-like” environment recognition. This is an example of signiﬁcant change 
Regarding the actions or operations of present networks, and the security issues 
involved are substantial.
•Optimal link-level communication details discovery through ML. Theoret-
ically, which is doable. The practicality within the technologies still is debat-
able, albeit with the required education expenses are covered by the optimization beneﬁts accrued.
•Interoperability and collaborative use of data and a technologies. The majority of modern systems operate within single organizations However, we pose the following query: “What are our options promote more interoperable technolo-
gies.” The need to identify as well as discuss pertinent security, privacy, and ethical issues, as well as the instruments to address them, is shared by diverse stakeholders the min trust worthy manner. 
3 Future Perspective on Cloud Computing 
The collaboration between future edge nodes and clouds shapes the future cloud 
computing perspective. Latency, bandwidth, data localization, scaling, ease of use, 
security, and fault tolerance are the key reasons for moving workloads from the cloud to the edge. This collaboration encompasses not just technical obstacles, but 
also business and stakeholder issues. The possibility conﬂict between the edges of the cloud must work closely together to deliver the greatest assistance available clients, is an important non-technical obstacle. We’ll go over the technology problems in detail 
from a cloud perspective in the following sections. These issues, in our perspective, are: Resource management, energy constraints and efﬁciency, security, trust, and privacy and sporadic connectivity are the other four topics. 
3.1 Resource Management 
Even though the number edge servers could supply a signiﬁcant number of resources, required proximity [
7] number edge servers severely limit the quantity of resources 
available when in contrast to a cloud setting. Owing to hardware restrictions and task 
execution lag, the quantity of different ML jobs that may be done at the same time is5 Recent Challenges on Edge AI with Its Application … 79
Fig. 2 V arious issues between devices [ 12] 
limited. While execution and instantaneous loading of the associated models could 
improve the amount of ML tasks that the edge can perform, this might lengthen the 
latency of jobs it needs to be loaded. Thus, for tasks that require low latency, proactive 
resource reservation may be required. Because resource reservations can substantially reduce how many ML tasks there are supported, the need for resource reservations 
should also be decided based on the possibility of failing and the repercussions of such error. The administration of these resources in crucial situations can be difﬁcult, 
especially when resources are scarce. The issues are discussed in Fig. 
2. 
Importantly, edge AI operations are frequently embedded in broader settings, 
such as a control workﬂow. As a result, various services provided via cutting-edge 
resources must engage with one another. Not accounting for the notion data items 
could need must be passed forward to services result in these services not being ready to receive either data or not having the computational capacity to handle data 
items. As a result, there may be a stream of data items. This is particularly true when stream processing is used, such as when fresh data items must be processed and then transmitted to various destination services based on the categorization. As a result, 
it could be fascinating to watch and analyze how jobs are completed based [
8] based 
on overall priority level in light of the various loads at the edge. 
3.2 Energy and Operational Constraints 
Edge nodes are projected to be less cost and energy efﬁcient compared to cloud data 
centers in general. Economies of scale, therefore may act in opposition to edge data80 K. Joshi et al.
centers by enabling greater cooling, for example. Renewable energy can be deployed 
at both cloud data centers and edge nodes, however Unknown where these units are 
produce greater energy efﬁciency. Large data centers, a single hand, offer greater chances Using renewable energy, enhancing their persistence and environmental 
but compatibility they also take in substantially more energy than edge nodes. The opposite is true for edge nodes may be able to they generate their own power more easily compared to cloud data centers because of modest geographic spread and 
scale, despite their restricted Amount of renewable energy units available. 
Furthermore, data must be transported over The Internet uses energy; hence it 
increases the likelihood that edge nodes may outperform cloud data centers that 
use less energy: Edge nodes can process data, for example, by detecting anoma-
lous instances and transmitting only pertinent information, by dynamically adapting monitoring rates. Therefore, the amount of processing that can be done at the edge 
determines how much energy can be saved. Another option is to shift part of the training process to the edge; using techniques like federated education (see Sect. 
4.3). 
Only learnt parameters (such as gradients) must be communicated in this situation, 
compared to raw data, which many various magnitudes bigger. 
Another possible the beneﬁt of edge nodes over general cloud server racks is the 
more speciﬁcally constructed hardware, which is increasing common during market 
(e.g., hardware accelerators) and makes better use of limited processing and graphical capacities. 
3.3 Security and Privacy Issues 
Edge AI, as seen in Fig. 3, provides prospects for increased trust, security, and 
privacy while also posing new obstacles. Because some threats may be discovered early and countermeasures taken, security may be enhanced. However, the number of potential targets and attack channels is substantially greater, resulting in a bigger 
assault potential. 
Fig. 3 Security and privacy in edge AI5 Recent Challenges on Edge AI with Its Application … 81
Because of its dispersed structure and potential liability issues, Edge AI may 
have a problem with trust in the system. Open implementation and speciﬁcations, 
on the other hand, may help to boost edge server conﬁdence. It may also be required to distinguish between devices within the user’s control and resources offered by 
providers (e.g., Azure, AWS, GCP). The implementation of a reputation system that analyses and controls trust, as seen in other ﬁelds, is one viable option for building trust. However, it will take time for new players to establish a reputation. 
Another feature the data of Edge AI is what needed to complete the task of ML 
is retained on servers at the edges. As a result, it’s widely assumed that edge servers offer more privacy and trust than the cloud. As opposed to popular belief, Edge 
cannot ensure privacy; rather, it creates additional obstacles for ensuring privacy. That is, because edge node trust is more difﬁcult to maintain and administer than cloud provider trust, data privacy and security are harmed, even while the data’s 
location prevents some assaults. 
3.4 Intermittent Connectivity 
Generally, we discuss utilizing edge servers with weak internet connections, i.e., edge servers in areas in which internet connection are bad. Edge intelligence has a 
lot of potential in those places as well, and it can be a driving force for digitalization in non-urban areas. Water and air pollution monitoring, as well as natural hazard prediction (wildﬁres, ﬂoods, volcanic eruptions, and so on) are examples of such 
systems. For example, in water pollution monitoring, edge resources could be used to identify chemicals in the water. 
Scenarios involving environmental monitoring where, streaming data must be 
analyzed in almost real-time an absence of a solid network access in those places could be the key driving reason. Monitoring systems are unable to continuously transmit raw data processing on the cloud due to intermittent connectivity and energy 
constraints, as opposed to less data-heavy control signals such as the result of machine learning still possible to communicate algorithms despite disruptions and restricted bandwidth. Furthermore, the participants who rely on the results Examples of ML 
algorithms frequently located near the sources of data. By avoiding transferring cloud storage for data, an entire processing stage may avoid, such as automatically initiating countermeasures. In the prior a good example of monitoring water contam-
ination edge hence, services instruct opening or closing pumps or valves based on the circumstance. 
4 Evolving AI and ML 
This paragraph discusses edge nodes from an AI perspective, and difﬁculties that it 
raises. Edge servers, cloud servers, and mobile devices, Fig. 4 depicts the various82 K. Joshi et al.
Fig. 4 Levels of AI with data connectivity 
AI complexity levels and data availability: While mobile devices have the most data 
at their disposal, their processing capabilities are often constrained. As a result, 
preprocessed information can sent to community edge servers for tasks that mobile devices can’t handle. The edge server offers extra capabilities and can do more 
complex tasks than the mobile devices; however data access [
9] is limited owing to 
preprocessing. The inability to ﬁnished will be forwarded to a cloud server, capable of execute even most sophisticated models, but only a fraction of the data is received. 
Tasks that are unable to be performed will be forwarded a connection to the cloud server, which execute most sophisticated models, but only is given a portion of the edge server’s information. 
The following are three challenges that have been highlighted and should be 
addressed by the community: (1) the accessibility of accelerators for applications using AI; (2) identifying a trade-off between precision and resource use; and (3) 
Using Federated edge server learning is the three issues. 
4.1 Accelerators for AI Usage 
Although we anticipate that hardware accelerators and AI applications would also be accessible at the edge, only modest hardware accelerators are anticipated to be 
accessible there. In other words, compared to the cloud, capabilities at the edge are5 Recent Challenges on Edge AI with Its Application … 83
used less elastically, which might lead to let hardware idle. It is anticipated that the 
huge hardware accelerators would be put in the cloud due to business considerations. 
To cooperate with them restricted resources, one current trend is to break down 
smaller the model and AI algorithm parts that could be processed these tiny hardware accelerators are used. The quality of service and the user experience of apps running 
too close to the edge improved by combining the interference of these small hardware 
accelerators. In general, huge models that require a lot of resources will need to be 
moved to both the ground and the cloud. 
Ofﬂoading and multi-tier architectures are also possible with machine learning. 
Unlike classical workloads, which have intrinsically contradictory criteria for expanding to higher tiers and nodes while holding the workload’s state consistent 
across nodes, ML, particularly neural network training, has a distinct state distri-
bution synchronization models. Additionally, in contrast to conventional distributed 
systems, overall consistency criteria easily possible reduced, resulting in a trade-off 
of network bandwidth and replica count for relatively minor accuracy losses. As a result, an essential question is what precision and delay constraints using the appli-
cation, as well as how to check [
10] the model’s quality during its whole life-cycle 
on the edge. 
4.2 Trade off Between Accuracy and Resource Demand 
Similar to before stated, edge servers face greater resource limitations than clouds 
of servers. Due to their complexity, certain models may only be able to operate in 
the cloud. 
The common goal today’s AI research frequently aims to attain the greatest 
feasible precision or to maximize the incentive mechanism. However, in the case of Edge AI, the quality of the outputs is [
11] not the sole criterion for success; other 
factors such as energy usage and memory must also be addressed. As a result, models 
must be ﬂexible in order to change for the current state of memory and resources 
at hand. There must be a way to adjust the dimensions of individual ML models for this purpose. Miniaturization techniques such as quantization and approximation 
can be used to achieve this. In general, these strategies provide a [
13] comparison 
of performance measures and model correctness. TinyML and Tensor-Lite are two examples of Edge AI minimization. The evaluation of adjusting the ML model in 
light of present accessible resources is an important research topic. Furthermore, we address the prospect of ofﬂoading speciﬁc jobs to the cloud: if the precision of 
the scaled-down model implemented near the edges is inadequate, ofﬂoading cloud-based task may be necessary. The clouds can then offer very accurate results and deliver them the edge once more [
14].84 K. Joshi et al.
4.3 F ederated Learning 
Then again to model downsizing, federated learning allows models to be trained on 
several devices at the same time. This is particularly crucial for Edge AI, because the dispersed the composition of edge servers makes federated learning more important. 
The data for federated learning is maintained locally [ 15] and not made avail-
able to the cloud. When a complicated running on such an edge server, however, the resources provided may not be adequate to execute or instruct the model. The 
introduction of a sophisticated model exclusively mostly in cloud, which would be trained collaboratively using fewer complicated models used in edge deployment, is one research topic. 
Federated education is also enables time-critical label [ 16] classiﬁcation directly 
while other labels are near the edge must be categorized in the cloud. The model gains 
additional layer of ﬂexibility as a result of this. Even if the model is reduced through 
downsizing by combining we anticipate that the cloud model will classify all non-time-critical labels, and that the time-critical labels will be categorized accurately to 
remain high. As a result, one ongoing research challenge is how to [
17] miniaturize a 
model while maintaining high a subset of labels accurately, as well as how to decide when a classiﬁcation should be performed in the cloud. Furthermore, the informa-
tion gathered by edge servers and created by mobile devices may be considerably different. Federated instruction can aid in the training of one model that uses a variety [
18] of sensor inputs, improving the precision of each model regardless arrangement 
of sensors. The ﬁnding of fraudulent material that could jeopardize the process of training is likewise a difﬁcult task. 
Another difﬁculty is the federated learning systems’ elastic adaptation to regularly 
updated client partnerships while taking the fulﬁllment of collaboration criteria into account. (e.g., the minimum needed of training partners) and keep improving the model’s accuracy. 
5 Outlook and Roadmap in Edge AI 
Overview of the Roadmap: Fig. 5 depicts the Edge AI roadmap as it is currently 
envisioned. Edge AI will advance in terms of data governance, service models, hard-ware, and software in addition to the three key pillars of evolving AI/ML, future 
cloud, and 5G beyond. The improvement in various Technology advancements and expanding commercial interests will propel Hardware, service paradigms, software, and data governance advancements for Edge AI.
The roadmap includes ﬁve main phases, starting with the current situation, which 
is Scalable framework, reliable co-design, equal accessibility, deployment that is sustainable, energy-efﬁcient, and characterized by pervasive intelligent infrastructure 
are led by companies that offer cellular, cloud, and AI/ML services. The order repre-sented in the roadmap could be swapped or mixed as changes occur. Nonetheless,5 Recent Challenges on Edge AI with Its Application … 85
Fig. 5 Road map of edge AI [ 19]
the combining their impacts technological enabling factors and non-tech require-
ments such user purchasing power, socioeconomic transformation behavior, as well 
as commercial interests are reﬂected in this Edge AI roadmap. 
5.1 Open Research Challenges 
Despite its potential and potential, when deployed on a large scale, Edge AI may 
encounter signiﬁcant difﬁculties, such as energy optimization, trust worthiness, 
ethical, security, and privacy concerns. 
The power consumption of Edge AI needs to be improved as a crucial compo-
nent of sustainability. On Edge AI embedding infrastructures (such as roadside units and micro base stations) to continuously enable sophisticated autonomous driving and services for Extended Reality (XR) in the upcoming years. Through the pipe 
line Edge AI has the option to exchange accuracy with efﬁciency of medical image data, [
20] data capture, transport, calculation, and storage reduced less time and less 
power is used. For instance, it is possible to transport and selectively process noisy 
signals from many sensors in order to conserve energy. Instead of being perfect and 
totally accurate, a group of applications would indeed be pleased with “acceptable” 
accuracy correct results. The optimization design can further increase energy efﬁ-
ciency by adding this different dimension of accuracy. Concerning trust worthiness, Edge AI gains from being so close to the end devices. But because of the scattered 
deployment with profound insights in to personal circumstances, perceived safety,86 K. Joshi et al.
and trust worthiness for Edge AI services have been causing stakeholders to express 
their worries (e.g., end users, public sectors, ISP). 
To succeed trust worthy Edge AI requires essential building components, such as 
systems for veriﬁcation and validation that guarantee transparency and explain ability, particularly while training and deployment of Edge AI in distributed, unregulated 
settings. The trust worthiness of Edge AI is a ﬁrst step toward establishing appropriate 
regulatory and governance frame work, which Edge AI’s potential can be built. 
5.2 Safety and Privacy/Ethical Issues 
When we talk about the security Regarding Edge AI’s privacy, there are two things 
to think about, i.e., 
(1) Use of edge AI for privacy and security; 
(2) Due to the deployment of Edge AI, there are data security and privacy risks, as 
shown in Fig. 3. 
Edge AI has the potential to be a key tool for ensuring network security and privacy. 
Edge AI uses edge computing to process AI rather than using the cloud processes the local info that is closer to that same user. That eliminates the need of transferring 
eliminates data transfer between both the user and the cloud. The possibility potential is ﬁxed during the data transfer phase of the backhaul. Furthermore, local storage and processing of data [
21] can enhance privacy. Processing’s primary objective is 
to move the interface of the AI workﬂow to the gadget while keeping data limited to the device. The potential for a decentralized AI algorithm, which would remove one single point of failure as in cloud AI system, is another security advantage of 
edge AI. Edge AI might in some ways be is a son due to increased intelligence, to reduce the impact of the attack on the immediate environment but also mitigate it there already. There are growing concerns about privacy and safety a rising Edge AI 
is used in this. Decentralization of Edge AI increases that number of access points that AI system hackers and exposes previously undiscovered attack vectors. Edge AI devices could lack the very same level of protection as the cloud, making them easy 
entry points for an assault the AI system. Additionally, edge devices are reachable physically, embedded Impact of AI on edge devices on capturing edge devices ever than a non-AI edge device. For example, seizing control of Edge Ai technologies 
might threaten or seize control of nearly all the local work services provided by a certain place. 
5.3 Conclusion and Future Scope 
Edge AI’s promises are accompanied by brand-new difﬁculties and uncertainties. 
This article is ours Endeavour Important players in the three key areas of 5G-beyond,5 Recent Challenges on Edge AI with Its Application … 87
future cloud, and AI/ML are needed to capture the most recent technology advance-
ments for the envisioned roadmap. In terms of future advancement, AI applications 
may be more elaborated for better environment and in this section, various issues has been discussed on industry 4.0 and industry 5.0 with the usage of 5G technology in 
future so here we expect the perspectives the ideas presented in this paper may offer the community an alternative perspective and, in the long term, help to generate the global adoption of Edge AI in future global world. 
References 
1. Y .L. Lee, P .K. Tsung, M. Wu, Techology trend of edge AI. in 2018 International Symposium 
on VLSI Design, Automation and T est (VLSI-DAT) (IEEE, 2018, April), pp. 1–2 
2. S. Greengard, AI on edge. Commun. ACM 63(9), 18–20 (2020) 
3. X. Wang, Y . Han, C. Wang, Q. Zhao, X. Chen, M. Chen, In-edge ai: Intelligentizing mobile edge computing, caching and communication by federated learning. IEEE Network 33(5), 156–165 
(2019) 
4. E. Li, L. Zeng, Z. Zhou, X. Chen, Edge AI: on-demand accelerating deep neural network inference via edge computing. IEEE Trans. Wireless Commun. 19(1), 447–457 (2019) 
5. T. Rausch, W. Hummer, V . Muthusamy, A. Rashed, S. Dustdar, Towards a serverless platform for edge {AI}. in 2nd USENIX W orkshop on Hot T opics in Edge Computing (HotEdge 19, 
2019) 
6. A.Y . Ding, E. Peltonen, T. Meuser, A. Aral, C. Becker, S. Dustdar, L. Wolf, Roadmap for edge AI: a Dagstuhl perspective. ACM SIGCOMM Comput. Commun. Rev. 52(1), 28–33 (2022) 
7. S. Yang, Q. Liu, X. Wan, J. Jiang, L. Ai, Edge-rich MoS2 nanosheets anchored on layered Ti3C2 MXene for highly efﬁcient and rapid catalytic reduction of 4-nitrophenol and methylene blue. J. Alloy. Compd. 891, 161900 (2022) 
8. G.K. Agarwal, M. Magnusson, A. Johanson, Edge AI driven technology advancements paving way towards new capabilities. Int. J. Innov. Technol. Manag. 18(01), 2040005 (2021) 
9. M.K. Mudunuru, X. Chen, S. Karra, G. Hammond, P . Jiang, K.C. Solander, T.D. Scheibe, EdgeAI: How to use AI to collect reliable and relevant watershed data (No. AI4ESP-1095). Artiﬁcial Intelligence for Earth System Predictability (AI4ESP) Collaboration (United States, 2021) 
10. C. Mwase, Y . Jin, T. Westerlund, H. Tenhunen, Z. Zou, Communication-efﬁcient distributed AI strategies for the IoT edge. Future Gener. Comput. Syst. (2022) 
11. O. V ermesan, J. Bacquet, (eds.) (2019). Next generation Internet of Things: Distributed intelligence at the edge and human machine-to-machine cooperation (River Publishers) 
12. V . Mazzia, A. Khaliq, F. Salvetti, M. Chiaberge, Real-time apple detection system using embedded systems with hardware accelerators: an edge AI application. IEEE Access 8, 
9102–9114 (2020) 
13. H. Y u, D. Y u, C. Wang, Y . Hu, Y . Li, Edge intelligence-driven digital twin of CNC system: Architecture and deployment. Rob. Comput. Integr. Manufac. 79, 102418 (2023) 
14. E. De Giovanni, F. Forooghifar, G. Surrel, T. Teijeiro, M. Peon, A. Aminifar, D. Atienza Alonso, Intelligent edge biomedical sensors in the internet of things (IoT) era. in Emerging Computing: From Devices to Systems (Springer, Singapore, 2023), pp. 407–433 
15. Y . Singh, Z.A. Sheikh, On efﬁcient and secure multi-access edge computing for internet of things. in Proceedings of Third International Conference on Computing, Communications, and Cyber-Security (Springer, Singapore, 2023), pp. 299–310 
16. T.T.H. Kim, L. Lu, Y . Chen, ReRAM-based processing-in-memory (PIM). in Processing-in-
Memory for AI (Springer, Cham, 2023), pp. 93–12088 K. Joshi et al.
17. A.K. Sharma, P . Singh, P . V ats, D. Jain, Deep learning and machine intelligence for opera-
tional management of strategic planning. in Proceedings of Third International Conference on Computing, Communications, and Cyber-Security (Springer, Singapore, 2023), pp. 475–485 
18. A. Bourbah, B. Meliani, Z. Madini, Y . Zouine, The next-generation 6g: trends, applications, technologies, challenges, and use cases. in Proceedings of Seventh International Congress on Information and Communication T echnology (Springer, Singapore, 2023), pp. 761–770 
19. L. Zanella, Y . Wang, N. Dall’Asen, A. Ancilotto, F. Paissan, E. Ricci, M. Pistore, Responsible AI at the edge: towards privacy-preserving smart cities. in Ital-IA 2022 Convegno del Laboratorio nazionale CINI-AIIS (2022, March) 
20. M. Kirola, M. Memoria, A. Dumka, K. Joshi, A comprehensive review study on: optimized data mining, machine learning and deep learning techniques for breast cancer prediction in big data context. Biomed. Pharmacol. J. 15(1), 13–25 (2022) 
21. M. Diwakar, A. Tripathi, K. Joshi, M. Memoria, P . Singh, Latest trends on heart disease prediction using machine learning and image fusion. Mater. Today: Proc. 37, 3213–3218 (2021)Chapter 6 
Explainable Artiﬁcial Intelligence 
in Health Care: How XAI Improves User Trust in High-Risk Decisions 
Sheeba Praveen and Kapil Joshi 
Abstract Explainable AI (XAI) is a set of methodologies, design concepts, and 
procedures that assist developers and organizations in adding a layer of transparency 
to AI algorithms so that their predictions can be justiﬁed. AI models, their predicted 
impact, and any biases may all be described using XAI. Human specialists can grasp 
the forecasts generated by this technology and have trust in the results. Medical 
AI applications must be transparent in order for doctors to trust them. Explain-
able artiﬁcial intelligence (XAI) research has lately gotten a lot of attention. XAI is critical for medical AI solutions to be accepted and adopted into practice. Health 
care workers utilize AI to speed up and enhance a variety of functions, including decision-making, forecasting, risk management, and even diagnosis, by analyzing medical pictures for abnormalities and patterns that are undetected to the naked 
eye. Many health care practitioners already use AI, but it is frequently difﬁcult to understand, causing irritation among clinicians and patients, especially when making high-stakes decisions. That’s why the health-care business requires explainable AI 
(XAI). Signiﬁcant AI recommendations, such as surgical treatments or hospitaliza-tions, require explanation from providers and patients. XAI delivers interpretable explanations in natural language or other simple-to-understand formats, allowing 
physicians, patients, and other stakeholders to better comprehend the logic behind a suggestion—and, if required, to dispute its validity. 
Keywords Artiﬁcial intelligence ·Machine learning ·Deep learning ·Explainable 
artiﬁcial intelligence (XAI) ·Diagnosis ·Black box algorithm ·Neural network ·
Medical and industrial applications
S. Praveen (B) 
Integral University Lucknow, Lucknow, India 
e-mail: sheeba@iul.ac.in 
K. Joshi 
Uttaranchal Institute of Technology, Uttaranchal University, Dehradun, India 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_6 8990 S. Praveen and K. Joshi
1 Introduction 
In a Simple Word Explainable artiﬁcial intelligence (XAI) is “to understand how 
a ML model has arrived to a particular decision”[ 1]. An enterprise must have a 
thorough grasp of the AI decision-making processes, including model monitoring 
and AI responsibility, and not blindly trust them. Explainable AI can help people 
comprehend and explain machine learning (ML) and deep learning algorithms and neural networks. Explaining the predictions is crucial for model credibility; yet, there 
is a compromise between explainability and precision. Implementing traditional AI is full of challenges, let alone its explainable and responsible version. Despite the obstacles, it will bring relief to your employees, who will be more motivated to act 
upon the system’s recommendations when they understand the rationale behind it. Moreover, XAI will help you comply with your industry’s regulations and ethical 
considerations. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. One use case where 
XAI might be beneﬁcial is loan issuing. If a client’s loan application is denied, the system will be able to defend its decision and provide a clear explanation [
2]. 
When most people think of artiﬁcial intelligence, they see black box algorithms 
that take large amounts of data as input, do their “magic,” and then provide unex-plained answers that users are expected to believe. This kind of system is derived straight from data, and even its developers are unable to explain its conclusion. Black 
box systems, such as neural networks, perform very well on difﬁcult prediction prob-lems. No one can comprehend how the algorithms arrived at their predictions, despite the fact that they provide very accurate outcomes. With explainable white box AI, on 
the other hand, people can comprehend the reasoning behind the system’s actions, making it more popular in corporate settings. Black box algorithms are more techni-cally remarkable than these models. Nonetheless, its openness is a trade-off since it 
provides a greater degree of dependability and is preferred in strict regulatory indus-tries [
3]. Regarding explainability, everything boils down to what you really want to 
explain. There are two potential outcomes: 
1.1. Describe the ancestry of the AI model: how the design was trained, which data was utilized, the sorts of bias that are conceivable and how they may be 
addressed. 
1.2. Explaining the whole model is also known as “model interpretability.” There are two ways to implement this strategy: 
A. Proxy modelling: an easier-to-understand model, such as a decision tree, is utilized to approximate a more complex AI model. Even while this approach 
provides a brief summary of what to anticipate, it is still an estimate and actual outcomes may vary. 
B. Design towards interpretability: creating AI models such that their behavior is 
clear and straightforward to explain. This method may result in less effective 
models since it excludes complex tools from the developer’s toolset (Figs. 1 
and 2).6 Explainable Artiﬁcial Intelligence in Health Care … 91
Fig. 1 Explainable AI principles 
Fig. 2 Relationship between 
AI, ML, DL and XAI 
Artificial 
Intelligence 
Machine 
Learning 
XAI 
Deep Learning 
2 Explainable AI Principles 
The US National Institute of Standards and Technology (NIST) developed four 
explainable AI principles [ 4]. Four comprehensible AI principles were created by 
the National Institute of Standards and Technology (NIST) in the US [ 4]. 
2.1. The system must be able to justify its results and provide evidence to back them up (at least). There are several sorts of explanations:92 S. Praveen and K. Joshi
a. Justiﬁcations for the end-user; 
b. Justiﬁcations for building system trust; 
c. Justiﬁcations for the end-user; 
2.2. The explanation provided must be helpful in assisting users in completing their 
responsibilities. The system must provide a variety of explanations that are 
tailored to the different user groups if there are a variety of users with different skill levels. 
2.3. Unlike output accuracy, this explanation has to be accurate and understandable. 
To guarantee a decent result, the system must function within the boundaries of 
its speciﬁed knowledge. 
3 Why Is Explainable AI Important? 
For AI to be recognized in certain sectors, an explanation may be required. This may be because of rules or because of people. Think about the categories of brain tumors. No surgeon will feel conﬁdent doing a procedure because “the algorithm 
says so.” What about the awarding of loans? Customers who had their application rejected would want to know the why. There are much more tolerant use scenarios, yes, where a justiﬁcation is not necessary. For instance, even when using predictive 
maintenance programmers, personnel would feel more comfortable understanding why certain equipment could need preventative maintenance. 
Although senior management often recognizes the beneﬁts of AI applications, 
they are not without reservations. There is always a “but,” claims Gaurav Deshpande, vice president of marketing at Tiger Graph: “but if you can’t explain how you got at the solution, I can’t utilize it. This is because to the possibility of bias in the black 
box AI system, which might result in legal action, serious liabilities, and harm to both the company’s reputation and ﬁnancial sheet. The best XAI system will be 
relatively accurate and able to communicate its ﬁndings to end users, executives, and practitioners. Intelligent software using explainable AI ideas [
5]:
•Helps people who use the system. They understand why decisions were made and 
can agree with them. For example, a loan ofﬁcer will feel better about telling a 
customer that their loan application was turned down if he knows why.
•Ensures compliance. By checking the explanation given, users can see if the rules 
of the algorithm are good and follow the law and ethics.
•It lets the system be improved. It’s easier for designers and developers to identify and correct problems when they read the explanation.
•Eliminates prejudice. When users look at the explanation, they can see if the 
system made a biased decision, override it, and ﬁx the algorithm so that it doesn’t 
make the same mistake again.
•Gives employees the power to act on what the system says. For instance, a XAI 
might guess that a certain business customer won’t renew their software license. 
The ﬁrst thing a manager might do is give a discount. But what if the reason6 Explainable Artiﬁcial Intelligence in Health Care … 93
Fig. 3 Example of XAI in loan processing 
they left was because they got bad service? In its explanation, the system will say 
this. Gives people the power to act. XAI lets people who are affected by certain 
decisions, like granting a mortgage, challenge and possibly change the outcome [6].
4 Explainable AI Example 
XAI can give a detailed explanation at the model level of why a certain decision was 
made. This explanation is in the form of a set of clear rules. Every person over the 
age of 40 who has less than $433 in savings per month and who requests for credit with a repayment term exceeding 38 years will be rejected a loan, according to the 
simpliﬁed application below. The same is true for candidates under the age of 35 
who have a monthly savings of less than $657 (Fig. 3). 
5 Explainable Artiﬁcial Intelligence in Medical 
and Industrial Applications 
Explainable artiﬁcial intelligence (XAI) is all about “Understanding how a machine learning model reached its decision.” A corporation must have a comprehensive 
understanding of AI decision-making processes, model monitoring, and account-
ability, and must not blindly trust AI. Explainable AI might aid people in under-standing and explaining machine learning methodologies, deep learning, and neural 
networks. Due to the importance of model credibility, there is a trade-off between 
explainability and precision. Implementing traditional AI is tough, much alone its explicable and responsible form. Regardless of the obstacles, it will give relief to 
your team, who will be more likely to accept the system’s suggestions if they have 
a better understanding of the problem.94 S. Praveen and K. Joshi
6 XAI in Finances 
Finance is another area that has a lot of rules and needs to explain its decisions. 
AI-powered solutions must be able to be checked, or they will have a hard time getting into the market. AI can, among other things, help give out credit scores, 
evaluate insurance claims, and optimize investment portfolios. But if the algorithms give biased results, it could hurt their reputation and even lead to lawsuits. Not too long ago, Apple got a lot of attention for its Apple Card, which was unfair to women 
because it lowered their credit limits. Steve Wozniak, who helped start Apple, said that this was true. He remembered that he and his wife don’t have separate bank accounts or assets, but when they both applied for an Apple Card, his limit was 
10 times greater than hers. New Y ork’s Department of Financial Services examined the ﬁrm after this incident. With AI that can explain its results, people can avoid 
embarrassing situations like this. For example, XAI can help with the process of giving out loans. If a client’s loan application is rejected, the system will be able to explain why in detail and defend that decision. This lets people improve their credit 
scores and apply again later [
7]. 
7 Explainable AI in the Automotive Industry 
Self-driving cars use a lot of data, which needs to be analyzed and made sense of by AI. In the event of any problems, the system’s judgments must be accessible to drivers, technicians, authorities, and insurance companies. Also, it’s very important 
to know how vehicles will act in an emergency. Paul Appleby, a former CEO of the data management software ﬁrm Kinetica, expressed his worry as follows: “If a self-driving vehicle ﬁnds itself in a situation where an accident is likely, what steps 
it should take? Put pedestrians at serious risk to safeguard the safety of the driver? Avoid people on the street while putting the safety of the passengers at risk?” These 
are hard questions to answer, and different people would have different ideas about how to handle them. But it is important to give the algorithm rules it can follow in these situations. This will help people decide if they are comfortable riding in a 
car that can make certain decisions on its own. Also, the explanation given after an incident will help developers make the algorithm better in the future [
8]. 
8 Explainable Artiﬁcial Intelligence in Manufacturing 
AI has many uses in manufacturing, such as predictive maintenance, managing inven-tory, and improving logistics. This technology may enhance the “tribal knowledge” 
of human workers thanks to its analytical skills. But it’s easier to make decisions when you know why they’re good. Heena Purohit, Senior Offering Manager for6 Explainable Artiﬁcial Intelligence in Health Care … 95
IBM Watson IoT, demonstrates how explainable AI is used in their AI-based mainte-
nance product. In order to ﬁx a piece of equipment, the system gives human workers 
a number of possibilities. The conﬁdence interval for each choice is shown as a percentage. Therefore, while making a decision, the user may still draw on their 
“tribal wisdom” and experience. As well, each recommendation can show the output of the knowledge graph along with the information used during the training phase [
9]. 
9 Achieving XAI in Health Care 
AI can be used in many ways in healthcare. Numerous AI-powered medical solutions 
may free up physicians’ time from tedious activities so they can concentrate on 
providing direct treatment to patients. Additionally, algorithms may be taught to recognize minute features that the human eye misses, making them effective in 
diagnosing a variety of medical conditions [
10]. However, clinicians are reluctant 
to employ this technology and follow its suggestions when they cannot explain the 
results. Duke University Hospital is one place that shows this. A group of researchers 
set up a machine learning programme called Sepsis Watch on a patient’s device. This application would transmit a warning if the patient was at danger of getting sepsis. 
The researchers found that doctors didn’t trust the algorithm and didn’t follow its advice because they didn’t understand it. Patients don’t want to be checked by AI because they don’t trust it. In a study that was published in the Harvard Business 
Review, people were asked to take a free stress test. 40 percent of the people who signed up for the test did so because they knew it would be done by a real doctor. When an algorithm did the diagnosis, only 26% of people signed up. When it comes 
to ﬁguring out what’s wrong and how to treat it, the choices you make can change your whole life. It’s not surprising that doctors want to know what’s going on. This is now possible, thanks to AI that can be explained [
11]. For instance, Keith Collins, 
the chief information ofﬁcer (CIO) of SAS, said that his company is already working on such a technology. He said, “Right now, we’re working on a case in which AI analytics are being used to help doctors ﬁnd cancerous lesions more accurately.” The 
technology acts as the doctor’s “virtual assistant,” and it explains how, for example, each variable in an MRI image helps the technology ﬁgure out which suspicious areas are likely to be cancer and which are not. 
AI is used by healthcare professionals to expedite and enhance a variety of func-
tions, including risk management, decision-making, and even diagnosis, by scanning 
medical pictures to ﬁnd abnormalities and patterns that are invisible to the human 
eye. Many health care professionals now use AI as a necessary tool, but it is some-times difﬁcult to understand, which frustrates both patients and doctors, particularly 
when making important decisions [
12] (Fig. 4).
According to article [ 13] review, they used XAI to perform a study of the most 
current developments in surgical and medical diagnostic applications between 2019 
and 2021 from PubMed, IEEE Xplore, the Association for Computing Machinery,96 S. Praveen and K. Joshi
Fig. 4 XAI process
and Google Scholar. In his article, they show how XAI can be used to diagnose 
breast cancer through an experiment and show how XAI can be used in medical 
applications. In the last part of the paper, a summary is given of the XAI methods 
used in medical XAI applications, the problems that researchers have faced, and where they think research should go in the future. The results of the survey show 
that medical XAI is a promising area of research, and the goal of this study is to give medical experts and AI scientists a place to start when making medical XAI applications [
14]. 
10 Why XAI is Important in Health Care 
The majority of artiﬁcial intelligence (AI)-based diagnostic tools for healthcare work as “black boxes,” which means that the ﬁndings do not provide an explanation of why the computer believes the patient has a certain illness or ailment. Although 
AI technologies are very strong, since physicians and regulators cannot indepen-dently check their outcomes, adoption of these algorithms in healthcare has been gradual. However, a brand-new kind of algorithm known as “explainable AI” (XAI) 
is simple for people to comprehend. Therefore, it is probable that providers will really apply the linked diagnoses since all indications point to XAI being quickly accepted throughout the health care industry [
11]. The black box feature of AI is 
acceptable—and maybe even desirable—for many industries outside of healthcare because it enables businesses to protect their valuable trade secrets. For instance, deep learning in AI recognizes speech patterns to enable a user’s preferred voice6 Explainable Artiﬁcial Intelligence in Health Care … 97
assistant to start their favorite movie. Deep learning algorithms identify connections 
and patterns without ever requiring the operators to be aware of the data elements 
that are most crucial to the outcome. For many uses of AI, there is no risk in relying on it to continue to provide accurate results since the outcomes verify the algorithms. 
However, the black box nature of AI makes it difﬁcult for physicians and regulators 
to trust it, maybe for good reason, in industries like health care where errors can have fatal consequences. Doctors are largely taught to recognize outliers, or unusual 
situations that don’t respond to conventional therapies. We cannot be certain that an AI system would recognize such outliers or otherwise accurately diagnose patients, for example, if it has not been trained correctly with the necessary data and we are 
unable to comprehend how it makes its decisions. 
The FDA, which now veriﬁes AI algorithms by looking at what kind of data is 
fed into the algorithms to make their judgments on the data, ﬁnds the black box 
component of AI to be troublesome for the same reasons. A doctor stands between 
the solution and the patient’s ultimate diagnosis or course of action, which is another reason why many AI-related developments bypass the FDA [
7]. 
For instance, the FDA continues to mandate that doctors be able to independently 
verify the rationale behind the software’s recommendations in its most recent draught 
guidance, which was published on September 28. This is done to prevent the software 
from drawing increased scrutiny as a medical “device.” Because physicians can verify the results of the algorithms, software is only loosely controlled. Think of a medical 
imaging where clinicians may double-check worrisome lumps that the algorithm has ﬂagged. The difﬁculty for doctors with algorithms like deep learning, however, is that they lack knowledge of the context in which a diagnosis was made. 
Because of this, the XAI algorithms being created for use in the health care 
industry can justify their ﬁndings in a way that people can comprehend. Many of the XAI algorithms created so far are rather basic, such decision trees, and have very 
speciﬁc applications. But as they advance, they will probably be the algorithms that dominate the healthcare industry. Companies in the healthcare industry would be advised to invest in its growth [
11]. 
11 A Case Study “Life or Death: West Nile Virus” 
This case study focused on how XAI assists a physician in saving the life of a 
patient with West Nile virus. Case study demonstrates the Key characteristics of XAI 
utilizing a key medical case of an early identiﬁcation of a West Nile virus infection 
in a person. Without AI and XAI, the patient’s life may have been jeopardized. There 
are four characters in this story: the patient, the West Nile virus, the physician, and the AI + XAI software. The doctor learns that AI and XAI just saved a life and 
comes to trust AI via XAI at the conclusion of the story. On the long road ahead, this symbolizes one of the ﬁrst stages of collaboration between people and machines [
15].98 S. Praveen and K. Joshi
12 XAI—Issues and Challenges 
There are still several current concerns and obstacles at the junction of machine 
learning and explanation [ 3].
•Starting with computers as opposed to humans. Should XAI systems person-alize explanations for speciﬁc users? Should they consider the lack of expertise among users? How can explanations be used to facilitate interactive and human-
in-the-loop learning, including allowing users to engage with explanations to offer feedback and drive learning?
•Precision versus interpretability an important stream of XAI research on explana-
tion investigates interpretability’s strategies and constraints. Interpretability must 
evaluate accuracy and ﬁdelity tradeoffs and achieve a balance between precision, interpretability, and tractability.
•Simplifying explanations by using abstractions. Patterns at the highest level provide the foundation for deﬁning large plans in large phases. It has long been difﬁcult to automate the discovery of abstractions, and today’s cutting-edge XAI 
research is focused on understanding how abstractions are discovered and shared in learning and explanation.
•Comparing the explanation of skills to that of choices. Reﬂecting on novel circum-
stances is an indication of expertise among highly skilled specialists. It is vital 
to assist end users comprehend the competences of AI systems in terms of what 
competencies a certain AI system has, how the competencies should be tested, 
and if an AI system has blind spots, i.e., whether there are classes of solutions it cannot discover. 
13 Conclusion 
This Chapter mainly focuses on the XAI versus AI Introduction. How XAI is a more 
effective technology in the medical industry. For the most part, medical diagnostics 
that make use of AI are “black boxes,” meaning that the ﬁndings don’t provide any context for why the computer came to this conclusion about the patient. Although AI 
technologies are exceptionally strong, their acceptance in health care has been gradual due to the inability of physicians and regulators to validate their outcomes. However, humans can readily comprehend XAI. All indications indicate to the quick adoption 
of XAI throughout the health care industry, making it probable that providers will use the accompanying diagnoses. 
From the standpoint of human-centered research, study on competences and 
knowledge might move XAI beyond the function of describing a speciﬁc XAI system and assisting its users in determining acceptable trust. In the future, XAIs could play signiﬁcant societal roles. These tasks may involve learning and explaining, working 
with other agents to link information, establishing cross-disciplinary insights and common ground, cooperating in educating people and other agents, and relying on6 Explainable Artiﬁcial Intelligence in Health Care … 99
previously acquired knowledge to expedite future discovery. From such a societal 
standpoint of comprehending and producing information, XAI’s future is just starting. 
References 
1. D. Gunning, M. Steﬁk, J. Choi, T. Miller, S. Stumpf, G.Z. Yang, XAI—Explainable artiﬁcial 
intelligence. Sci. Rob. 4(37), eaay7120 (2019) 
2. A.B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, F. Herrera, Explainable artiﬁcial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58, 82–115 (2020) 
3. A. Das, P . Rad, Opportunities and challenges in explainable artiﬁcial intelligence (xai): A survey. arXiv preprint 
arXiv:2006.11371 (2020) 
4. D. Gunning, Explainable artiﬁcial intelligence (xai). Defense Adv. Res. Projects Agency (DARPA), nd Web 2(2), 1 (2017) 
5. J. Gerlings, A. Shollo, I. Constantiou, Reviewing the need for explainable artiﬁcial intelligence (xAI). arXiv preprint 
arXiv:2012.01007 (2020) 
6. R.M. Byrne, Counterfactuals in explainable artiﬁcial intelligence (XAI): evidence from human reasoning. In IJCAI (2019, August), pp. 6276–6282 
7. E. Tjoa, C. Guan, A survey on explainable artiﬁcial intelligence (xai): toward medical xai. IEEE Trans. Neural Netw. Learn. Syst. 32(11), 4793–4813 (2020) 
8. F. Bodendorf, S. Merbele, J. Franke, Deep learning based cost estimation of circuit boards: a case study in the automotive industry. Int. J. Prod. Res. 1–22 (2021) 
9. J. Liu, H. Tu, F. Xai, X. Yang, W. Zhang, A calculation model of equipment similarity on manufacturing capability and its application. in 2010 8th World Congress on Intelligent Control 
and Automation (IEEE, 2010, July), pp. 1686–1689 
10. U. Pawar, D. O’Shea, S. Rea, R. O’Reilly, Explainable AI in healthcare. in 2020 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA) (IEEE, 2020, June), pp. 1–2 
11. M. Nazar, M.M. Alam, E. Yaﬁ, M.S. Mazliham, A systematic review of human-computer inter-action and explainable artiﬁcial intelligence in healthcare with artiﬁcial intelligence techniques. IEEE Access (2021) 
12. A. Adadi, M. Berrada, Explainable AI for healthcare: from black box to interpretable models. In Embedded Systems and Artiﬁcial Intelligence (Springer, Singapore, 2020), pp. 327–337 
13. Y . Zhang, Y . Weng, J. Lund, Applications of explainable artiﬁcial intelligence in diagnosis and surgery. Diagnostics 12(2), 237 (2022) 
14. M. Ahmed, S. Zubair, Explainable artiﬁcial intelligence in sustainable smart healthcare. in Explainable Artiﬁcial Intelligence for Cyber Security (Springer, Cham, 2022), pp. 265–280 
15. The XAI medical diagnosis timeline|Hands-On Explainable AI (XAI) with Python (pack-tpub.com)Chapter 7 
Role of Explainable Edge AI to Resolve 
Real Time Problem 
Ambeshwar Kumar, T. M. Rajesh, Manikandan Ramachandran, 
and Deepak Gupta 
Abstract The growth of information technology (IT) has resulted in physical de-
vices being connected to the internet and having the ability to recognize other devices. 
Artiﬁcial Intelligence algorithms are processed on edge or the devices of users. Edge 
Computing based on the same premise, stores, processes, and manages data directly 
at Internet of Things (IoT) endpoints. Edge artiﬁcial intelligence uses the device’s 
hardware to process data and performs machine learning and deep learning proce-
dures. In the model, you can troubleshoot and improve model performance while also assisting others in understanding the behavior of your models. To make the area 
more real, explainable edge devices come in a wide range of costs and capabilities. A decade ago, we couldn’t imagine that explainable edge artiﬁcial intelligence would be at today’s level. Now it is a part of industries and even devices for customer service. 
The best example of explainable edge AI is virtual assistants such as Alexa, google assistant. They learn from the user’s world and phrases and can store them directly on the device. These are just a few examples later, and we have possible applica-
tions in future works on the explainable edge artiﬁcial intelligence. Edge Computing Platform facilitates the development and elastic operation of apps and services. Its beneﬁts the AI assisting in overcoming the technical obstacles that AI-enabled apps 
experience. Combination of edge and AI is buzzwords within the industry to deliver the performance and reduce the cost compared to state of arts XAI applications. Moreover edge artiﬁcial intelligence are reducing the latency, improving user expe-
rience, and reducing the necessary bandwidth, consequently reducing the costs of internet services. It surfs this movement since the need for data processing on the 
device themselves also represents the increasing use of artiﬁcial intelligence. Arti-ﬁcial intelligence edge processing focused on model which trained them in central data center using historical datasets. Compression techniques of data that enables
A. Kumar (B) · T. M. Rajesh 
Dayananda Sagar University, Bangalore, Karnataka, India 
e-mail: ambeshwar.kumar@gmail.com 
M. Ramachandran 
SASTRA Deemed University, Thanjavur, Tamilnadu, India 
D. Gupta 
Maharaja Agrasen Institute of Technology, Delhi, India 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_7 101102 A. Kumar et al.
squeezing large artiﬁcial intelligence models into small hardware form factors could 
push some training to the edge over time. 
Keywords Explainable artiﬁcial intelligence (XAI) ·Edge computing ·Deep 
learning ·Machine learning ·Internet of things (IoT) ·Neural network ·Cognitive 
science 
1 Introduction 
Explainable artiﬁcial intelligence (XAI) is a set of processes and methods that allows 
human users to comprehend and trust the results and output created by machine 
learning algorithms. Explainable AI is used to describe an AI model, its expected 
impact and potential biases. It helps characterize model accuracy, fairness, trans-parency and outcomes in AI-powered decision making. Explainable AI is crucial for 
an organization in building trust and conﬁdence when putting AI models into produc-tion. AI explainability also helps an organization adopt a responsible approach to AI 
development. 
As AI becomes more advanced, humans are challenged to comprehend and retrace 
how the algorithm came to a result. The whole calculation process is turned into 
what is commonly referred to as a “black box” that is impossible to interpret. These 
black box models are created directly from the data. And, not even the engineers or data scientists who create the algorithm can understand or explain what exactly is 
happening inside them or how the AI algorithm arrived at a speciﬁc result. There are many advantages to understanding how an AI-enabled system has led to a speciﬁc output. Explainability can help developers ensure that the system is working as 
expected, it might be necessary to meet regulatory standards, or it might be important in allowing those affected by a decision to challenge or change that outcome. 
1.1 Deﬁnition of Explainable Artiﬁcial Intelligence 
The explainable artiﬁcial intelligence (XAI) is a strong tool for addressing ethical 
and legal concerns regarding AI systems by answering important how? and Why? 
Questions. As a result, AI researchers have recognized XAI as a necessary component of reliable AI, and explainability has recently gotten a lot of attention. Despite the 
increased interest in XAI research and the requirement for explainability across a wide range of areas, XAI still has a number of drawbacks. This blog post provides an overview of the current state of XAI, as well as its advantages and disadvantages. 
Interpretability aids in ensuring objectivity in decision-making by detecting and correcting bias in the training dataset. It provides resilience by indicating potential adversarial disturbances that modify the prediction. It also assure that only relevant7 Role of Explainable Edge AI … 103
Fig. 1 Brain tumor MR 
image and heat map showing tumor region 
variables are used to infer the result, i.e., that the model reasoning is based on genuine 
causality. 
Explanations can take many different forms depending on the goal and situation. 
Human-language and heat-map explanations of model activities are shown in Fig. 1. 
The machine learning model presented below is designed for doctors to utilize and 
can diagnose brain tumor using MRI. Based on the MRI on the far left, the original 
report gives a “ground-truth” report from a doctor. The Generated report includes an 
explanation of the model’s diagnosis as well as a heat-map of the tumor locations 
that inﬂuenced the conclusion. The Generated report offers doctors with an easy-to-understand and validated explanation of the model’s diagnosis. 
1.2 Edge AI Differ from Artiﬁcial Intelligence 
Explainable AI is about making sense of black box models and AI is about making 
sense of transparent AI models. XAI implements efﬁcient techniques and methods 
to ensure that every decision made during the process can be traced and explained whereas in AI it often arrives at a result using the algorithms but the arrangements 
of the systems do not fully understand how the algorithms reached to the results. AI has achieved growing momentum in many ﬁelds to deal with the complexity. Humans do not grasp the complex mechanisms by which AI systems work or how 
they make certain decisions because AI-powered systems have progressed to the point where humans do not understand the complex processes by which AI systems 
work or how they make certain decisions. There’s a risk that an AI model won’t be considered trustworthy or authentic if the core functionality and judgments it makes aren’t explained. The role of domain knowledge interacting with an XAI system 
in a realistic complex decision-making scenario, XAI brings the needed clarity and openness to AI-based solutions, allowing for increased trust [
1]. As a result, XAI is 
recognized as a critical feature for the practical deployment of AI models in systems 
and, more critically, for ensuring those AI users’ fundamental rights to AI decision-
making are respected. Our ultimate objective is to provide a comprehensive taxonomy to newcomers in the ﬁeld of XAI that can be used as a reference tool to encourage 
future research advances, as well as to encourage experts and professionals from other disciplines to embrace the beneﬁts of AI in their respective activity sectors, without prejudice due to its lack of interpretability.104 A. Kumar et al.
1.3 Explainable Edge AI Can Assist Internet of Behavior 
Effort 
According to academics, applying XAI to Internet of Behavior approaches could 
enable provide a more trustworthy and understandable foundation for modifying 
human behavior. According to a study, the convergence of Internet of Things devices, artiﬁcial intelligence, data analytics, and behavioral science can provide 
both consumer and corporate beneﬁts. The explainable technique also aids in over-coming the ostrich effect, which is a major problem when using internet of Behavior. 
“The ostrich effect,” she explains, “is a difﬁculty for the Internet of Behaviors because people are essentially terriﬁed”. “And, because it’s such a delicate subject, you may ﬁnd opposition when attempting to inﬂuence and modify people’s behavior” [
2], 
you’re dealing with sensitive information as well as their actions. To avoid this, as well as to manage this resistance and other psychological aspects connected to comfort and stress, XAI assists us in providing the user with the necessary understanding and 
trust for the AI-based system.“ 
The internet behavior model refers the collection of data through emerging tech-
nologies. It generates useful knowledge about the customer behaviors, desire and 
preferences. It is also useful for predicting user behavior. It is shown in Fig. 2.T h e 
use of XAI approaches to interpret black-box generalization proved to be promising, 
providing reliable outputs that could be compared to experts’ interpretations of the 
same problem and prompting further studies in which it analyzed learning generaliza-
tion [ 3]. Given that it focuses on human nature, IoB can be a strong instrument. The 
tendency to act is caused by the behavioral component of humans, not necessarily 
other attributes such as cognition, emotion, personality, or communication, and is a 
potent factor when paired with digital networks and technologies. In 2012, IoB was ﬁrst discussed. Professor Göte Nyman of the University of Helsinki’s Department 
of Psychology is quoted as saying that if human behavior was assigned to devices
Fig. 2 Internet behavior model7 Role of Explainable Edge AI … 105
with speciﬁc addresses, there would be an opportunity to beneﬁt from the knowledge 
gained by analyzing the history of patterns in many ﬁelds, including business, soci-
etal, health, political, and many others. According to a CISCO report cited by the researcher, the IoT sensor commercial market is estimated to reach $22.48 billion 
by 2023, with a predicted 29.3 billion linked devices accessible [
4].
1.4 Organization of the Chapter 
The rest of the chapter is organized as follow: Sect. 9.2 represents the concepts of 
explainable artiﬁcial intelligence with beneﬁts and challenges. Section 9.3 illustrates 
the example of explainable artiﬁcial intelligence in real world with the future of XAI. 
Finally, Sect. 9.4 discussed the future of XAI summarizes the work with conclusion 
follow with references. 
2 Concepts of XAI 
XAI will develop a set of algorithms that will enable human users to com-prehend, trust, and manage the next generation of artiﬁcially intelligent part-ners. This deﬁni-
tion combines two notions (understanding and trust) that must be addressed before 
proceeding. It describes a model’s ability to explain its function—how it works—to 
a person without having to explain its internal structure or the computational means 
by which the model processes data in-side. Trust can be deﬁned as the belief that a model will behave as planned, a speciﬁc issue although it should undoubtedly be a 
feature of any explainable model [
5], and this does not imply that every trustworthy 
model is explainable on its own, nor does it indicate that trustworthiness is a property of each explainable model a feature that is easily quantiﬁable. 
2.1 Major Principles in XAI 
The major principles in XAI deals with the Intelligence to interpret predictions from 
machine learning models, resiliency, reliability, bias, and account-ability are some of 
the other characteristics. In most cases, these phrases are deﬁned as part of a larger set 
of principles or pillars. The four principles of explainable artiﬁcial intelligence are 
highly affected by the relationship of AI system with human recipient information. Before proceeding to the principles of the XAI, the key term i.e. the output of an 
AI system. [
6] For an example grammar checking system, the output is grammatical 
error and correction recommendation. For disease detection system, the output is detected disease and suggested precautions. The XAI objective is to provide the 
information with trust, conﬁdence, informative and transparency, Fig. 
3 provides the106 A. Kumar et al.
Fig. 3 Principles in XAI 
pictorial representation of principles in XAI. 
The four principles are as follow:
•Explanation
•Meaningful
•Explanation accuracy
•Knowledge limits 
2.1.1 Explanation 
This principle obligates AI system to provide proof, support and logic for each output. 
It does not require that the evidence be correct, informative, or understandable in and 
of itself; it only implies that a system can provide an explanation. Currently, a corpus of work is being done to build and validate explainable AI approaches. It imposes 
no demands on those explanations in terms of quality. The ideas of Relevance and Explanation Accuracy establish a framework for assessing explanations. 
2.1.2 Meaningful 
If the recipient understands the system’s explanations, it satisﬁes the Meaningful principle. The user understands the explanation, if it is helpful in completing a task. 
This idea does not imply that there is a one-size-ﬁts-all answer. For a system, multiple explanations may be required for distinct groups of users. Because of the Meaningful principle, explanations can be adapted to each user group. Developers of a system 
versus end-users of a system, Doctor/practitioner versus researcher and so on are7 Role of Explainable Edge AI … 107
examples of wide groups. The goal of this principle may vary, e.g. what is mean-
ingful to a doctor may be different than what is meaningful for a researcher. So, 
the conﬂuence of the AI system’s explanation and a person’s existing knowledge, experiences, and beliefs inﬂuences meaningfulness. The challenge in modeling the 
interface between AI and humans is due to all of the factors that determine meaning-fulness. Both computational and human variables must be taken into account when developing systems that produce relevant explanations. 
2.1.3 Explanation Accuracy 
The Explanation and Meaningful principles only need a system to create explanations that are meaningful to a user group when used together. This principle exposes 
accuracy on a system’s explanations. In the same way as the Meaningful principle allows for distinct explanation accuracy metrics for different groups and individuals, this principle allows for different explanation accuracy metrics for different groups 
and individuals. Some clients will require concise explanations that focus on the important point(s) but lack the intricacies required to fully explain the algorithm’s 
output generation process. However sometimes nuances data may be meaningful to experts. Overall, if a system can create multiple types of explanations, it may be deemed more explainable. Because of the various layers of explanation, the metrics 
used to assess an explanation’s accuracy may not be universal or absolute. 
2.1.4 Knowledge Limits 
This principle asserts that systems detect situations in which they were not designed 
or approved. To operate in which their responses are not trustworthy. This method 
protects answers by identifying and disclosing knowledge boundaries, ensuring that no judgment is made when it is not necessary. Two ways, through which system can 
reach its knowledge limits, initially the question can be outside of the domain of the system. For an example, system has to classify the wild animals but the user may input the image of parrot. The system may produce an answer indicating that it was 
unable to locate any wild animals in the input image and hence could not provide an answer; this is a response as well as an explanation. Another ways depending on an internal trust level, a knowledge limit can be reached, and the conﬁdence of the most 
likely solution may be too low, for an e.g. reptiles classiﬁcation system, the input image of a lizard is too blurry to identify. In this situation the system may recognize 
the image of reptiles, but it is in low quality, explanation is found the image of reptiles but the quality of an image is too low to identify it.108 A. Kumar et al.
Fig. 4 Flow process of how XAI works 
2.2 Working Scenario of XAI Principles 
The description of XAI principles explained in Sect. 9.2.1 , this section explains the 
working environment of XAI. It examines each aspect and its impact on the ﬁnal 
result. Users can use this analysis to create new situations and see how different input 
values affect the output. Users may see the factors that have a positive or negative impact. 
Success in machine learning has led towards artiﬁcial intelligence applications, 
continuity to deliver better to the end client. Developing an autonomous system to learn and decide automatically. To maintain high level of accuracy and prediction 
explainable model work efﬁciently. It enables human user to understand, trust and efﬁciently manage the artiﬁcially intelligent partners. In the Fig. 
4 new machine 
learning system will be able to justify their actions, identify their advantages and 
disadvantages, and communicate how they will act in the future. It is shown in 
Fig. 4. 
2.3 Beneﬁts of XAI 
Number of advantage in XAI which put them in limelight and emphasizes the need 
for faster large-scale adaptation around the world, the beneﬁts of XAI can be found 
across wide range of sectors and job activities. Some of the beneﬁts of XAI are listed below:
•Building user trust
•Reducing cost of mistakes
•Satisfying legal requirements
•Code conﬁdence and compliance7 Role of Explainable Edge AI … 109
2.3.1 Building User Trust 
Using the XAI in the system the user can understand how the output is generated with 
relevant inputs. When the error occurs and the procedure to reduce the error. It also 
provides the risk assessment report for the risky posed by the automated decision 
making system, so the user trust the system. 
2.3.2 Reducing Cost of Mistakes 
In the event of incorrect prediction, decision-sensitive professions such as healthcare system, ﬁnance system, and law administrative system are severely impacted. Over-sight of the ﬁndings decreases the impact of incorrect results while also identifying 
the main problem, allowing the underlying model to be improved. 
2.3.3 Satisfying Legal Requirements 
AI model have shown most signiﬁcance evidence of bias. It has proven a game-changer for many users. User is seeing improved to increase productivity and efﬁciency hence the legal requirements is satisfy using the XAI. 
2.3.4 Code Conﬁdence and Compliance 
XAI provide the explanation knowledge with the accuracy tends to increase the system’s conﬁdence. Critical system such as banking and healthcare system re-quires 
high code conﬁdence from user for optimal solution. Increasing regulatory pressure implies that companies should easily adapt and implement XAI in order to comply with the government. 
2.4 Challenges in XAI 
Using black box strategies to make an unexplained conclusion might result in legal, 
ethical, and operational issues. Because black-box models are not veriﬁable or trace-
ability prior to deployment making assurances about their behavior difﬁcult, explain-
ability is an issue that is both scientiﬁcally and socially relevant, and it is at the core 
of several areas of contemporary machine learning and AI research. The following are some of XAI’s challenges:110 A. Kumar et al.
2.4.1 Bias 
It’s a difﬁcult challenge to make sure that the AI algorithm doesn’t learn biased or 
unbiased world views as a result of gaps in the training data, model, or objective 
function. 
2.4.2 Safety 
It’s tough to tell whether AI is trustworthy or not without looking at how it came to its conclusions. Because of the generalization in statistical learning theory, which 
essentially shows how organizations cover the gaps in unseen knowledge, this is a 
challenge. 
2.4.3 Fairness 
The impression of fairness is contextual and depends on the information provided to 
the machine learning algorithms, therefore determining whether the choice made by 
AI systems was fair or not is a difﬁculty for XAI. 
2.4.4 Possible Way to Overcome the XAI Challenges Provides 
Meaningful Explanations 
Model Agnostic Technique 
This strategy can be applied to any number of algorithms or learning methods. The model agnostic approach will aid businesses in approaching XAI’s internal workings as a mysterious black box. 
Model-Speciﬁc Technique 
This method can be used for a small number of algorithms or a speciﬁc collection of algorithms. The internal workings of XAI are treated as a white box in the model 
agnostic approach. 
3 Example of XAI in Real World 
XAI is a key research ﬁeld. Our AI systems could become more reliable, legal, efﬁcient, fair, and robust with the help of explanations of their workings, which could 
increase their acceptance and economic value. General opportunities and challenges7 Role of Explainable Edge AI … 111
in XAI are analyzed potentially pros and cons from their perspective to overcome 
the challenges in XAI [ 7]. Most of the organizations are working on their proprietary 
XAI solutions. Real world implementation of XAI is discussed in this section. 
3.1 XAI in Defense 
Artiﬁcial general intelligence (AGI), a relatively new branch of AI research that 
focuses on brain growth and, eventually, the development of sentient machines, might 
give rise to robots. However, the use of machine learning a computer’s capacity to 
learn without explicit programming and neural networks computer systems based after the human brain and nervous system to improve human decision-making is 
much more common. In order to enable human users to comprehend and trust (while managing) the artiﬁcially intelligent agents are being built, a range of explainable machine learning models must be developed while preserving their predictive accu-
racy [
8]. The XAI programed selected 12 areas of focus for its varied areas of interest 
after receiving hundreds of ideas. One of them is deep explanation, which is effec-
tively an inversion of deep learning in which the machine examines and then anal-
yses its own explanation using machine learning. It attempts to explain the inner workings of the neural network’s own “black box” processes. Consider the hours of 
drone-recovered video that you want to go through for crucial frames or sequences. 
Figure 5 illustrates the framework of robot developed by Indian army with XAI 
multi-layered architecture is capable of providing multitude of military applications. 
Indian army has already built the snake robot, legged robot and wheeled robot etc. 
Fig. 5 Multi agent robotics 
framework
112 A. Kumar et al.
3.2 XAI in Autonomous V ehicle 
The concept of explainable AI (XAI) is becoming increasingly popular. Users of 
AI systems will almost probably seek an explanation and expect it. A machine-produced explanation of what the AI has done or is doing will be in great demand 
given the rapidly expanding number of AI systems [
9, 10]. One such research area 
is autonomous vehicles (A Vs). It will gradually develop autonomous methods of transportation with the aim of realizing the slogan “mobility for everyone. “There 
will be a variety of autonomous vehicles available, including self-driving trucks, motorcycles, submarines, drones, and planes. To be clear, fully autonomous vehicles are those in which the AI operates without any help from a human while the vehicle 
is being driven. While cars that require a human driver to share the driving effort are typically classiﬁed as Level 2 or Level 3, these driverless vehicles are considered 
Level 4 and Level 5. At Level 5, there isn’t a real autonomous vehicle—we don’t even know if it will be possible or how long it will take to get there. While there is debate about whether this testing should be permitted, the Level 4 attempts are 
progressively attempting to gain traction by participating in very limited and selected public roadway trials. One thing to mention right away is that the AI used in modern AI driving systems is not sentient. In other words, the AI is mostly a collection 
of computer-based algorithms and programming, and it is deﬁnitely not capable of reasoning in the same way that people can work. 
Figure 6 explain the framework of driverless vehicles feasible for all humans. 
The XAI will charge of driving with all necessity equipment’s required in driving a vehicle. The controller actions are limited by visual attention. The XAI features vehicle is ready to handle the scenario. 
Fig. 6 Autonomous vehicle framework7 Role of Explainable Edge AI … 113
3.3 XAI in Fraudulent Activities 
The developing ﬁeld of explainable AI (or XAI) can give banks more clarity on 
their AI governance while assisting them in navigating concerns of transparency and trust [
11]. The goal of XAI is to improve the explainability, usability, and compre-
hension of AI models for human users without compromising performance or fore-cast accuracy. A strong XAI programmed can beneﬁt ﬁrms in a variety of other ways as well. Depending on the questions being asked and the modeling methods 
being employed, explainability tools can reveal various forms of information about a model. To lead explainability initiatives in conjunction with their AI teams and business departments, several banks are forming speciﬁc task forces. The accuracy 
of a company’s total fraud detection is impacted by the insights that XAI provides to teams by explaining why models are performing or not performing. Teams cannot 
be expected to know how to increase fraud detection rates without this important understanding. The two fundamental elements of XAI—feature importance (which is technically connected to XAI) and model evaluation and performance—deliver on 
these promises. Features look over a period of time speciﬁed data, such as payment type, transaction amount, location, and so forth. Before deploying the model, teams can test the model’s performance. Prior to relying on a machine learning model for 
fraud detection and prevention, organizations should test the model’s performance to determine its accuracy and false positive rate. 
3.4 XAI in Marketing 
Artiﬁcial intelligence (AI) marketing makes judgments automatically using data collecting, analysis, and further observations of audience or economic trends that 
could have an impact on marketing activities. One of the most important steps in starting a XAI marketing programmed is choosing the appropriate platform or 
platforms. Marketing professionals need to be astute in spotting the holes that the platform is attempting to ﬁll and wise in choosing solutions based on skills. This will depend on the objective that marketers are attempting to fulﬁll. For instance, 
products used to increase speed and productivity will need to have different function-ality than those used to raise XAI consumer satisfaction levels generally [
12, 13]. 
XAI Reduces the Latency and Increases Precision & Relevancy in the Campaign 
Process of product. The current method is simply too sluggish, too inaccurate, and 
too irrelevant to the customer in a one-to-moment environment where the relevance of marketing messages and customer experiences are crucial for establishing trust 
and keeping hold of existing clients. Through similarity-based XAI, not in a narrow sense but in a broader sense, it is possible to reduce latency by 50% (for those processes that bookend the campaign window itself) and increase precision and rele-
vance by 20–100% or more. This technology can be activated across the workﬂow in a phased approach for maximum impact. By drawing the most important insights114 A. Kumar et al.
from their datasets and acting on them in real time, marketers can use XAI to alter 
their entire marketing campaign if it is used properly. Customers can receive individ-
ualized messages at the right times throughout the consumer life cycle with the aid of XAI. Additionally, XAI can assist marketers in identifying at-risk clients and deliv-
ering information to them that will encourage them to re-engage with the business. When compared to humans, XAI can analyses tactical data more quickly and apply machine learning to reach quick decisions depending on campaign and customer 
context. Team members now have more time to concentrate on strategic projects that will later guide XAI-enabled campaigns. With AI, marketers can use real-time data to choose better media instead of waiting until the conclusion of a campaign 
to make selections. Consumers react differently to communications across channels; some may be moved by comedy, others by logic, and others by an emotional appeal. XAI and machine learning can track which messages customers have interacted with 
and build a more thorough user proﬁle. From there, marketing departments can send users more tailored communications based on their choices. 
3.5 Summary 
This chapter deﬁnes the brieﬂy introduction of explainable artiﬁcial intelligence and 
explain about the internet behavior efforts with real world examples of XAI. The 
major points to recall are as follow:
•Explainable Artiﬁcial Intelligence is useful in building trust and conﬁdence when 
putting AI models into production.
•AI is differing from XAI in terms of black box decision.
•XAI approaches to interpret black-box testing.
•Behavioral efforts assist through XAI
•XAI deals with the Intelligence to interpret predictions from machine learning models, resiliency, reliability, bias, and accountability are some of the other 
characteristics.
•Users can use this analysis to create new situations and see how different input values affect the output.
•In defense XAI to improve human decision-making in more effective to deal with 
the real time problems.
•Autonomous vehicle is the rapidly expanding on road with effective features and ease of access.
•XAI helps in fraudulent activities to detect the intruder and block their hacking 
activities through blocking the card immediately.
•In marketing sector XAI achieves a tremendous success to deliver appropriate 
messages to customer and establishing trust and keeping hold of existing clients.7 Role of Explainable Edge AI … 115
4 Future of XAI 
Since 2000, there have been 14 times as many active AI startups. Recent develop-
ments in deep learning have enabled AI to handle a variety of marketing and sales choices as well as search engines, virtual assistants, and online translators. The level 
of autonomous driving technology is quickly rising to that of fully automated driving. Despite Elon Musk’s claim that “over a million robo taxis would be on the road for sure by next year,” Modern GPUs are now effective enough to be utilized for activ-
ities other than visual rendering, such machine learning or crypto currency mining. Data scientists found that these are repeated parallel jobs even if CPUs are typically employed to do them. GPUs are therefore commonly employed in AI models to facil-
itate effective learning. A type of autonomous supervised learning is self-supervised learning (also known as self-supervision). This method, unlike supervised learning, 
handles the labeling process on its own and does not rely on humans to categories the data. 
5 Conclusion 
The objective of this chapter has been contributed towards understanding the concepts of explainable artiﬁcial intelligence to stores, processes, and manages data directly at 
Internet of Things (IoT) endpoints. The internet behavior efforts promising to provide reliable outputs that could be compared to experts’ interpretation, the explainable artiﬁcial intelligence is growing rapidly in every sector of organization to complete 
the task in easiest and user friendly. It will provide the steps through which end user trust the system. The overall structure of this chapter provides an efﬁcient knowledge to the user to understand the concept of XAI. 
Consent for Publication No any Consent for Publication. 
References 
1. Dikmen, T.L. Webb, B.P . Chang, Y . Benn, ‘The ostrich problem’: Motivated avoidance or 
rejection of information about goal progress. Soc. Pers. Psychol. Compass, 7(11), pp. 794–807 
(2013) 
2. T.L. Webb, B.P . Chang, Y . Benn, ‘The ostrich problem’: Motivated avoidance or rejection of information about goal progress. Soc. Pers. Psychol. Compass 7(11), 794–807 (2013) 
3. L.A. de Souza Jr, R. Mendel, S. Strasser, A. Ebigbo, A. Probst, H. Messmann, J.P . Papa, C. Palm, Convolutional neural networks for the evaluation of cancer in Barrett’s esophagus: Explainable AI to lighten up the black-box. Comput. Biol. Med. 135, p. 104578 
4. G. Nyman, J. Peltonen, M. Nelson, J. Karjalainen, M. Laine, T. Nyberg, H. Tuomisaari, On the behavioral theory of the networked ﬁrm. In Big data and smart service systems, (pp. 179–192). Academic press (2017)116 A. Kumar et al.
5. A.B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, 
S. Gil-López, D. Molina, R. Benjamins, R. Chatila, F. Herrera (2020). Explainable artiﬁcial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inform. Fusion, 58, 82–115 (2020) 
6. P .J. Phillips, C.A. Hahn, P .C. Fontana, D.A. Broniatowski, M.A. Przybocki, Four principles of explainable artiﬁcial intelligence, Gaithersburg, Maryland (2020) 
7. S.N. Payrovnaziri, Z. Chen, P . Rengifo-Moreno, T. Miller, J. Bian, J.H. Chen, Z. He, Explain-able artiﬁcial intelligence models using real-world electronic health record data: a systematic scoping review. J. Am. Med. Inf. Assoc. 27(7), 1173–1185 (2020) 
8. D.W. Gunning, D. Aha. DARPA ’s explainable artiﬁcial intelligence program. AI Mag 
9. S. Gupta, B. Amaba, M. McMahon, K. Gupta, The Evolution of Artiﬁcial Intelligence in the Automotive Industry. 2021 In Annual reliability and maintainability symposium (RAMS) (pp. 1–7). IEEE (2021) 
10. H. Mankodiya, M.S. Obaidat, R. Gupta, S. Tanwar, XAI-A V: Explainable Artiﬁcial Intelli-gence for Trust Management in Autonomous V ehicles. In 2021 international conference on communications, computing, cybersecurity, and informatics (CCCI). IEEE, pp 1–5 (2021) 
11. X. Zhu, X. Ao, Z. Qin, Y . Chang, Y . Liu, Q. He, J. Li, Intelligent ﬁnancial fraud detection 
practices in post-pandemic era. Innovation 2(4), 100176 (2021) 
12. M. Bellucci, N. Delestre, N. Malandain, C. Zanni-Merk, Towards a terminology for a fully 
contextualized XAI. Proc. Comput. Sci. 192, 241–250 (2021) 
13. Y .K. Dwivedi, E. Ismagilova, D.L. Hughes, J. Carlson, R. Filieri, J. Jacobson, V . Jain, H. Karjaluoto, H. Keﬁ, A.S. Krishen, V . Kumar, Y . Wang, Setting the future of digital and social media marketing research: Perspectives and research propositions. Int. J. Inf. Manage. 59, 102 
(2021)Chapter 8 
Explainable Data Fusion on Edge: 
Challenges and Opportunities 
Shweta Sinha and Priyanka Vashisht 
Abstract Advancements in technology and the availability of small electronic 
devices have led to the availability of big data. This humungous amount of data 
is nowadays available in nearly every ﬁeld, be it science or social domain. As much 
of these collected data are time-sensitive, they need to be utilized timely to give 
an effective outcome. Data fusion is of paramount signiﬁcance in enhancing the 
collected data’s effectiveness. But, today, as AI has gained signiﬁcant momentum, 
machine learning (ML) techniques, mainly deep neural network (DNN), are widely utilized to deliver what is expected from the data. Edge intelligence tries to fulﬁl 
the promise of giving efﬁcient and timely output. Deep learning, undoubtedly being robust, also possesses some disadvantages, and one of these relates to the explain-ability of ML methods that help solve any problem. With dense layers in DNN and 
millions of parameters, it is difﬁcult to interpret the model creating a black box. The chapter aims to outline the explainability of data fusion at the edge. It high-lights different data models of fusion, discusses a framework for AI and data fusion 
at the edge and identiﬁes potential challenges and possible solutions in this regard. While doing so, the chapter intends to cover the fundamentals of explainability in AI, the need to convert the black box system into a transparent one, and the associated 
opportunities for explainable artiﬁcial intelligence. 
Keywords XAI ·Explainable AI ·Data fusion ·Edge computing ·Intelligent 
machine learning 
1 Introduction 
Artiﬁcial Intelligence (AI) has achieved tremendous growth in many activity sectors. In the last few years, the existence of AI at the core of almost all developments can 
be easily observed. The inclusion of learning, reasoning and adaptation capabilities has helped AI enable systems to perform better than any human being [
1]. Besides,
S. Sinha · P. Vashisht (B) 
Amity University Haryana, Gurugram, India 
e-mail: pvashisht@ggn.amity.edu 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_8 117118 S. Sinha and P. Vashisht
AI-based systems have been deployed in several susceptible decision-making tasks 
that range from biometric surveillance systems and criminal justice systems to areas 
that have diverse social and political connections [ 2]. Research and innovation based 
on AI techniques in the area of voice, automotive and healthcare have continuously 
seen an increasing growth rate, and it is projected to expand at the rate of 38.1% 
from 2022 to 2030 [ 3]. The more these AI systems have started affecting human life, 
the more the quest to understand the decision-making mechanism of these systems 
is increasing. 
The traditional AI methods are typically hard-coded rules to execute step by step 
and are easily interpretable. But, in the last few years, with the increased availability of a massive quantum of data AI market has witnessed frameworks that leverage 
the power associated with the enormous volume of data. In ML methods, logic 
and inference provide interpretation of any problem and give solutions. To further 
improve the performance, a combination of efﬁcient learning algorithms with huge 
parametric space was utilized, and DNN came into existence. Deep learning is a range of algorithms using neural networks with deep layers. These DNNs comprise 
millions of parameters and hundreds of layers making the complete system opaque. This opacity further leads to naming it the Black-box model [
4]. Their vast parametric 
space allows them to easily model any complex problem [ 3]. Several tools are now 
available that can increase insight into the inner working of DNN, but transparency is still a problem. 
In general, humans don’t readily adopt techniques that are not trustworthy or are 
not tractable and interpretable. The explanations for supporting outputs are generally necessary to better understand the system and its behaviour. And sometimes they will 
be even helpful in removing deﬁciencies from the system. With the employment of AI in critical application areas like precision medicine, autonomous vehicles etc., the demand to provide transparency is increasing among AI stakeholders [
5]. On the 
other hand, the focus is always on increasingly improving the system performance, and it will, in turn, increase the opacity. So, there must always be a trade-off between a model’s transparency and performance [
6]. Besides, interpretability is also one of 
the key design drivers for any ML system for its easy implementability. It can help in detecting and correcting bias from the datasets. Further, interpretability can act as insurance that the inference is only due to meaningful variables. 
Lack of insight into the internal mechanism of work limits the effectiveness of 
AI methods. Particularly the ML models. To address this issue, explainable Artiﬁ-cial intelligence (XAI) [
7] proposes to create more transparent AI, particularly ML 
techniques. The aim is to generate a suite of methods that produce more explanation of the inside working while maintaining a high level of performance. The need for 
XAI (Fig. 
1) can well stem from the reasons mentioned below.
•Explanation to identify weaknesses; a model that can be understood and explained 
can easily be improved for its shortcomings and perform as per users’ needs.
•An explanation for monitoring the system so that no wrongs can be done. Under-standing system behaviour closely gives better control over vulnerabilities and 
helps quickly identify ﬂaws.8 Explainable Data Fusion on Edge … 119
Fig. 1 Need for Explainable 
AI
Explain to 
Identify 
Weakness 
Explain to 
Justify 
Decision XAI 
Explain to 
Monitor 
Performance 
•An explanation is needed to justify the decision taken. This helps in building trust, 
mainly 
when unexpected outcomes are observed. XAI systems will provide the 
required information to explain the results. 
With the tremendous growth in information sources, data fusion approaches are 
being 
utilized to exploit them simultaneously for solving any learning task. Undoubt-
edly, using the fusion of heterogeneous data sources has drastically improved ML 
node performance. Different data fusion techniques can also enrich the explainability of ML models besides maintaining the privacy of training data used by ML models. 
Data fusion can be done at a centralized or distributed level, where it can be either 
knowledge fusion, data fusion or model fusion. Either of these is going to impact the ML models and help provide results. But, the acceptability requires explanation, 
and hence the fusion approaches need to be exploited to achieve explanation while 
maintaining privacy. 
The objective of this chapter is to
•Provide an overview of the essential concepts and terms used in XAI-related research.
•Deﬁne diverse approaches to post-hoc explainability.
•Elaborate on different data fusion models and discuss the need for XAI for data fusion.
•Highlight some of the application domains that demand XAI.
•Enumerate a series of challenges XAI, especially in the context of edge intelligence.120 S. Sinha and P. Vashisht
2 Data Fusion Architecture and Models 
Data fusion is the combined examination of various interconnected datasets that offer 
different perspectives on the same phenomenon. In general, more accurate inferences 
can be drawn from the correlation and fusion of data from multiple sources than from 
the examination of a single dataset. In the context of combining multiple data sets for analysis, this section frames the primary Data Fusion Models and their levels. 
Both in the research and business arenas, many data fusion frameworks have been 
created. By choosing the best method for the given challenge, these frameworks have been applied in several projects to assist in the creation of fusion systems. Conven-
tional data fusion models are discussed in this section, along with their existing capa-bilities. It should be highlighted, however, that these structures can be redeﬁned and enhanced in order to use more modern techniques, such as switching from static to 
dynamic processing, from human–machine teaming, from centralized to distributed processing, and from the insigniﬁcant amount of data to enormous real-time data. 
2.1 JDL/DGIF Model 
The Data Fusion Subpanel, later known as the Data Fusion Group, was established 
in the middle of the 1980s by the Joint Directors of Laboratories. Data fusion 
now includes sensor and information fusion because of the World Wide Web. The JDL/DFIG announced a data fusion model that separated the various processes. The 
model speciﬁes the number of stages of fusion, each of which takes advantage of recent advancements in AI to facilitate evaluation at a lower level (tier 0–3) and reﬁne-ment at mid-level (tier 4–6). However, contextual restrictions are based on mission, 
objectives, and goals in system management (tier 6). The Data Fusion Information Group (DFIG) model currently has seven tiers, and they are depicted in Fig. 
2 and 
are discussed as follows:
•Tier 0: Subject Evaluation, based on the relationship between the pixel and signal 
levels, provides approximation and forecast the signal and object observable 
states.
•Tier 1: Object Assessment, is constructed on data and their relationship and state 
estimation, which offers valuation and prediction of entities and objects.
•Tier 2: Situation Assessment offers assessment and forecast of relationships between objects and entities.
•Tier 3: Threat Reﬁnement, estimates and predicts the impacts of participant’s 
planned or anticipated activities.
•Tier 4: Process optimization is an aspect of resource management that offers adaptive information collection and processing to give an assessment of sensing targets and impact forecasting for planned or estimated actions by participants.8 Explainable Data Fusion on Edge … 121
Fig. 2 The JDL model
•Tier 5: User Reﬁnement is an aspect of managing knowledge that offers adap-
ti
ve access to control and information display to help decision-making through 
human–machine interfaces.
•Tier 6: Mission Management is an aspect of managing systems that permits the 
spatial–temporal management of resources, preparation, and target setting 
to aid in taking decision while taking into account societal, ﬁnancial, and political constraints. 
The model is useful for general understanding, but it does not direct a developer 
in 
determining the methods that should be utilized [8 ]—as a result, it is ineffective 
for 
creating an architecture for a practical system. In addition to broadening the func-
tional model and connecting the taxonomy to ﬁelds outside of the original military 
focus, Steinberg, Bowman, and White proposed expanding and revising the JDL 
model [ 9]. These changes included integrating a data fusion tree architecture model 
for 
system description, design, and development. 
2.2 W aterfall Model 
The Waterfall model is a three-tiered hierarchal model and is presented in Fig. 3. 
From 
the data level to the decision-making level, data ﬂow is operational. Feed-
back data from the decision-making module is continuously updated by the sensor 
system. The feedback component gives the multi-sensor system advice on recali-
brating, reconﬁguring, and data collection issues. The description of each tier of the 
Waterfall model is as follows:
Level 1: In order to offer the necessary information about the environment, the ra
w data is suitably converted at this level. Models of the sensors and, if possible,122 S. Sinha and P. Vashisht
Fig. 3 The waterfall model 
of the detected phenomena are required to complete this task. These models might 
draw from the experimental analysis.
Level 2: The feature extraction and feature fusion processes make up level 2. These 
procedures 
are used to draw conclusions about the data on a symbolic level. While 
maximizing information given, they seek to minimize the data content. Lists of 
estimates with associated probabilities (and beliefs) are the result of this level. 
Level 3: The association of object and events are drawn at this level. The infor-
mation 
that has been obtained, the libraries and databases that are available, and 
the interactions between people are used to assemble potential courses of action. 
The waterfall approach has the same ﬂaws as the JDL model since they are iden-
tical. 
The waterfall model’s main drawback is the absence of any feedback data ﬂow, 
despite being more accurate than other models in its analysis of the fusion process. 
Although not widely utilized abroad, the waterfall approach has been used in the 
defence data fusion sector in Great Britain [ 10]. 
2.3 Omnibus Model 
In 1999, Bedworth and O’Brien introduced the Omnibus model which is shown in 
Fig. 4 [10]. This model combines the Boyd loop, Dasarathy, and Waterfall models. 
The 
Boyd control loop is described by the authors as an iterative process with four 
elements (observe, orient, decide, and act) acting in a close loop. The Dasarathy model, in contrast, is made up of the three fundamental stages of data fusion: data, 
feature, and decision.
The model was developed after examining the advantages and disadvantages of 
pre
vious models, and it incorporates the majority of the strong points of other method-
ologies. Although the model offers a much more ﬁne-grained organization of the8 Explainable Data Fusion on Edge … 123
Fig. 4 The omnibus model [ 10]
processing layers, it exhibits a cyclic nature similar to the Boyd loop. The concept 
is designed to be applied recursively at two different abstraction levels numerous 
times within the same application. The model is ﬁrst used to describe and structure the entire system. Second, the system’s individual subtasks are modelled using the 
same structures. In the Omnibus model, highly sophisticated hierarchical separa-tion of the sensor fusion jobs does not permit a horizontal division into tasks that reﬂect distributed sensing and data processing. Decomposition into modules that may 
be independently implemented, independently tested, and independently reused for other purposes is therefore not supported by the paradigm. 
Data fusion is a complex idea with apparent beneﬁts but also many difﬁculties, 
especially with new paradigms with artiﬁcial intelligence integrated applications. The development of artiﬁcial intelligence (AI) has led to a meteoric rise in AI-based services. AI technology, such as machine learning (ML) and deep learning (DL), 
achieves state-of-the-art performance in a variety of sectors. Additionally, present applications frequently use centralized data management, which necessitates user uploading of data to the central data centre. But there is an enormous amount of 
data generated and gathered by billions of mobile users and Internet of Things (IoT) devices that are dispersed at the network edge. Such a large amount of data requires 
enormous bandwidth resources to upload to the distributed environment such as the cloud, which may also cause users to experience intolerable latency [
11]. With the 
development of cloud computing, edge computing [ 12–16] brings cloud services 
closer to end users. Edge computing provides platforms for computer resources that are typically found at the edge of networks, such as networking, storage, and processing. Edge servers, which can include IoT gateways, routers, and microdata 
centres in mobile network base stations, on automobiles, among other locations, are the devices that provide services for end devices. Edge intelligence is the result of combining edge computing and AI to address the signiﬁcant issues facing AI-based 
applications.124 S. Sinha and P. Vashisht
The discussion of edge intelligence is provided in the next part, along with exam-
ining several fundamental unresolved problems apart from potential theoretical and 
technical directions. 
3 Edge Intelligence 
An era of the “Internet of Things” has emerged as a result of technological advance-ments and the downsizing of electronic gadgets that can not only sense and process 
data but also connect with other devices (IoT). These Internet of Things (IoT) devices frequently have a large number of sensors, which generate enormous amounts of data at the network edge. Many of the applications that need to use this data have embraced 
artiﬁcial intelligence (AI) and machine learning (ML). Due to constrained network capacity and real-time requirements of numerous applications, such as medical, aerospace, data analytics etc., moving this enormous amount of data to the cloud 
is frequently not viable. Consequently, there is a drive to push AI boundaries to the network edge to use the massive amount of data generated by IoT devices closer to 
the data source. Due to this requirement, edge computing moves computing capacity away from the centralized nodes to the logical extreme edges of a network [
17], 
and AI have combined to create a new ﬁeld known as Edge Intelligence. In order to 
fuel AI applications without solely relying on the cloud, edge intelligence makes use 
of the widely available edge resources. There is yet no ofﬁcial deﬁnition for edge intelligence, despite the early stages of research. Most organizations currently refer 
to Edge Intelligence as “the paradigm of running AI algorithms locally on an end device, using data that are created on the device.” [
18]. Edge intelligence is supported 
by a wide range of ML techniques, which fall under the categories of unsupervised 
learning, supervised learning, semi-supervised learning, and reinforcement learning 
(RL). 
An ML model must ﬁrst be trained before it can be deployed, and it must then 
be used to make inferences. Since training requires more time and resources than inference, in the edge AI model, training is frequently carried out in the cloud while inference is carried out at the edge. While AI inference will be carried out at edge 
devices, AI training will probably continue on robust cloud-based computers due to the resource limitations of many edge devices [
19]. Deep learning, one of the ML 
techniques now in use, excels in many tasks. Convolutional neural networks (CNN), 
for instance, have been used for object detection and recognition as well as image 
classiﬁcation. Recurrent neural networks (RNNs) are used in multitarget tracking and 
natural language processing. An example of a task that can be accomplished using 
deep reinforcement learning (DRL) is the trajectory optimization of autonomous vehicles. Edge computing offers a solution for the performance- and energy-efﬁcient 
execution of AI/ML algorithms, independent of any particular AI/ML techniques.8 Explainable Data Fusion on Edge … 125
3.1 Challenges and Resolutions of Edge Intelligence 
In edge intelligence, there are certain outstanding problems that remain unsolved. It is 
essential to recognize, examine, and seek out fresh theoretical and technical answers to these problems. In this article, we examine several signiﬁcant edge intelligence 
challenges and potential solutions. Scarcity of data at the edge, data consistency on edge devices, poor adaptability of statically trained models, privacy and security concerns, and incentive mechanisms are some of these difﬁculties [
20]. 
3.1.1 Scarcity of Data at the Edge 
The majority of machine learning methods mainly supervised machine learning, 
and depend on having enough adequate high-quality training instances to achieve 
good performance. However, when the acquired data is sparse and unlabelled, it frequently fails in edge intelligence applications, such as speech recognition and 
HAR. In contrast to conventional cloud-based intelligent services, which pool all of the training instances into a single database, edge devices create models using data that they either generate themselves or that they collect from their surroundings. 
There aren’t many high-quality training examples in these datasets, such as good image features. Since most existing works assume that the training instances are of high quality, they neglect this problem. The training dataset is frequently unla-
belled as well. To address the issue of unlabeled training examples, which necessi-tate human annotation, a few articles [
21, 22] propose using active learning. Only 
scenarios with a limited number of occurrences and classiﬁcations would be suitable 
for such an approach. Federated learning strategies make use of data decentraliza-
tion to efﬁciently address the issue. However, federated learning is only appropriate for collaborative training as opposed to the solitary training required for customized 
models. 
Following are some of the issues that are explored and their potential solutions:
•Use shallow entities can be trained on a small amount of data. In general, a machine learning algorithm will learn from tiny datasets more effectively the simpler it is. 
In some cases, a straightforward model, like Naive Bayes, a linear model, or a decision tree, is sufﬁcient to solve the issue, as opposed to more complex models, like neural networks, as they are essentially trying to learn less. Consequently, 
when addressing practical issues, it is important to select an appropriate model.
•Adopt techniques based on incremental learning. Edge devices could incremen-tally retrain a widely-used pre-trained model to take into account their new data. 
In this way, a customized model that may be created with just a few training knowledge from other models is leveraged to improve the performance of a related model. The cold-start issue is often avoided, and the amount of training 
data needed is decreased. This technique is known as transfer learning. Transfer126 S. Sinha and P. Vashisht
learning may therefore be an option when there is a lack of target training data, 
and there are some similarities between the source and target domains.
•A model can be made more reliable through data augmentation by having more 
data during the training process [ 23]. For instance, expanding the number of 
photos while maintaining the semantic meaning of the labels through ﬂipping, rotating, scaling, translating, cropping, etc. The network would become invariant to these deformations and perform better with unseen data through training on 
enhanced data. 
3.1.2 Consistency of Data on Edge Devices 
Applications that rely on edge intelligence, such as voice recognition, activity recog-
nition, emotion recognition, etc., typically gather information from a signiﬁcant 
number of sensors that are dispersed throughout the edge network. The informa-tion collected might not, however, be reliable. Different sensing contexts and sensor 
heterogeneity are two variables that contribute to this issue. The environment (such as the street and library) and its weather (such as rainy, and windy) introduce back-ground noise to the sensor data that is collected, which may have an effect on the 
model’s accuracy. The unexpected difference in the acquired data from sensors may also be due to the heterogeneity of those sensors (such as hardware and software). Various sensors, for instance, have various sensitivities, sampling speeds, and sensing 
efﬁciency. Even the sensor data that was gathered from the same source may differ between sensors. As a result, the model training would vary depending on how the data were different, for example, the parameters of the features [
24]. Existing sensing 
applications still struggle with this variation. 
If the model is trained centrally, this issue might be readily resolved. The ability 
to learn the invariant characteristics to the variations is ensured by the centralized 
huge training set. However, edge intelligence does not cover this ground. Future 
research into this issue should concentrate on ﬁnding ways to counteract the varia-
tion’s detrimental impact on model accuracy. To achieve this, representation learning 
and data augmentation are two potential research approaches that may be taken into consideration. In order to make the model more resistant to noise, data augmentation 
could be used to supplement the data during model training. For instance, in speech recognition software for mobile devices, several types of background noise can be added to mask environmental variance. 
The performance of models is strongly inﬂuenced by data representation. In order 
to develop models with more useful characteristics, representation learning focuses on learning how to represent data [
25], which could also be used to disguise hardware 
disparities. The model’s performance would be much enhanced if we could “trans-late” the representations between two sensors that are using the same data source for this issue. As a result, representation learning offers a promising way to lessen 
the effects of inconsistent data. Future efforts could be made in this regard, such as developing processing pipelines and data transformations that are more efﬁcient.8 Explainable Data Fusion on Edge … 127
3.1.3 Bad Compliance of Statically Trained Model 
Most edge intelligence-based AI applications train the model on a central server 
before deploying it on edge hardware. Upon completion of the training process, 
the trained model will not need to be retrained. Low performance and a poor user 
experience are the results of these statically trained models’ inability to deal with the unknown new data and jobs in unfamiliar situations. For models trained using a 
decentralized learning approach, only local knowledge is utilized. As a result, these models may only become authorities in their limited geographic regions. A wider serving area results in a decline in service quality. 
Two options may be taken into consideration to deal with this issue: sharing 
knowledge and lifetime machine learning. An advanced learning paradigm called lifetime machine learning (LML) [
26] permits ongoing knowledge building and 
self-learning on new tasks. Instead of being taught by humans, machines are to independently acquire new knowledge based on previously acquired knowledge. Meta-learning [
27], which enables machines to automatically learn new models, and 
LML are slightly dissimilar. LML could be used by edge devices with a variety of learnt tasks to adjust to changing environments and deal with unknowable data. It 
is important to remember that the LML is not primarily intended for edge devices; therefore, powerful computing hardware is anticipated for the machines. Therefore, if LML is used, model design, model compression, and ofﬂoading techniques should 
also be considered. Communication between many edge servers is made possible through knowledge sharing [
28]. When an edge server receives a job for which 
it lacks the knowledge necessary to deliver quality service, it may send knowledge 
requests to other edge servers. Since the knowledge is distributed around various edge 
servers, the server with the necessary knowledge answers the query and completes the task for users. In such a knowledge-sharing paradigm, a method for knowledge 
appraisal and a system for knowledge querying are needed. 
3.1.4 Security and Privacy Concerns 
Heterogeneous edge devices and edge servers must cooperate to offer processing 
power in order to realize edge intelligence. This technique involves sending locally 
cached data and computation tasks (training or inference tasks) to unfamiliar machines for additional processing. The data could include personally identiﬁable 
information about users, such as tokens and images, which raises the possibility of privacy breaches and attacks from hostile users. If the data is not encrypted, 
malevolent people could easily get access to conﬁdential information. A few initia-tives [
29–31] propose performing some local preparatory processing, which could 
conceal sensitive data and cut down on the amount of data provided. The processed 
data can still be used to extract personal information, albeit [ 32]. Additionally, by 
adding malicious code, users might attack and take over a computerized device by128 S. Sinha and P. Vashisht
controlling computer tasks being infected by a virus. It is the main obstacle that insuf-
ﬁcient security and privacy protection measures methods to safeguard the security 
and privacy of users attacked. 
A viable solution is the credit system. The credit system employed by banks, which 
veriﬁes each user’s participation in the system and examines their credit information, is comparable to this. The system would remove users with a history of poor credit. As a result, everyone who uses computing equipment can trust them, and they are 
all secure. Privacy could be protected via encryption, which is currently employed in several works [
20, 33]. However, before the training or inference tasks are carried 
out, the encrypted data must be decrypted, which increases the amount of computing 
required. Future work might focus more on homomorphic encryption to address the 
issue [ 20]. A form of encryption known as homomorphic encryption enables direct 
computation on ciphertexts and produces encrypted results. Once the data has been 
decrypted, the outcome is identical to what would have been obtained by computing 
the unencrypted data. Therefore, by using homomorphic encryption, the training or inference process might be directly done on encrypted data. 
3.1.5 Mechanism for Incentives 
The two most crucial phases of edge intelligence are model training and inference 
and data collection. Securing the accuracy and usefulness of the information from 
the gathered data during data gathering is difﬁcult. The time, battery, and bandwidth that data collectors use to perceive and gather data are all of their own resources. 
It is unrealistic to believe that every data collector will be willing to contribute, let alone for pre-processing data cleaning, feature extraction, and encryption, which further uses resources. Each member must put aside personal interests in order to 
cooperate for the purpose of collaborative model training/inference. For instance, one master and multiple employees make up the architecture suggested in [
34]. Workers 
identify objects in a certain mobile visual domain and supply training instances for 
masters via pipelines. Such an architecture is effective in private settings, such as at 
home, where all of the equipment is driven to work together to develop a superior intelligent model for their owner or master. It would not, however, function well 
in public settings where the master initializes a job and assigns subtasks to new players. In this situation, extra incentive issues develop that are not normally taken into account in smart settings when all devices are not owned by a single master. 
Participants must be motivated to collect data and complete tasks. 
Future initiatives should consider reasonable incentive mechanisms. Participants 
have a variety of missions, such as collecting data, processing it, and analyzing it, all of which require different resources. Every participant wants to receive the greatest potential prize. The operator, on the other hand, seeks to maximize model accuracy 
at the lowest cost. The difﬁculties in creating the best incentive mechanism are determining how to quantify the workloads of various tasks to match commensurate rewards and how to jointly optimize these two competing objectives. Overcoming 
these difﬁculties could be the focus of future work.8 Explainable Data Fusion on Edge … 129
4 Explainable Artiﬁcial Intelligence 
Although the deep learning tools available today can generate reliable results, they 
are tough to comprehend due to their opaque nature. As the demand for trustable AI worldwide is increasing, attempts are being made to integrate ethical standards during 
design and implementation. Explainable AI, one of the components of trustable AI, tries to dive deep into the rationale of the decision taken by the AI system. It is considered a new research ﬁeld that tries to make AI systems such that their behaviour 
can be explained and the outputs are understandable to humans [
35]. The primary aim 
of XAI is to explore the logic involved in the decision-making process and outline the merits and drawbacks of the system. This information can be utilized for further 
improvements. Any XAI-powered AI system removes the confusion related to the output of the system, making it a trustable system. 
4.1 T erminology in XAI 
Better understanding of XAI can be achieved if there is a common point of under-
standing for explainability. The desired characteristics of XAI and terminologies 
used in research and the public domain need to be adequately explained. These terminologies are mentioned below in Table 
1. 
In all these characteristics of XAI, Interpretability, also referred to as understand-
ability, is an essential feature of XAI [ 36]. The terms interpretability and transparency 
are strongly tied to this concept. Justiﬁability is also connected in the sense that a 
clear understanding of the system can easily identify the decision as right or wrong. 
Understandability can refer to either model understandability or human understand-ability. With the importance of the understandability of the system, the presence of 
a human is the cornerstone of XAI.
Table 1 Characteristics of XAI 
Term Description 
Intelligibility It refers to the easy understandability of the internal working of a model for 
any human, without any need for explicit demand for an explanation 
Justiﬁability It supports the outcome of the AI system by understanding its decision but may not always give an explanation for the same 
Interpretability The ability of the system to showcase its functionality to humans in an easily understandable term. It is closely related to explainability 
Explainability The ability of the system to provide an explanation about the decisions being taken by the system. It refers to the description of the internal working of the system 
Transparency Refers to the self-explanatory aspect of the system that leads to understandability without any external interventions 130 S. Sinha and P. Vashisht
Table 2 Summary of XAI purposes 
Purpose Discussion 
Informativeness The ML models are used to provide a solution that matches human 
performance. But, the problem being solved by ML methods is not exactly the same as humans in their environment. It is a mapping from one domain to the other but not an exact mapping. The information regarding this mapping and other aspects of the problem solving is required to understand the system working and its decision making. Hence, explainability here targets to capture the information 
Trustworthiness Building trust is the primary aim of the XAI model. It gives conﬁdence regarding the model’s performance in an adverse situation. Also, as almost all explainable models are trustworthy, the vice-versa may not be true. Out of the signiﬁcant limitations associated with trustworthiness is its fuzziness which makes it difﬁcult to quantify 
Causality Causality refers to the relationships between the variables. As the black box models are generally implemented in complex systems with a huge number of variables or parameters, explainability helps deﬁne the causal relationship between the variables and helps better understand them [
39] 
Accessibility One of the essential goals of explainable AI is accessibility [ 40]. It helps end 
users to be more involved during the ML model development phase. Accessibility also eases the understandability of non-expert users of the system 
Interactivity Researchers [ 41] have highlighted interactivity with the user as one of the 
goals behind XAI. This can be beneﬁcial when the user’s response or interactions inﬂuence system performance 
Fairness One of the goals sought by researchers of XAI is fairness. Explainability tries to capture each detail of the black box model, leading to a clear visualization of the inside process. This guarantees fairness in ML methods 
4.2 Purpose of XAI 
Several research activities around XAI have been carried out [ 37, 38] to date. Most 
of this research highlights the goals that the explainable model possesses or should 
possess, but the purpose behind achieving these goals is different from most of the 
research carried out in XAI. A summary of some of the purposes of XAI is presented 
in Table 2. 
5 The How of Explainability in AI 
There are a few intriguing queries when the demand for XAI is generated. They start 
with understanding the contextual aspects of XAI, i.e. what exactly is XAI, followed by covering the identiﬁcation of the purpose for which XAI has been demanded, 
i.e. Why needed. And lastly, when one is conceived about exploring the inside of the black-box models, they query how it can be done. The explainability of any AI8 Explainable Data Fusion on Edge … 131
system can be considered not only after the black-box development has been done 
rather, it can be done at all stages of development, namely, at the Pre-modelling stage, 
at the model development stage known as Explanation modelling and also at Post-
modelling stage. At each stage, many ways of explainability exist, which depend 
upon factors contributing to the AI model operations. Figure 5 shows the stages of 
explainability of AI system [ 42]. 
•Pre-modelling Explainability—A suite of methods diverse in nature but having 
a similar goal of understanding exists for pre-modelling explainability that works 
mainly at the data level. These methods can be categorized as dataset description 
standardization, exploratory data analysis, data set summarization and feature engineering [
43]. 
In real-time, the datasets usually come with incomplete documentation. This 
makes the interpretation difﬁcult. Standardization can make this interpretation consistent, and smooth communication between users and creators regarding the 
data can be done. Some speciﬁcation standards are recommended as data state-
ments and data nutrition labels [
42]. The exploratory data analysis aims to extract 
a statistical summary that ranges from mean and standard deviation to the dimen-
sionality of the dataset. Dataset summarization provides a prediction on the given 
dataset with the help of already used training data. As these training data are too huge, posing storage challenges, a possible way is to store only a relevant subset 
of the training data set. A good mechanism should be employed for selecting the representative dataset. Explanation of any AI system can be as good as the 
selection of features used to explain the predictions of model. Feature engineering 
helps in identifying the relative importance of input features.
•Explainable Modelling—Explainable AI approaches can be applied at a local or a global level which is either inherent or, Post-hoc. With the choice of speciﬁc 
AI models that are inherently explainable, we can avoid the problem of explain-
ability due to the black box model. Linear models, decision trees etc., are some
Pre-Modelling Explainability 
(Understand data used to develop model) 
Explanable Modelling 
(Develop models that are intrinsically explainable) 
Post-Modelling Explainability 
(Extract explanation from surrogate models to describe 
pre-developed opaque models) 
Fig. 5 Stages of AI explainability132 S. Sinha and P. Vashisht
of the models that are inherently transparent and have low-performing power. 
But, for high dimensional input, these simple, inherently explainable models also 
do not guarantee explainability. Like, the linear regression model, when applied to high dimensional data, may not be simulatable or, we can say, explainable. 
It is customary to use an inherently simple or explainable model approach with 
the black box method to obtain a high-performing explainable model. To better understand, deep K-nearest neighbour is the amalgamation of K-nearest neigh-
bour inference applied on opaque training set learnt through deep hidden layers. 
Self-explanation neural network and deep weighted averaging classiﬁer (DW AC) are a few more models obtained from the combination of the traditional transparent 
approach and the black box model.
•Post-modelling Explainability —After the explainability, attention to data and 
model 
next comes the explainability after the fact. The basic idea behind this 
is to provide an explanation of the simpliﬁcation or, Feature level explanation. 
The post-hoc explainability techniques can be grouped based upon their target, i.e. what they aim to explain about the model. The drivers are the cause for the 
explanation, the explanation family that communicates with the user about the 
driver and the target and ﬁnally, the estimator that deals with the overall computa-
tional process of obtaining the explanation. The local interpretable model-agnostic 
explanation tool (LIME) [ 44] explains an instance of a complex deep learning 
model, 
which is the model’s target. Input features serve as the driver, and the 
importance score of each feature is communicated to the user that is computed through the local evaluation of the model input termed as an estimator. The targets are objects of an explainability method. They vary in their scope and complexity 
and can be of inside or outside type. Inside targets are bothered about the details 
internal to the model, and outside targets refer to the functionality or output of any model. Drivers are a set of factors that impact the target, and explainability deals 
with the causal relationship between the two. Training samples, input features and 
choice of optimization algorithms can affect AI model development; hence, they are the explanation drivers. 
The post-hoc explainability methods aim at providing information content that 
is 
easily understandable by a human. The impact of the driver on the target should 
be communicated in an easy and understandable way, ensuring interpretability and 
completeness. The importance score of each of the drivers, if–then based on decision 
rules and decision trees, are some of the ways to explain the relationship between 
the target and the drivers. In the fourth category, the estimators have a wide range, and they vary in terms of their applicability and the underlying mechanism. The 
estimation methods are either model speciﬁc, which works only for a speciﬁc method, 
or model-agnostic, which can be used for any black box model. These models can be used at the local level that offers interpretation for a particular instance or can 
be applied to a global scenario where surrogate representation can be developed to 
approximate any black box model. 
Even though post-hoc explainability provides an understanding of the black-box 
models, 
there are certain limitations to it [ 43]:8 Explainable Data Fusion on Edge … 133
•These methods do not precisely match; instead only approximate the underlying 
models while creating a surrogate white box model to explain the decisions made 
by the system. But since it is only based on approximation, it cannot truly rely on critical applications.
•These methods work for sample data collected randomly instead of the complete data set. Due to randomness in the sampling process, the post-hoc explainability suffers instability. 
These shortcomings highlight the instability and low ﬁdelity of post-hoc explain-
ability methods and simply points out that the post-hoc explainability methods are 
helpful for model design and development but cannot be relied upon for regulatory 
use. 
6 Application of XAI for Data Fusion 
A brief description of XAI highlights that it can bring signiﬁcant beneﬁts to an extensive range of application domains that rely on AI. Most AI applications are 
data-centric, and nowadays, for creating a robust and reliable system, the input data is taken from several sources that may be heterogeneous. Data fusion is one of the key players in the creation of such a system. This section presents some critical domains 
that rely on AI, and XAI may ﬁnd opportunities in these domains. A few mentioned here have witnessed exponential growth in XAI applicability and research in the last few years.
•Transportation Vehicles running on the road without human intervention are the latest buzz in 
the technology world. These automated vehicles promise a reduced mortality rate with increased mobility features. As the motor vehicle runs on the designated path, it must take decisions regarding objects coming into its way. These decisions 
must be taken in fractions of seconds. The decisions are dependent upon the efﬁciency and effectiveness of the object classiﬁcation algorithms. Challenges lie in obtaining accurate results and are due to the explainability of intelligent 
systems. Uber killed a woman in Arizona. The AI systems are mostly opaque; their internal working and decision-making criteria are difﬁcult to understand. Only a simple explainable system can explain the decision taken and can open 
the door for further improvements. A few pieces of research in the direction of XAI for transportation have been initiated [
45]. Still, owing to the importance and 
criticality of the domain, explainability for self-driven vehicles are crucial.
•Finance In ﬁnancial sectors, AI tools can be used to predict risks associated with the loan 
and can also assess other ﬁnancial risks. It can provide customer interactions and solve domain-related queries throughout the day without any manual inter-ventions. The ML-based intelligent system also helps in fraud detection. The AI 
tools used for the purpose possess security risks that can be related to data theft. It134 S. Sinha and P. Vashisht
is needed that the tools used here must be explainable so that the reasons behind 
the decisions taken and selections made should be clear, making the tool/system 
trustable.
•Military 
The need for XAI was ﬁrst felt in the military domain, and after a call for research 
by DARPA [ 7] project, it widely spread in other domains too. AI has several 
application areas in the military where it can showcase its potential. It can be used 
in automated surveillance for tracing suspicious activities. Machine learning is 
being applied for internal security and used to forecast and protect unauthorized intruders. All these need to be a trustable and reliable system. They require XAI 
to get better understandability as they all involve life and death.
•Legal The law is the force to maintain decorum in society. All aspects of business, be 
it merger, acquisition, selling or purchase, require a legal contract. As a powerful abiding force in everyday’s life, the law is also inﬂuenced by advanced techno-logical developments. In the legal domain, AI is leveraged for contract analytics, 
criminal justice, litigation predictions, lawyer bots, etc. All these use ML power, so complex models can be easily presented in a simple form. As the impact of all 
these legal services critically inﬂuences human life, understanding their working and decision-making is very much needed. XAI is essentially required to obtain transparency, and recently, a few works have been initiated for the same [
46, 47].
•Healthcare The continuous growth in medicine and other healthcare monitoring aspects is credited to the technological developments in recent years. The advanced devices 
available for patient monitoring continuously capture data, and their analysis can give much information regarding patient conditioning that is impossible even for humans to explore. With this large amount of collected data, the information 
provided by traditional AI techniques and even ML methods is sometimes insuf-ﬁcient. Recently, these enormous amounts of data are being used by deep learning methods to identify the association between different types of patient data and 
use them effectively. Even though the DNN-based AI techniques lead to a new era of digital healthcare, the challenges still persist. With the number of parame-ters approximating millions in number in the deep learning model, it is difﬁcult 
to exactly comprehend what the model portrays of the data, for example, inter-preting the radiological images [
48]. From the black box of DNN computations, 
expressing intuitive interpretations that can clarify model uncertainty, support 
model results, or provide new clinical insights is difﬁcult. Explainability and 
understandability are very important for the success of these models. Recently, 
AI-based visualization methods have been developed to describe the AI models. 
Some widely used models include occlusion maps [ 49], salience maps [ 50], and 
class activation maps [ 51]. A few studies [ 52–54] have explored explainability in 
medical AI in recent years. The work in each of the domains supports the necessity for XAI. Such study is, however, still in its early stages, and signiﬁcant research efforts must yet be made8 Explainable Data Fusion on Edge … 135
in this regard. Moreover, cybersecurity, education, entertainment, government and 
image recognition etc., also ﬁnd exciting applications in XAI. 
7 Conclusion 
The conventional “Black Box” models that are traditionally being used in AI systems are often marked for their lack of transparency and the way they hide potential biases. 
XAI in ﬁelds like criminal justice, medical decisions and search engine outputs can be helpful in removing potential biases while giving the proper explanation for the 
decisions taken by the systems. Use of XAI leads to fair and ethical practices for 
development of intelligent and responsible AI. They help in building trust for the 
system. In future, XAI may ﬁnd itself deep rooted in the ﬁeld of medicine where it 
provides the advantage of explanation that helps in easy understanding. The intent of this chapter was to provide the brief about XAI that can help in better understanding 
of the decisions made. In this chapter, the authors have discussed the emerging discipline of edge intelligence. With the tremendous increase in the availability of data and decreasing cost of computing devices, the rise of data-driven applications will 
continue to proliferate. A Survey of available research highlights the long existence of cloud and AI. With the increasing applicability of intelligence in real-time data-centric applications, the cloud will be used to provide storage for AI training data, 
and the edge will be utilized for inference and decision making. This chapter presents a brief discussion on data fusion models and edge intelligence. With the increasing performance of intelligent systems, the opacity of the system has increased. The 
article takes a deep dive into the explainability of artiﬁcially intelligent systems. The fundamentals of XAI are presented here in brief. At the same time, reviewing the application domain of AI, the chapter details the opportunities in data-centric 
applications and outlines the challenges and their solutions in data fusion and edge intelligence. 
References 
1. D.M. West, The future of work: Robots, AI, and automation. Brookings Institution Press (2018) 
2. A.B. Arrieta, et al. Explainable artiﬁcial intelligence (XAI): concepts, taxonomies, opportuni-
ties and challenges toward responsible AI. Inf. Fusion 58, 82–115 (2020) 
3. G. Yang, Q. Ye, J. Xia, Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond. Inf. Fusion 77, 29–52 
(2022) 
4. D. Castelvecchi, Can we open the black box of AI? Nature News 538(7623), 20 (2016) 
5. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty, Stakeholders in Explainable AI, (2018) 
6. F.K. Došilovi´ c, M. Brˇ ci´ c, N. Hlupi´ c, Explainable artiﬁcial intelligence: A survey, in 2018 41st 
International Convention On Information And Communication Technology, Electronics And 
Microelectronics (MIPRO) (IEEE, 2018)136 S. Sinha and P. Vashisht
7. Gunning, David. “Explainable artiﬁcial intelligence (xai).“ Defense advanced research projects 
agency (DARPA), nd Web 2.2 (2017): 1. 
8. J. Llinas, D. L. Hall, An introduction to multi-sensor data fusion, in Proceedings of the International Symposium on Circuits and Systems, vol 6537–540 (1998) 
9. Elmenreich, Wilfried, A review on system architectures for sensor fusion applications, in IFIP International Workshop on Software Technolgies for Embedded and Ubiquitous Systems (Springer, Berlin, Heidelberg, 2007) 
10. M. D. Bedworth and J. O’Brien, The omnibus model: A new architecture for data fusion? In Proceedings of the 2nd International Conference on Information Fusion (FUSION’99), 
(Helsinki, Finnland, 1999) 
11. D. Xu et al., Edge intelligence: Empowering intelligence to the edge of network, in Proc. IEEE 109(11), 1778–1837 (2021) 
12. W. Shi, J. Cao, Q. Zhang, Y . Li, L. Xu, Edge computing: Vision and challenges. IEEE Internet Things J. 3(5), 637–646 (2016) 
13. T.X. Tran, A. Hajisami, P. Pandey, D. Pompili, Collaborative mobile edge computing in 5G networks: New paradigms, scenarios, and challenges. IEEE Commun. Mag. 55(4), 54–61 
(2017) 
14. P.G. Lopez et al., Edge-centric computing: Vision and challenges. ACM SIGCOMM Comput. Commun. Rev. 45(5), 37–42 (2015) 
15. Y . C. Hu, M. Patel, D. Sabella, N. Sprecher, and V . Young, Mobile edge computing—A key technology towards 5G, ETSI White Paper, vol 11, no 11 (2015), pp. 1–16 
16. F. Bonomi, R. Milito, J. Zhu, S. Addepalli, Fog computing and its role in the Internet of Things, in Proceedings of the ﬁrst edition of the MCC workshop on Mobile cloud computing (MCC), 
(2012), pp. 13–16 
17. R, P. Kansakar, S. U. Khan, IFCIoT: Integrated fog cloud iot: a novel architectural paradigm for the future internet of things. IEEE Consum. Electron. Mag. 6(3), 74–82 (2017) 
18. Edge Intelligence in 2022—Deep Learning And Edge Computing—viso.ai 
19. A. Munir et al., Artiﬁcial intelligence and data fusion at the edge. IEEE Aerosp. Electron. Syst. Mag. 36(7), 62–78 (2021) 
20. D. Xu et al., Edge intelligence: Architectures, challenges, and applications. arXiv:2003.12172 . 
(2020) 
21. T. Xing, S. S. Sandha, B. Balaji, S. Chakraborty, and M. Srivastava, Enabling edge devices that learn from each other: Cross modal training for activity recognition, in Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking (ACM, 2018), pp. 37–42 
22. F. Shahmohammadi, A. Hosseini, C. E. King, M. Sarrafzadeh, Smartwatch based activity recog-nition using active learning, in Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (IEEE Press, 2017), pp. 321–329 
23. Mathur, T. Zhang, S. Bhattacharya, P. Velickovic, L. Joffe, N. D. Lane, F. Kawsar, P. Lio, Using deep data augmentation training’ to address software and hardware heterogeneities in wearable and smartphone sensing devices, in 2018 17th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN) (IEEE, 2018), pp. 200–211 
24. Mathur, A. Isopoussu, F. Kawsar, R. Smith, N. D. Lane, N. Berthouze, On robustness of cloud speech apis: An early characterization, in Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers (ACM, 2018), pp. 1409–1413 
25. Y . Bengio, A. Courville, P. Vincent, Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell. 35(8), 1798–1828 (2013) 
26. Z. Chen, B. Liu, Lifelong machine learning. Synth. Lect. Artif. Intell. Mach. Learn. 10(3), 
1–145 (2016) 
27. R. Vilalta, Y . Drissi, A perspective view and survey of meta learning. Artif. Intell. Rev. 18(2), 
77–95 (2002)8 Explainable Data Fusion on Edge … 137
28. Soller, J. Wiebe, A. Lesgold, A machine learning approach to assessing knowledge sharing 
during collaborative learning activities, in Proceedings of the Conference on Computer Support for Collaborative Learning: Foundations for a CSCL Community. International Society of the Learning Sciences, 2002, pp. 128–137 
29. H. Li, K. Ota, M. Dong, Learning IOT in edge: Deep learning for the internet of things with edge computing. IEEE Netw. 32(1), 96–101 (2018) 
30. A. E. Eshratifar and M. Pedram, “Energy and performance efﬁcient computation ofﬂoading for deep neural networks in a mobile cloud computing environment, in Proceedings of the 2018 on Great Lakes Symposium on VLSI. (ACM, 2018), pp. 111–116 
31. Y . Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and L. Tang, Neurosurgeon: Collaborative intelligence between the cloud and mobile edge, in ACM SIGARCH Comput. Archit. News 45(1), (ACM, 2017) 
32. W. Yang, S. Wang, J. Hu, G. Zheng, J. Yang, and C. Valli, Securing deep learning based edge 
ﬁnger-vein biometrics with binary decision diagram, IEEE Trans. Ind. Inf. (2019) 
33. J. Konecnˇ y, B. McMahan, D. Ramage, Federated optimiza- ‘ tion: Distributed optimization 
beyond the datacentre. arXiv:1511.03575 , (2015) 
34. Y . Huang, Y . Zhu, X. Fan, X. Ma, F. Wang, J. Liu, Z. Wang, Y . Cui, Task scheduling with optimized transmission time in collaborative cloud-edge learning, in 2018 27th International Conference on Computer Communication and Networks (ICCCN). IEEE, (2018), pp. 1–9 
35. M. van Lent, W. Fisher, and M. Mancuso, An explainable articial intelligence system for small-unit tactical behaviour, in Proceedings of the 16th International Conference on Innovative Application of Artiﬁcial. Intelligence, (2004), pp. 900–907 
36. A. Adadi, M. Berrada, Peeking inside the black-box: A survey on explainable artiﬁcial intelligence (XAI). IEEE access 6, 52138–52160 (2018) 
37. D. Doran, S. Schulz, T.R. Besold, What does explainable AI really mean? A new conceptual-ization of perspectives. 
arXiv:1710.00794 , (2017) 
38. Gilpin, Leilani H., et al., Explaining explanations: An overview of interpretability of machine learning, in 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). IEEE, (2018) 
39. P. Rani, C. Liu, N. Sarkar, E. Vanman, An empirical study of machine learning techniques for affect recognition in human–robot interaction. Pattern Anal. Appl. 9(1), 58–69 (2006) 
40. A. Chander, R. Srinivasan, S. Chelian, J. Wang, K. Uchino, Working with beliefs: AI trans-parency in the enterprise, in Workshops of the ACM Conference on Intelligent User Interfaces, 
(2018) 
41. P. Langley, B. Meadows, M. Sridharan, D. Choi, Explainable agency for intelligent autonomous systems, in AAAI Conference on Artiﬁcial Intelligence, 2017, pp. 4762–4763. 
42. Online Source: https://towardsdatascience.com/the-how-of-explainable-ai-post-modelling-
explainability-8b4cbc7adf5f . Accessed on 12 jul 2022 
43. Online Source: https://www.infoworld.com/article/3634602/explainable-ai-explained.html . 
Accessed on 14 Jul 2022 
44. M.T. Ribeiro, S. Singh, C. Guestrin, “Why should I trust you?” Explaining the predictions of any classiﬁer, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1135–1144 
45. Haspiel, Jacob, et al. “Explanations and expectations: Trust building in automated vehicles. In Companion of the 2018 ACM/IEEE international conference on human-robot interaction. 
2018. 
46. C. Howell, A framework for addressing fairness in consequential machine learning, in Proc. FAT Conf., Tuts., (2018), pp. 12 
47. R. Berk, J. Bleich, Statistical procedures for forecasting criminal behavior: A comparative assessment. Criminol. Public Policy 12(3), 513–544 (2013) 
48. J.R. England, P.M. Cheng, Artiﬁcial intelligence for medical image analysis: a guide for authors and reviewers. Am. J. Roentgenol. 212(3), 513–519 (2019) 
49. M.D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in European Conference on Computer Vision, (Springer, 2014), pp. 818–833138 S. Sinha and P. Vashisht
50. K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualizing 
image classiﬁcation models and saliency maps, in Workshop at International Conference on Learning Representations, (2014) 
51. R.R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Gradcam: Visual explanations from deep networks via gradient-based localization, in Proceedings of the IEEE International Conference on Computer Vision, (2017), pp. 618–626 
52. Z. Zhang, Y . Xie, F. Xing, M. McGough, L. Yang, Mdnet, A semantically and visually inter-pretable medical image diagnosis network, inProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017), pp. 6428–6436 
53. S. Tonekaboni, S. Joshi, M.D. McCradden, A. Goldenberg, What clinicians want: Contextu-alizing explainable machine learning for clinical end use, in Proceedings of the 4th Machine Learning for Healthcare Conference, Proceedings of Machine Learning Research, vol. 106, PMLR, Ann Arbor, Michigan, pp. 359–380, (2019), eds. by F. DoshiVelez, J. Fackler, K. Jung, D. Kale, R. Ranganath, B. Wallace, J. Wiens 
54. E. Khodabandehloo, D. Riboni, A. Alimohammadi, Health XAI: Collaborative and explainable AI for supporting early diagnosis of cognitive decline, future gener. Comput. Syst. 116, 168–189 
(2021)Chapter 9 
Trust Model Based Data Fusion 
in Explainable Artiﬁcial Intelligence for Edge Computing Using Secure Sequential Discriminant Auto Encoder with Lightweight Optimization Algorithm 
D. Prabakar, M. Sundarrajan, S. Prasath Alias Surendhar, 
Manikandan Ramachandran, and Deepak Gupta 
Abstract The amount of data generated, collected, and processed through computer netw orks has increased exponentially in recent years. Network attacks have also 
become an inherent concern in complex networks as a result of this increase of data. 
The practise of assessing trust using attributes that inﬂuence trust is known as trust 
evaluation. It is confronted with a number of serious challenges, including a shortage of critical assessment data, a requirement for big data processing, a call for a simple 
trust relationship expression, and the expectation of automation. Machine learning (ML) has been applied to trust evaluation in order to overcome these issues and intel-
ligently and automatically evaluate trust. This research propose novel technique in data fusion model with security and data optimization technique in edge computing. Here the proposed data fusion is carried out using secure sequential discriminant 
auto encoder in which the improvement of data accuracy, as well as for the maxi-mizing of Edge-cloud based sensor networks lifespan. The fusion of edge cloud data has been carried out using discriminant auto encoder which is integrated with
D. Prabakar 
Department of CSE, SRM Institute of Science and Technology, Ramapuram Campus, 
Chennai 600089, India 
M. Sundarrajan 
Department of CSE, SRM Institute of Science and Technology, Ramapuram Campus, Chennai 600089, India 
S. Prasath Alias Surendhar 
Department of Biomedical Engineering, Aarupadai veedu Institute of Technology(A VIT), Chennai, India 
M. Ramachandran (B) 
School of Computing, SASTRA Deemed University, Thanjavur, India 
e-mail: manikandan75@core.sastra.edu 
D. Gupta 
CSE Department, Maharaja Agrasen Institute of Technology, Delhi, India 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_9 139140 D. Prabakar et al.
distributed edge cloud users, where the security of the network has been enhanced 
using secure sequential fuzzy based trust model. The data optimization has been 
established using Genetic swarm lightweight optimization algorithm. The experi-mental analysis has been carried out based on data fusion as well as network security 
model. The parametric analysis is carried out in terms of network security analysis, throughput, Coverage fraction, Delay time, Energy consumption, Storage efﬁciency. 
Keywords Data fusion ·Security ·Data optimization ·Edge computing ·Trust 
evaluation 
1 Introduction 
Cloud computing (CC) has evolved into a viable alternative for providing an on-demand platform for processing as well as sifting large amounts of data. Cloud 
computing is being used in various ﬁelds, including but not limited to education, ﬁnance, manufacturing, and healthcare [
1]. As more methods rely on CC, novel-
methods that utilise beneﬁts of CC while remaining lightweight and free of cloud’s complexity are required to meet the properties of lightweight systems like IoT devices. Although IoT devices can perform some essential activities like regulating, 
actuating, and sensing, they cannot perform complex as well as sophisticated oper-ations like controlling massive smart transportation methods or smart medical treat-ments. Numerous IoT applications are time-critical, requiring fast decisions in order 
to provide best possible performance. Examples include automotive networks as well as aided healthcare systems [
2]. Furthermore, CC is too sophisticated for these 
devices to operate, and it lacks support for some of the most basic characteristics 
of IoT methods, such as location awareness as well asbandwidth constraints. Fog 
computing and edge computing are two novel technologies that can provide cloud computing beneﬁts while also addressing the unique peculiarities of IoT systems. 
Although these new methods open the door to IoT system innovation as well as growth, they also pose signiﬁcant security and privacy concerns that may hinder IoT system implementation and use [
3]. 
With growth of IoT, a slew of smart applications are being developed that take 
use of the ability to connect a variety of devices to internet. These apps will produce 
a large volume of data, which will require to be analysed quickly in order to produce 
useful as well as actionable data. EI refers to capacity to bring ML tasks from a remote cloud closer to IoT devices, either partially or completely. Data fusion [
4], also called 
as data fusion or multi-sensor data fusion, refers to multisensor data resources. To achieve goal of improving method stability, the system is able to obtain more precise data. Data fusion is classiﬁed into three types based on the level of information 
representation: datalevel fusion, featurelevel fusion, and decisionlevel fusion. The datalevel fusion, which is the lowest level of fusion, immediately fuses collected raw data, then extracts feature vector from fused data before being rated as well as 
recognised.There is no risk of data loss as well as obtained result is most precise. In9 Trust Model Based Data Fusion … 141
greenhouse WSNs, data fusion is a critical method for resolving low precision as well 
as poor stability. It removes redundant dataas well as minimises data transmission 
from various time and space multi-source data, meeting the goal of improving data collecting accuracy as well as system stability. 
Numerous activity areas that have adopted new data technologies have placed 
Artiﬁcial Intelligence (AI) at the centre [ 5]. AI methods attains unprecedented levels 
of performance while learning to solve gradually difﬁcult computational tasks as 
a result of these characteristics. AI-powered methods have lately advanced to point 
where their design as well as implementation practically need no human intervention. When such methods make decisions that affect people’s lives, there is a growing need 
to comprehend how AI technologies deliver such data. While original AI methods were simple to understand, opaque decision approaches such as DNNs (Deep Neural Networks) have gained popularity in recent years. DL (Deep Learning) methods 
like DNNs are empirically successful due to a combination of efﬁcient learning methods as well as their large parametric space [
6]. However, a better understanding 
of a method might lead to its shortcomings being corrected. Interpretability as an 
additional method driver can enhance implementability of a machine learning model 
for three reasons: 
• Interpretability aids in ensuring objectivity in decision-making by detecting and correcting bias in training dataset. 
• Interpretability aids resilience by revealing potential adversarial disturbances that 
could cause prediction to shift. 
• Interpretability can serve as a guarantee that only relevant variables are utilized 
to infer the result, i.e., that method reasoning is based on true causation. 
All of this means that, in order to be regarded realistic, the system interpreta-
tion should provide either a knowledge of the model operations as well as predic-
tions, a visualisation of methods discrimination rules, or clues on what can disrupt 
method. To prevent limiting effectiveness of today’s AI methods, eXplainable AI (XAI) recommends developing a set of machine learning approaches. (1) build more 
explainable models while preserving high learning performance (for example, predic-tion accuracy), and (2) assist people in comprehending, suitably trusting, and effec-tively managing the next generation of AI partners. XAI also analyses psychology 
of explanation and draws on insights from Social Sciences [
7]. 
Contribution of this chapter is as follows: 
1. To propose novel technique in data fusion model with security and data optimization technique in edge computing 
2. To develop data fusion using secure sequential discriminant auto encoder in which 
the improvement of data accuracy, as well as for the maximizing of Edge-cloud 
based sensor networks lifespan 
3. To enhance security using secure sequential fuzzy based trust model 
4. To optimize the data transmission using Genetic swarm lightweight optimization algorithm.142 D. Prabakar et al.
2 Background 
Statistical methods, hidden Markov methods, ANNs, and fuzzy logic are some of 
solutions presented for IDS. Support vector machines (SVMs) were shown to be highly accurate in creating IDS in recent studies [
8]. However, its fundamental 
drawback is that it necessitates a lengthy training period, limiting its use. Classic data mining methods, such as association rule mining, are also used in IDS. For example, to design and create IDS, [
9] used rule-based approaches, where expert 
knowledge is considered a rule set. As an analytical model, association rules were utilised instead of human specialists. The extraction of a high number of association rules, which improves model’s complexity, is a limitation of such methods. Feature 
selection is one of the most important parts of a supervised classiﬁcation model [
10]. 
The computing time of the algorithms will be reduced by selecting essential features. 
Many IDSs have been designed with feature selection because of the signiﬁcant 
aspects of network data [ 11]. In creating an IDS, work [ 12] classiﬁed main aspects 
that are critical for real-world intrusion detection. In [ 13], the author used a feature 
selection method to build a lightweight IDS. In big data framework for IDS, work [
14] created a PCA (principal component analysis) feature selection method. Work 
[15] produced an IDS method that used a gain ratio as a feature selection strategy as 
well as two classiﬁcation methods, namely SVM as well as rule-based classiﬁcation, to identify class label as well as displayed greater accuracy levels for DoS attacks. One disadvantage of this method is the high computational cost of using a separate 
gain ratio and classiﬁcation algorithm. They do, however, take a different approach from the one taken in this SLR. In [
16], the authors propose as well as assess a 
framework based on distributed edge model that uses Docker method to create a 
very lightweight as well as signiﬁcant virtualization solution. Work [ 17] focuses on 
relationship between DL as well as edge, whether it’s to utilise DL to optimise edge 
or to execute DL algorithms on the edge. The research is separated into ﬁve sections: 
edge DL applications, inference, edge computing and optimization. In [ 18], author 
took a comprehensive look into edge intelligence from perspectives of edge caching, training, inference and ofﬂoading [
19] take a broad look at edge intelligence from 
perspectives of edge caching, training, inference and ofﬂoading [ 20] created a multi-
agent method for boosting monitoring as well as security in cloud IoT environment. Security is provided by intelligent agents sharing trust as well as certiﬁcates for 
authentication. 
3 System Model 
This section discuss novel technique in data fusion model with security and data 
optimization technique in edge computing. Here the proposed data fusion is carried 
out using secure sequential discriminant auto encoder in which the improvement of 
data accuracy, as well as for the maximizing of Edge-cloud based sensor networks9 Trust Model Based Data Fusion … 143
lifespan. The fusion of edge cloud data has been carried out using discriminant auto 
encoder which is integrated with distributed edge cloud users, where the security of 
the network has been enhanced using secure sequential fuzzy based trust model. The data optimization has been established using Genetic swarm lightweight optimization 
algorithm. 
3.1 Secure Sequential Discriminant Auto Encoder (SSDAE) 
Based Data Fusion 
Edge servers have far more resources and computing power than sensors and IoT 
devices at the network’s edge. Because edge servers receive data from a variety of 
edge of network devices with varying application needs, edge server performance needs are similarly varied. Edge servers are created with high-performance ﬁxed 
evaluating servers with steady power sources as well as high bandwidth network connections when AI/ML processing with severe latency constraints is required. Edge 
servers are power/energy restricted for mobile edge computation. UA Vs are utilised as mobile edge servers or devices because they are often powered by batteries. For quick and energy-efﬁcient AI/ML computation, these mobile edge devices frequently 
use AI accelerators. Furthermore, as shown in Fig. 
1, edge servers or fog nodes are 
more agreeable to the incorporation of ML accelerators such as CNN, RNN, and MLP accelerators. 
Each edge server is in charge of a cluster of sensors and IoT devices at network’s 
edge. Edge servers offer edge-of-network IoT devices with apps, content, context, services, and storage. Edge servers, for example, can help with complicated image
Fig. 1 Framework for data fusion and AI at edge 144 D. Prabakar et al.
processing as well as storage responsibilities for imaging data collected from IoT 
devices’ camera sensors. Our framework’s edge servers are linked to a top-tier 
centralised cloud server. The core network sends locally processed data from edge to cloud for a variety of reasons, including analytics, archiving, and large-scale decision-
making. High-performance servers are commonly employed in cloud since the cloud must fulﬁl a large number of requests from network edge devices and edge servers. 
As shown by data fusion blocks at every hierarchical layer in Fig. 1, data fusion 
plays an essential role in the proposed system alongside AI. We present a high-level overview of data fusion at several tiers of proposed method here, whereas Section III provides a thorough study of data fusion within data fusion blocks in Fig. 
1.I o T 
devices then employ AI to improve accuracy, performance, and energy efﬁciency of AI jobs by fusing data. The data obtained from different IoT devices is then fused by the edge servers. The edge servers also use topological, contextual, and 
environmental data in data fusion to determine topological links between sensors. The edge servers employ AI to process the fused data as well as then send sanitised and fused data, analytics, to cloud. Data obtained from network edge is merged in 
the cloud, and then AI is used to the merged data to generate global analytics and insights. Edge servers and fog nodes can also ofﬂoad duties to the cloud. 
Utilizing2 blocks: an encoder as well as decoder, we aim to learn probability 
distribution (PD) of data: X in the V AE architecture. The encoder uses q(Z/X) to map X . Decoder block will work in same way as encoder, transferring latent variables to 
a new set of speciﬁcations that deﬁne a new set of related PDs: p(Xb/Z), from which we will gather samples once more. Our network’s output will be these ﬁnal samples: Xb. The end goal is to resemble the network’s input and output, X and Xb, as closely 
as feasible. However, to achieve that goal, we must map data’s internal structure to PDs q(Z/X) and p(Xb/Z). Conditional PD p(Xb/Z) and q(Z/X) model likelihood of Xb and Z but are dependent on their certain inputs: Z and X are the letters Z and 
X, respectively. We train the PD: q(Z/X) and p(Xb/Z) in a V AE by employing a variational technique, which converts learning method to a reduction method, which can be simply expressed in terms of SGD in a NN. Method Speciﬁcations: θ and 
ϕ are utilized to reﬂect architecture as well as weights of NN employed in Fig. 
2. 
These parameters are ﬁne-tuned during the V AE training process as well as treated as constants.
We strive to maximise the chance of getting the desired data as an output in the 
variational approach by maximising a number known as Evidence Lower Bound (ELBO) [22]. ELBO is made up of 2 parts: (1) a measure of distance between 
PD of q(Z/X) and some similar reference PD, where distance is usually Kullback– Leibler (KL) divergence; and (2) log likelihood of p(X) under PD p(Xb/Z), which is 
probability of obtaining desired data (X) with ﬁnal PD that generatesXb. All learnt PDs are parameterized PD, which means that a set of parameters entirely deﬁnes them. This is critical for the model’s operation because we use these parameters, 
which are obtained as network node values, to represent corresponding PD: p(Xb/Z) and q(Z/X). 
Parts of loss function to be minimised using SGD for ID-CV AE method are 
shown in Fig. 2. As previously stated, the loss function is divided into 2 parts: a KL9 Trust Model Based Data Fusion … 145
Fig. 2 Comparison of SSDAE with a typical V AE method
divergence as well as log likelihood portion. Second portion uses the distribution 
p(Xb/Z, L), which is a distance between X and Xb, to determine how likely it is to generate X. By decreasing this distance, we are effectively preventing q(Z/X) from 
deviating too far from its previous, and therefore functioning as a regularisation term. This regularisation term has the advantage of being automatically modiﬁed, therefore cross-validation is not required to adjust a hyper-parameter connected with 
regularisation, as it is in other methods. 
We utilize multivariate Bernoulli distribution for distribution p(Xb/Z, L). Because 
the output speciﬁcations that describes distribution is mean, which is same as likeli-
hood of success, the Bernoulli distribution does not require a ﬁnal sampling. Ground 
truth X, which has already been scaled to [0–1], can be regarded as a [0–1] scaled value for this likelihood. The distributions chosen for q(Z/X) and p(Xb/Z, L) are in 
line with ones selected; they are simple as well as produce decent results. Boxes in lower half of loss function’s certain choice. This is a speciﬁc choice for generic loss 
function shown in Fig. 
2. 
Before entering following section, denote sample set {xi , y i}n 
i=1 b yam a t r i xf o r m 
X = [x1; x2; ... ;x n] ∈ Rnx and Y = {y1; y2; ... ; y n} ∈ Rn×r . Here, yi is an r-
dimensional one-hot row vector with a value of 1 in ith dimension and 0 in other dimensions, and r is number of fault categories. 
DIAE encoding method is same as AE method discussed in Sect. 3.1, in that it 
uses Eq. to translate the input sample xi into a hidden layer (1). While introducing 
an extra dimension to sample x i as: xi =[
xi1, x i2,..., x in−1, 1]
. The effect will be 
the same as if you used bias term in Eq. ( 1). 
Unlike traditional AE paradigm, DIAE decoding method is divided into two 
stages. One method is to use the hidden representation hi Eq. ( 1) to reconstruct 
input data: 
ˆxi = g(
hi W T 
x)
(1)146 D. Prabakar et al.
The other element is the input data prediction using hi Eq. ( 2): 
ˆyi = g(hi WT ) (2) 
The discriminant weight matrix from hidden layer to output yi is WT ∈ Rm×r . 
In the DIAE model, Eq. ( 4) can be used to introduce discriminant information. 
Furthermore, we include a symmetric matrix L ∈ Rr×r into the DIAE method to 
capture structural data among various fault types. Degree of relatedness between each 
pair of fault categories is indicated by each element of L. The input data prediction can then be rewritten as Eq. (
3): 
˜yi =ˆy i L = g (hi WT )L(3) 
We introduce structural information as well as discriminant information among 
different fault kinds from Eq. ( 4). The loss function of DIAE is constructed as follows 
using Eq. ( 5): 
J = α 
2n nΣ 
i=1 (xi −˜x i )2 + 1 
n nΣ 
i=1(
−κσ(
yi , ˆy i L))
+ β 
2∥L − LT∥2 
F + λ 
2(
∥W x∥2 
F +∥W T∥2 
F)
(4) 
where α, β, λ are regularization parameters. Determine the optimal WX , W r and L 
by minimising J, because the matrices W ∗
x , W : 
T and L− are all randomly initialised. 
Every term in Eq. ( 6) is described as follows for a better understanding: 
• Reconstruction error is 1 
2nΣ n 
i=1(xi −˙x i )2 , which is the same as the traditional 
AE. By reducing the use of this phrase. The ability of features to represent input 
data is enhanced. 
• Discriminant information is represented by the second term 
1 
nΣ n 
i=1(
−κσ(
yi , ˆy i L))
. The extracted features’ discriminant ability would 
be improved if this term was reduced. Here κσ (yi , y i L) is Gaussian kernel by 
Eq. ( 5): 
κσ(
yi , ˆyL)
= 1/1(√
2πσ)(√
2πσ)
exp(
−(
yi −ˆy i L)2 /2σ 2)
(5) 
As pointed by 1 
nΣ n 
i=1 κσ (a, b) are viewed as an approximate evaluation of 
correntropy Vσ (A, B) Eq. ( 6): 
Vσ ( A, B) = E(κθ (A, B)) = ∫ κθ (A, B)dF AB(a, b) (6) 
where A = [a1, a2,..., a N ]T and B = [b1, b2,..., b N ]T is two stochastic variables, 
E(κσ (A, B)) is expectation of κσ (A, B),κσ (·, +) is Mercer kernel, and FAB(a, b)9 Trust Model Based Data Fusion … 147
is joint PDF. The PDF of ﬁnite samples is uncertain, as it is in many real-world 
situations, hence the above approximate estimate of correntropy is frequently used. 
Minimizing 2nd term in Eq. ( 8) is thought to be capable of enhancing discriminant 
ability of extracted features. 
We simplify reduction of Eq. ( 7) by utilising matrix form, as follows: 
min 
wx ,w1 L J1 + J 2 + J 3 + J 4 (7) 
where 
J1 =−tr(
Kσ(
ˆYL , Y))
J2 = α 
2∥˙X − X∥2 
F 
J3 = β 
2∥L − LT∥2 
F 
J4 = λ 
2(
∥W X∥2 
F +∥W I∥2 
F)(8) 
To solve Eq. ( 9), we use a different optimization strategy. Fix Wγ and L ﬁrst, then 
optimise Wx using the gradient descent algorithm: Second, ﬁx Wx and L then opti-
mise WI using the gradient descent approach. Finally, ﬁx WX and WT , then optimise 
L using the gradient descent approach. These three phases run in order until conver-
gence, starting with initialising WX , W r , L. The following equations can be used to 
calculate the values of WX , W T , L: 
Wτ = W T + η ∂ J 
∂W T 
L = L + η ∂ J 
∂ L (9) 
where η is learning rate. Partial derivatives of J based on Wx , W T , L, i.e. d 
Wx , d 
Wτ , A 
π 
by Eq. ( 10) 
∂ J 
∂W X = ∂ J1 
∂W X + ∂ J2 
∂W X ∂y 
ωT = ∂1 
WT + ∂ W T 
WT 
= c 
σ 2 H T((
D(
˜YL − Y)
LT)
⨀dg(HW T ))
+ λW T (10) 
∂ J 
∂ L = ∂ J1 
∂ L + ∂ J4 
∂ L = C 
σ 2 ˆY T D(
ˆYL − Y)
+ β(
L − LT)
H = [h1; h2 ......, h n] ∈ Rn×m is output of hidden layer, ⨀is Hadamard product. 
We reduce the often appearing formulas in Eq. ( 11) to make it easier to show the 
derivative results:148 D. Prabakar et al.
B =−(
ˆYL − Y)
( ˆYL − Y )T /2σ 2 
C =−1/ √
2πσ D = 1⨀exp (B) 
G =(
D(
ˆYL − Y)
LT)
⨀dg(HW T ) 
E =(
ˆX − X)
⨀dg(
HW T 
X)(11) 
Algorithm: 1—SSDAE: 
Input: Training sample set {X, Y} ={
xi: yi}n 
i=1, the number of hidden layers K , neuron 
number in every hidden layer{
m,}K 
j−1, maximum iteration number max_iter 
Output: Weight matrix of every hidden layer W x ={
W i 
x}K 
i=1, and output of every hidden 
layer{
Hi}K 
i=1 
1. Evaluate H0 = X 
2. for i ← 1, K do 
3. Start randomly W i 
X and W i 
T ;S t a r t L as a unit diagonal matrix 
4. Construct DlAE network with mi hidden neurons: 
5. Set iter = 1; 
6. Utilize an alternative optimization technique to update W,
X , Wl 
T , L,
7. while iter < max iter do 
8. If difference of loss function J in 2 consecutive iterations is smaller than threshold, break loop: 
9. iter ← iter + 1: 
10. end while 
11. Build i- th hidden layer of SDIAE utilizing W i 
x : 
12. Evaluate Hi utilizing Hi = g(
Hi−1W i 
X)
: 
13. Xi+1 ÷ Hi 
14. end for 
15. return WX 
3.2 Secure Sequential Fuzzy Based Trust Model (SSFTM) 
Resource constraints in the edge computing environment frequently result in large 
disparities in number of requests as well as responses to services. Some devices 
refuse to reveal identity data out of concern for privacy, but still want to interact; some require a high-level security method due to real-time services; and some only 
expect to beneﬁt from resource interaction. As a result, establishing a targeted trust evaluation strategy is difﬁcult. Furthermore, relying on a single factor to assess trust is unreliable. The authentication process, for example, normally has only two 
outcomes: pass or fail. To highlight the complexity of edge devices in executing9 Trust Model Based Data Fusion … 149
resource interaction or application service, we must analyse many trust metrics with 
multiple critical attributes. The proposed approach for assessing trust is based on 
Fig. 4. If there have been multiple contacts between ECUs clustered by common 
tasks in past period of time, the corresponding trust degree of ECX is anticipated 
using a capsule network to examine trust value of every ECU. As a result, we can 
determine the overall level of conﬁdence in the edge computing ecosystem. 
The trust property of ECUs is described as Eq. ( 12): 
Vt =< Vi, Vb, Vc >(12) 
where Vi, Vb, Vc represents identity trust, behavior trust and capacity trust. 
(1) Identity Trust: When assessing a device’s identiﬁcation trustworthiness, privacy 
concerns must be taken into account. Exposing a device’s ID, for example, 
exposes it to risk of identity theft, fraudulent data production, and so on. In edge computing, public key infrastructure (PKI) has been widely employed 
as a trustworthy authentication technique that can readily implement one-to-one interaction in a distributed context. Identiﬁcation trust Vi is deﬁned as the believability of an ECU’s identity during the resource request procedure. To 
compute Vi, we assume that ECUs’ identity trust is assured by an anonymous authentication mechanism based on ECC, which is utilized to evaluate each ECU’s key. ECC generates a discrete elliptic curve E ﬁrst, then generates public 
as well as private keys based on one point O(x, y) on E in following 2 steps: 
Step 1: Creating a Server Key The local edge server generates a key pair (Kp, 
Ks) as shown in Eq. (
13): 
K p = H (Il , Ie) 
Ks = M(
K p)
(13) 
The hash function H calculates the public key Kp based on the ID Il of the 
edge server and the ID Ie of the ECUs. According to Kp, the private key Ks is calculated using the encoding function M. Following the calculation of Kp and 
Ks, the local edge server provides identity data D = {Kp, E, O} to ECUs. 
Step 2: ECUs Key Generation: After the ECUs get D as Eq. (
14), three keys 
Kvi, Kep, and Kes are calculated: 
Kvi = R 1(O(x, y))Kep = R 2(O(x, y)) 
Kep = R 2(O(x, y)) (14) 
where virtual key is used Kvi is used to safeguard ECUs’ genuine IDs, whereas 
Kep and Kes are ECUs’ public and private keys, respectively, and R1() and R2()150 D. Prabakar et al.
are PRNG functions. Then ECUs sends a message to the local edge server with 
the format F = {IDe, Kv, Kep, a}, where an is a nonce integer. The public key 
is used to contact edge servers and ECUs in the later authentication step, while 
private key is used to complete identity matching. Vi is set to 1 if the ECUs pass 
the authentication; otherwise, Vi is set to 0. 
The proposed framework would calculate trust values utilizing fuzzy rules, 
with fuzzy inputs being objective and subjective trust values as well as fuzzy outputs being trust values.Low, medium, and high are the 3 states of fuzzy 
outputs, and their ranges are 0 to 1. CSPj trust value maps to distrust if the 
obtained trust value is low. CSPj trust value maps to trust if the acquired trust 
value is high, and uncertain if the obtained trust value is low. 
We deﬁne behaviour trust of ECUs as Vb as a weighted total of three terms by 
Eq. (
15), taking into account the process of service request as well as response: 
Vb = w bcVbc + w beVbc + w br Vbr (15) 
where Vbc stands for behaviour constraint, Vbe for behaviour experience, and 
Vbr for behaviour change rate. wbc+wbe+wbr = 1 are weight coefﬁcients that 
can be modiﬁed depending on the task at hand. 
(2) Behavior Constraint: V arying ECUs observe different numbers of behaviour restrictions or interaction speciﬁcations, which are found by certain tasks using 
Eq. (
16): 
Vbe = w 1ξ1 + w 2ξ2 + ··· + w eξc (16) 
where ξi is constraint or speciﬁcation that ECU s follows,Σ c 
i=1 wi = 1 are 
weight coefﬁcients. 
Behavior Experience: Because the edge computing environment is always 
evolving, it is vital to evaluate trust changes on a regular basis.ECUc is used to 
deﬁne the ECUs that will be examined.Behavior experience Vbh indicates the inter-
active credibility of the recent past for interacting behaviours of ECUc with many ECUs, where minimum engagement can also lead to effective trust development. 
Assume that ECUs have interacted with k other ECUs, namely ECU1, ECU2, and ECUk. Let’s see what happens if we use Eq. (
17) 
sj (Δt) ={
τ 1 
j ,τ 2 
j ,...,τ s 
j ,...,τ t 
j}
(17) 
where 0 ≤ τ s 
j ≤ 1 is the interaction observation between itself and ECUj during 
time unit s. Assign a threshold to ECUj based on the size of interactive resources 
α j ∈ (0, 1). 
H+ 
j (Δt) =Σ t 
i=1 1(
τ i 
j ≥ α j)
if the condition is true, the 1(.) function returns 1; 
otherwise, it returns 0. Equation ( 18) can be used to deﬁne Vbe9 Trust Model Based Data Fusion … 151
Vbe = 1 
k kΣ 
j=1 H+ 
j (Δt) 
t Ri (t) 
ˆFi (18) 
Rate of Behavior Change: Because behaviour varies over time, Vbr, which indicates 
the real change in behaviour trust, is deﬁned as the rate of behaviour change. There are last time series
Δ(t − 1), mainly consider t − H+ 
j (Δ(t − 1)) of ECU j with 
ECU c. For the time series Δ(t), the basic exponential smoothing factor is H+ 
j (Δt) 
of ECU j with ECU c,λ. 
Because ECUs typically do not keep a lot of previous data due to restricted 
resources, the rate of behaviour change is based on two time series: Δ(t − 1), and
Δ(t). We believe historical data that is closer to the present moment to be more 
suggestive, so recent data is given more weight Δ(t). 
Capability Trust is described as degree of conﬁdence in a device’s capability 
property, such as accessibility, available bandwidth, response speed, and so on. 
L = {e1, e2,..., e n} is a set of ECU gives an ECU s. C = {c1, c2,..., c m} is 
capability property set of ECU. V alue of m of any ECU under an ECU n. is identical. 
Cs ={
cs1 , c s2 ,..., c sm}
indicates any cak ∈ C n requires based on ck ∈ C , where cak 
is standard value preset based on task requirements. Vs =⟨
Vs = C1 
s , C2 
s ,..., Cn 
s⟩
notes a vector of ECU s which contains Cs of ECU , and V =⟨
C1, C2,..., Cn⟩
is a 
vector where C is actual C s when ECU is working. 
The correlation coefﬁcient of Vs and V is described as ρ. We propose that value 
of denotes capacity trust of ECU s, which can be expressed as coefﬁcient vectors with the values of 
Vc · ωT 
v and ωT 
v,. 
CCA [ 12] method is used to evaluate ρ as follows: 
Wx ={
W i 
x}K 
i=1. 
3.3 Data Optimization Using Genetic Swarm Lightweight 
Optimization Algorithm 
The gateways are relay nodes that must be monitored to ensure that the load on edge servers is balanced. An appropriate Gateway-Edge setup is necessary for this. For an 
in-depth analysis of network trafﬁc, we use a variety of Key Performance Indicators (KPIs) to determine the best conﬁguration. The transmission path for each gateway is determined by an SDN controller. It checks the load on each server and raises an 
alarm if it exceeds a certain threshold value, requiring the existing Gateway-Edge (GE) connection to be re-conﬁgured. Determining ideal balanced GE conﬁguration at time t + 1 is a key.GE conﬁguration at a given time t can be given by a vector G 
t = { G t1 ,Gt2 , … ,Gtn } , w h e r eGtn ∈ { 1 ,2 , … ,N } . Gtn = m, for example, 
indicates that at time t, the nth gateway is communicating to mth server.We use two KPIs to solve Gateway-Edge conﬁguration issue: the network’s Average Residual 
Energy (KPIARE) and the servers’ Load Fairness Index (KPILFI). To maximise NP at time t, the normalised weighted sum of these two KPIs is used, as stated in Eq. 
(
19).152 D. Prabakar et al.
Max(NP) = αKP I AR E + β KP I LF I (19) 
NP is an optimization problem’s principal objective function, α and β are weights 
allocated to every KPI. As indicated in Eq. ( 20), these weights gives priority level of 
every KPI in goal function. 
NP = α(
1 
Nn NnΣ 
i=1 Ri (t) 
E)
+ β( 1 
M(Σ M 
i=1Σ N 
n=1 In,i ϕn(t)\ϕmax)2
Σ M 
i=1(Σ N 
n=1 In,i ϕn(t)\ϕmax)2(20) 
where Ri (t) 
ˆE is residual energy of a sensor node I at time t and is described as ratio of 
node i’s remaining energy (Ri(t)) to each node’s initial energy ( ˆE)a tt i m et .A tt h e 
moment of deployment, ˆE is the same for all nodes in the network. We examine load 
fairness at the edge servers for the second KPI. In, i is a binary indication, meaning 
it is 1 if an nth gateway sends φn packets to ith server at time t, and 0 otherwise. 
With an enhance in number of M and N, number of alternative conﬁgurations grows 
exponentially. We use evolutionary methods, such as GA and DPSO, to address GE 
conﬁguration as an optimization issue. To reach an optimal setup as well as balanced 
load, these algorithms go through the processes below. 
1. Produce random population R 0 of sizeΔ. Best possible position for every 
particle, i.e., Gateway, is started such that Pbest 0 
i = r0 
i , ∀1 ≤ i ≤Δ
2. Determine each particle’s ﬁtness value for DPSO and every chromosome’s ﬁtness 
value for GA in R 0 and its global best position Gbest0 by Eq. ( 21) 
Gbest0 = arg max 
1≤i≤ΔF(
Pbestl 
i)
(21) 
3. If best candidate solution for GE conﬁguration is found or maximum number of 
generations is achieved, search for GA is completed; otherwise, Step 4 is carried 
out. If best candidate solution is found in DPSO, particle velocities in current 
population must be updated utilizing Eq. ( 22). 
vI 
i = j wvI −1 
i + a 1r1(
PbestI 
i − xl 
i)
+ a 2r2(
GbestI 
i − xl 
i)
.(22) 
xI Igives particle I’s current position at the I th iteration, r1 and r2 are random 
variables in (0, 1) range, a1 and a2 are acceleration constants utilized to pull 
particles toward best position, and jw reﬂects inertia effect of the preceding particle’s velocity. 
4. Best accessible γ chromosomes from the present population are then extracted 
for GA. Present population is R I, and probability of selection is Ps. Iteration number is simply updated in DPSO. i.e., I = I + 1. 
5. In case of GA, crossover as well as mutation are done on γ. If best candidate 
solution for GE conﬁguration is found in DPSO, search is completed; otherwise, 
Step 6 is performed.9 Trust Model Based Data Fusion … 153
6. All of the processes from Step 2 are repeated for GA. In DPSO, each particle’s 
personal best position is updated utilizing Eq. ( 23). 
PbestI 
i =⎧
PbestI −1 
i, if F(
r I 
i)
≤ F PbestI −1 
i) 
r I 
i , otherwise. (23) 
7. The global optimum position for DPSO is updated using Eq. ( 24). 
Gbestl 
i = ⎧ 
⎨ 
⎩ arg max 
1≤i≤ΔF(
Pbestl 
i)
,if F(
Pbestl 
i)
> F(
Pbestl−1 
i)
Gbestl−1 
i, otherwise. (24) 
8. For DPSO, repeat all steps from Step 1. 
Based on NP as well as KPI values, SDN controller continuously conﬁgures 
the Gateway-Edge. The above technique can be applied to edge servers that are 
homogeneous or have the ability to estimate at nearly identical times. It does not take into consideration dynamic scenarios in which some data is processed ahead 
of others. As a result, a service migration method is suggested to further balance load—which is doable because most edge servers run Linux-based OS. 
Algorithm: 2—GSLOA 
Input: dynamic CPU and memory utilization threshold 
values i.e. U tc and U tm, respectively—computed periodically using Eq. ( 12); channel 
condition Ct 
Output: migration list map → input for load optimization 
module in Fig. 3 
1. compute CPU utilization level of the edge node (EN uc); 
2. compute memory utilization slevel of the edge node (EN um ); 
3. compute channel condition (Cc) 
4. For each node edge do 
5. Select application m from edge node 
6. Choose edge node n as destination node 
7. Map = m,n 
8. End if 
9. End for 
10. Return map 
Fig. 3 Details on loss function elements for SSDAE method154 D. Prabakar et al.
Fig. 4 Illustration of edge computing architecture 
4 Performance Analysis 
Performance of trust evaluation with data fusion as well as data optimization 
approaches is addressed in this section. The experimental infrastructure for analysing 
the suggested trustworthy data gathering algorithm was sbuilt using MA TLAB 
R2018a. The experimental setup consists of randomly placing 100 nodes in a 300 × 300 m area as well as selecting 30 nodes to serve as CH nodes. Experimental 
setup includes two IoTs, one cloud, and one BS, with each IoT having its own edge platform as well as cloud sitting on top of IoT. Users are positioned in lowest layer of 
the cloud, where underlying nodes are clustered independently as well as adjacent to the BS nodes. The data transfer rate is 2 × 106 bit/s, and the transmission time from edge node to cloud is sixteen milliseconds. Moving edge node is considered to move 
at a constant speed from the chosen beginning point, speed as well as communication radius are changed. Following creation of initial path without crossover, a path with highest trust value within required moving distance is constructed. 
Tables 1, 2, 3, 4, 5 and 6 shows the comparative analysis between existing and 
proposed technique in terms of network security analysis, throughput, Coverage fraction, Delay time, Energy consumption, Storage efﬁciency. Here comparison has9 Trust Model Based Data Fusion … 155
been carried out for enhancing data fusion model with security and data optimization 
technique in edge computing. The proposed technique obtained network security 
analysis of 90% for 500 epochs since the analysis has been carried out based on number of epochs in NN as represented in Fig. 
3. Then the throughput analysis of 
proposed technique has been carried out and it obtained 98% for 500 epochs as shown in Fig. 
4. The analysis of coverage fraction obtained of 81% for 500 epochs 
as shown in Fig. 5. The proposed technique obtained as 65% for 500 epochs of 
energy consumption as shown in Fig. 6. Delay time of proposed technique is 55% 
as shown in Fig. 7 for 500 epochs. Comparative analysis has been carried out for 
storage efﬁciency of 92% for 500 epochs as shown in Fig. 8. From the above analysis, 
proposed technique obtained optimal results (Figs. 9 and 10). 
Table 1 Comparative analysis of network security analysis 
Number of nodes SVM PCA_IDS SSDAE_SSFTM-GSLOA 
100 65 71 75 
200 69 75 79 
300 71 79 83 
400 75 81 85 
500 78 83 90 
Table 2 Comparative analysis of throughput 
Number of nodes SVM PCA_IDS SSDAE_SSFTM-GSLOA 
100 75 81 85 
200 79 83 89 
300 81 86 91 
400 83 89 95 
500 85 93 98 
Table 3 Comparative analysis of coverage fraction 
Number of nodes SVM PCA_IDS SSDAE_SSFTM-GSLOA 
100 61 65 68 
200 63 69 71 
300 65 73 75 
400 69 75 79 
500 71 79 81156 D. Prabakar et al.
Table 4 Comparative analysis of energy consumption 
Number of nodes SVM PCA_IDS SSDAE_SSFTM-GSLOA 
100 71 65 55 
200 73 68 59 
300 75 69 61 
400 79 72 63 
500 80 75 65 
Table 5 Comparative analysis of Delay time 
Number of nodes SVM PCA_IDS SSDAE_SSFTM-GSLOA 
100 61 51 45 
200 63 53 49 
300 64 55 51 
400 65 59 53 
500 69 61 55 
Table 6 Comparative analysis of storage efﬁciency 
Number of nodes SVM PCA_IDS SSDAE_SSFTM-GSLOA 
100 66 75 81 
200 69 79 83 
300 75 83 85 
400 79 85 89 
500 81 89 92 
Fig. 5 Comparison of network security analysis9 Trust Model Based Data Fusion … 157
Fig. 6 Comparison of throughput 
Fig. 7 Comparative analysis of coverage fraction
5 Conclusion 
This research proposes novel technique in data fusion model with security and data 
optimization technique in edge computing. Here the proposed data fusion is carried out using secure sequential discriminant auto encoder in which the improvement of 
data accuracy, as well as for the maximizing of Edge-cloud based sensor networks lifespan. The fusion of edge cloud data has been carried out using discriminant auto158 D. Prabakar et al.
Fig. 8 Comparative analysis of energy consumption 
Fig. 9 Comparative analysis of delay time
encoder which is integrated with distributed edge cloud users, where the security of 
the network has been enhanced using secure sequential fuzzy based trust model. The data optimization has been established using Genetic swarm lightweight optimization 
algorithm. The experimental analysis is carried out based on data fusion and network security model. The parametric analysis is carried out in terms of network security 
analysis of 90%, throughput of 98%, coverage fraction obtained of 81%, Energy9 Trust Model Based Data Fusion … 159
Fig. 10 Comparative analysis of storage efﬁciency
consumption of 65%, Delay time of 55%, Storage efﬁciency of 92%.The limitation 
of this research is to enhance security as well as enhance data transmission rate. 
In future this technique can be implemented for industrial based application and 
medical application with enhanced security and storage efﬁciency. 
References 
1. W. Mao, W. Feng, Y . Liu, D. Zhang, X. Liang, A new deep auto-encoder method with fusing 
discriminant information for bearing fault diagnosis. Mech. Syst. Signal Process. 150, 107233 
(2021) 
2. M. Lopez-Martin, B. Carro, A. Sanchez-Esguevillas, J. Lloret, Conditional variational autoen-coder for prediction and feature recovery applied to intrusion detection in iot. Sensors 17(9), 1967 (2017) 
3. C. Jia, K. Lin, J. Deng, A multi-property method to evaluate trust of edge computing based on data driven capsule network. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications W orkshops (INFOCOM WKSHPS), (IEEE, 2020), pp. 616–621 
4. M.A. Jan, M. Zakarya, M. Khan, S. Mastorakis, V .G. Menon, V . Balasubramanian, A.U. Rehman, An AI-enabled lightweight data fusion and load optimization approach for Internet 
of Things. Futur. Gener. Comput. Syst. 122, 40–51 (2021) 
5. B. Mahbooba, M. Timilsina, R. Sahal, M. Serrano, Explainable artiﬁcial intelligence (xai) to enhance trust management in intrusion detection systems using decision tree model. Complexity (2021) 
6. A.B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins, R. Chatila, F. Herrera, Explainable Artiﬁcial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58, 82–115160 D. Prabakar et al.
7. A. Krajna, M. Brcic, M. Kovac, A. Sarcevic, Explainable artiﬁcial intelligence: An updated 
perspective. Under Rev (2022) 
8. F. Hussain, R. Hussain, E. Hossain, Explainable artiﬁcial intelligence (XAI): An engineering perspective. 
arXiv:2101.03613 (2021) 
9. G. Vilone, L. Longo, Notions of explainability and evaluation approaches for explainable artiﬁcial intelligence. Inf. Fusion 76, 89–106 (2021) 
10. T. Rojat, R. Puget, D. Filliat, J. Del Ser, R. Gelin, N. Díaz-Rodríguez, Explainable artiﬁcial intelligence (xai) on timeseries data: A survey. 
arXiv:2104.00950 (2021) 
11. A. Holzinger, M. Dehmer, F. Emmert-Streib, R. Cucchiara, I. Augenstein, J. Del Ser, N. Díaz-Rodríguez, Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artiﬁcial intelligence. Inf. Fusion 79, 263–278 (2022) 
12. C.K. Leung, F. Jiang, Y . Zhang, Explainable machine learning and mining of inﬂuential patterns from sparse web. In 2020 IEEE/WIC/ACM International Joint Conference on W eb Intelligence and Intelligent Agent T echnology (WI-IAT). (IEEE, 2020), pp. 829–836 
13. N. Mehdiyev, P . Fettke, Explainable artiﬁcial intelligence for process mining: A general overview and application of a novel local explanation approach for predictive process monitoring. Interpretable Artif. Intell. Perspect. Granular Comput. 1–28 (2021) 
14. I. Kakogeorgiou, K. Karantzalos, Evaluating explainable artiﬁcial intelligence methods for multi-label deep learning classiﬁcation tasks in remote sensing. Int. J. Appl. Earth Obs. Geoinf. 103, 102520 (2021) 
15. D. Thakker, B.K. Mishra, A. Abdullatif, S. Mazumdar, S. Simpson, Explainable artiﬁcial intelligence for developing smart cities solutions. Smart Cities 3(4), 1353–1382 (2020) 
16. A. Anguita-Ruiz, A. Segura-Delgado, R. Alcalá, C.M. Aguilera, J. Alcalá-Fdez, explainable Artiﬁcial Intelligence (XAI) for the identiﬁcation of biologically relevant gene expression patterns in longitudinal human studies, insights from obesity research. PLoS Comput. Biol. 16(4), e1007792 (2020) 
17. S.R. Islam, W. Eberle, S.K. Ghafoor, A. Siraj, M. Rogers, Domain knowledge aided explainable artiﬁcial intelligence for intrusion detection and response. 
arXiv:1911.09853 (2019) 
18. C. Dindorf, J. Konradi, C. Wolf, B. Taetz, G. Bleser, J. Huthwelker, F. Werthmann, E. Bartaguiz, J. Kniepert, P . Drees, U. Betz, M. Fröhlich, Classiﬁcation and automated interpretation of spinal posture data using a pathology-independent classiﬁer and explainable artiﬁcial intelligence (Xai). Sensors, 21(18), 6323 
19. Y . Xu, Y . Wu, H. Gao, S. Song, Y . Yin, X. Xiao, Collaborative APIs recommendation for artiﬁcial intelligence of things with information fusion. Futur. Gener. Comput. Syst. 125, 471– 
479 (2021) 
20. S. Atakishiyev, H. Babiker, N. Farruque, R. Goebel, M.Y . Kima, M.H. Motallebi, J. Rabelo, T. Syed, O.R. Zaïane, A multi-component framework for the analysis and design of explainable artiﬁcial intelligence. 
arXiv:2005.01908 (2020)Chapter 10 
A Deep Learning Based T arget Coverage 
Protocol for Edge Computing Enabled Wireless Sensor Networks 
Pooja Chaturvedi, A. K. Daniel, and Umesh Bodkhe 
Abstract The sensor networks have attracted a numerous research attention due to 
its diverse applications ranging from surveillance and monitoring applications. The 
sensor nodes are usually characterized as having scarce resources; hence energy efﬁ-
cient mechanisms which can enhance the resource utilization are of great signiﬁcance. 
The integration of edge computing framework with the sensor network can aid in the 
data collection, dissemination and decision making. Scheduling approaches which 
divide the nodes into a number of set covers and monitor the given points of interest with the desired conﬁdence level along with the objective of maximizing coverage 
and network lifetime have been proved a prominent approach. The determination of set covers is a NP hard problem and is dependent on different network parameters such as node contribution, trust values and coverage probability. In this scheme, the 
node has to monitor the neighboring node parameters at regular intervals, which incurs a huge number of communication overhead. The nodes in sensor network can employ the learning strategy to determine its best possible action to enhance the 
network coverage as well as network lifetime. The chapter proposes a LSTM based strategy for an edge computing enabled WSN to determine the status of the node depending on the network parameters such as number of communications, number of 
packets transmitted and initial energy of the nodes. The proposed protocol is imple-mented using tensor ﬂow and keras libraries in the python language. The keras tuner package has been used to determine the best parameters such as number of hidden 
layers and number of neurons in each layer. The obtained parameters are used to construct a hyper model and the efﬁciency of the model is evaluated in terms of 
the loss function. The explainability of the proposed model is investigated using the
P . Chaturvedi (B) · U. Bodkhe (B) 
Institute of Technology, Nirma University, Ahmedabad, Gujarat, India 
e-mail: pooja.chaturvedi@nirmauni.ac.in 
U. Bodkhe 
e-mail: umesh.bodkhe@nirmauni.ac.in 
A. K. Daniel 
Madan Mohan Malaviya University of Technology, Gorakhpur, India 
e-mail: danielak@rediffmail.com 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 A. E. Hassanien et al. (eds.), Explainable Edge AI: A Futuristic Computing Perspective, 
Studies in Computational Intelligence 1072, 
https://doi.org/10.1007/978-3-031-18292-1_10 161162 P . Chaturvedi et al.
Local Interpretable Model-agnostic Explanations (LIME) framework and the effect 
of all the features on the status prediction have been determined. 
Keywords Target coverage ·Network lifetime ·LSTM ·Deep learning ·Energy 
efﬁciency ·Edge enabled WSN 
1 Introduction 
WSNs are extensively applicable in different environmental investigation and 
reporting applications due to their minimal energy need, lesser cost and diverse applications. The different issues arises in the sensor network incorporating a large 
number of nodes such as increased energy consumption, increase latency, low QoS and more communication overhead. 
Apart from these issues, ensuring the coverage to the environmental parameters 
is also a major issue. The node scheduling strategy is often used in the network design which ensures the signiﬁcant observation of the environment and energy 
optimization. The scheduling strategy is based on the approach of keeping a small subset of nodes in active state to monitor the required target region, as compared to the activation of all the nodes[
1, 2]. One such strategy is shown in [ 3], in which the 
nodes are activated according to the network parameters such as coverage probability detection, node trust worthiness and contribution in monitoring of the targets by the nodes. The nodes are scheduled to be in the active state depending on their capability 
to be included in the set cover. The larger the number of set covers, the larger will be network lifetime. 
Predictive analysis based techniques aid in the network applications such as 
reducing the redundant data transmission, predicting the network conditions and forecasting the tangible multistep information. The paper presents a prediction based approach in which the status of the nodes is predicted on the basis of historical data 
using the LSTM based neural network. The LSTM based neural network consist of the memory element which can improve the learning capability of the neural network. 
The main contributions of the paper are as follows: 
1. The deep learning based prediction model is proposed for the target coverage 
problem in the wireless sensor network to determine the status of the node to be 
considered as active or sleep. 
2. The different parameters of the network are considered such as probability of 
coverage, trust value of the nodes. 
3. The objective of the approach is determining the number of set covers consisting 
of minimum number of nodes. 
4. The determination of set cover is considered as the NP hard problem and is computationally complex, so machine learning based strategy can be adopted to predict the node status on the basis of various parameters. 
The organization of the paper is: the different machine learning based approaches 
have been discussed in the Sect. 2, network model in Sect. 3, the proposed protocol10 A Deep Learning Based Target Coverage Protocol for Edge … 163
in Sect. 4, the simulation setup and parameters is described in Sect. 5, simulation 
results and analysis is discussed in the Sect. 6 and Sect. 7 consist of conclusion and 
future scope. 
2 Machine Learning in WSN 
The machine learning based approaches are considered as efﬁcient because it requires 
less complexity. The different applications in which machine learning approaches can 
be applied are: localization [ 3–6], coverage and connectivity [ 7–13], anomaly detec-
tion [ 14–18], fault detection [ 19–22], routing [ 23–25], MAC [ 26–31], data aggrega-
tion [ 32–35], synchronization [ 36–38], congestion control [ 39–43], and quality of 
service [ 44–46]. The survey on the applications of machine learning technique in 
different applications in WSNs can be found in [ 47–50]. 
The beneﬁts of employing machine learning based approaches for the coverage 
problem are as: 
(i) The set cover consisting of minimum number of nodes can be determined quickly and easily. 
(ii) The nodes can be classiﬁed as active/idle in the dynamic environment without 
losing the data. 
In [51], authors have proposed a regression based interface design approach 
which maintains the connectivity of the network. In [ 52], SVM based communi-
cation approach has been proposed which reduces the communication complexity and hence improves the connectivity of the network. In [
53], random forest coverage 
and connectivity based approach is proposed which improves the coverage area by 
changing the attributes of the network. In [ 54], reinforcement learning based coverage 
optimization approach is proposed which enhances the network lifetime. 
3 Network Model 
Figure 1 represents the network model considered for the proposed prediction based 
strategy in edge computing based environment. At the lowest level the number of sensor nodes is deployed to monitor the given environment. The sensor nodes collect 
the information from the environment and transmit it to the edge server where it is used for further processing and analysis. The advantage of the edge server is that 
the data is processed near the source of the data generation. The edge server then 
forwards the data towards the application layer which usually comprise of different 
applications which may utilize the sensed information.164 P . Chaturvedi et al.
Fig. 1 The edge enabled network model for WSN 
4 Proposed Protocol 
The steps of the proposed protocol are as shown in the Fig. 2:
1. Identiﬁcation and deﬁnition of the problem 
In this paper we have considered the problem of determination of the node status on the basis of various network parameters such as coverage probability, trust, 
node degree, number of communications, number of packets transmitted, CH or not and energy consumed. The problem is considered as a binary classiﬁcation problem which classiﬁes the nodes into one of the two states as active or sleep. 
The machine learning based prediction model is used so that the communication overhead will be reduced and the energy conservation is achieved. We have deployed the network consisting of 30 nodes and 10 targets in the square region 
of dimension of 100*100. The parameter values for the network are obtained by executing the simulation for the 100 iterations. The results of the simulation are stored in .csv ﬁle. 
2. Identiﬁcation of the input and output parameters In the next step we have identiﬁed the output parameter as Status and the remaining attributes are considered as the input parameters. 
3. Importing the required packages In the next step, the necessary package required for the simulation and analysis 
purpose are imported. 
(i) pandas—It is used to load, visualize and analyze the dataset. The dataset is read using the read_csv method available in this package.10 A Deep Learning Based Target Coverage Protocol for Edge … 165
Fig. 2 Flow chart for the 
proposed approach
Yes No 
End Transformation of dataset using different 
encoding mechanism 
Split the dataset into training and 
validation set and train the model 
Make the prediction and determine if the 
optimal parameters are achieved? 
Construct the hypermodel 
Make the prediction using the hypermodel 
and determine the efficiency 
Explain the prediction of the model using the 
Local Feature Importance on LIME 
Boost the model performance using LGBM 
Classifier 
Identify the global importance of features Start 
Identification and definition of problem 
Identification of input and output 
parameters 
Import the required packages 
Understanding the dataset by performing 
the Exploratory Data Analysis 166 P . Chaturvedi et al.
(ii) numpy—It is used to perform the mathematical operations on the dataset. 
(iii) matplotlib.pyplot—This package is used to plot the different ﬁgures and 
plots 
related to the dataset. 
(iv) label encoder from sklearn.preprocessing—The label encoder method 
from 
sklearn.preprocessing is used to encode the categorical attributes 
such as node staus, CH or not into numberical values. 
(v) seaborn—This package is used to plot the correlation matrix for the dif
ferent parameters available in the dataset. 
(vi) tensorﬂow keras—The tensorﬂow package is used to build using the 
deep 
learning based models. The keras package is also imported from 
tensorﬂow library. 
(vii) kerastuner—This package is used to perform the hyper parameter tuning of 
the LSTM based neural network. 
(viii) kerastuner.tuners random search—The random search from the kerastuner is 
used to generate the different models by varying the number of hidden 
layers and neurons in the layer. 
(ix) train_test_split from sklearn.model_selection—The train_test_split method 
from sklearn.model_selection is used to divide the original 
dataset into training and testing dataset. 
(x) lime and lime_tabular—The lime package is used to achieve the explain-ability 
of the machine learning model. It explains the reason behind the 
particular output of the designed model. The lime_tabular method is used to visualize the impact of different parameters on the positive and negative classes in the notebook. 
(xi) lightgbm—Gradient descent based boosting mechanism is a collection of 
ensemble machine learning methods which aims to minimize the loss 
function. In the boosting method the hard to classify data instances are 
given more weights. There are different implementations available for 
the gradient descent based boosting methods such as Extreme Gradient Descent boosting (Xgboost) and Light Gradient Descent boosting model. 
In this paper we have used light gradient boosting model to improve the 
prediction model which is based on decision trees. The advantages of the light gradient descent boosting model are: 
a. The memory requirement is low. 
b. Accuracy of the model is more. 
c. It supports the GPU learning, parallel and distributed modeling. 
d. These methods are capable of handling large number of datasets. 
e. The efﬁciency of the model is high. 
f. The training of the model is faster.
4. Understanding the dataset by performing the Exploratory data analysis The 
Exploratory data analysis phase consists of following steps: 
a. Identifying the number of attributes having missed or zero values. 
b. Identifying the number of duplicate entries.10 A Deep Learning Based Target Coverage Protocol for Edge … 167
Fig. 3 Count of active and 
sleep nodes 
c.Identifying the size of the data set. The considered data set consist of 100 
rows and 9 columns. The column names are Node id, coverage probability, 
trust, no of communications, no of packets transmitted, whether the node is 
cluster head or not, degree of the nodes, energy consumed, no of targets and the status of the node as either active or sleep. 
d. Determining the number of nodes in the active and sleep state using the pandas library in python language. The number of active and sleep nodes 
based on the simulation of 100 nodes is as shown in the Fig. 
3. The results 
show that there are 60 active and 40 sleep nodes, so we can say that the data 
set is balanced. 
5. Transformation of dataset using the different encoding mechanisms 
a. The next step is preprocessing of the data set. Since the column CH or not and Status contains the categorical data, so we have used the label encoding 
mechanism to transform the data into numerical values. 
b. In the next step, the correlation plot is determined to identify the most inﬂu-ential parameter on the node status. From the Fig. 
4 it is clear that coverage 
probability , CH or not has the most signiﬁcant impact on the node status.
6. Spiting our data to train and validation data In the next step, the dataset is divided into training and test dataset using sklearn’s 
inbuilt train_test_split method. The test data set is considered as 30%. For determining the efﬁciency of the proposed approach we have used the validation dataset size as 30%. 
7. Using keras tuner to determine the optimal parameters for the LSTM based neural network 
In this step the keras tuner is used to determine the optimal number of layers 
and neurons in each layer. The random search method is used to evaluate the 
loss function of the model for the varying number of layers and neurons.168 P . Chaturvedi et al.
Fig. 4 Correlation plot 
8. Designing the hyper model 
The hypermodel is designed using the optimal parameters obtained in the step 
7. The hyper model is then evaluated by executing the model for 1000 iterations. 
9. Making the prediction and determining the efﬁciency 
The efﬁciency of the hyper model is determined in terms of the loss function 
by executing the model for 1000 iterations. 
10. Explaining the model with Lime The local impact of the various parameters on the node status prediction is 
determined using the LIME package which is available in the python language. 
The explain method is used to display the impact of the different parameters on 
the node status as active or sleep. 
11. Using extreme gradient boosting machine learning model (Lightgbm) for 
prediction 
The gradient boosting machine learning model is used to predict the node status. 
The efﬁciency of the model is determined for the validation dataset of size 30%. 
The metric method is used to visualize the training and testing accuracy of the 
model. 
12. Identifying the global importance of the features 
The global importance parameter is used to identify the signiﬁcant parameters 
using the feature importance method available in lightgbm.10 A Deep Learning Based Target Coverage Protocol for Edge … 169
Ta b l e 1 Parameter valuesParameter Va l u e s 
No. of nodes 20 
No. of targets 5 
Sensing range 5 
Communication range 10 
No. of layers 2–20 
No of neurons in each layer 32–512 
Step size 32 
Learning rate 0.001, 0.0001, 0.00001 
Activation function at hidden layer Rectiﬁed linear unit 
Activation function at output layer Linear 
Loss function Mean absolute error 
Performance metric Mean absolute error 
Test data set size 30% 
Max trial 100 
Execution per trial 5 
No. of iterations 1000 
5 Simulation Setup and Parameters 
The dataset obtained through the simulation of the sensor network is provided as 
input to the LSTM based neural network. The keras tuner package is used to obtain 
the optimal parameter such as number of hidden layers and number of neurons in 
each layer. The number of layers is varied from 2 to 20 and number of neurons is varied between 32 and 512 with a step size of 32. The learning rate is considered 
as 0.001, 0.0001 and 0.00001. The other simulation parameters are as shown in the Table 
1. 
The hypermodel is constructed based on the parameter values obtained and then it 
is trained for 100 trials with 5 executions per trial. The hypermodel is then executed for the 1000 iterations to analyze its efﬁciency. The results of the training and validation are discussed in the next section. 
6 Simulation Results and Discussion 
The performance of the proposed prediction based model is evaluated using the simulation setup and parameters deﬁned in the previous section as discussed in the subsections:170 P . Chaturvedi et al.
6.1 Model Analysis 
The network parameters obtained through simulation are provided as input to LSTM 
based neural network model. To determine the optimal number of layers and neurons the keras tuner package is used. The keras tuner regularly vary the number of hidden 
layers, number of neurons and learning and train the model for the different learning rates. The mea absolute error is considered as the loss function. The validation split is considered as the 30% and the loss function for the validation data set is obtained 
as val_mean_absolute_error: 0.179. The results of 10 best iterations are summarized in the Table 
2. It can be seen from the results that the best loss function is obtained 
as 0.1604 for the number of layers as 4 and learning rate at 0.001. 
The number of neurons in each layer for the 10 best trials is as shown in the 
Table 3.
The score of the 10 best iterations is shown in the Fig. 5.
Using the best parameters thus obtained, a hyper model is constructed and the 
prediction is done for the test dataset. The hypermodel is trained for the training 
dataset and tested for the test dataset by executing the model for the 1000 iterations. 
The results show that after 1000th iteration, the loss function is almost stable. The 
loss function using the hypermodel for the training and test dataset is as follow: 
[test loss, train loss]: [0.1099, 0.1099]. 
The comparison results of the loss function for the training and validation dataset 
is as shown in the Fig. 6. The results show that the hypermodel accurately predicts 
the node status for the testing as well as the validation dataset.
Ta b l e 2 Result of 10 
iterations Trial Num_layers Learning rate Score 
1 4 0.01 0.16047 
2 6 0.001 0.1706 
3 6 0.01 0.1752 
4 3 0.001 0.1778 
5 3 0.01 0.1790 
6 4 0.01 0.1791 
7 4 0.01 0.1793 
8 7 0.001 0.1798 
9 2 0.001 0.1833 
10 5 0.01 0.1853 10 A Deep Learning Based Target Coverage Protocol for Edge … 171
Ta b l e 3 Number of neurons in each layer in each trial 
Trial/union 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 
1 224 352 192 96 64 192 288 352 384 448 384 288 320 384 64 320 352 256 224 288 
2 224 96 384 448 320 480 352 32 320 288 160 32 96 384 384 160 160 96 352 480 
3 352 32 192 96 160 128 480 320 192 256 128 256 224 416 192 256 256 224 416 288 
4 128 192 288 32 352 320 416 416 64 32 416 256 448 96 448 256 416 320 32 448 
5 224 480 320 224 256 96 480 480 160 256 224 416 320 352 128 192 128 128 96 384 
6 384 192 96 192 256 320 448 32 512 416 480 480 448 64 
7 160 448 512 512 384 480 256 384 448 192 320 480 160 160 384 448 416 256 416 384 
8 288 320 32 160 480 192 352 32 192 96 64 160 192 320 256 256 96 96 
9 128 384 416 320 352 448 256 256 384 416 224 64 352 416 192 288 96 256 224 192 
10 320 416 320 256 320 352 480 192 352 352 256 128 512 96 384 448 320 64 384 32172 P . Chaturvedi et al.
Fig. 5 Score of the 10 best 
iterations using hypermodel
Fig. 6 Model loss versus Epoch 
6.2 Explainability of the Proposed Model Using Lime 
The machine learning models are usually considered as the black box which only 
produces the results in the form of prediction and the impact of the different param-
eters are usually hidden from the outside world. So the LIME package is used to understand the most signiﬁcant parameter in the prediction of the node status.
a. Positive Figure 
7 represents the local explanation of the various features on the node 
status being as active. The green color represents the positive correlation with10 A Deep Learning Based Target Coverage Protocol for Edge … 173
Fig. 7 Impact of network parameters on the positive class 
the node status and the red color represents the negative correlation on the node 
status. It can be inferred from the ﬁgure that the attributes CH or not, energy consumed, number of packets and number of targets have positive correlation on 
the node status, whereas the coverage probability, node degree, trust and number of communications have negative correlation on the node status.
b. Negative Figure 
8 represents the local explanation of the various features on the node 
status being as sleep. The green color represents the positive correlation with 
the node status and the red color represents the negative correlation on the node status. It can be inferred from the ﬁgure that the attributes CH or not, energy 
consumed, number of packets and number of targets have positive correlation on the node status, whereas the coverage probability, node degree, trust and number of communications have negative correlation on the node status. 
Fig. 8 Impact of the network parameters on the negative class174 P . Chaturvedi et al.
Fig. 9 The impact of 
network parameters in the list form on both the classes 
The correlation of various features on the node status is shown in the list form in 
the Fig. 9. The active status is represented as 0 and sleep status is represented as 1. 
Row 0 
The local importance of various features for the ﬁrst row in the dataset has been 
analyzed for the node status as active or sleep as shown in the Fig. 10. The parameter 
values for the row are as shown in the Table 4.
Row 1 
The local importance of various features for the ﬁrst row in the dataset has been 
analyzed for the node status as active or sleep as shown in the Fig. 11. The parameter 
values for the row are as shown in the Table 5.
6.3 LGBM Classiﬁer 
In this section we have implemented the LGBM classiﬁer for the considered dataset to improve the training efﬁciency and faster convergence. The other simulation parameters are: 
Learning_rate = 0.09, max depth = 5 random state = 42, V erbose = 20, 
eval_metric = ‘logloss’. 
The results of the model prediction for the training and validation dataset are as 
shown in the Table 6.
The binary logloss function is used to evaluate the efﬁciency of the LGBM clas-
siﬁer as shown in the Fig. 12. The loss value is approaching to minimum in 200 
iterations for the training as well as the validation, which proves the efﬁciency of the proposed model.10 A Deep Learning Based Target Coverage Protocol for Edge … 175
Fig. 10 Local impact of feature on the node status for the row 0176 P . Chaturvedi et al.
Ta b l e 4 Parameter values for 
the row 0 Parameter Intercept Prediction_local Right 
Va l u e 0.180 0.230 0.235
The accuracy of LGBM classiﬁer is obtained as: 
Training accuracy: 1.0000 
Testing accuracy: 0.9667 
6.4 Global F eature Importance 
The global feature importance on the node prediction is plotted as shown in the Fig . 
13. The results show that the most dominant features for the node status prediction are 
Column 0–coverage probability, Column 1–trust, column 3–CH or not and Column 
4–node degree.
7 Conclusion and Future Scope 
The paper presented a LSTM based data prediction approach which can be used 
to determine the node status in the scheduling approach to monitor a given set of 
targets. The keras tuner package has been used to determine the optimal number of neurons and number of hidden layers. The results are then applied to construct 
the hypermodel which is then used to predict the node status. The loss function for the model is determined for the different learning rate, number of hidden layers and number of neurons. The lime package is used to determine the local feature impact 
on the target attribute. The LGBM based classiﬁer is used as a boosting algorithm to improve the prediction model. The simulation results show that the LGBM based boosting strategy achieves the accuracy of 100% and 97% for the training and testing 
dataset. 
As a future work, we aim to determine the different performance parameters for 
the proposed prediction model such as network lifetime, energy consumption and other QoS parameters.10 A Deep Learning Based Target Coverage Protocol for Edge … 177
Fig. 11 Local impact of feature on the node status for the row 1178 P . Chaturvedi et al.
Ta b l e 5 Parameter values for 
the row 1 Parameter Intercept Prediction_local Right 
Va l u e 0.188 0.236 0.251
Ta b l e 6 Loss values for the LGBM classiﬁer for training and validation dataset for 100 Epochs 
Epochs Training loss V alidation loss 
0 1 0 1 
20 0.1118 0.1118 0.1277 0.1277 
40 0.0356 0.0356 0.0618 0.0618 
60 0.0118 0.0118 0.0554 0.0554 
80 0.0038 0.0038 0.0637 0.0637 
100 0.0014 0.0014 0.0747 0.0747
Fig. 12 Loss function for the training and validation datasets
Fig. 13 Feature importance of the model using LGBM classiﬁer
10 A Deep Learning Based Target Coverage Protocol for Edge … 179
References 
1. J. Yick, B. Mukherjee, D. Ghosal, Wireless sensor network survey. Comput. Netw. 52(12), 
2292–2330 (2008) 
2. R. Mulligan, H.M. Ammari, Coverage in wireless sensor networks: a survey. Netw. Protoc. Algorithms 2(2), ISSN: 1943–3581 
3. P . Chaturvedi, A.K. Daniel, An energy efﬁcient node scheduling protocol for target coverage in wireless sensor networks. Paper presented at the 5th international conference on communication system and network technologies (CSNT-2015), 2015 
4. J. Kuriakose, S. Joshi, R.V . Raju, A. Kilaru, A review on localization in wireless sensor networks. Adv. Sig. process. Intel. Recogn. Syst. 599–610 (2014) 
5. P . Cottone, S. Gaglio, G.L. Re, M. Ortolani, A machine learning approach for user localization exploiting connectivity data. Eng. Appl. Artif. Intell. 50, 125–134 (2016) 
6. M. Bernas, B. Paczek, Fully connected neural networks ensemble with signal strength clustering for indoor localization in wireless sensor networks. Int. J. Distrib. Sens. Netw. 11(12), 1–10 
(2015) 
7. I. Ullah, Y . Shen, X. Su, C. Esposito, C. Choi, A localization based on unscented Kalman ﬁlter and particle ﬁlter localization algorithms. IEEE Access 8, 2233–2246 (2020) 
8. S M. Mohamed, H.S. Hamza, I.A. Saroit, Coverage in mobile wireless sensor networks (M-WSN): a survey, Comput. Commun. 110(C), 133–150 (2017) 
9. W. Fang, X. Song, X. Wu, J. Sun, M. Hu, Novel efﬁcient deployment schemes for sensor coverage in mobile wireless sensor networks. Inf. Fusion 41, 25–36 (2018) 
10. H. Li, S. Wang, M. Gong, Q. Chen, L. Chen, IM2DCA: Immune mechanism based multipath decoupling connectivity algorithm with fault tolerance under coverage optimization in wireless sensor networks. Appl. Soft. Comput. 58, 540–552 (2017) 
11. M. Abo-Zahhad, N. Sabor, S. Sasaki, S.M. Ahmed, A centralized immune-voronoi deploy-ment algorithm for coverage maximization and energy conservation in mobile wireless sensor networks. Inf. Fusion 30, 36–51 (2016) 
12. W. Wang, H. Huang, Q. Li, F. He, C. Sha, Generalized intrusion detection mechanism for empowered intruders in wireless sensor networks. IEEE Access 8, 25170–25183 (2020) 
13. C. Miranda, G. Kaddoum, E. Bou-Harb, S. Garg, K. Kaur, A collaborative security framework for software-deﬁned wireless sensor networks. IEEE Trans. Inf. Forensics Secur. 15, 2602–2615 
(2020) 
14. Y . Gao, F. Xiao, J. Liu, R. Wang, Distributed soft fault detection for interval type-2 fuzzy-model-based stochastic systems with wireless sensor networks. IEEE Trans. Ind. Informat. 15(1), 334–347 (2019) 
15. J. Tan et al., An efﬁcient information maximization based adaptive congestion control scheme in wireless sensor network. IEEE Access 7, 64878–64896 (2019) 
16. F. Hajjej, M. Hamdi, R. Ejbali, M. Zaied, A distributed coverage hole recovery approach based on reinforcement learning for wireless sensor networks. Ad Hoc Netw. 101 (2020). 102082 
17. Z. Sun, L. Wei, C. Xu, Z. Lv, An event-driven mechanism coverage algorithm based on sensing-cloud-computing in sensor networks. IEEE Access, 7, 84668–84679 (2019) 
18. B. Khalifa, A.M. Khedr, Z. Al Aghbari, A coverage maintenance algorithm for mobile WSNs with adjustable sensing range. IEEE Sens. J. 20(3), 1582–1591 (2020) 
19. A. Farhat, C. Guyeux, A. Makhoul, A. Jaber, R. Tawil, A. Hijazi, Impacts of wireless sensor networks strategies and topologies on prognostics and health management. J. Intell. Manuf. 1–27 (2017) 
20. P . Oluwasanya, Anomaly detection in wireless sensor networks. arXiv preprint arXiv:1708. 
08053 . 
21. A.D. Paola, S. Gaglio, G.L. Re, F. Milazzo, M. Ortolani, Adaptive distributed outlier detection for WSNs. IEEE Trans. Cybern. 45(5), 902–913 (2015) 
22. M. Wazid, A.K. Das, An efﬁcient hybrid anomaly detection scheme using K-means clustering for wireless sensor networks. Wireless Pers. Commun. 90(4), 1971–2000 (2016)180 P . Chaturvedi et al.
23. H.H. Bosman, G. Iacca, A. Tejada, H.J. Wörtche, A. Liotta, Spatial anomaly detection in sensor 
networks using neighborhood information. Inf. Fusion, 33, 41–56 (2017) 
24. C. O’Reilly, A. Gluhak, M.A. Imran, S. Rajasegarar, Anomaly detection in wireless sensor networks in a non-stationary environment. IEEE Commun. Surv. Tutorials 16(3), 1413–1432 
(2014) 
25. T. Muhammed, R.A. Shaikh, An analysis of fault detection strategies in wireless sensor networks. J. Netw. Comput. Appl. 78, 267–287 (2017) 
26. H. Geng, Y . Liang, F. Yang, L. Xu, Q. Pan, Model-reduced fault detection for multi-rate sensor fusion with unknown inputs. Inf Fusion 33, 1–14 (2017) 
27. R.R. Swain, P .M. Khilar, S.K. Bhoi, Heterogeneous fault diagnosis for wireless sensor networks. Ad Hoc Netw. 69, 15–37 (2018) 
28. R. Palanikumar, K. Ramasamy, Effective failure nodes detection using matrix calculus algorithm in wireless sensor networks. Cluster Comput. 1–10 (2018) 
29. M. Hammoudeh, R. Newman, Adaptive routing in wireless sensor networks: QoS optimisation for enhanced application performance. Inf. Fusion 22, 3–15 (2015) 
30. X. Liu, Routing protocols based on ant colony optimization in wireless sensor networks: a survey. IEEE Access 5, 26303–26317 (2017) 
31. M. Asif, S. Khan, R. Ahmad, M. Sohail, D. Singh, Quality of service of routing protocols in wireless sensor networks: a review. IEEE Access 5, 1846–1871 (2017) 
32. K.-L.A. Yau, H.G. Goh, D. Chieng, K.H. Kwong, Application of reinforcement learning to wireless sensor networks: models and algorithms. Computing 97(11), 1045–1075 (2015) 
33. J. Kabara, M. Calle, MAC protocols used by wireless sensor networks and a general method of performance evaluation. Int. J. Distrib. Sens. Netw. 8(1), 1–11 (2012) 
34. I. Mustapha, B.M. Ali, A. Sali, M.F.A. Rasid, H. Mohamad, An energy efﬁcient reinforcement learning based cooperative channel sensing for cognitive radio sensor networks. Pervasive Mob. Comput. 35, 165–184 (2017) 
35. S. Kosunalp, Y . Chu, P .D. Mitchell, D. Grace, T. Clarke, Use of Q-learning approaches for practical medium access control in wireless sensor networks. Eng. Appl. Artif. Intell. 55, 
146–154 (2016) 
36. M. Rovcanin, E. De Poorter, I. Moerman, P . Demeester, A reinforcement learning based solu-tion for cognitive network cooperation between co-located, heterogeneous wireless sensor networks. Ad Hoc Netw. 17, 98–113 (2014) 
37. M. Rovcanin, E. De Poorter, D. van den Akker, I. Moerman, P . Demeester, C. Blondia, Exper-imental validation of a reinforcement learning based approach for a service-wise optimisation of heterogeneous wireless sensor networks. Wireless Netw. 21(3), 931–948 (2015) 
38. M. Ambigavathi, D. Sridharan, Energy-aware data aggregation techniques in wireless sensor network. Adv. Power Syst. Energy Manag. 165–173 (2018) 
39. k. xie, L. Wang, X. Wang, G. Xie, J. Wen, Low cost and high accuracy data gathering in WSNs with matrix completion. IEEE Trans. Mob. Comput. 17(7), 1595–1608 (2017) 
40. H. Lin, D. Bai, Y . Liu, Maximum data collection rate routing for data gather trees with data aggregation in rechargeable wireless sensor networks. Cluster Comput. 1–11 (2017) 
41. E. Kanjo, E.M. Y ounis, N. Sherkat, Towards unravelling the relationship between on-body, 
environmental and emotion data using sensor information fusion approach. Inf. Fusion 40, 
18–31 (2018) 
42. D. Capriglione, D. Casinelli, L. Ferrigno, Analysis of quantities inﬂuencing the perfor-
mance of time synchronization based on linear regression in low cost WSNs. Measurement 77(Supplement C), 105–116 (2016) 
43. J.J. Prez-Solano, S. Felici-Castell, Adaptive time window linear regression algorithm for accu-rate time synchronization in wireless sensor networks. Ad Hoc Netw. 24 (Part A), 92–108 
(2015) 
44. G. Betta, D. Casinelli, L. Ferrigno, Some Notes on the Performance of Regression-based Time Synchronization Algorithms in Low Cost WSNs (Springer International Publishing, Cham, 2015)10 A Deep Learning Based Target Coverage Protocol for Edge … 181
45. J.J. ´Perez-Solano, S. Felici-Castell, Improving time synchronization in wireless sensor 
networks using Bayesian inference. J. Netw. Comput. Appl. 82, 47–55 (2017) 
46. Pau, A.V . Bobovich, A fuzzy data fusion solution to enhance the QoS and the energy 
consumption in wireless sensor networks. Wireless Commun. Mob. Comput. 1–10 (2017) 
47. E. Alpaydin, Introduction to Machine Learning (MIT Press, Cambridge, MA, USA, 2020) 
48. D. Praveen Kumar, T. Amgoth, C.S.R. Annavarapu, Machine learning algorithms for wireless sensor networks: a survey. Inf. Fusion 9, 1–25 (2019) 
49. W. Sun, W. Lu, Q. Li, L. Chen, D. Mu, X. Y uan, WNN-LQE: wavelet-neural-network-based link quality estimation for smart grid WSNs. IEEE Access 5, 12788–12797 (2017) 
50. E.K. Lee, H. Viswanathan, D. Pompili, RescueNet: reinforcement-learning-based communi-cation framework for emergency networking. Comput. Netw. 98, 14–28 (2016) 
51. X. Chang, J. Huang, S. Liu, G. Xing, H. Zhang, J. Wang, L. Huang, Y . Zhuang, Accuracy-aware interference modeling and measurement in wireless sensor networks. IEEE Trans. Mob. Comput. 15(2), 278–291 (2016) 
52. W. Kim, M.S. Stankovi, K.H. Johansson, H.J. Kim, A distributed support vector machine learning over wireless sensor networks. IEEE Trans Cybern. 45(11), 2599–2611 (2015) 
53. W. Elghazel, K. Medjaher, N. Zerhouni, J. Bahi, A. Farhat, C. Guyeux, M. Hakem, Random 
forests for industrial device functioning diagnostics using wireless sensor networks, in 
Aerospace Conference (IEEE, 2015), pp. 1–9 
54. H. Chen, X. Li, F. Zhao, A reinforcement learning-based sleep scheduling algorithm for desired area coverage in solar-powered wireless sensor networks. IEEE Sens. J. 16(8), 2763–2774 
(2016)